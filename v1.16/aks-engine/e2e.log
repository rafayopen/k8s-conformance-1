I0918 19:34:12.510032      14 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-815280101
I0918 19:34:12.510168      14 e2e.go:92] Starting e2e run "30115aef-52cd-4ac4-beca-5d5a9b55b49c" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1568835250 - Will randomize all specs
Will run 274 of 4897 specs

Sep 18 19:34:12.566: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 19:34:12.568: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep 18 19:34:12.599: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep 18 19:34:12.632: INFO: 25 / 25 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep 18 19:34:12.632: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Sep 18 19:34:12.632: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep 18 19:34:12.641: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'azure-cni-networkmonitor' (0 seconds elapsed)
Sep 18 19:34:12.641: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'azure-ip-masq-agent' (0 seconds elapsed)
Sep 18 19:34:12.641: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'blobfuse-flexvol-installer' (0 seconds elapsed)
Sep 18 19:34:12.641: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'keyvault-flexvolume' (0 seconds elapsed)
Sep 18 19:34:12.641: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Sep 18 19:34:12.641: INFO: e2e test version: v1.16.0
Sep 18 19:34:12.642: INFO: kube-apiserver version: v1.16.0
Sep 18 19:34:12.642: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 19:34:12.647: INFO: Cluster IP family: ipv4
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:34:12.648: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename webhook
Sep 18 19:34:12.720: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Sep 18 19:34:12.732: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2370
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 18 19:34:13.455: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 18 19:34:15.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432053, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432053, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432053, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432053, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 18 19:34:17.468: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432053, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432053, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432053, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432053, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 18 19:34:19.469: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432053, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432053, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432053, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432053, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 18 19:34:22.487: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Sep 18 19:34:22.512: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:34:22.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2370" for this suite.
Sep 18 19:34:28.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:34:28.654: INFO: namespace webhook-2370 deletion completed in 6.108817207s
STEP: Destroying namespace "webhook-2370-markers" for this suite.
Sep 18 19:34:34.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:34:34.758: INFO: namespace webhook-2370-markers deletion completed in 6.103964122s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:22.122 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:34:34.770: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1223
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:35:04.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1223" for this suite.
Sep 18 19:35:10.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:35:10.305: INFO: namespace container-runtime-1223 deletion completed in 6.103390895s

â€¢ [SLOW TEST:35.536 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:35:10.307: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2552
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:35:26.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2552" for this suite.
Sep 18 19:35:32.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:35:32.697: INFO: namespace resourcequota-2552 deletion completed in 6.105750176s

â€¢ [SLOW TEST:22.391 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:35:32.697: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6601
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 18 19:35:32.870: INFO: Waiting up to 5m0s for pod "pod-753e2c2d-f9b1-4784-8f21-1b3d9d7f8a9d" in namespace "emptydir-6601" to be "success or failure"
Sep 18 19:35:32.874: INFO: Pod "pod-753e2c2d-f9b1-4784-8f21-1b3d9d7f8a9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.576181ms
Sep 18 19:35:34.879: INFO: Pod "pod-753e2c2d-f9b1-4784-8f21-1b3d9d7f8a9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008938045s
Sep 18 19:35:36.883: INFO: Pod "pod-753e2c2d-f9b1-4784-8f21-1b3d9d7f8a9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012869144s
STEP: Saw pod success
Sep 18 19:35:36.883: INFO: Pod "pod-753e2c2d-f9b1-4784-8f21-1b3d9d7f8a9d" satisfied condition "success or failure"
Sep 18 19:35:36.886: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-753e2c2d-f9b1-4784-8f21-1b3d9d7f8a9d container test-container: <nil>
STEP: delete the pod
Sep 18 19:35:36.934: INFO: Waiting for pod pod-753e2c2d-f9b1-4784-8f21-1b3d9d7f8a9d to disappear
Sep 18 19:35:36.938: INFO: Pod pod-753e2c2d-f9b1-4784-8f21-1b3d9d7f8a9d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:35:36.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6601" for this suite.
Sep 18 19:35:42.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:35:43.038: INFO: namespace emptydir-6601 deletion completed in 6.096237846s

â€¢ [SLOW TEST:10.341 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:35:43.039: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1018
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-1018/configmap-test-7f6e315c-dc16-4c89-979b-22f74fdc2422
STEP: Creating a pod to test consume configMaps
Sep 18 19:35:43.227: INFO: Waiting up to 5m0s for pod "pod-configmaps-cb448a82-c65f-42e6-bc05-c828f6384873" in namespace "configmap-1018" to be "success or failure"
Sep 18 19:35:43.231: INFO: Pod "pod-configmaps-cb448a82-c65f-42e6-bc05-c828f6384873": Phase="Pending", Reason="", readiness=false. Elapsed: 4.085184ms
Sep 18 19:35:45.235: INFO: Pod "pod-configmaps-cb448a82-c65f-42e6-bc05-c828f6384873": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008243511s
Sep 18 19:35:47.239: INFO: Pod "pod-configmaps-cb448a82-c65f-42e6-bc05-c828f6384873": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012801466s
STEP: Saw pod success
Sep 18 19:35:47.240: INFO: Pod "pod-configmaps-cb448a82-c65f-42e6-bc05-c828f6384873" satisfied condition "success or failure"
Sep 18 19:35:47.242: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod pod-configmaps-cb448a82-c65f-42e6-bc05-c828f6384873 container env-test: <nil>
STEP: delete the pod
Sep 18 19:35:47.292: INFO: Waiting for pod pod-configmaps-cb448a82-c65f-42e6-bc05-c828f6384873 to disappear
Sep 18 19:35:47.295: INFO: Pod pod-configmaps-cb448a82-c65f-42e6-bc05-c828f6384873 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:35:47.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1018" for this suite.
Sep 18 19:35:53.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:35:53.405: INFO: namespace configmap-1018 deletion completed in 6.105848859s

â€¢ [SLOW TEST:10.366 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:35:53.406: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-491
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-722cc539-be2a-4089-8ea5-275a1cf02444
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:35:53.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-491" for this suite.
Sep 18 19:35:59.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:35:59.707: INFO: namespace configmap-491 deletion completed in 6.132182394s

â€¢ [SLOW TEST:6.300 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:35:59.707: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3920
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Sep 18 19:35:59.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-3920 -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep 18 19:36:01.682: INFO: stderr: ""
Sep 18 19:36:01.682: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Sep 18 19:36:01.682: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep 18 19:36:01.682: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3920" to be "running and ready, or succeeded"
Sep 18 19:36:01.686: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.580186ms
Sep 18 19:36:03.690: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007725046s
Sep 18 19:36:05.710: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.027967764s
Sep 18 19:36:05.710: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep 18 19:36:05.710: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Sep 18 19:36:05.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 logs logs-generator logs-generator --namespace=kubectl-3920'
Sep 18 19:36:05.831: INFO: stderr: ""
Sep 18 19:36:05.831: INFO: stdout: "I0918 19:36:03.936987       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/gf7h 480\nI0918 19:36:04.137137       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/l52 542\nI0918 19:36:04.337231       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/gg8 238\nI0918 19:36:04.537178       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/dq5 558\nI0918 19:36:04.737196       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/97hs 505\nI0918 19:36:04.937109       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/9zh 266\nI0918 19:36:05.137175       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/5wwp 501\nI0918 19:36:05.337133       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/fcth 265\nI0918 19:36:05.537152       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/hx4n 529\nI0918 19:36:05.737172       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/mmgh 288\n"
STEP: limiting log lines
Sep 18 19:36:05.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 logs logs-generator logs-generator --namespace=kubectl-3920 --tail=1'
Sep 18 19:36:05.938: INFO: stderr: ""
Sep 18 19:36:05.938: INFO: stdout: "I0918 19:36:05.737172       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/mmgh 288\n"
STEP: limiting log bytes
Sep 18 19:36:05.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 logs logs-generator logs-generator --namespace=kubectl-3920 --limit-bytes=1'
Sep 18 19:36:06.035: INFO: stderr: ""
Sep 18 19:36:06.035: INFO: stdout: "I"
STEP: exposing timestamps
Sep 18 19:36:06.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 logs logs-generator logs-generator --namespace=kubectl-3920 --tail=1 --timestamps'
Sep 18 19:36:06.141: INFO: stderr: ""
Sep 18 19:36:06.141: INFO: stdout: "2019-09-18T19:36:05.937402775Z I0918 19:36:05.937214       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/rjbs 206\n"
STEP: restricting to a time range
Sep 18 19:36:08.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 logs logs-generator logs-generator --namespace=kubectl-3920 --since=1s'
Sep 18 19:36:08.739: INFO: stderr: ""
Sep 18 19:36:08.739: INFO: stdout: "I0918 19:36:07.737109       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/47k 386\nI0918 19:36:07.937204       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/kwd2 297\nI0918 19:36:08.137207       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/6m6p 451\nI0918 19:36:08.337215       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/kube-system/pods/vzgw 276\nI0918 19:36:08.537218       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/s8z 488\n"
Sep 18 19:36:08.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 logs logs-generator logs-generator --namespace=kubectl-3920 --since=24h'
Sep 18 19:36:08.836: INFO: stderr: ""
Sep 18 19:36:08.836: INFO: stdout: "I0918 19:36:03.936987       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/gf7h 480\nI0918 19:36:04.137137       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/l52 542\nI0918 19:36:04.337231       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/gg8 238\nI0918 19:36:04.537178       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/dq5 558\nI0918 19:36:04.737196       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/97hs 505\nI0918 19:36:04.937109       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/9zh 266\nI0918 19:36:05.137175       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/5wwp 501\nI0918 19:36:05.337133       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/fcth 265\nI0918 19:36:05.537152       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/hx4n 529\nI0918 19:36:05.737172       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/mmgh 288\nI0918 19:36:05.937214       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/rjbs 206\nI0918 19:36:06.138169       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/j58r 588\nI0918 19:36:06.337109       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/gpqz 558\nI0918 19:36:06.537113       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/9r6 273\nI0918 19:36:06.737106       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/qts 533\nI0918 19:36:06.937106       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/cp5 483\nI0918 19:36:07.137128       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/lkm9 456\nI0918 19:36:07.337191       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/6j8 580\nI0918 19:36:07.537123       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/rnm 300\nI0918 19:36:07.737109       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/47k 386\nI0918 19:36:07.937204       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/kwd2 297\nI0918 19:36:08.137207       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/6m6p 451\nI0918 19:36:08.337215       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/kube-system/pods/vzgw 276\nI0918 19:36:08.537218       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/s8z 488\nI0918 19:36:08.738373       1 logs_generator.go:76] 24 POST /api/v1/namespaces/kube-system/pods/vzqf 431\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Sep 18 19:36:08.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 delete pod logs-generator --namespace=kubectl-3920'
Sep 18 19:36:13.549: INFO: stderr: ""
Sep 18 19:36:13.549: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:36:13.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3920" for this suite.
Sep 18 19:36:19.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:36:19.673: INFO: namespace kubectl-3920 deletion completed in 6.113640093s

â€¢ [SLOW TEST:19.966 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:36:19.675: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3224
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-9tfv
STEP: Creating a pod to test atomic-volume-subpath
Sep 18 19:36:19.859: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9tfv" in namespace "subpath-3224" to be "success or failure"
Sep 18 19:36:19.864: INFO: Pod "pod-subpath-test-configmap-9tfv": Phase="Pending", Reason="", readiness=false. Elapsed: 5.274279ms
Sep 18 19:36:21.870: INFO: Pod "pod-subpath-test-configmap-9tfv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010609506s
Sep 18 19:36:23.874: INFO: Pod "pod-subpath-test-configmap-9tfv": Phase="Running", Reason="", readiness=true. Elapsed: 4.015363751s
Sep 18 19:36:25.879: INFO: Pod "pod-subpath-test-configmap-9tfv": Phase="Running", Reason="", readiness=true. Elapsed: 6.019742713s
Sep 18 19:36:27.883: INFO: Pod "pod-subpath-test-configmap-9tfv": Phase="Running", Reason="", readiness=true. Elapsed: 8.023652992s
Sep 18 19:36:29.887: INFO: Pod "pod-subpath-test-configmap-9tfv": Phase="Running", Reason="", readiness=true. Elapsed: 10.027674586s
Sep 18 19:36:31.891: INFO: Pod "pod-subpath-test-configmap-9tfv": Phase="Running", Reason="", readiness=true. Elapsed: 12.031899192s
Sep 18 19:36:33.895: INFO: Pod "pod-subpath-test-configmap-9tfv": Phase="Running", Reason="", readiness=true. Elapsed: 14.036390412s
Sep 18 19:36:35.899: INFO: Pod "pod-subpath-test-configmap-9tfv": Phase="Running", Reason="", readiness=true. Elapsed: 16.040426146s
Sep 18 19:36:37.904: INFO: Pod "pod-subpath-test-configmap-9tfv": Phase="Running", Reason="", readiness=true. Elapsed: 18.044750092s
Sep 18 19:36:39.908: INFO: Pod "pod-subpath-test-configmap-9tfv": Phase="Running", Reason="", readiness=true. Elapsed: 20.04938325s
Sep 18 19:36:41.913: INFO: Pod "pod-subpath-test-configmap-9tfv": Phase="Running", Reason="", readiness=true. Elapsed: 22.05386002s
Sep 18 19:36:43.917: INFO: Pod "pod-subpath-test-configmap-9tfv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.057921903s
STEP: Saw pod success
Sep 18 19:36:43.917: INFO: Pod "pod-subpath-test-configmap-9tfv" satisfied condition "success or failure"
Sep 18 19:36:43.920: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod pod-subpath-test-configmap-9tfv container test-container-subpath-configmap-9tfv: <nil>
STEP: delete the pod
Sep 18 19:36:43.957: INFO: Waiting for pod pod-subpath-test-configmap-9tfv to disappear
Sep 18 19:36:43.960: INFO: Pod pod-subpath-test-configmap-9tfv no longer exists
STEP: Deleting pod pod-subpath-test-configmap-9tfv
Sep 18 19:36:43.960: INFO: Deleting pod "pod-subpath-test-configmap-9tfv" in namespace "subpath-3224"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:36:43.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3224" for this suite.
Sep 18 19:36:49.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:36:50.074: INFO: namespace subpath-3224 deletion completed in 6.107361058s

â€¢ [SLOW TEST:30.399 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:36:50.075: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3441
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Sep 18 19:36:54.813: INFO: Successfully updated pod "labelsupdatea1d50b9b-5ed7-4c78-bc5a-8936d74a32ce"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:36:56.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3441" for this suite.
Sep 18 19:37:24.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:37:24.958: INFO: namespace downward-api-3441 deletion completed in 28.121979653s

â€¢ [SLOW TEST:34.883 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:37:24.958: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1797
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 18 19:37:25.139: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c9c9a71d-6299-4296-b93b-655402bb542d" in namespace "projected-1797" to be "success or failure"
Sep 18 19:37:25.151: INFO: Pod "downwardapi-volume-c9c9a71d-6299-4296-b93b-655402bb542d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.689086ms
Sep 18 19:37:27.154: INFO: Pod "downwardapi-volume-c9c9a71d-6299-4296-b93b-655402bb542d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015359381s
Sep 18 19:37:29.158: INFO: Pod "downwardapi-volume-c9c9a71d-6299-4296-b93b-655402bb542d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019348641s
STEP: Saw pod success
Sep 18 19:37:29.158: INFO: Pod "downwardapi-volume-c9c9a71d-6299-4296-b93b-655402bb542d" satisfied condition "success or failure"
Sep 18 19:37:29.161: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod downwardapi-volume-c9c9a71d-6299-4296-b93b-655402bb542d container client-container: <nil>
STEP: delete the pod
Sep 18 19:37:29.207: INFO: Waiting for pod downwardapi-volume-c9c9a71d-6299-4296-b93b-655402bb542d to disappear
Sep 18 19:37:29.216: INFO: Pod downwardapi-volume-c9c9a71d-6299-4296-b93b-655402bb542d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:37:29.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1797" for this suite.
Sep 18 19:37:35.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:37:35.312: INFO: namespace projected-1797 deletion completed in 6.091387228s

â€¢ [SLOW TEST:10.354 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:37:35.312: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-59
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Sep 18 19:37:35.476: INFO: namespace kubectl-59
Sep 18 19:37:35.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 create -f - --namespace=kubectl-59'
Sep 18 19:37:35.837: INFO: stderr: ""
Sep 18 19:37:35.837: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep 18 19:37:36.841: INFO: Selector matched 1 pods for map[app:redis]
Sep 18 19:37:36.841: INFO: Found 0 / 1
Sep 18 19:37:37.842: INFO: Selector matched 1 pods for map[app:redis]
Sep 18 19:37:37.842: INFO: Found 0 / 1
Sep 18 19:37:38.841: INFO: Selector matched 1 pods for map[app:redis]
Sep 18 19:37:38.841: INFO: Found 0 / 1
Sep 18 19:37:39.841: INFO: Selector matched 1 pods for map[app:redis]
Sep 18 19:37:39.841: INFO: Found 0 / 1
Sep 18 19:37:40.841: INFO: Selector matched 1 pods for map[app:redis]
Sep 18 19:37:40.841: INFO: Found 0 / 1
Sep 18 19:37:41.842: INFO: Selector matched 1 pods for map[app:redis]
Sep 18 19:37:41.842: INFO: Found 1 / 1
Sep 18 19:37:41.842: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 18 19:37:41.846: INFO: Selector matched 1 pods for map[app:redis]
Sep 18 19:37:41.846: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 18 19:37:41.846: INFO: wait on redis-master startup in kubectl-59 
Sep 18 19:37:41.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 logs redis-master-qfs58 redis-master --namespace=kubectl-59'
Sep 18 19:37:41.948: INFO: stderr: ""
Sep 18 19:37:41.948: INFO: stdout: "1:C 18 Sep 2019 19:37:40.712 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 18 Sep 2019 19:37:40.712 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 18 Sep 2019 19:37:40.712 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 18 Sep 2019 19:37:40.714 * Running mode=standalone, port=6379.\n1:M 18 Sep 2019 19:37:40.714 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 18 Sep 2019 19:37:40.714 # Server initialized\n1:M 18 Sep 2019 19:37:40.714 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 18 Sep 2019 19:37:40.714 * Ready to accept connections\n"
STEP: exposing RC
Sep 18 19:37:41.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-59'
Sep 18 19:37:42.086: INFO: stderr: ""
Sep 18 19:37:42.086: INFO: stdout: "service/rm2 exposed\n"
Sep 18 19:37:42.099: INFO: Service rm2 in namespace kubectl-59 found.
STEP: exposing service
Sep 18 19:37:44.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-59'
Sep 18 19:37:44.232: INFO: stderr: ""
Sep 18 19:37:44.232: INFO: stdout: "service/rm3 exposed\n"
Sep 18 19:37:44.243: INFO: Service rm3 in namespace kubectl-59 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:37:46.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-59" for this suite.
Sep 18 19:38:14.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:38:14.361: INFO: namespace kubectl-59 deletion completed in 28.107108045s

â€¢ [SLOW TEST:39.048 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:38:14.361: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-474
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 18 19:38:14.536: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cc2be8ca-5e1e-4fc4-a9e8-426cd7ca4eda" in namespace "downward-api-474" to be "success or failure"
Sep 18 19:38:14.540: INFO: Pod "downwardapi-volume-cc2be8ca-5e1e-4fc4-a9e8-426cd7ca4eda": Phase="Pending", Reason="", readiness=false. Elapsed: 3.637771ms
Sep 18 19:38:16.545: INFO: Pod "downwardapi-volume-cc2be8ca-5e1e-4fc4-a9e8-426cd7ca4eda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008633483s
Sep 18 19:38:18.550: INFO: Pod "downwardapi-volume-cc2be8ca-5e1e-4fc4-a9e8-426cd7ca4eda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013920307s
STEP: Saw pod success
Sep 18 19:38:18.550: INFO: Pod "downwardapi-volume-cc2be8ca-5e1e-4fc4-a9e8-426cd7ca4eda" satisfied condition "success or failure"
Sep 18 19:38:18.553: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod downwardapi-volume-cc2be8ca-5e1e-4fc4-a9e8-426cd7ca4eda container client-container: <nil>
STEP: delete the pod
Sep 18 19:38:18.583: INFO: Waiting for pod downwardapi-volume-cc2be8ca-5e1e-4fc4-a9e8-426cd7ca4eda to disappear
Sep 18 19:38:18.589: INFO: Pod downwardapi-volume-cc2be8ca-5e1e-4fc4-a9e8-426cd7ca4eda no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:38:18.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-474" for this suite.
Sep 18 19:38:24.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:38:24.691: INFO: namespace downward-api-474 deletion completed in 6.097967602s

â€¢ [SLOW TEST:10.330 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:38:24.692: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2700
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 19:38:24.862: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:38:24.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2700" for this suite.
Sep 18 19:38:31.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:38:31.144: INFO: namespace custom-resource-definition-2700 deletion completed in 6.182096993s

â€¢ [SLOW TEST:6.453 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:38:31.145: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9287
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-7dtt2 in namespace proxy-9287
I0918 19:38:31.340894      14 runners.go:184] Created replication controller with name: proxy-service-7dtt2, namespace: proxy-9287, replica count: 1
I0918 19:38:31.341124      14 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 19:38:31.341156      14 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 19:38:32.392084      14 runners.go:184] proxy-service-7dtt2 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0918 19:38:33.392373      14 runners.go:184] proxy-service-7dtt2 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0918 19:38:34.392585      14 runners.go:184] proxy-service-7dtt2 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0918 19:38:35.392823      14 runners.go:184] proxy-service-7dtt2 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0918 19:38:36.393130      14 runners.go:184] proxy-service-7dtt2 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0918 19:38:37.393353      14 runners.go:184] proxy-service-7dtt2 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0918 19:38:38.393612      14 runners.go:184] proxy-service-7dtt2 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0918 19:38:39.393931      14 runners.go:184] proxy-service-7dtt2 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0918 19:38:40.394238      14 runners.go:184] proxy-service-7dtt2 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0918 19:38:41.394595      14 runners.go:184] proxy-service-7dtt2 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0918 19:38:42.394962      14 runners.go:184] proxy-service-7dtt2 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 18 19:38:42.399: INFO: setup took 11.086924352s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep 18 19:38:42.411: INFO: (0) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">test<... (200; 11.389417ms)
Sep 18 19:38:42.417: INFO: (0) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">... (200; 16.738778ms)
Sep 18 19:38:42.419: INFO: (0) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/tlsrewritem... (200; 19.475758ms)
Sep 18 19:38:42.423: INFO: (0) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/rewriteme">test</a> (200; 22.465236ms)
Sep 18 19:38:42.423: INFO: (0) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 22.055239ms)
Sep 18 19:38:42.424: INFO: (0) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 22.847734ms)
Sep 18 19:38:42.427: INFO: (0) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 24.890018ms)
Sep 18 19:38:42.434: INFO: (0) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname1/proxy/: foo (200; 33.683355ms)
Sep 18 19:38:42.434: INFO: (0) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 31.810068ms)
Sep 18 19:38:42.436: INFO: (0) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:460/proxy/: tls baz (200; 34.921446ms)
Sep 18 19:38:42.437: INFO: (0) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname2/proxy/: bar (200; 35.381742ms)
Sep 18 19:38:42.438: INFO: (0) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname1/proxy/: tls baz (200; 38.236122ms)
Sep 18 19:38:42.438: INFO: (0) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname1/proxy/: foo (200; 36.822832ms)
Sep 18 19:38:42.439: INFO: (0) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname2/proxy/: bar (200; 38.872716ms)
Sep 18 19:38:42.440: INFO: (0) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:462/proxy/: tls qux (200; 38.905417ms)
Sep 18 19:38:42.440: INFO: (0) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname2/proxy/: tls qux (200; 39.045316ms)
Sep 18 19:38:42.451: INFO: (1) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:462/proxy/: tls qux (200; 11.452217ms)
Sep 18 19:38:42.455: INFO: (1) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">... (200; 14.147996ms)
Sep 18 19:38:42.457: INFO: (1) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:460/proxy/: tls baz (200; 16.111382ms)
Sep 18 19:38:42.457: INFO: (1) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 16.595879ms)
Sep 18 19:38:42.457: INFO: (1) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/rewriteme">test</a> (200; 16.47378ms)
Sep 18 19:38:42.457: INFO: (1) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 16.942276ms)
Sep 18 19:38:42.457: INFO: (1) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 16.988677ms)
Sep 18 19:38:42.458: INFO: (1) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">test<... (200; 17.164975ms)
Sep 18 19:38:42.458: INFO: (1) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname2/proxy/: tls qux (200; 17.78327ms)
Sep 18 19:38:42.460: INFO: (1) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 19.510458ms)
Sep 18 19:38:42.461: INFO: (1) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname2/proxy/: bar (200; 20.267553ms)
Sep 18 19:38:42.462: INFO: (1) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname1/proxy/: foo (200; 21.439943ms)
Sep 18 19:38:42.462: INFO: (1) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/tlsrewritem... (200; 22.132039ms)
Sep 18 19:38:42.463: INFO: (1) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname1/proxy/: tls baz (200; 22.101439ms)
Sep 18 19:38:42.463: INFO: (1) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname2/proxy/: bar (200; 22.882034ms)
Sep 18 19:38:42.464: INFO: (1) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname1/proxy/: foo (200; 22.961732ms)
Sep 18 19:38:42.469: INFO: (2) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 5.355261ms)
Sep 18 19:38:42.478: INFO: (2) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/tlsrewritem... (200; 13.750199ms)
Sep 18 19:38:42.479: INFO: (2) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">test<... (200; 14.813492ms)
Sep 18 19:38:42.479: INFO: (2) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">... (200; 15.022191ms)
Sep 18 19:38:42.480: INFO: (2) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname2/proxy/: tls qux (200; 15.697485ms)
Sep 18 19:38:42.480: INFO: (2) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 15.877884ms)
Sep 18 19:38:42.481: INFO: (2) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname1/proxy/: foo (200; 16.329581ms)
Sep 18 19:38:42.482: INFO: (2) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:460/proxy/: tls baz (200; 17.518873ms)
Sep 18 19:38:42.482: INFO: (2) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname1/proxy/: tls baz (200; 17.906369ms)
Sep 18 19:38:42.483: INFO: (2) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname2/proxy/: bar (200; 18.375366ms)
Sep 18 19:38:42.483: INFO: (2) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/rewriteme">test</a> (200; 18.486566ms)
Sep 18 19:38:42.483: INFO: (2) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname2/proxy/: bar (200; 18.138367ms)
Sep 18 19:38:42.483: INFO: (2) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:462/proxy/: tls qux (200; 18.222367ms)
Sep 18 19:38:42.484: INFO: (2) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 18.971762ms)
Sep 18 19:38:42.484: INFO: (2) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname1/proxy/: foo (200; 19.16326ms)
Sep 18 19:38:42.484: INFO: (2) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 20.008755ms)
Sep 18 19:38:42.496: INFO: (3) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/rewriteme">test</a> (200; 10.869621ms)
Sep 18 19:38:42.497: INFO: (3) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:460/proxy/: tls baz (200; 11.428317ms)
Sep 18 19:38:42.498: INFO: (3) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:462/proxy/: tls qux (200; 12.487309ms)
Sep 18 19:38:42.500: INFO: (3) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 14.817792ms)
Sep 18 19:38:42.500: INFO: (3) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 15.04509ms)
Sep 18 19:38:42.500: INFO: (3) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/tlsrewritem... (200; 15.125689ms)
Sep 18 19:38:42.501: INFO: (3) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 15.19869ms)
Sep 18 19:38:42.501: INFO: (3) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 15.561886ms)
Sep 18 19:38:42.502: INFO: (3) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">test<... (200; 16.145883ms)
Sep 18 19:38:42.502: INFO: (3) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname1/proxy/: tls baz (200; 17.668171ms)
Sep 18 19:38:42.502: INFO: (3) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">... (200; 16.626479ms)
Sep 18 19:38:42.503: INFO: (3) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname1/proxy/: foo (200; 17.656572ms)
Sep 18 19:38:42.505: INFO: (3) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname2/proxy/: bar (200; 19.799156ms)
Sep 18 19:38:42.505: INFO: (3) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname2/proxy/: tls qux (200; 19.770856ms)
Sep 18 19:38:42.505: INFO: (3) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname2/proxy/: bar (200; 19.276559ms)
Sep 18 19:38:42.505: INFO: (3) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname1/proxy/: foo (200; 19.371159ms)
Sep 18 19:38:42.516: INFO: (4) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">... (200; 10.241125ms)
Sep 18 19:38:42.516: INFO: (4) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 10.955321ms)
Sep 18 19:38:42.518: INFO: (4) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/rewriteme">test</a> (200; 12.149412ms)
Sep 18 19:38:42.518: INFO: (4) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 12.973105ms)
Sep 18 19:38:42.519: INFO: (4) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 13.357203ms)
Sep 18 19:38:42.519: INFO: (4) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/tlsrewritem... (200; 13.345702ms)
Sep 18 19:38:42.519: INFO: (4) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">test<... (200; 13.384902ms)
Sep 18 19:38:42.522: INFO: (4) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:460/proxy/: tls baz (200; 16.863177ms)
Sep 18 19:38:42.522: INFO: (4) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname2/proxy/: bar (200; 17.200375ms)
Sep 18 19:38:42.523: INFO: (4) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname1/proxy/: foo (200; 17.418573ms)
Sep 18 19:38:42.523: INFO: (4) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname2/proxy/: bar (200; 18.077469ms)
Sep 18 19:38:42.523: INFO: (4) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname1/proxy/: foo (200; 17.84187ms)
Sep 18 19:38:42.524: INFO: (4) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:462/proxy/: tls qux (200; 18.628365ms)
Sep 18 19:38:42.524: INFO: (4) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname2/proxy/: tls qux (200; 19.111961ms)
Sep 18 19:38:42.524: INFO: (4) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 19.090661ms)
Sep 18 19:38:42.525: INFO: (4) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname1/proxy/: tls baz (200; 19.23866ms)
Sep 18 19:38:42.533: INFO: (5) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">... (200; 8.033441ms)
Sep 18 19:38:42.535: INFO: (5) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 9.574531ms)
Sep 18 19:38:42.536: INFO: (5) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:460/proxy/: tls baz (200; 10.271225ms)
Sep 18 19:38:42.537: INFO: (5) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/tlsrewritem... (200; 11.547016ms)
Sep 18 19:38:42.538: INFO: (5) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:462/proxy/: tls qux (200; 12.38451ms)
Sep 18 19:38:42.540: INFO: (5) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 14.711992ms)
Sep 18 19:38:42.541: INFO: (5) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 15.360588ms)
Sep 18 19:38:42.543: INFO: (5) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname1/proxy/: foo (200; 17.243374ms)
Sep 18 19:38:42.544: INFO: (5) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname1/proxy/: foo (200; 18.860762ms)
Sep 18 19:38:42.545: INFO: (5) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">test<... (200; 19.045361ms)
Sep 18 19:38:42.545: INFO: (5) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname2/proxy/: bar (200; 19.795256ms)
Sep 18 19:38:42.545: INFO: (5) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 19.452059ms)
Sep 18 19:38:42.545: INFO: (5) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/rewriteme">test</a> (200; 19.362659ms)
Sep 18 19:38:42.546: INFO: (5) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname1/proxy/: tls baz (200; 20.519251ms)
Sep 18 19:38:42.547: INFO: (5) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname2/proxy/: tls qux (200; 21.390244ms)
Sep 18 19:38:42.547: INFO: (5) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname2/proxy/: bar (200; 21.204846ms)
Sep 18 19:38:42.556: INFO: (6) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 8.846536ms)
Sep 18 19:38:42.556: INFO: (6) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 8.604338ms)
Sep 18 19:38:42.557: INFO: (6) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/rewriteme">test</a> (200; 9.61573ms)
Sep 18 19:38:42.557: INFO: (6) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">... (200; 9.61803ms)
Sep 18 19:38:42.557: INFO: (6) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/tlsrewritem... (200; 9.68403ms)
Sep 18 19:38:42.560: INFO: (6) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:460/proxy/: tls baz (200; 12.484809ms)
Sep 18 19:38:42.562: INFO: (6) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">test<... (200; 14.910391ms)
Sep 18 19:38:42.563: INFO: (6) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 15.657786ms)
Sep 18 19:38:42.567: INFO: (6) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 19.25316ms)
Sep 18 19:38:42.567: INFO: (6) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:462/proxy/: tls qux (200; 19.416859ms)
Sep 18 19:38:42.567: INFO: (6) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname1/proxy/: foo (200; 20.216853ms)
Sep 18 19:38:42.567: INFO: (6) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname2/proxy/: tls qux (200; 20.419051ms)
Sep 18 19:38:42.567: INFO: (6) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname1/proxy/: foo (200; 20.086554ms)
Sep 18 19:38:42.568: INFO: (6) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname1/proxy/: tls baz (200; 20.53275ms)
Sep 18 19:38:42.568: INFO: (6) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname2/proxy/: bar (200; 19.945354ms)
Sep 18 19:38:42.568: INFO: (6) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname2/proxy/: bar (200; 20.479351ms)
Sep 18 19:38:42.576: INFO: (7) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/tlsrewritem... (200; 7.514546ms)
Sep 18 19:38:42.577: INFO: (7) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 7.983442ms)
Sep 18 19:38:42.582: INFO: (7) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname2/proxy/: tls qux (200; 12.564509ms)
Sep 18 19:38:42.585: INFO: (7) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">test<... (200; 16.007783ms)
Sep 18 19:38:42.586: INFO: (7) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 17.128075ms)
Sep 18 19:38:42.587: INFO: (7) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">... (200; 17.563572ms)
Sep 18 19:38:42.587: INFO: (7) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:462/proxy/: tls qux (200; 16.893677ms)
Sep 18 19:38:42.587: INFO: (7) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname1/proxy/: foo (200; 17.694371ms)
Sep 18 19:38:42.587: INFO: (7) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 17.669471ms)
Sep 18 19:38:42.588: INFO: (7) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname1/proxy/: foo (200; 19.362059ms)
Sep 18 19:38:42.588: INFO: (7) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname1/proxy/: tls baz (200; 19.620957ms)
Sep 18 19:38:42.589: INFO: (7) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:460/proxy/: tls baz (200; 18.860162ms)
Sep 18 19:38:42.589: INFO: (7) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname2/proxy/: bar (200; 19.481458ms)
Sep 18 19:38:42.589: INFO: (7) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 19.841156ms)
Sep 18 19:38:42.590: INFO: (7) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname2/proxy/: bar (200; 20.56905ms)
Sep 18 19:38:42.590: INFO: (7) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/rewriteme">test</a> (200; 21.604143ms)
Sep 18 19:38:42.602: INFO: (8) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/rewriteme">test</a> (200; 10.823621ms)
Sep 18 19:38:42.605: INFO: (8) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/tlsrewritem... (200; 14.574394ms)
Sep 18 19:38:42.605: INFO: (8) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 14.410095ms)
Sep 18 19:38:42.605: INFO: (8) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 14.470994ms)
Sep 18 19:38:42.605: INFO: (8) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:460/proxy/: tls baz (200; 14.649193ms)
Sep 18 19:38:42.605: INFO: (8) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">test<... (200; 14.437795ms)
Sep 18 19:38:42.605: INFO: (8) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 15.07059ms)
Sep 18 19:38:42.606: INFO: (8) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">... (200; 14.785892ms)
Sep 18 19:38:42.606: INFO: (8) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 15.447188ms)
Sep 18 19:38:42.606: INFO: (8) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:462/proxy/: tls qux (200; 15.690185ms)
Sep 18 19:38:42.608: INFO: (8) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname2/proxy/: tls qux (200; 17.703071ms)
Sep 18 19:38:42.609: INFO: (8) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname1/proxy/: foo (200; 17.569872ms)
Sep 18 19:38:42.609: INFO: (8) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname1/proxy/: tls baz (200; 18.249267ms)
Sep 18 19:38:42.609: INFO: (8) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname2/proxy/: bar (200; 18.494265ms)
Sep 18 19:38:42.610: INFO: (8) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname2/proxy/: bar (200; 18.681764ms)
Sep 18 19:38:42.610: INFO: (8) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname1/proxy/: foo (200; 19.105561ms)
Sep 18 19:38:42.620: INFO: (9) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/tlsrewritem... (200; 9.805428ms)
Sep 18 19:38:42.621: INFO: (9) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 10.706822ms)
Sep 18 19:38:42.621: INFO: (9) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/rewriteme">test</a> (200; 10.859021ms)
Sep 18 19:38:42.622: INFO: (9) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 11.463516ms)
Sep 18 19:38:42.624: INFO: (9) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">... (200; 13.382702ms)
Sep 18 19:38:42.624: INFO: (9) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 13.064805ms)
Sep 18 19:38:42.625: INFO: (9) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:462/proxy/: tls qux (200; 14.214496ms)
Sep 18 19:38:42.625: INFO: (9) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname2/proxy/: tls qux (200; 14.658193ms)
Sep 18 19:38:42.627: INFO: (9) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname1/proxy/: foo (200; 16.136083ms)
Sep 18 19:38:42.628: INFO: (9) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:460/proxy/: tls baz (200; 17.069176ms)
Sep 18 19:38:42.628: INFO: (9) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">test<... (200; 17.599272ms)
Sep 18 19:38:42.628: INFO: (9) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 17.481973ms)
Sep 18 19:38:42.628: INFO: (9) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname2/proxy/: bar (200; 18.175968ms)
Sep 18 19:38:42.629: INFO: (9) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname1/proxy/: foo (200; 18.007069ms)
Sep 18 19:38:42.629: INFO: (9) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname2/proxy/: bar (200; 18.935663ms)
Sep 18 19:38:42.630: INFO: (9) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname1/proxy/: tls baz (200; 18.788463ms)
Sep 18 19:38:42.638: INFO: (10) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 8.30684ms)
Sep 18 19:38:42.639: INFO: (10) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 9.549031ms)
Sep 18 19:38:42.640: INFO: (10) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">test<... (200; 9.59653ms)
Sep 18 19:38:42.642: INFO: (10) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">... (200; 11.838714ms)
Sep 18 19:38:42.643: INFO: (10) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 11.930913ms)
Sep 18 19:38:42.644: INFO: (10) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 12.903306ms)
Sep 18 19:38:42.644: INFO: (10) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/rewriteme">test</a> (200; 14.078497ms)
Sep 18 19:38:42.648: INFO: (10) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname1/proxy/: tls baz (200; 17.862269ms)
Sep 18 19:38:42.648: INFO: (10) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:462/proxy/: tls qux (200; 17.181475ms)
Sep 18 19:38:42.649: INFO: (10) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/tlsrewritem... (200; 18.128368ms)
Sep 18 19:38:42.649: INFO: (10) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname1/proxy/: foo (200; 19.156861ms)
Sep 18 19:38:42.650: INFO: (10) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname2/proxy/: bar (200; 19.25086ms)
Sep 18 19:38:42.650: INFO: (10) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:460/proxy/: tls baz (200; 19.432259ms)
Sep 18 19:38:42.650: INFO: (10) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname2/proxy/: tls qux (200; 19.804655ms)
Sep 18 19:38:42.650: INFO: (10) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname2/proxy/: bar (200; 19.23056ms)
Sep 18 19:38:42.650: INFO: (10) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname1/proxy/: foo (200; 19.346559ms)
Sep 18 19:38:42.660: INFO: (11) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:460/proxy/: tls baz (200; 9.095333ms)
Sep 18 19:38:42.662: INFO: (11) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/tlsrewritem... (200; 10.614823ms)
Sep 18 19:38:42.671: INFO: (11) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 19.500858ms)
Sep 18 19:38:42.673: INFO: (11) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 20.459351ms)
Sep 18 19:38:42.674: INFO: (11) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/rewriteme">test</a> (200; 21.605242ms)
Sep 18 19:38:42.674: INFO: (11) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">... (200; 21.484843ms)
Sep 18 19:38:42.674: INFO: (11) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 22.738734ms)
Sep 18 19:38:42.675: INFO: (11) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 23.498629ms)
Sep 18 19:38:42.676: INFO: (11) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname1/proxy/: foo (200; 22.940333ms)
Sep 18 19:38:42.676: INFO: (11) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">test<... (200; 23.748327ms)
Sep 18 19:38:42.676: INFO: (11) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname2/proxy/: tls qux (200; 24.994818ms)
Sep 18 19:38:42.676: INFO: (11) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname2/proxy/: bar (200; 25.033017ms)
Sep 18 19:38:42.676: INFO: (11) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname1/proxy/: foo (200; 24.271923ms)
Sep 18 19:38:42.676: INFO: (11) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:462/proxy/: tls qux (200; 24.497221ms)
Sep 18 19:38:42.677: INFO: (11) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname1/proxy/: tls baz (200; 25.112217ms)
Sep 18 19:38:42.677: INFO: (11) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname2/proxy/: bar (200; 25.762012ms)
Sep 18 19:38:42.686: INFO: (12) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 8.543238ms)
Sep 18 19:38:42.687: INFO: (12) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 9.930828ms)
Sep 18 19:38:42.687: INFO: (12) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 10.140826ms)
Sep 18 19:38:42.687: INFO: (12) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/tlsrewritem... (200; 10.237026ms)
Sep 18 19:38:42.688: INFO: (12) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">... (200; 10.231425ms)
Sep 18 19:38:42.690: INFO: (12) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:460/proxy/: tls baz (200; 12.097912ms)
Sep 18 19:38:42.690: INFO: (12) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 12.255411ms)
Sep 18 19:38:42.691: INFO: (12) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:462/proxy/: tls qux (200; 13.457602ms)
Sep 18 19:38:42.692: INFO: (12) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname1/proxy/: tls baz (200; 14.173597ms)
Sep 18 19:38:42.693: INFO: (12) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname2/proxy/: bar (200; 15.264389ms)
Sep 18 19:38:42.695: INFO: (12) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">test<... (200; 17.080775ms)
Sep 18 19:38:42.695: INFO: (12) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/rewriteme">test</a> (200; 16.931477ms)
Sep 18 19:38:42.695: INFO: (12) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname1/proxy/: foo (200; 17.285674ms)
Sep 18 19:38:42.695: INFO: (12) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname2/proxy/: tls qux (200; 17.300174ms)
Sep 18 19:38:42.696: INFO: (12) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname2/proxy/: bar (200; 17.85517ms)
Sep 18 19:38:42.696: INFO: (12) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname1/proxy/: foo (200; 18.916063ms)
Sep 18 19:38:42.707: INFO: (13) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/tlsrewritem... (200; 10.078926ms)
Sep 18 19:38:42.711: INFO: (13) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname2/proxy/: tls qux (200; 13.650501ms)
Sep 18 19:38:42.713: INFO: (13) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/rewriteme">test</a> (200; 15.924884ms)
Sep 18 19:38:42.713: INFO: (13) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 16.944476ms)
Sep 18 19:38:42.713: INFO: (13) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:460/proxy/: tls baz (200; 15.652586ms)
Sep 18 19:38:42.713: INFO: (13) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname1/proxy/: tls baz (200; 16.837777ms)
Sep 18 19:38:42.714: INFO: (13) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 16.545579ms)
Sep 18 19:38:42.715: INFO: (13) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname2/proxy/: bar (200; 17.86197ms)
Sep 18 19:38:42.715: INFO: (13) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">test<... (200; 18.054868ms)
Sep 18 19:38:42.716: INFO: (13) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">... (200; 18.789863ms)
Sep 18 19:38:42.717: INFO: (13) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname1/proxy/: foo (200; 19.269359ms)
Sep 18 19:38:42.717: INFO: (13) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:462/proxy/: tls qux (200; 19.486858ms)
Sep 18 19:38:42.717: INFO: (13) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname1/proxy/: foo (200; 20.48025ms)
Sep 18 19:38:42.717: INFO: (13) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname2/proxy/: bar (200; 19.609757ms)
Sep 18 19:38:42.718: INFO: (13) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 19.968655ms)
Sep 18 19:38:42.718: INFO: (13) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 20.070554ms)
Sep 18 19:38:42.725: INFO: (14) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 7.136448ms)
Sep 18 19:38:42.729: INFO: (14) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">... (200; 10.710522ms)
Sep 18 19:38:42.729: INFO: (14) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">test<... (200; 11.184218ms)
Sep 18 19:38:42.729: INFO: (14) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:460/proxy/: tls baz (200; 10.857321ms)
Sep 18 19:38:42.730: INFO: (14) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 10.98252ms)
Sep 18 19:38:42.731: INFO: (14) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 12.166611ms)
Sep 18 19:38:42.732: INFO: (14) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/tlsrewritem... (200; 14.013998ms)
Sep 18 19:38:42.734: INFO: (14) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname2/proxy/: tls qux (200; 15.908984ms)
Sep 18 19:38:42.735: INFO: (14) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:462/proxy/: tls qux (200; 16.298881ms)
Sep 18 19:38:42.736: INFO: (14) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname2/proxy/: bar (200; 17.587572ms)
Sep 18 19:38:42.736: INFO: (14) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname1/proxy/: foo (200; 17.82847ms)
Sep 18 19:38:42.736: INFO: (14) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 17.552372ms)
Sep 18 19:38:42.737: INFO: (14) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname1/proxy/: tls baz (200; 18.616965ms)
Sep 18 19:38:42.737: INFO: (14) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname2/proxy/: bar (200; 18.776664ms)
Sep 18 19:38:42.738: INFO: (14) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname1/proxy/: foo (200; 19.028661ms)
Sep 18 19:38:42.738: INFO: (14) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/rewriteme">test</a> (200; 20.079554ms)
Sep 18 19:38:42.748: INFO: (15) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">... (200; 9.151834ms)
Sep 18 19:38:42.748: INFO: (15) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 9.203133ms)
Sep 18 19:38:42.749: INFO: (15) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/rewriteme">test</a> (200; 10.545523ms)
Sep 18 19:38:42.750: INFO: (15) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:462/proxy/: tls qux (200; 11.110119ms)
Sep 18 19:38:42.750: INFO: (15) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 11.521316ms)
Sep 18 19:38:42.750: INFO: (15) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 11.103619ms)
Sep 18 19:38:42.750: INFO: (15) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:460/proxy/: tls baz (200; 11.676715ms)
Sep 18 19:38:42.750: INFO: (15) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">test<... (200; 11.458717ms)
Sep 18 19:38:42.751: INFO: (15) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/tlsrewritem... (200; 12.047312ms)
Sep 18 19:38:42.751: INFO: (15) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 12.667908ms)
Sep 18 19:38:42.754: INFO: (15) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname1/proxy/: foo (200; 15.157189ms)
Sep 18 19:38:42.754: INFO: (15) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname1/proxy/: foo (200; 15.382888ms)
Sep 18 19:38:42.755: INFO: (15) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname2/proxy/: tls qux (200; 16.46998ms)
Sep 18 19:38:42.756: INFO: (15) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname2/proxy/: bar (200; 16.49658ms)
Sep 18 19:38:42.756: INFO: (15) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname1/proxy/: tls baz (200; 16.927076ms)
Sep 18 19:38:42.756: INFO: (15) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname2/proxy/: bar (200; 17.251375ms)
Sep 18 19:38:42.761: INFO: (16) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 4.856964ms)
Sep 18 19:38:42.764: INFO: (16) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 7.518145ms)
Sep 18 19:38:42.765: INFO: (16) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 8.083241ms)
Sep 18 19:38:42.765: INFO: (16) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname1/proxy/: foo (200; 8.431338ms)
Sep 18 19:38:42.767: INFO: (16) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:460/proxy/: tls baz (200; 10.054327ms)
Sep 18 19:38:42.768: INFO: (16) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/rewriteme">test</a> (200; 10.647922ms)
Sep 18 19:38:42.768: INFO: (16) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname2/proxy/: bar (200; 11.713514ms)
Sep 18 19:38:42.773: INFO: (16) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 16.253682ms)
Sep 18 19:38:42.774: INFO: (16) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname1/proxy/: foo (200; 16.43798ms)
Sep 18 19:38:42.774: INFO: (16) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:462/proxy/: tls qux (200; 17.112575ms)
Sep 18 19:38:42.775: INFO: (16) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname2/proxy/: bar (200; 16.957176ms)
Sep 18 19:38:42.775: INFO: (16) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname1/proxy/: tls baz (200; 17.204975ms)
Sep 18 19:38:42.775: INFO: (16) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/tlsrewritem... (200; 16.926077ms)
Sep 18 19:38:42.775: INFO: (16) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">... (200; 17.418573ms)
Sep 18 19:38:42.775: INFO: (16) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">test<... (200; 18.306967ms)
Sep 18 19:38:42.775: INFO: (16) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname2/proxy/: tls qux (200; 17.604772ms)
Sep 18 19:38:42.783: INFO: (17) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/rewriteme">test</a> (200; 7.218648ms)
Sep 18 19:38:42.784: INFO: (17) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 8.26424ms)
Sep 18 19:38:42.786: INFO: (17) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:460/proxy/: tls baz (200; 9.596031ms)
Sep 18 19:38:42.789: INFO: (17) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:462/proxy/: tls qux (200; 12.685308ms)
Sep 18 19:38:42.789: INFO: (17) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">... (200; 13.274004ms)
Sep 18 19:38:42.789: INFO: (17) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/tlsrewritem... (200; 13.431602ms)
Sep 18 19:38:42.789: INFO: (17) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 13.287003ms)
Sep 18 19:38:42.790: INFO: (17) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 13.526801ms)
Sep 18 19:38:42.792: INFO: (17) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 15.384788ms)
Sep 18 19:38:42.792: INFO: (17) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">test<... (200; 16.035583ms)
Sep 18 19:38:42.792: INFO: (17) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname2/proxy/: bar (200; 15.538487ms)
Sep 18 19:38:42.793: INFO: (17) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname2/proxy/: tls qux (200; 16.633679ms)
Sep 18 19:38:42.793: INFO: (17) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname2/proxy/: bar (200; 17.025276ms)
Sep 18 19:38:42.793: INFO: (17) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname1/proxy/: foo (200; 17.136175ms)
Sep 18 19:38:42.795: INFO: (17) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname1/proxy/: foo (200; 18.543765ms)
Sep 18 19:38:42.795: INFO: (17) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname1/proxy/: tls baz (200; 19.381958ms)
Sep 18 19:38:42.800: INFO: (18) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">... (200; 4.604367ms)
Sep 18 19:38:42.802: INFO: (18) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/tlsrewritem... (200; 5.678659ms)
Sep 18 19:38:42.809: INFO: (18) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 12.117012ms)
Sep 18 19:38:42.809: INFO: (18) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/rewriteme">test</a> (200; 12.089912ms)
Sep 18 19:38:42.811: INFO: (18) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname1/proxy/: foo (200; 15.556186ms)
Sep 18 19:38:42.811: INFO: (18) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">test<... (200; 14.364995ms)
Sep 18 19:38:42.812: INFO: (18) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 16.273081ms)
Sep 18 19:38:42.813: INFO: (18) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:462/proxy/: tls qux (200; 16.43578ms)
Sep 18 19:38:42.813: INFO: (18) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 16.802178ms)
Sep 18 19:38:42.813: INFO: (18) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname2/proxy/: bar (200; 17.306974ms)
Sep 18 19:38:42.814: INFO: (18) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:460/proxy/: tls baz (200; 17.398673ms)
Sep 18 19:38:42.815: INFO: (18) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname2/proxy/: tls qux (200; 18.944062ms)
Sep 18 19:38:42.815: INFO: (18) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname1/proxy/: tls baz (200; 18.305367ms)
Sep 18 19:38:42.816: INFO: (18) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname2/proxy/: bar (200; 20.212152ms)
Sep 18 19:38:42.816: INFO: (18) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname1/proxy/: foo (200; 19.061761ms)
Sep 18 19:38:42.816: INFO: (18) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 19.931855ms)
Sep 18 19:38:42.830: INFO: (19) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">test<... (200; 13.056105ms)
Sep 18 19:38:42.830: INFO: (19) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl/proxy/rewriteme">test</a> (200; 13.137204ms)
Sep 18 19:38:42.830: INFO: (19) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 13.436402ms)
Sep 18 19:38:42.830: INFO: (19) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:1080/proxy/rewriteme">... (200; 13.290003ms)
Sep 18 19:38:42.831: INFO: (19) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname1/proxy/: foo (200; 14.265896ms)
Sep 18 19:38:42.835: INFO: (19) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:160/proxy/: foo (200; 16.862877ms)
Sep 18 19:38:42.835: INFO: (19) /api/v1/namespaces/proxy-9287/pods/proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 17.041176ms)
Sep 18 19:38:42.835: INFO: (19) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:462/proxy/: tls qux (200; 19.13526ms)
Sep 18 19:38:42.835: INFO: (19) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname2/proxy/: tls qux (200; 18.055069ms)
Sep 18 19:38:42.836: INFO: (19) /api/v1/namespaces/proxy-9287/services/http:proxy-service-7dtt2:portname2/proxy/: bar (200; 19.286859ms)
Sep 18 19:38:42.836: INFO: (19) /api/v1/namespaces/proxy-9287/pods/http:proxy-service-7dtt2-xm5kl:162/proxy/: bar (200; 18.434966ms)
Sep 18 19:38:42.837: INFO: (19) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/: <a href="/api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:443/proxy/tlsrewritem... (200; 19.17196ms)
Sep 18 19:38:42.837: INFO: (19) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname2/proxy/: bar (200; 20.122553ms)
Sep 18 19:38:42.838: INFO: (19) /api/v1/namespaces/proxy-9287/services/https:proxy-service-7dtt2:tlsportname1/proxy/: tls baz (200; 21.040947ms)
Sep 18 19:38:42.838: INFO: (19) /api/v1/namespaces/proxy-9287/services/proxy-service-7dtt2:portname1/proxy/: foo (200; 20.051454ms)
Sep 18 19:38:42.839: INFO: (19) /api/v1/namespaces/proxy-9287/pods/https:proxy-service-7dtt2-xm5kl:460/proxy/: tls baz (200; 20.822848ms)
STEP: deleting ReplicationController proxy-service-7dtt2 in namespace proxy-9287, will wait for the garbage collector to delete the pods
I0918 19:38:42.842034      14 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 19:38:42.842362      14 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 18 19:38:42.898: INFO: Deleting ReplicationController proxy-service-7dtt2 took: 6.598152ms
Sep 18 19:38:43.199: INFO: Terminating ReplicationController proxy-service-7dtt2 pods took: 300.487616ms
I0918 19:38:43.199229      14 controller_utils.go:810] Ignoring inactive pod proxy-9287/proxy-service-7dtt2-xm5kl in state Running, deletion time 2019-09-18 19:38:44 +0000 UTC
[AfterEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:38:46.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9287" for this suite.
Sep 18 19:38:52.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:38:52.208: INFO: namespace proxy-9287 deletion completed in 6.103294047s

â€¢ [SLOW TEST:21.064 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:38:52.209: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9767
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Sep 18 19:38:52.376: INFO: Waiting up to 5m0s for pod "client-containers-78f99130-c656-4bcf-80c0-03afc6c9ee6a" in namespace "containers-9767" to be "success or failure"
Sep 18 19:38:52.386: INFO: Pod "client-containers-78f99130-c656-4bcf-80c0-03afc6c9ee6a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.777724ms
Sep 18 19:38:54.391: INFO: Pod "client-containers-78f99130-c656-4bcf-80c0-03afc6c9ee6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015013104s
Sep 18 19:38:56.395: INFO: Pod "client-containers-78f99130-c656-4bcf-80c0-03afc6c9ee6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019564565s
STEP: Saw pod success
Sep 18 19:38:56.395: INFO: Pod "client-containers-78f99130-c656-4bcf-80c0-03afc6c9ee6a" satisfied condition "success or failure"
Sep 18 19:38:56.399: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod client-containers-78f99130-c656-4bcf-80c0-03afc6c9ee6a container test-container: <nil>
STEP: delete the pod
Sep 18 19:38:56.419: INFO: Waiting for pod client-containers-78f99130-c656-4bcf-80c0-03afc6c9ee6a to disappear
Sep 18 19:38:56.422: INFO: Pod client-containers-78f99130-c656-4bcf-80c0-03afc6c9ee6a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:38:56.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9767" for this suite.
Sep 18 19:39:02.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:39:02.536: INFO: namespace containers-9767 deletion completed in 6.109125324s

â€¢ [SLOW TEST:10.326 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:39:02.536: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8891
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Sep 18 19:39:02.709: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 19:39:05.708: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:39:17.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8891" for this suite.
Sep 18 19:39:23.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:39:23.663: INFO: namespace crd-publish-openapi-8891 deletion completed in 6.106011733s

â€¢ [SLOW TEST:21.127 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:39:23.663: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1381
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 18 19:39:23.834: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5459dcec-95c6-416e-a85e-a54bb14f19e6" in namespace "downward-api-1381" to be "success or failure"
Sep 18 19:39:23.858: INFO: Pod "downwardapi-volume-5459dcec-95c6-416e-a85e-a54bb14f19e6": Phase="Pending", Reason="", readiness=false. Elapsed: 24.467442ms
Sep 18 19:39:25.864: INFO: Pod "downwardapi-volume-5459dcec-95c6-416e-a85e-a54bb14f19e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029651993s
Sep 18 19:39:27.868: INFO: Pod "downwardapi-volume-5459dcec-95c6-416e-a85e-a54bb14f19e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033990615s
STEP: Saw pod success
Sep 18 19:39:27.868: INFO: Pod "downwardapi-volume-5459dcec-95c6-416e-a85e-a54bb14f19e6" satisfied condition "success or failure"
Sep 18 19:39:27.871: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod downwardapi-volume-5459dcec-95c6-416e-a85e-a54bb14f19e6 container client-container: <nil>
STEP: delete the pod
Sep 18 19:39:27.905: INFO: Waiting for pod downwardapi-volume-5459dcec-95c6-416e-a85e-a54bb14f19e6 to disappear
Sep 18 19:39:27.910: INFO: Pod downwardapi-volume-5459dcec-95c6-416e-a85e-a54bb14f19e6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:39:27.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1381" for this suite.
Sep 18 19:39:33.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:39:34.062: INFO: namespace downward-api-1381 deletion completed in 6.144830823s

â€¢ [SLOW TEST:10.399 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:39:34.064: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-3419
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Sep 18 19:39:34.239: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Sep 18 19:39:34.875: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep 18 19:39:36.930: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 18 19:39:38.935: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 18 19:39:40.935: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 18 19:39:42.934: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 18 19:39:44.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 18 19:39:46.935: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 18 19:39:48.935: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432374, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 18 19:39:51.768: INFO: Waited 827.408684ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:39:52.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3419" for this suite.
Sep 18 19:39:58.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:39:58.460: INFO: namespace aggregator-3419 deletion completed in 6.184427023s

â€¢ [SLOW TEST:24.397 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:39:58.467: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7798
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:40:09.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7798" for this suite.
Sep 18 19:40:15.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:40:15.776: INFO: namespace resourcequota-7798 deletion completed in 6.101720804s

â€¢ [SLOW TEST:17.310 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:40:15.779: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8245
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Sep 18 19:40:16.013: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 18 19:40:16.029: INFO: Waiting for terminating namespaces to be deleted...
Sep 18 19:40:16.032: INFO: 
Logging pods the kubelet thinks is on node k8s-agentpool1-40209065-vmss000000 before test
Sep 18 19:40:16.068: INFO: keyvault-flexvolume-n9xxt from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 19:40:16.068: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
Sep 18 19:40:16.068: INFO: kubernetes-dashboard-65966766b9-86cqc from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 19:40:16.068: INFO: 	Container kubernetes-dashboard ready: true, restart count 1
Sep 18 19:40:16.068: INFO: kube-proxy-rhg4h from kube-system started at 2019-09-18 19:32:35 +0000 UTC (1 container statuses recorded)
Sep 18 19:40:16.068: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 18 19:40:16.068: INFO: blobfuse-flexvol-installer-6ghd6 from kube-system started at 2019-09-18 19:32:36 +0000 UTC (1 container statuses recorded)
Sep 18 19:40:16.068: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
Sep 18 19:40:16.068: INFO: azure-ip-masq-agent-l2c2n from kube-system started at 2019-09-18 19:32:37 +0000 UTC (1 container statuses recorded)
Sep 18 19:40:16.068: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Sep 18 19:40:16.068: INFO: sonobuoy-systemd-logs-daemon-set-13153e5f2c4542ef-86g6v from sonobuoy started at 2019-09-18 19:33:36 +0000 UTC (2 container statuses recorded)
Sep 18 19:40:16.068: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 18 19:40:16.068: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 18 19:40:16.068: INFO: azure-cni-networkmonitor-9x9wr from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 19:40:16.068: INFO: 	Container azure-cnms ready: true, restart count 0
Sep 18 19:40:16.068: INFO: 
Logging pods the kubelet thinks is on node k8s-agentpool1-40209065-vmss000001 before test
Sep 18 19:40:16.099: INFO: azure-cni-networkmonitor-ppx7z from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 19:40:16.099: INFO: 	Container azure-cnms ready: true, restart count 0
Sep 18 19:40:16.099: INFO: keyvault-flexvolume-428z4 from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 19:40:16.099: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
Sep 18 19:40:16.099: INFO: kube-proxy-cp8gq from kube-system started at 2019-09-18 19:32:35 +0000 UTC (1 container statuses recorded)
Sep 18 19:40:16.099: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 18 19:40:16.099: INFO: blobfuse-flexvol-installer-bbdb6 from kube-system started at 2019-09-18 19:32:36 +0000 UTC (1 container statuses recorded)
Sep 18 19:40:16.099: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
Sep 18 19:40:16.099: INFO: azure-ip-masq-agent-62ddt from kube-system started at 2019-09-18 19:32:37 +0000 UTC (1 container statuses recorded)
Sep 18 19:40:16.099: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Sep 18 19:40:16.099: INFO: sonobuoy-e2e-job-326e453a1b3e45d6 from sonobuoy started at 2019-09-18 19:33:36 +0000 UTC (2 container statuses recorded)
Sep 18 19:40:16.099: INFO: 	Container e2e ready: true, restart count 0
Sep 18 19:40:16.099: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 18 19:40:16.099: INFO: sonobuoy-systemd-logs-daemon-set-13153e5f2c4542ef-9l9xz from sonobuoy started at 2019-09-18 19:33:36 +0000 UTC (2 container statuses recorded)
Sep 18 19:40:16.099: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 18 19:40:16.099: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 18 19:40:16.099: INFO: 
Logging pods the kubelet thinks is on node k8s-agentpool1-40209065-vmss000002 before test
Sep 18 19:40:16.130: INFO: azure-cni-networkmonitor-86wjx from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 19:40:16.130: INFO: 	Container azure-cnms ready: true, restart count 0
Sep 18 19:40:16.130: INFO: blobfuse-flexvol-installer-n7wtj from kube-system started at 2019-09-18 19:32:36 +0000 UTC (1 container statuses recorded)
Sep 18 19:40:16.130: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
Sep 18 19:40:16.130: INFO: metrics-server-855b565c8f-rrds2 from kube-system started at 2019-09-18 19:32:37 +0000 UTC (1 container statuses recorded)
Sep 18 19:40:16.130: INFO: 	Container metrics-server ready: true, restart count 0
Sep 18 19:40:16.130: INFO: keyvault-flexvolume-f9jq8 from kube-system started at 2019-09-18 19:32:35 +0000 UTC (1 container statuses recorded)
Sep 18 19:40:16.130: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
Sep 18 19:40:16.130: INFO: kube-proxy-vnxpl from kube-system started at 2019-09-18 19:32:35 +0000 UTC (1 container statuses recorded)
Sep 18 19:40:16.130: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 18 19:40:16.130: INFO: azure-ip-masq-agent-q7dfv from kube-system started at 2019-09-18 19:32:37 +0000 UTC (1 container statuses recorded)
Sep 18 19:40:16.130: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Sep 18 19:40:16.130: INFO: sonobuoy from sonobuoy started at 2019-09-18 19:33:29 +0000 UTC (1 container statuses recorded)
Sep 18 19:40:16.130: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 18 19:40:16.130: INFO: sonobuoy-systemd-logs-daemon-set-13153e5f2c4542ef-djfs2 from sonobuoy started at 2019-09-18 19:33:36 +0000 UTC (2 container statuses recorded)
Sep 18 19:40:16.130: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 18 19:40:16.130: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-88854594-fd5f-4432-b8c7-e7d491fdb26a 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-88854594-fd5f-4432-b8c7-e7d491fdb26a off the node k8s-agentpool1-40209065-vmss000001
STEP: verifying the node doesn't have the label kubernetes.io/e2e-88854594-fd5f-4432-b8c7-e7d491fdb26a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:40:32.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8245" for this suite.
Sep 18 19:40:46.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:40:46.385: INFO: namespace sched-pred-8245 deletion completed in 14.103533913s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:30.607 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:40:46.386: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
I0918 19:40:46.386060      14 request.go:706] Error in request: resource name may not be empty
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4820
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 18 19:40:46.559: INFO: Waiting up to 5m0s for pod "pod-d2c21c76-72c3-4a88-a8a6-aa21a8c962c6" in namespace "emptydir-4820" to be "success or failure"
Sep 18 19:40:46.566: INFO: Pod "pod-d2c21c76-72c3-4a88-a8a6-aa21a8c962c6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.485859ms
Sep 18 19:40:48.571: INFO: Pod "pod-d2c21c76-72c3-4a88-a8a6-aa21a8c962c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012089122s
Sep 18 19:40:50.575: INFO: Pod "pod-d2c21c76-72c3-4a88-a8a6-aa21a8c962c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016198422s
STEP: Saw pod success
Sep 18 19:40:50.575: INFO: Pod "pod-d2c21c76-72c3-4a88-a8a6-aa21a8c962c6" satisfied condition "success or failure"
Sep 18 19:40:50.578: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-d2c21c76-72c3-4a88-a8a6-aa21a8c962c6 container test-container: <nil>
STEP: delete the pod
Sep 18 19:40:50.606: INFO: Waiting for pod pod-d2c21c76-72c3-4a88-a8a6-aa21a8c962c6 to disappear
Sep 18 19:40:50.610: INFO: Pod pod-d2c21c76-72c3-4a88-a8a6-aa21a8c962c6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:40:50.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4820" for this suite.
Sep 18 19:40:56.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:40:56.774: INFO: namespace emptydir-4820 deletion completed in 6.159125112s

â€¢ [SLOW TEST:10.388 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:40:56.775: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8834
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:40:56.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8834" for this suite.
Sep 18 19:41:02.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:41:03.064: INFO: namespace services-8834 deletion completed in 6.115587757s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:6.290 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:41:03.065: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-641
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 18 19:41:06.266: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:41:06.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-641" for this suite.
Sep 18 19:41:12.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:41:12.391: INFO: namespace container-runtime-641 deletion completed in 6.096360989s

â€¢ [SLOW TEST:9.326 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:41:12.396: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8585
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 18 19:41:12.567: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c9b8da90-8eae-443c-bb93-0f85604a8123" in namespace "projected-8585" to be "success or failure"
Sep 18 19:41:12.576: INFO: Pod "downwardapi-volume-c9b8da90-8eae-443c-bb93-0f85604a8123": Phase="Pending", Reason="", readiness=false. Elapsed: 9.701149ms
Sep 18 19:41:14.580: INFO: Pod "downwardapi-volume-c9b8da90-8eae-443c-bb93-0f85604a8123": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013532922s
Sep 18 19:41:16.584: INFO: Pod "downwardapi-volume-c9b8da90-8eae-443c-bb93-0f85604a8123": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017410322s
STEP: Saw pod success
Sep 18 19:41:16.585: INFO: Pod "downwardapi-volume-c9b8da90-8eae-443c-bb93-0f85604a8123" satisfied condition "success or failure"
Sep 18 19:41:16.587: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod downwardapi-volume-c9b8da90-8eae-443c-bb93-0f85604a8123 container client-container: <nil>
STEP: delete the pod
Sep 18 19:41:16.618: INFO: Waiting for pod downwardapi-volume-c9b8da90-8eae-443c-bb93-0f85604a8123 to disappear
Sep 18 19:41:16.620: INFO: Pod downwardapi-volume-c9b8da90-8eae-443c-bb93-0f85604a8123 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:41:16.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8585" for this suite.
Sep 18 19:41:22.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:41:22.727: INFO: namespace projected-8585 deletion completed in 6.101676699s

â€¢ [SLOW TEST:10.333 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:41:22.727: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5882
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-0ea4931b-da46-44a1-a1e3-947c5950b891
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-0ea4931b-da46-44a1-a1e3-947c5950b891
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:42:29.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5882" for this suite.
Sep 18 19:42:57.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:42:57.395: INFO: namespace configmap-5882 deletion completed in 28.10001155s

â€¢ [SLOW TEST:94.668 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:42:57.397: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8283
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 18 19:42:57.602: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:42:57.610: INFO: Number of nodes with available pods: 0
Sep 18 19:42:57.610: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 19:42:58.615: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:42:58.619: INFO: Number of nodes with available pods: 0
Sep 18 19:42:58.619: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 19:42:59.615: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:42:59.619: INFO: Number of nodes with available pods: 0
Sep 18 19:42:59.619: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 19:43:00.616: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:43:00.622: INFO: Number of nodes with available pods: 0
Sep 18 19:43:00.622: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 19:43:01.615: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:43:01.619: INFO: Number of nodes with available pods: 0
Sep 18 19:43:01.619: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 19:43:02.616: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:43:02.619: INFO: Number of nodes with available pods: 0
Sep 18 19:43:02.619: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 19:43:03.616: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:43:03.619: INFO: Number of nodes with available pods: 0
Sep 18 19:43:03.619: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 19:43:04.616: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:43:04.620: INFO: Number of nodes with available pods: 0
Sep 18 19:43:04.620: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 19:43:05.615: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:43:05.619: INFO: Number of nodes with available pods: 0
Sep 18 19:43:05.619: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 19:43:06.615: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:43:06.620: INFO: Number of nodes with available pods: 0
Sep 18 19:43:06.620: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 19:43:07.615: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:43:07.618: INFO: Number of nodes with available pods: 1
Sep 18 19:43:07.618: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 19:43:08.617: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:43:08.621: INFO: Number of nodes with available pods: 2
Sep 18 19:43:08.621: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 19:43:09.616: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:43:09.620: INFO: Number of nodes with available pods: 2
Sep 18 19:43:09.620: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 19:43:10.615: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:43:10.619: INFO: Number of nodes with available pods: 3
Sep 18 19:43:10.619: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep 18 19:43:10.642: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:43:10.645: INFO: Number of nodes with available pods: 2
Sep 18 19:43:10.646: INFO: Node k8s-agentpool1-40209065-vmss000001 is running more than one daemon pod
Sep 18 19:43:11.667: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:43:11.672: INFO: Number of nodes with available pods: 2
Sep 18 19:43:11.673: INFO: Node k8s-agentpool1-40209065-vmss000001 is running more than one daemon pod
Sep 18 19:43:12.651: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:43:12.654: INFO: Number of nodes with available pods: 2
Sep 18 19:43:12.654: INFO: Node k8s-agentpool1-40209065-vmss000001 is running more than one daemon pod
Sep 18 19:43:13.651: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:43:13.655: INFO: Number of nodes with available pods: 2
Sep 18 19:43:13.655: INFO: Node k8s-agentpool1-40209065-vmss000001 is running more than one daemon pod
Sep 18 19:43:14.651: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:43:14.656: INFO: Number of nodes with available pods: 2
Sep 18 19:43:14.656: INFO: Node k8s-agentpool1-40209065-vmss000001 is running more than one daemon pod
Sep 18 19:43:15.652: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:43:15.656: INFO: Number of nodes with available pods: 2
Sep 18 19:43:15.656: INFO: Node k8s-agentpool1-40209065-vmss000001 is running more than one daemon pod
Sep 18 19:43:16.650: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:43:16.654: INFO: Number of nodes with available pods: 3
Sep 18 19:43:16.654: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8283, will wait for the garbage collector to delete the pods
I0918 19:43:16.660200      14 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 19:43:16.660230      14 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 18 19:43:16.719: INFO: Deleting DaemonSet.extensions daemon-set took: 9.056957ms
Sep 18 19:43:17.019: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.314189ms
I0918 19:43:17.019612      14 controller_utils.go:810] Ignoring inactive pod daemonsets-8283/daemon-set-t8tgn in state Running, deletion time 2019-09-18 19:43:46 +0000 UTC
I0918 19:43:17.019671      14 controller_utils.go:810] Ignoring inactive pod daemonsets-8283/daemon-set-z5l4b in state Running, deletion time 2019-09-18 19:43:46 +0000 UTC
I0918 19:43:17.019683      14 controller_utils.go:810] Ignoring inactive pod daemonsets-8283/daemon-set-g6576 in state Running, deletion time 2019-09-18 19:43:46 +0000 UTC
Sep 18 19:43:29.923: INFO: Number of nodes with available pods: 0
Sep 18 19:43:29.923: INFO: Number of running nodes: 0, number of available pods: 0
Sep 18 19:43:29.928: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8283/daemonsets","resourceVersion":"2944"},"items":null}

Sep 18 19:43:29.930: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8283/pods","resourceVersion":"2944"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:43:29.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8283" for this suite.
Sep 18 19:43:35.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:43:36.060: INFO: namespace daemonsets-8283 deletion completed in 6.113913817s

â€¢ [SLOW TEST:38.664 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:43:36.062: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7682
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:43:40.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7682" for this suite.
Sep 18 19:44:24.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:44:24.380: INFO: namespace kubelet-test-7682 deletion completed in 44.097533721s

â€¢ [SLOW TEST:48.318 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:44:24.380: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-2025
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 18 19:44:24.872: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep 18 19:44:26.883: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432664, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432664, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432664, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704432664, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 18 19:44:29.924: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 19:44:29.928: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:44:31.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2025" for this suite.
Sep 18 19:44:37.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:44:37.407: INFO: namespace crd-webhook-2025 deletion completed in 6.098540186s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:13.039 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:44:37.420: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9127
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Sep 18 19:44:37.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 create -f - --namespace=kubectl-9127'
Sep 18 19:44:37.927: INFO: stderr: ""
Sep 18 19:44:37.927: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 18 19:44:37.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9127'
Sep 18 19:44:38.038: INFO: stderr: ""
Sep 18 19:44:38.038: INFO: stdout: "update-demo-nautilus-4xjhx update-demo-nautilus-r8sns "
Sep 18 19:44:38.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-4xjhx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9127'
Sep 18 19:44:38.132: INFO: stderr: ""
Sep 18 19:44:38.132: INFO: stdout: ""
Sep 18 19:44:38.132: INFO: update-demo-nautilus-4xjhx is created but not running
Sep 18 19:44:43.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9127'
Sep 18 19:44:43.253: INFO: stderr: ""
Sep 18 19:44:43.253: INFO: stdout: "update-demo-nautilus-4xjhx update-demo-nautilus-r8sns "
Sep 18 19:44:43.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-4xjhx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9127'
Sep 18 19:44:43.353: INFO: stderr: ""
Sep 18 19:44:43.354: INFO: stdout: "true"
Sep 18 19:44:43.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-4xjhx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9127'
Sep 18 19:44:43.441: INFO: stderr: ""
Sep 18 19:44:43.441: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 18 19:44:43.441: INFO: validating pod update-demo-nautilus-4xjhx
Sep 18 19:44:43.447: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 18 19:44:43.447: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 18 19:44:43.447: INFO: update-demo-nautilus-4xjhx is verified up and running
Sep 18 19:44:43.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-r8sns -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9127'
Sep 18 19:44:43.543: INFO: stderr: ""
Sep 18 19:44:43.543: INFO: stdout: "true"
Sep 18 19:44:43.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-r8sns -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9127'
Sep 18 19:44:43.636: INFO: stderr: ""
Sep 18 19:44:43.636: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 18 19:44:43.636: INFO: validating pod update-demo-nautilus-r8sns
Sep 18 19:44:43.643: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 18 19:44:43.643: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 18 19:44:43.643: INFO: update-demo-nautilus-r8sns is verified up and running
STEP: rolling-update to new replication controller
Sep 18 19:44:43.649: INFO: scanned /root for discovery docs: <nil>
Sep 18 19:44:43.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-9127'
Sep 18 19:45:07.219: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep 18 19:45:07.219: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 18 19:45:07.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9127'
Sep 18 19:45:07.322: INFO: stderr: ""
Sep 18 19:45:07.322: INFO: stdout: "update-demo-kitten-mrnjn update-demo-kitten-vqfvt "
Sep 18 19:45:07.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-kitten-mrnjn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9127'
Sep 18 19:45:07.409: INFO: stderr: ""
Sep 18 19:45:07.409: INFO: stdout: "true"
Sep 18 19:45:07.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-kitten-mrnjn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9127'
Sep 18 19:45:07.500: INFO: stderr: ""
Sep 18 19:45:07.500: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep 18 19:45:07.500: INFO: validating pod update-demo-kitten-mrnjn
Sep 18 19:45:07.507: INFO: got data: {
  "image": "kitten.jpg"
}

Sep 18 19:45:07.508: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep 18 19:45:07.508: INFO: update-demo-kitten-mrnjn is verified up and running
Sep 18 19:45:07.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-kitten-vqfvt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9127'
Sep 18 19:45:07.605: INFO: stderr: ""
Sep 18 19:45:07.605: INFO: stdout: "true"
Sep 18 19:45:07.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-kitten-vqfvt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9127'
Sep 18 19:45:07.693: INFO: stderr: ""
Sep 18 19:45:07.693: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep 18 19:45:07.693: INFO: validating pod update-demo-kitten-vqfvt
Sep 18 19:45:07.702: INFO: got data: {
  "image": "kitten.jpg"
}

Sep 18 19:45:07.702: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep 18 19:45:07.702: INFO: update-demo-kitten-vqfvt is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:45:07.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9127" for this suite.
Sep 18 19:45:35.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:45:35.803: INFO: namespace kubectl-9127 deletion completed in 28.095500534s

â€¢ [SLOW TEST:58.383 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:45:35.803: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4930
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Sep 18 19:45:40.491: INFO: Successfully updated pod "adopt-release-dcsr8"
STEP: Checking that the Job readopts the Pod
Sep 18 19:45:40.491: INFO: Waiting up to 15m0s for pod "adopt-release-dcsr8" in namespace "job-4930" to be "adopted"
Sep 18 19:45:40.504: INFO: Pod "adopt-release-dcsr8": Phase="Running", Reason="", readiness=true. Elapsed: 12.934242ms
Sep 18 19:45:42.508: INFO: Pod "adopt-release-dcsr8": Phase="Running", Reason="", readiness=true. Elapsed: 2.017425185s
Sep 18 19:45:42.509: INFO: Pod "adopt-release-dcsr8" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Sep 18 19:45:43.017: INFO: Successfully updated pod "adopt-release-dcsr8"
STEP: Checking that the Job releases the Pod
Sep 18 19:45:43.017: INFO: Waiting up to 15m0s for pod "adopt-release-dcsr8" in namespace "job-4930" to be "released"
Sep 18 19:45:43.028: INFO: Pod "adopt-release-dcsr8": Phase="Running", Reason="", readiness=true. Elapsed: 10.500153ms
Sep 18 19:45:45.032: INFO: Pod "adopt-release-dcsr8": Phase="Running", Reason="", readiness=true. Elapsed: 2.01440509s
Sep 18 19:45:45.032: INFO: Pod "adopt-release-dcsr8" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:45:45.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4930" for this suite.
Sep 18 19:46:35.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:46:35.143: INFO: namespace job-4930 deletion completed in 50.105162326s

â€¢ [SLOW TEST:59.340 seconds]
[sig-apps] Job
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:46:35.143: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-518
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-518
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-518
STEP: Deleting pre-stop pod
Sep 18 19:46:48.357: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:46:48.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-518" for this suite.
Sep 18 19:47:32.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:47:32.532: INFO: namespace prestop-518 deletion completed in 44.125581796s

â€¢ [SLOW TEST:57.388 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:47:32.538: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3907
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 18 19:47:32.740: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:47:32.748: INFO: Number of nodes with available pods: 0
Sep 18 19:47:32.748: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 19:47:33.755: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:47:33.758: INFO: Number of nodes with available pods: 0
Sep 18 19:47:33.758: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 19:47:34.754: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:47:34.758: INFO: Number of nodes with available pods: 0
Sep 18 19:47:34.758: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 19:47:35.754: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:47:35.757: INFO: Number of nodes with available pods: 1
Sep 18 19:47:35.757: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 19:47:36.757: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:47:36.763: INFO: Number of nodes with available pods: 2
Sep 18 19:47:36.763: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 19:47:37.755: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:47:37.758: INFO: Number of nodes with available pods: 3
Sep 18 19:47:37.758: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep 18 19:47:37.776: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:47:37.791: INFO: Number of nodes with available pods: 2
Sep 18 19:47:37.791: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 19:47:38.797: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:47:38.800: INFO: Number of nodes with available pods: 2
Sep 18 19:47:38.800: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 19:47:39.797: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:47:39.800: INFO: Number of nodes with available pods: 2
Sep 18 19:47:39.800: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 19:47:40.797: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 19:47:40.800: INFO: Number of nodes with available pods: 3
Sep 18 19:47:40.800: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3907, will wait for the garbage collector to delete the pods
I0918 19:47:40.808638      14 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 19:47:40.808681      14 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 18 19:47:40.864: INFO: Deleting DaemonSet.extensions daemon-set took: 5.734154ms
Sep 18 19:47:41.164: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.378592ms
I0918 19:47:41.164813      14 controller_utils.go:810] Ignoring inactive pod daemonsets-3907/daemon-set-jl8mn in state Running, deletion time 2019-09-18 19:48:11 +0000 UTC
I0918 19:47:41.164862      14 controller_utils.go:810] Ignoring inactive pod daemonsets-3907/daemon-set-kjrkw in state Running, deletion time 2019-09-18 19:48:11 +0000 UTC
I0918 19:47:41.164878      14 controller_utils.go:810] Ignoring inactive pod daemonsets-3907/daemon-set-tnmf8 in state Running, deletion time 2019-09-18 19:48:11 +0000 UTC
Sep 18 19:47:53.567: INFO: Number of nodes with available pods: 0
Sep 18 19:47:53.567: INFO: Number of running nodes: 0, number of available pods: 0
Sep 18 19:47:53.570: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3907/daemonsets","resourceVersion":"3820"},"items":null}

Sep 18 19:47:53.573: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3907/pods","resourceVersion":"3820"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:47:53.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3907" for this suite.
Sep 18 19:47:59.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:47:59.701: INFO: namespace daemonsets-3907 deletion completed in 6.106980655s

â€¢ [SLOW TEST:27.165 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:47:59.702: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-395
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-395
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-395
I0918 19:47:59.918898      14 runners.go:184] Created replication controller with name: externalname-service, namespace: services-395, replica count: 2
I0918 19:47:59.919044      14 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 19:47:59.919065      14 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 19:48:02.970316      14 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 18 19:48:05.970: INFO: Creating new exec pod
I0918 19:48:05.970535      14 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0918 19:48:09.995342      14 reflector.go:120] Starting reflector *v1.Endpoints (0s) from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
I0918 19:48:09.995376      14 reflector.go:158] Listing and watching *v1.Endpoints from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
Sep 18 19:48:10.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=services-395 execpodv8nvm -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep 18 19:48:12.806: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 18 19:48:12.806: INFO: stdout: ""
Sep 18 19:48:12.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=services-395 execpodv8nvm -- /bin/sh -x -c nc -zv -t -w 2 10.0.1.142 80'
Sep 18 19:48:13.087: INFO: stderr: "+ nc -zv -t -w 2 10.0.1.142 80\nConnection to 10.0.1.142 80 port [tcp/http] succeeded!\n"
Sep 18 19:48:13.087: INFO: stdout: ""
Sep 18 19:48:13.087: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:48:13.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-395" for this suite.
Sep 18 19:48:19.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:48:19.236: INFO: namespace services-395 deletion completed in 6.091703664s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:19.534 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:48:19.236: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2366
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Sep 18 19:48:19.397: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-815280101 proxy --unix-socket=/tmp/kubectl-proxy-unix338272092/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:48:19.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2366" for this suite.
Sep 18 19:48:25.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:48:25.571: INFO: namespace kubectl-2366 deletion completed in 6.10493446s

â€¢ [SLOW TEST:6.335 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:48:25.571: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8450
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-4e382a16-e532-4989-b86d-d693ebf1a8b0
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-4e382a16-e532-4989-b86d-d693ebf1a8b0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:48:33.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8450" for this suite.
Sep 18 19:48:45.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:48:45.969: INFO: namespace projected-8450 deletion completed in 12.096147245s

â€¢ [SLOW TEST:20.398 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:48:45.970: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9331
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Sep 18 19:48:50.723: INFO: Successfully updated pod "annotationupdate5e411014-2239-44f3-91ac-9b8ba9217663"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:48:52.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9331" for this suite.
Sep 18 19:49:04.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:49:04.857: INFO: namespace projected-9331 deletion completed in 12.103075186s

â€¢ [SLOW TEST:18.887 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:49:04.857: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6432
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-6432
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 18 19:49:05.031: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 18 19:49:33.179: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.240.0.47:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6432 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 18 19:49:33.180: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 19:49:33.602: INFO: Found all expected endpoints: [netserver-0]
Sep 18 19:49:33.607: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.240.0.72:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6432 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 18 19:49:33.607: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 19:49:33.761: INFO: Found all expected endpoints: [netserver-1]
Sep 18 19:49:33.765: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.240.0.20:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6432 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 18 19:49:33.766: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 19:49:33.912: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:49:33.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6432" for this suite.
Sep 18 19:49:45.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:49:46.030: INFO: namespace pod-network-test-6432 deletion completed in 12.112971975s

â€¢ [SLOW TEST:41.172 seconds]
[sig-network] Networking
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:49:46.032: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6421
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 18 19:49:56.234: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 18 19:49:56.237: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 18 19:49:58.238: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 18 19:49:58.243: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 18 19:50:00.238: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 18 19:50:00.243: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 18 19:50:02.238: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 18 19:50:02.244: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 18 19:50:04.238: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 18 19:50:04.242: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:50:04.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6421" for this suite.
Sep 18 19:50:16.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:50:16.372: INFO: namespace container-lifecycle-hook-6421 deletion completed in 12.115049464s

â€¢ [SLOW TEST:30.340 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:50:16.378: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4812
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 18 19:50:16.552: INFO: Waiting up to 5m0s for pod "pod-09e85ad2-55f9-4411-b2f8-fadbbaf729b3" in namespace "emptydir-4812" to be "success or failure"
Sep 18 19:50:16.563: INFO: Pod "pod-09e85ad2-55f9-4411-b2f8-fadbbaf729b3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.767229ms
Sep 18 19:50:18.567: INFO: Pod "pod-09e85ad2-55f9-4411-b2f8-fadbbaf729b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01469837s
Sep 18 19:50:20.571: INFO: Pod "pod-09e85ad2-55f9-4411-b2f8-fadbbaf729b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019082934s
STEP: Saw pod success
Sep 18 19:50:20.571: INFO: Pod "pod-09e85ad2-55f9-4411-b2f8-fadbbaf729b3" satisfied condition "success or failure"
Sep 18 19:50:20.574: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-09e85ad2-55f9-4411-b2f8-fadbbaf729b3 container test-container: <nil>
STEP: delete the pod
Sep 18 19:50:20.599: INFO: Waiting for pod pod-09e85ad2-55f9-4411-b2f8-fadbbaf729b3 to disappear
Sep 18 19:50:20.603: INFO: Pod pod-09e85ad2-55f9-4411-b2f8-fadbbaf729b3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:50:20.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4812" for this suite.
Sep 18 19:50:26.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:50:26.710: INFO: namespace emptydir-4812 deletion completed in 6.099476281s

â€¢ [SLOW TEST:10.332 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:50:26.712: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9582
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Sep 18 19:50:27.446: INFO: created pod pod-service-account-defaultsa
Sep 18 19:50:27.446: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep 18 19:50:27.465: INFO: created pod pod-service-account-mountsa
Sep 18 19:50:27.465: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep 18 19:50:27.480: INFO: created pod pod-service-account-nomountsa
Sep 18 19:50:27.480: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep 18 19:50:27.491: INFO: created pod pod-service-account-defaultsa-mountspec
Sep 18 19:50:27.491: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep 18 19:50:27.508: INFO: created pod pod-service-account-mountsa-mountspec
Sep 18 19:50:27.508: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep 18 19:50:27.560: INFO: created pod pod-service-account-nomountsa-mountspec
Sep 18 19:50:27.560: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep 18 19:50:27.584: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep 18 19:50:27.584: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep 18 19:50:27.621: INFO: created pod pod-service-account-mountsa-nomountspec
Sep 18 19:50:27.621: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep 18 19:50:27.655: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep 18 19:50:27.655: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:50:27.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9582" for this suite.
Sep 18 19:50:55.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:50:55.786: INFO: namespace svcaccounts-9582 deletion completed in 28.112988371s

â€¢ [SLOW TEST:29.075 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:50:55.788: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4820
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 19:50:55.956: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:51:00.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4820" for this suite.
Sep 18 19:51:06.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:51:06.597: INFO: namespace custom-resource-definition-4820 deletion completed in 6.099101409s

â€¢ [SLOW TEST:10.809 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:51:06.597: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9610
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 18 19:51:07.356: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 18 19:51:09.364: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433067, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433067, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433067, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433067, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 18 19:51:11.369: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433067, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433067, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433067, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433067, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 18 19:51:14.388: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 19:51:14.392: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-839-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:51:15.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9610" for this suite.
Sep 18 19:51:21.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:51:21.697: INFO: namespace webhook-9610 deletion completed in 6.106898843s
STEP: Destroying namespace "webhook-9610-markers" for this suite.
Sep 18 19:51:27.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:51:27.821: INFO: namespace webhook-9610-markers deletion completed in 6.124041524s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:21.237 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:51:27.835: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3013
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 18 19:51:28.540: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 18 19:51:30.550: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433088, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433088, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433088, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433088, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 18 19:51:33.569: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:51:33.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3013" for this suite.
Sep 18 19:51:39.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:51:39.826: INFO: namespace webhook-3013 deletion completed in 6.102464816s
STEP: Destroying namespace "webhook-3013-markers" for this suite.
Sep 18 19:51:45.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:51:45.927: INFO: namespace webhook-3013-markers deletion completed in 6.101389696s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:18.104 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:51:45.939: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-8705
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 18 19:51:46.687: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep 18 19:51:48.699: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433106, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433106, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433106, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433106, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 18 19:51:51.719: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 19:51:51.722: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:51:52.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8705" for this suite.
Sep 18 19:51:58.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:51:59.004: INFO: namespace crd-webhook-8705 deletion completed in 6.09121512s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:13.077 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:51:59.016: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6270
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-bf9eaa81-db7c-423c-b6b7-73285f4bbb7a
STEP: Creating configMap with name cm-test-opt-upd-d356017c-5557-48c1-ae70-9c3d20e1bc15
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-bf9eaa81-db7c-423c-b6b7-73285f4bbb7a
STEP: Updating configmap cm-test-opt-upd-d356017c-5557-48c1-ae70-9c3d20e1bc15
STEP: Creating configMap with name cm-test-opt-create-17e9dce6-7a22-4921-9770-65b6f1663bb7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:52:07.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6270" for this suite.
Sep 18 19:52:31.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:52:31.445: INFO: namespace projected-6270 deletion completed in 24.102893145s

â€¢ [SLOW TEST:32.429 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:52:31.446: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9728
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 18 19:52:31.634: INFO: Waiting up to 5m0s for pod "pod-323c7fcc-c9be-48f7-94cf-b4d8b81e671f" in namespace "emptydir-9728" to be "success or failure"
Sep 18 19:52:31.646: INFO: Pod "pod-323c7fcc-c9be-48f7-94cf-b4d8b81e671f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.508732ms
Sep 18 19:52:33.652: INFO: Pod "pod-323c7fcc-c9be-48f7-94cf-b4d8b81e671f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017506622s
Sep 18 19:52:35.656: INFO: Pod "pod-323c7fcc-c9be-48f7-94cf-b4d8b81e671f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021927438s
STEP: Saw pod success
Sep 18 19:52:35.656: INFO: Pod "pod-323c7fcc-c9be-48f7-94cf-b4d8b81e671f" satisfied condition "success or failure"
Sep 18 19:52:35.659: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-323c7fcc-c9be-48f7-94cf-b4d8b81e671f container test-container: <nil>
STEP: delete the pod
Sep 18 19:52:35.680: INFO: Waiting for pod pod-323c7fcc-c9be-48f7-94cf-b4d8b81e671f to disappear
Sep 18 19:52:35.685: INFO: Pod pod-323c7fcc-c9be-48f7-94cf-b4d8b81e671f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:52:35.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9728" for this suite.
Sep 18 19:52:41.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:52:41.793: INFO: namespace emptydir-9728 deletion completed in 6.103698803s

â€¢ [SLOW TEST:10.348 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:52:41.796: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7725
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Sep 18 19:52:41.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 create -f - --namespace=kubectl-7725'
Sep 18 19:52:42.404: INFO: stderr: ""
Sep 18 19:52:42.404: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 18 19:52:42.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7725'
Sep 18 19:52:42.504: INFO: stderr: ""
Sep 18 19:52:42.505: INFO: stdout: "update-demo-nautilus-qbm62 update-demo-nautilus-xb2ln "
Sep 18 19:52:42.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-qbm62 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7725'
Sep 18 19:52:42.604: INFO: stderr: ""
Sep 18 19:52:42.605: INFO: stdout: ""
Sep 18 19:52:42.605: INFO: update-demo-nautilus-qbm62 is created but not running
Sep 18 19:52:47.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7725'
Sep 18 19:52:47.690: INFO: stderr: ""
Sep 18 19:52:47.690: INFO: stdout: "update-demo-nautilus-qbm62 update-demo-nautilus-xb2ln "
Sep 18 19:52:47.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-qbm62 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7725'
Sep 18 19:52:47.778: INFO: stderr: ""
Sep 18 19:52:47.778: INFO: stdout: "true"
Sep 18 19:52:47.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-qbm62 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7725'
Sep 18 19:52:47.870: INFO: stderr: ""
Sep 18 19:52:47.870: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 18 19:52:47.870: INFO: validating pod update-demo-nautilus-qbm62
Sep 18 19:52:47.876: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 18 19:52:47.876: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 18 19:52:47.876: INFO: update-demo-nautilus-qbm62 is verified up and running
Sep 18 19:52:47.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-xb2ln -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7725'
Sep 18 19:52:47.966: INFO: stderr: ""
Sep 18 19:52:47.966: INFO: stdout: "true"
Sep 18 19:52:47.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-xb2ln -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7725'
Sep 18 19:52:48.053: INFO: stderr: ""
Sep 18 19:52:48.053: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 18 19:52:48.053: INFO: validating pod update-demo-nautilus-xb2ln
Sep 18 19:52:48.063: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 18 19:52:48.063: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 18 19:52:48.063: INFO: update-demo-nautilus-xb2ln is verified up and running
STEP: scaling down the replication controller
Sep 18 19:52:48.068: INFO: scanned /root for discovery docs: <nil>
Sep 18 19:52:48.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-7725'
Sep 18 19:52:49.209: INFO: stderr: ""
Sep 18 19:52:49.209: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 18 19:52:49.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7725'
Sep 18 19:52:49.295: INFO: stderr: ""
Sep 18 19:52:49.295: INFO: stdout: "update-demo-nautilus-qbm62 update-demo-nautilus-xb2ln "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep 18 19:52:54.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7725'
Sep 18 19:52:54.392: INFO: stderr: ""
Sep 18 19:52:54.392: INFO: stdout: "update-demo-nautilus-qbm62 update-demo-nautilus-xb2ln "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep 18 19:52:59.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7725'
Sep 18 19:52:59.487: INFO: stderr: ""
Sep 18 19:52:59.487: INFO: stdout: "update-demo-nautilus-qbm62 update-demo-nautilus-xb2ln "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep 18 19:53:04.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7725'
Sep 18 19:53:04.601: INFO: stderr: ""
Sep 18 19:53:04.601: INFO: stdout: "update-demo-nautilus-qbm62 "
Sep 18 19:53:04.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-qbm62 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7725'
Sep 18 19:53:04.691: INFO: stderr: ""
Sep 18 19:53:04.691: INFO: stdout: "true"
Sep 18 19:53:04.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-qbm62 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7725'
Sep 18 19:53:04.781: INFO: stderr: ""
Sep 18 19:53:04.781: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 18 19:53:04.781: INFO: validating pod update-demo-nautilus-qbm62
Sep 18 19:53:04.786: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 18 19:53:04.786: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 18 19:53:04.786: INFO: update-demo-nautilus-qbm62 is verified up and running
STEP: scaling up the replication controller
Sep 18 19:53:04.791: INFO: scanned /root for discovery docs: <nil>
Sep 18 19:53:04.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-7725'
Sep 18 19:53:05.913: INFO: stderr: ""
Sep 18 19:53:05.913: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 18 19:53:05.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7725'
Sep 18 19:53:06.005: INFO: stderr: ""
Sep 18 19:53:06.005: INFO: stdout: "update-demo-nautilus-qbm62 update-demo-nautilus-xcf7q "
Sep 18 19:53:06.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-qbm62 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7725'
Sep 18 19:53:06.099: INFO: stderr: ""
Sep 18 19:53:06.099: INFO: stdout: "true"
Sep 18 19:53:06.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-qbm62 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7725'
Sep 18 19:53:06.197: INFO: stderr: ""
Sep 18 19:53:06.197: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 18 19:53:06.197: INFO: validating pod update-demo-nautilus-qbm62
Sep 18 19:53:06.207: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 18 19:53:06.207: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 18 19:53:06.207: INFO: update-demo-nautilus-qbm62 is verified up and running
Sep 18 19:53:06.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-xcf7q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7725'
Sep 18 19:53:06.304: INFO: stderr: ""
Sep 18 19:53:06.304: INFO: stdout: ""
Sep 18 19:53:06.305: INFO: update-demo-nautilus-xcf7q is created but not running
Sep 18 19:53:11.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7725'
Sep 18 19:53:11.395: INFO: stderr: ""
Sep 18 19:53:11.395: INFO: stdout: "update-demo-nautilus-qbm62 update-demo-nautilus-xcf7q "
Sep 18 19:53:11.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-qbm62 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7725'
Sep 18 19:53:11.500: INFO: stderr: ""
Sep 18 19:53:11.500: INFO: stdout: "true"
Sep 18 19:53:11.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-qbm62 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7725'
Sep 18 19:53:11.611: INFO: stderr: ""
Sep 18 19:53:11.611: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 18 19:53:11.611: INFO: validating pod update-demo-nautilus-qbm62
Sep 18 19:53:11.616: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 18 19:53:11.616: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 18 19:53:11.616: INFO: update-demo-nautilus-qbm62 is verified up and running
Sep 18 19:53:11.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-xcf7q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7725'
Sep 18 19:53:11.719: INFO: stderr: ""
Sep 18 19:53:11.719: INFO: stdout: "true"
Sep 18 19:53:11.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-xcf7q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7725'
Sep 18 19:53:11.820: INFO: stderr: ""
Sep 18 19:53:11.820: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 18 19:53:11.820: INFO: validating pod update-demo-nautilus-xcf7q
Sep 18 19:53:11.826: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 18 19:53:11.827: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 18 19:53:11.827: INFO: update-demo-nautilus-xcf7q is verified up and running
STEP: using delete to clean up resources
Sep 18 19:53:11.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 delete --grace-period=0 --force -f - --namespace=kubectl-7725'
Sep 18 19:53:11.926: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 18 19:53:11.926: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 18 19:53:11.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7725'
Sep 18 19:53:12.033: INFO: stderr: "No resources found in kubectl-7725 namespace.\n"
Sep 18 19:53:12.033: INFO: stdout: ""
Sep 18 19:53:12.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods -l name=update-demo --namespace=kubectl-7725 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 18 19:53:12.131: INFO: stderr: ""
Sep 18 19:53:12.131: INFO: stdout: "update-demo-nautilus-qbm62\nupdate-demo-nautilus-xcf7q\n"
Sep 18 19:53:12.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7725'
Sep 18 19:53:12.773: INFO: stderr: "No resources found in kubectl-7725 namespace.\n"
Sep 18 19:53:12.773: INFO: stdout: ""
Sep 18 19:53:12.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods -l name=update-demo --namespace=kubectl-7725 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 18 19:53:12.907: INFO: stderr: ""
Sep 18 19:53:12.907: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:53:12.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7725" for this suite.
Sep 18 19:53:40.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:53:41.017: INFO: namespace kubectl-7725 deletion completed in 28.100388511s

â€¢ [SLOW TEST:59.222 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:53:41.018: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2845
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 18 19:53:41.189: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3eb25b0d-60db-4cc5-bcc0-9c7e5dbbde04" in namespace "downward-api-2845" to be "success or failure"
Sep 18 19:53:41.192: INFO: Pod "downwardapi-volume-3eb25b0d-60db-4cc5-bcc0-9c7e5dbbde04": Phase="Pending", Reason="", readiness=false. Elapsed: 3.283881ms
Sep 18 19:53:43.196: INFO: Pod "downwardapi-volume-3eb25b0d-60db-4cc5-bcc0-9c7e5dbbde04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007450548s
Sep 18 19:53:45.200: INFO: Pod "downwardapi-volume-3eb25b0d-60db-4cc5-bcc0-9c7e5dbbde04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011288129s
STEP: Saw pod success
Sep 18 19:53:45.200: INFO: Pod "downwardapi-volume-3eb25b0d-60db-4cc5-bcc0-9c7e5dbbde04" satisfied condition "success or failure"
Sep 18 19:53:45.203: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod downwardapi-volume-3eb25b0d-60db-4cc5-bcc0-9c7e5dbbde04 container client-container: <nil>
STEP: delete the pod
Sep 18 19:53:45.242: INFO: Waiting for pod downwardapi-volume-3eb25b0d-60db-4cc5-bcc0-9c7e5dbbde04 to disappear
Sep 18 19:53:45.245: INFO: Pod downwardapi-volume-3eb25b0d-60db-4cc5-bcc0-9c7e5dbbde04 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:53:45.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2845" for this suite.
Sep 18 19:53:51.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:53:51.356: INFO: namespace downward-api-2845 deletion completed in 6.106953771s

â€¢ [SLOW TEST:10.338 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:53:51.357: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3227
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Sep 18 19:53:51.515: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:53:56.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3227" for this suite.
Sep 18 19:54:02.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:54:02.182: INFO: namespace init-container-3227 deletion completed in 6.099923294s

â€¢ [SLOW TEST:10.825 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:54:02.183: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1631
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep 18 19:54:02.439: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1631 /api/v1/namespaces/watch-1631/configmaps/e2e-watch-test-resource-version c8a22b02-33ec-4a79-be27-6af82b55ecea 5332 0 2019-09-18 19:54:02 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 18 19:54:02.439: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1631 /api/v1/namespaces/watch-1631/configmaps/e2e-watch-test-resource-version c8a22b02-33ec-4a79-be27-6af82b55ecea 5333 0 2019-09-18 19:54:02 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:54:02.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1631" for this suite.
Sep 18 19:54:08.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:54:08.548: INFO: namespace watch-1631 deletion completed in 6.103950776s

â€¢ [SLOW TEST:6.365 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:54:08.550: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1883
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-b1291dcc-6873-410c-99c7-6b879d0ae5f3
STEP: Creating a pod to test consume secrets
Sep 18 19:54:08.733: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b744edea-169b-4ba8-8ab1-bf4c5ffce9fc" in namespace "projected-1883" to be "success or failure"
Sep 18 19:54:08.749: INFO: Pod "pod-projected-secrets-b744edea-169b-4ba8-8ab1-bf4c5ffce9fc": Phase="Pending", Reason="", readiness=false. Elapsed: 16.05671ms
Sep 18 19:54:10.753: INFO: Pod "pod-projected-secrets-b744edea-169b-4ba8-8ab1-bf4c5ffce9fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019487433s
Sep 18 19:54:12.758: INFO: Pod "pod-projected-secrets-b744edea-169b-4ba8-8ab1-bf4c5ffce9fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024733558s
STEP: Saw pod success
Sep 18 19:54:12.758: INFO: Pod "pod-projected-secrets-b744edea-169b-4ba8-8ab1-bf4c5ffce9fc" satisfied condition "success or failure"
Sep 18 19:54:12.760: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod pod-projected-secrets-b744edea-169b-4ba8-8ab1-bf4c5ffce9fc container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 18 19:54:12.817: INFO: Waiting for pod pod-projected-secrets-b744edea-169b-4ba8-8ab1-bf4c5ffce9fc to disappear
Sep 18 19:54:12.821: INFO: Pod pod-projected-secrets-b744edea-169b-4ba8-8ab1-bf4c5ffce9fc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:54:12.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1883" for this suite.
Sep 18 19:54:18.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:54:18.943: INFO: namespace projected-1883 deletion completed in 6.118141961s

â€¢ [SLOW TEST:10.394 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:54:18.945: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9039
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 18 19:54:19.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-9039'
Sep 18 19:54:19.245: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 18 19:54:19.245: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Sep 18 19:54:19.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 delete jobs e2e-test-httpd-job --namespace=kubectl-9039'
Sep 18 19:54:19.367: INFO: stderr: ""
Sep 18 19:54:19.367: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:54:19.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9039" for this suite.
Sep 18 19:54:25.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:54:25.480: INFO: namespace kubectl-9039 deletion completed in 6.104626038s

â€¢ [SLOW TEST:6.536 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:54:25.481: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2223
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep 18 19:54:25.662: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2223 /api/v1/namespaces/watch-2223/configmaps/e2e-watch-test-configmap-a 5006a23f-d212-4f83-948a-59293a31d8a5 5432 0 2019-09-18 19:54:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 18 19:54:25.662: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2223 /api/v1/namespaces/watch-2223/configmaps/e2e-watch-test-configmap-a 5006a23f-d212-4f83-948a-59293a31d8a5 5432 0 2019-09-18 19:54:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep 18 19:54:35.670: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2223 /api/v1/namespaces/watch-2223/configmaps/e2e-watch-test-configmap-a 5006a23f-d212-4f83-948a-59293a31d8a5 5448 0 2019-09-18 19:54:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep 18 19:54:35.670: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2223 /api/v1/namespaces/watch-2223/configmaps/e2e-watch-test-configmap-a 5006a23f-d212-4f83-948a-59293a31d8a5 5448 0 2019-09-18 19:54:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep 18 19:54:45.677: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2223 /api/v1/namespaces/watch-2223/configmaps/e2e-watch-test-configmap-a 5006a23f-d212-4f83-948a-59293a31d8a5 5466 0 2019-09-18 19:54:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 18 19:54:45.677: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2223 /api/v1/namespaces/watch-2223/configmaps/e2e-watch-test-configmap-a 5006a23f-d212-4f83-948a-59293a31d8a5 5466 0 2019-09-18 19:54:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep 18 19:54:55.686: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2223 /api/v1/namespaces/watch-2223/configmaps/e2e-watch-test-configmap-a 5006a23f-d212-4f83-948a-59293a31d8a5 5482 0 2019-09-18 19:54:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 18 19:54:55.686: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2223 /api/v1/namespaces/watch-2223/configmaps/e2e-watch-test-configmap-a 5006a23f-d212-4f83-948a-59293a31d8a5 5482 0 2019-09-18 19:54:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep 18 19:55:05.693: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2223 /api/v1/namespaces/watch-2223/configmaps/e2e-watch-test-configmap-b cd7123c9-eba9-4f25-9167-edd4f76af954 5498 0 2019-09-18 19:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 18 19:55:05.694: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2223 /api/v1/namespaces/watch-2223/configmaps/e2e-watch-test-configmap-b cd7123c9-eba9-4f25-9167-edd4f76af954 5498 0 2019-09-18 19:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep 18 19:55:15.700: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2223 /api/v1/namespaces/watch-2223/configmaps/e2e-watch-test-configmap-b cd7123c9-eba9-4f25-9167-edd4f76af954 5514 0 2019-09-18 19:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 18 19:55:15.700: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2223 /api/v1/namespaces/watch-2223/configmaps/e2e-watch-test-configmap-b cd7123c9-eba9-4f25-9167-edd4f76af954 5514 0 2019-09-18 19:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:55:25.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2223" for this suite.
Sep 18 19:55:31.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:55:31.811: INFO: namespace watch-2223 deletion completed in 6.105548218s

â€¢ [SLOW TEST:66.330 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:55:31.812: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7046
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 19:55:31.993: INFO: Waiting up to 5m0s for pod "busybox-user-65534-c7a71b6d-349a-4527-9859-4d6fe8c98591" in namespace "security-context-test-7046" to be "success or failure"
Sep 18 19:55:32.010: INFO: Pod "busybox-user-65534-c7a71b6d-349a-4527-9859-4d6fe8c98591": Phase="Pending", Reason="", readiness=false. Elapsed: 16.805109ms
Sep 18 19:55:34.014: INFO: Pod "busybox-user-65534-c7a71b6d-349a-4527-9859-4d6fe8c98591": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0209594s
Sep 18 19:55:36.018: INFO: Pod "busybox-user-65534-c7a71b6d-349a-4527-9859-4d6fe8c98591": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024675502s
Sep 18 19:55:36.018: INFO: Pod "busybox-user-65534-c7a71b6d-349a-4527-9859-4d6fe8c98591" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:55:36.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7046" for this suite.
Sep 18 19:55:42.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:55:42.135: INFO: namespace security-context-test-7046 deletion completed in 6.113725292s

â€¢ [SLOW TEST:10.323 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:55:42.136: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7064
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 18 19:55:42.735: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 18 19:55:44.744: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433342, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433342, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433342, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433342, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 18 19:55:47.783: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Sep 18 19:55:51.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 attach --namespace=webhook-7064 to-be-attached-pod -i -c=container1'
Sep 18 19:55:51.944: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:55:51.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7064" for this suite.
Sep 18 19:56:03.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:56:04.056: INFO: namespace webhook-7064 deletion completed in 12.101773773s
STEP: Destroying namespace "webhook-7064-markers" for this suite.
Sep 18 19:56:10.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:56:10.149: INFO: namespace webhook-7064-markers deletion completed in 6.093152802s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:28.026 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:56:10.162: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4649
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 18 19:56:10.329: INFO: Waiting up to 5m0s for pod "pod-2270a0e2-e690-4aa7-8d7b-06d59674881b" in namespace "emptydir-4649" to be "success or failure"
Sep 18 19:56:10.334: INFO: Pod "pod-2270a0e2-e690-4aa7-8d7b-06d59674881b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.797475ms
Sep 18 19:56:12.338: INFO: Pod "pod-2270a0e2-e690-4aa7-8d7b-06d59674881b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008853901s
Sep 18 19:56:14.343: INFO: Pod "pod-2270a0e2-e690-4aa7-8d7b-06d59674881b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013352532s
STEP: Saw pod success
Sep 18 19:56:14.343: INFO: Pod "pod-2270a0e2-e690-4aa7-8d7b-06d59674881b" satisfied condition "success or failure"
Sep 18 19:56:14.346: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-2270a0e2-e690-4aa7-8d7b-06d59674881b container test-container: <nil>
STEP: delete the pod
Sep 18 19:56:14.398: INFO: Waiting for pod pod-2270a0e2-e690-4aa7-8d7b-06d59674881b to disappear
Sep 18 19:56:14.409: INFO: Pod pod-2270a0e2-e690-4aa7-8d7b-06d59674881b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:56:14.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4649" for this suite.
Sep 18 19:56:20.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:56:20.537: INFO: namespace emptydir-4649 deletion completed in 6.124208137s

â€¢ [SLOW TEST:10.375 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:56:20.538: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8341
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:56:36.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8341" for this suite.
Sep 18 19:56:42.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:56:42.861: INFO: namespace resourcequota-8341 deletion completed in 6.099979674s

â€¢ [SLOW TEST:22.323 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:56:42.861: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4488
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 19:56:43.031: INFO: Creating deployment "test-recreate-deployment"
Sep 18 19:56:43.035: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep 18 19:56:43.057: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep 18 19:56:45.063: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep 18 19:56:45.066: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433403, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433403, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433403, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433403, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 18 19:56:47.069: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433403, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433403, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433403, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433403, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 18 19:56:49.069: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433403, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433403, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433403, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433403, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 18 19:56:51.069: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep 18 19:56:51.076: INFO: Updating deployment test-recreate-deployment
Sep 18 19:56:51.076: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Sep 18 19:56:51.172: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4488 /apis/apps/v1/namespaces/deployment-4488/deployments/test-recreate-deployment 4f57c833-df67-4842-9a39-6fe9deb7cc61 5871 2 2019-09-18 19:56:43 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002e46638 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-09-18 19:56:51 +0000 UTC,LastTransitionTime:2019-09-18 19:56:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2019-09-18 19:56:51 +0000 UTC,LastTransitionTime:2019-09-18 19:56:43 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep 18 19:56:51.176: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-4488 /apis/apps/v1/namespaces/deployment-4488/replicasets/test-recreate-deployment-5f94c574ff b08fa54f-fb3d-4dba-8cd1-412d152ddc88 5870 1 2019-09-18 19:56:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 4f57c833-df67-4842-9a39-6fe9deb7cc61 0xc002e46cb7 0xc002e46cb8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002e46de8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 18 19:56:51.176: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep 18 19:56:51.176: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-4488 /apis/apps/v1/namespaces/deployment-4488/replicasets/test-recreate-deployment-68fc85c7bb 7c573517-641d-439b-a9d3-72e5bda68c91 5860 2 2019-09-18 19:56:43 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 4f57c833-df67-4842-9a39-6fe9deb7cc61 0xc002e46e47 0xc002e46e48}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002e46ed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 18 19:56:51.181: INFO: Pod "test-recreate-deployment-5f94c574ff-m6jfz" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-m6jfz test-recreate-deployment-5f94c574ff- deployment-4488 /api/v1/namespaces/deployment-4488/pods/test-recreate-deployment-5f94c574ff-m6jfz 9f31c07e-28eb-4c0c-b904-76297b931862 5866 0 2019-09-18 19:56:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff b08fa54f-fb3d-4dba-8cd1-412d152ddc88 0xc002f9b287 0xc002f9b288}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v46x4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v46x4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v46x4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 19:56:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:56:51.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4488" for this suite.
Sep 18 19:56:57.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:56:57.318: INFO: namespace deployment-4488 deletion completed in 6.129559441s

â€¢ [SLOW TEST:14.457 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:56:57.322: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1623
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:56:57.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1623" for this suite.
Sep 18 19:57:09.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:57:09.656: INFO: namespace pods-1623 deletion completed in 12.112441414s

â€¢ [SLOW TEST:12.337 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:57:09.657: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4349
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep 18 19:57:09.824: INFO: Waiting up to 5m0s for pod "pod-3472f76d-d152-415d-aab8-1cf0e68e4435" in namespace "emptydir-4349" to be "success or failure"
Sep 18 19:57:09.871: INFO: Pod "pod-3472f76d-d152-415d-aab8-1cf0e68e4435": Phase="Pending", Reason="", readiness=false. Elapsed: 46.833152ms
Sep 18 19:57:11.875: INFO: Pod "pod-3472f76d-d152-415d-aab8-1cf0e68e4435": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050886951s
Sep 18 19:57:13.879: INFO: Pod "pod-3472f76d-d152-415d-aab8-1cf0e68e4435": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055004355s
STEP: Saw pod success
Sep 18 19:57:13.879: INFO: Pod "pod-3472f76d-d152-415d-aab8-1cf0e68e4435" satisfied condition "success or failure"
Sep 18 19:57:13.882: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod pod-3472f76d-d152-415d-aab8-1cf0e68e4435 container test-container: <nil>
STEP: delete the pod
Sep 18 19:57:13.927: INFO: Waiting for pod pod-3472f76d-d152-415d-aab8-1cf0e68e4435 to disappear
Sep 18 19:57:13.931: INFO: Pod pod-3472f76d-d152-415d-aab8-1cf0e68e4435 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:57:13.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4349" for this suite.
Sep 18 19:57:19.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:57:20.048: INFO: namespace emptydir-4349 deletion completed in 6.113057111s

â€¢ [SLOW TEST:10.391 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:57:20.050: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2305
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-6f323309-1a27-4abf-b8c3-58e5376d52e5
STEP: Creating a pod to test consume secrets
Sep 18 19:57:20.235: INFO: Waiting up to 5m0s for pod "pod-secrets-191a3967-9299-41bf-aec1-8594653e5fea" in namespace "secrets-2305" to be "success or failure"
Sep 18 19:57:20.246: INFO: Pod "pod-secrets-191a3967-9299-41bf-aec1-8594653e5fea": Phase="Pending", Reason="", readiness=false. Elapsed: 10.304546ms
Sep 18 19:57:22.250: INFO: Pod "pod-secrets-191a3967-9299-41bf-aec1-8594653e5fea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014381671s
Sep 18 19:57:24.254: INFO: Pod "pod-secrets-191a3967-9299-41bf-aec1-8594653e5fea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018289302s
STEP: Saw pod success
Sep 18 19:57:24.254: INFO: Pod "pod-secrets-191a3967-9299-41bf-aec1-8594653e5fea" satisfied condition "success or failure"
Sep 18 19:57:24.256: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-secrets-191a3967-9299-41bf-aec1-8594653e5fea container secret-volume-test: <nil>
STEP: delete the pod
Sep 18 19:57:24.282: INFO: Waiting for pod pod-secrets-191a3967-9299-41bf-aec1-8594653e5fea to disappear
Sep 18 19:57:24.285: INFO: Pod pod-secrets-191a3967-9299-41bf-aec1-8594653e5fea no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:57:24.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2305" for this suite.
Sep 18 19:57:30.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:57:30.395: INFO: namespace secrets-2305 deletion completed in 6.104741233s

â€¢ [SLOW TEST:10.345 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:57:30.396: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1070
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-1598d70b-561a-4be7-a377-4c326f94b36c
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:57:34.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1070" for this suite.
Sep 18 19:58:02.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:58:02.727: INFO: namespace configmap-1070 deletion completed in 28.102288411s

â€¢ [SLOW TEST:32.331 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:58:02.729: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1796
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Sep 18 19:58:06.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec pod-sharedvolume-0f26e717-cae0-487d-b3cb-5036d348176e -c busybox-main-container --namespace=emptydir-1796 -- cat /usr/share/volumeshare/shareddata.txt'
Sep 18 19:58:07.227: INFO: stderr: ""
Sep 18 19:58:07.227: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:58:07.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1796" for this suite.
Sep 18 19:58:13.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:58:13.330: INFO: namespace emptydir-1796 deletion completed in 6.094805875s

â€¢ [SLOW TEST:10.601 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:58:13.331: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2034
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-c07150e5-f555-400b-b25c-a945349494c6
STEP: Creating a pod to test consume secrets
Sep 18 19:58:13.566: INFO: Waiting up to 5m0s for pod "pod-secrets-314deddf-6267-4def-9acc-eee2171f13ad" in namespace "secrets-2034" to be "success or failure"
Sep 18 19:58:13.576: INFO: Pod "pod-secrets-314deddf-6267-4def-9acc-eee2171f13ad": Phase="Pending", Reason="", readiness=false. Elapsed: 10.193047ms
Sep 18 19:58:15.580: INFO: Pod "pod-secrets-314deddf-6267-4def-9acc-eee2171f13ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014768088s
Sep 18 19:58:17.584: INFO: Pod "pod-secrets-314deddf-6267-4def-9acc-eee2171f13ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018698637s
STEP: Saw pod success
Sep 18 19:58:17.584: INFO: Pod "pod-secrets-314deddf-6267-4def-9acc-eee2171f13ad" satisfied condition "success or failure"
Sep 18 19:58:17.587: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-secrets-314deddf-6267-4def-9acc-eee2171f13ad container secret-volume-test: <nil>
STEP: delete the pod
Sep 18 19:58:17.608: INFO: Waiting for pod pod-secrets-314deddf-6267-4def-9acc-eee2171f13ad to disappear
Sep 18 19:58:17.620: INFO: Pod pod-secrets-314deddf-6267-4def-9acc-eee2171f13ad no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:58:17.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2034" for this suite.
Sep 18 19:58:23.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:58:23.722: INFO: namespace secrets-2034 deletion completed in 6.097637524s

â€¢ [SLOW TEST:10.391 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:58:23.722: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-3302
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep 18 19:58:27.902: INFO: &Pod{ObjectMeta:{send-events-cb29a03d-759d-47e6-b994-799b0fb91cc5  events-3302 /api/v1/namespaces/events-3302/pods/send-events-cb29a03d-759d-47e6-b994-799b0fb91cc5 75e540af-3fab-4819-9261-b330ca89f450 6227 0 2019-09-18 19:58:23 +0000 UTC <nil> <nil> map[name:foo time:879459246] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dd7hf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dd7hf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dd7hf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 19:58:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 19:58:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 19:58:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 19:58:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.66,PodIP:10.240.0.79,StartTime:2019-09-18 19:58:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-18 19:58:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:docker://4a071854ff36b25f679d8aaea7007094150940e366ff0316bf5e7406d3158c85,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Sep 18 19:58:29.906: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep 18 19:58:31.911: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:58:31.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3302" for this suite.
Sep 18 19:59:15.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:59:16.025: INFO: namespace events-3302 deletion completed in 44.09598454s

â€¢ [SLOW TEST:52.304 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:59:16.035: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3256
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-5b12de52-7b4d-4b15-ae9b-a179bb8136d8
STEP: Creating secret with name s-test-opt-upd-f2fb10ed-58ac-4e91-8cdd-bc1354561d46
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-5b12de52-7b4d-4b15-ae9b-a179bb8136d8
STEP: Updating secret s-test-opt-upd-f2fb10ed-58ac-4e91-8cdd-bc1354561d46
STEP: Creating secret with name s-test-opt-create-d6514fc7-43da-459b-9bb4-e7b6e01f4d93
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:59:24.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3256" for this suite.
Sep 18 19:59:36.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:59:36.500: INFO: namespace projected-3256 deletion completed in 12.130625808s

â€¢ [SLOW TEST:20.465 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:59:36.500: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-41
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 18 19:59:36.667: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f44ce23e-5d0c-456a-97b8-855c0b436f57" in namespace "projected-41" to be "success or failure"
Sep 18 19:59:36.677: INFO: Pod "downwardapi-volume-f44ce23e-5d0c-456a-97b8-855c0b436f57": Phase="Pending", Reason="", readiness=false. Elapsed: 10.048848ms
Sep 18 19:59:38.681: INFO: Pod "downwardapi-volume-f44ce23e-5d0c-456a-97b8-855c0b436f57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014221334s
Sep 18 19:59:40.689: INFO: Pod "downwardapi-volume-f44ce23e-5d0c-456a-97b8-855c0b436f57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022191604s
STEP: Saw pod success
Sep 18 19:59:40.689: INFO: Pod "downwardapi-volume-f44ce23e-5d0c-456a-97b8-855c0b436f57" satisfied condition "success or failure"
Sep 18 19:59:40.696: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod downwardapi-volume-f44ce23e-5d0c-456a-97b8-855c0b436f57 container client-container: <nil>
STEP: delete the pod
Sep 18 19:59:40.718: INFO: Waiting for pod downwardapi-volume-f44ce23e-5d0c-456a-97b8-855c0b436f57 to disappear
Sep 18 19:59:40.728: INFO: Pod downwardapi-volume-f44ce23e-5d0c-456a-97b8-855c0b436f57 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 19:59:40.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-41" for this suite.
Sep 18 19:59:46.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 19:59:46.839: INFO: namespace projected-41 deletion completed in 6.106500001s

â€¢ [SLOW TEST:10.339 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 19:59:46.841: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2796
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-2796
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Sep 18 19:59:47.018: INFO: Found 0 stateful pods, waiting for 3
Sep 18 19:59:57.022: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 18 19:59:57.022: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 18 19:59:57.022: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Sep 18 20:00:07.023: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 18 20:00:07.023: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 18 20:00:07.023: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep 18 20:00:07.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-2796 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 18 20:00:09.274: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 18 20:00:09.274: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 18 20:00:09.274: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Sep 18 20:00:19.307: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep 18 20:00:29.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-2796 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:00:29.609: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 18 20:00:29.609: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 18 20:00:29.609: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 18 20:00:39.630: INFO: Waiting for StatefulSet statefulset-2796/ss2 to complete update
Sep 18 20:00:39.630: INFO: Waiting for Pod statefulset-2796/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 18 20:00:39.630: INFO: Waiting for Pod statefulset-2796/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 18 20:00:39.630: INFO: Waiting for Pod statefulset-2796/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 18 20:00:49.637: INFO: Waiting for StatefulSet statefulset-2796/ss2 to complete update
Sep 18 20:00:49.637: INFO: Waiting for Pod statefulset-2796/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 18 20:00:49.637: INFO: Waiting for Pod statefulset-2796/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 18 20:00:59.637: INFO: Waiting for StatefulSet statefulset-2796/ss2 to complete update
Sep 18 20:00:59.637: INFO: Waiting for Pod statefulset-2796/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 18 20:00:59.637: INFO: Waiting for Pod statefulset-2796/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 18 20:01:09.637: INFO: Waiting for StatefulSet statefulset-2796/ss2 to complete update
Sep 18 20:01:09.637: INFO: Waiting for Pod statefulset-2796/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 18 20:01:19.637: INFO: Waiting for StatefulSet statefulset-2796/ss2 to complete update
Sep 18 20:01:19.637: INFO: Waiting for Pod statefulset-2796/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 18 20:01:29.638: INFO: Waiting for StatefulSet statefulset-2796/ss2 to complete update
STEP: Rolling back to a previous revision
Sep 18 20:01:39.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-2796 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 18 20:01:39.983: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 18 20:01:39.983: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 18 20:01:39.983: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 18 20:01:50.017: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep 18 20:02:00.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-2796 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:02:00.295: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 18 20:02:00.295: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 18 20:02:00.295: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 18 20:02:10.315: INFO: Waiting for StatefulSet statefulset-2796/ss2 to complete update
Sep 18 20:02:10.315: INFO: Waiting for Pod statefulset-2796/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Sep 18 20:02:10.315: INFO: Waiting for Pod statefulset-2796/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Sep 18 20:02:10.315: INFO: Waiting for Pod statefulset-2796/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Sep 18 20:02:20.327: INFO: Waiting for StatefulSet statefulset-2796/ss2 to complete update
Sep 18 20:02:20.327: INFO: Waiting for Pod statefulset-2796/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Sep 18 20:02:30.322: INFO: Deleting all statefulset in ns statefulset-2796
Sep 18 20:02:30.326: INFO: Scaling statefulset ss2 to 0
Sep 18 20:02:50.348: INFO: Waiting for statefulset status.replicas updated to 0
Sep 18 20:02:50.355: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:02:50.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2796" for this suite.
Sep 18 20:02:56.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:02:56.499: INFO: namespace statefulset-2796 deletion completed in 6.123003859s

â€¢ [SLOW TEST:189.658 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:02:56.499: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4440
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 18 20:02:56.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-4440'
Sep 18 20:02:56.791: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 18 20:02:56.791: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Sep 18 20:02:56.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 delete deployment e2e-test-httpd-deployment --namespace=kubectl-4440'
Sep 18 20:02:56.917: INFO: stderr: ""
Sep 18 20:02:56.917: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:02:56.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4440" for this suite.
Sep 18 20:03:08.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:03:09.053: INFO: namespace kubectl-4440 deletion completed in 12.130421528s

â€¢ [SLOW TEST:12.554 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:03:09.053: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6154
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Sep 18 20:03:09.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 --namespace=kubectl-6154 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Sep 18 20:03:12.244: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Sep 18 20:03:12.244: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:03:14.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6154" for this suite.
Sep 18 20:03:20.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:03:20.356: INFO: namespace kubectl-6154 deletion completed in 6.09989819s

â€¢ [SLOW TEST:11.303 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:03:20.357: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1626
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Sep 18 20:03:20.514: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:03:24.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1626" for this suite.
Sep 18 20:03:30.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:03:30.283: INFO: namespace init-container-1626 deletion completed in 6.094697823s

â€¢ [SLOW TEST:9.926 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:03:30.283: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7548
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 18 20:03:33.484: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:03:33.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7548" for this suite.
Sep 18 20:03:39.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:03:39.614: INFO: namespace container-runtime-7548 deletion completed in 6.110798817s

â€¢ [SLOW TEST:9.330 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:03:39.616: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4777
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-a51588f1-bfb7-4880-8edd-b658dafae720
STEP: Creating a pod to test consume configMaps
Sep 18 20:03:39.790: INFO: Waiting up to 5m0s for pod "pod-configmaps-cd0df006-2465-4413-91b1-2dc54a40d0ad" in namespace "configmap-4777" to be "success or failure"
Sep 18 20:03:39.796: INFO: Pod "pod-configmaps-cd0df006-2465-4413-91b1-2dc54a40d0ad": Phase="Pending", Reason="", readiness=false. Elapsed: 6.128542ms
Sep 18 20:03:41.800: INFO: Pod "pod-configmaps-cd0df006-2465-4413-91b1-2dc54a40d0ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010051393s
Sep 18 20:03:43.804: INFO: Pod "pod-configmaps-cd0df006-2465-4413-91b1-2dc54a40d0ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013872277s
STEP: Saw pod success
Sep 18 20:03:43.804: INFO: Pod "pod-configmaps-cd0df006-2465-4413-91b1-2dc54a40d0ad" satisfied condition "success or failure"
Sep 18 20:03:43.807: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod pod-configmaps-cd0df006-2465-4413-91b1-2dc54a40d0ad container configmap-volume-test: <nil>
STEP: delete the pod
Sep 18 20:03:43.852: INFO: Waiting for pod pod-configmaps-cd0df006-2465-4413-91b1-2dc54a40d0ad to disappear
Sep 18 20:03:43.854: INFO: Pod pod-configmaps-cd0df006-2465-4413-91b1-2dc54a40d0ad no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:03:43.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4777" for this suite.
Sep 18 20:03:49.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:03:49.969: INFO: namespace configmap-4777 deletion completed in 6.111399999s

â€¢ [SLOW TEST:10.354 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:03:49.971: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8707
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8707.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8707.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8707.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8707.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 18 20:04:12.165: INFO: DNS probes using dns-test-2f258174-41a2-401a-98de-dbf6a8048e4a succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8707.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8707.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8707.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8707.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 18 20:04:18.235: INFO: DNS probes using dns-test-056cf621-51e4-4650-ad10-9221924f550e succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8707.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8707.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8707.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8707.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 18 20:04:38.337: INFO: DNS probes using dns-test-c33068b6-680e-4d02-b892-d3a63275d037 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:04:38.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8707" for this suite.
Sep 18 20:04:44.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:04:44.502: INFO: namespace dns-8707 deletion completed in 6.102405091s

â€¢ [SLOW TEST:54.531 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:04:44.502: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4026
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep 18 20:04:44.712: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4026 /api/v1/namespaces/watch-4026/configmaps/e2e-watch-test-label-changed 147fdceb-0312-48a7-9f16-821dba048231 7577 0 2019-09-18 20:04:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 18 20:04:44.712: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4026 /api/v1/namespaces/watch-4026/configmaps/e2e-watch-test-label-changed 147fdceb-0312-48a7-9f16-821dba048231 7578 0 2019-09-18 20:04:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep 18 20:04:44.712: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4026 /api/v1/namespaces/watch-4026/configmaps/e2e-watch-test-label-changed 147fdceb-0312-48a7-9f16-821dba048231 7579 0 2019-09-18 20:04:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep 18 20:04:54.741: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4026 /api/v1/namespaces/watch-4026/configmaps/e2e-watch-test-label-changed 147fdceb-0312-48a7-9f16-821dba048231 7596 0 2019-09-18 20:04:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 18 20:04:54.741: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4026 /api/v1/namespaces/watch-4026/configmaps/e2e-watch-test-label-changed 147fdceb-0312-48a7-9f16-821dba048231 7597 0 2019-09-18 20:04:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Sep 18 20:04:54.742: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4026 /api/v1/namespaces/watch-4026/configmaps/e2e-watch-test-label-changed 147fdceb-0312-48a7-9f16-821dba048231 7598 0 2019-09-18 20:04:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:04:54.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4026" for this suite.
Sep 18 20:05:00.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:05:00.846: INFO: namespace watch-4026 deletion completed in 6.097803405s

â€¢ [SLOW TEST:16.344 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:05:00.847: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8809
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:05:01.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8809" for this suite.
Sep 18 20:05:07.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:05:07.142: INFO: namespace custom-resource-definition-8809 deletion completed in 6.122739437s

â€¢ [SLOW TEST:6.295 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:05:07.148: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1372
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Sep 18 20:05:07.323: INFO: Waiting up to 5m0s for pod "downward-api-4fbdee63-3b37-4ebb-910a-53947227144e" in namespace "downward-api-1372" to be "success or failure"
Sep 18 20:05:07.341: INFO: Pod "downward-api-4fbdee63-3b37-4ebb-910a-53947227144e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.554436ms
Sep 18 20:05:09.345: INFO: Pod "downward-api-4fbdee63-3b37-4ebb-910a-53947227144e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02265973s
Sep 18 20:05:11.350: INFO: Pod "downward-api-4fbdee63-3b37-4ebb-910a-53947227144e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026889649s
STEP: Saw pod success
Sep 18 20:05:11.350: INFO: Pod "downward-api-4fbdee63-3b37-4ebb-910a-53947227144e" satisfied condition "success or failure"
Sep 18 20:05:11.353: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod downward-api-4fbdee63-3b37-4ebb-910a-53947227144e container dapi-container: <nil>
STEP: delete the pod
Sep 18 20:05:11.418: INFO: Waiting for pod downward-api-4fbdee63-3b37-4ebb-910a-53947227144e to disappear
Sep 18 20:05:11.428: INFO: Pod downward-api-4fbdee63-3b37-4ebb-910a-53947227144e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:05:11.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1372" for this suite.
Sep 18 20:05:17.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:05:17.542: INFO: namespace downward-api-1372 deletion completed in 6.109620666s

â€¢ [SLOW TEST:10.393 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:05:17.542: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9226
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 18 20:05:18.226: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 18 20:05:20.236: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433918, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433918, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433918, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433918, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 18 20:05:23.261: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:05:23.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9226" for this suite.
Sep 18 20:05:29.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:05:29.381: INFO: namespace webhook-9226 deletion completed in 6.104292272s
STEP: Destroying namespace "webhook-9226-markers" for this suite.
Sep 18 20:05:35.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:05:35.484: INFO: namespace webhook-9226-markers deletion completed in 6.102081224s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:17.954 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:05:35.497: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4586
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Sep 18 20:05:35.673: INFO: Waiting up to 5m0s for pod "var-expansion-b15827fe-ed61-4e5a-b546-fef7498002cf" in namespace "var-expansion-4586" to be "success or failure"
Sep 18 20:05:35.676: INFO: Pod "var-expansion-b15827fe-ed61-4e5a-b546-fef7498002cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.868575ms
Sep 18 20:05:37.680: INFO: Pod "var-expansion-b15827fe-ed61-4e5a-b546-fef7498002cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006620532s
Sep 18 20:05:39.684: INFO: Pod "var-expansion-b15827fe-ed61-4e5a-b546-fef7498002cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010839809s
STEP: Saw pod success
Sep 18 20:05:39.684: INFO: Pod "var-expansion-b15827fe-ed61-4e5a-b546-fef7498002cf" satisfied condition "success or failure"
Sep 18 20:05:39.689: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod var-expansion-b15827fe-ed61-4e5a-b546-fef7498002cf container dapi-container: <nil>
STEP: delete the pod
Sep 18 20:05:39.745: INFO: Waiting for pod var-expansion-b15827fe-ed61-4e5a-b546-fef7498002cf to disappear
Sep 18 20:05:39.748: INFO: Pod var-expansion-b15827fe-ed61-4e5a-b546-fef7498002cf no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:05:39.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4586" for this suite.
Sep 18 20:05:45.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:05:45.848: INFO: namespace var-expansion-4586 deletion completed in 6.095919265s

â€¢ [SLOW TEST:10.351 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:05:45.852: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3420
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:06:02.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3420" for this suite.
Sep 18 20:06:08.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:06:08.234: INFO: namespace resourcequota-3420 deletion completed in 6.120472369s

â€¢ [SLOW TEST:22.383 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:06:08.235: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-515
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 20:06:08.410: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Sep 18 20:06:10.455: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:06:10.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-515" for this suite.
Sep 18 20:06:16.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:06:16.565: INFO: namespace replication-controller-515 deletion completed in 6.096239268s

â€¢ [SLOW TEST:8.330 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:06:16.566: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5374
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 18 20:06:17.195: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 18 20:06:19.204: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433977, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433977, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433977, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704433977, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 18 20:06:22.224: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:06:22.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5374" for this suite.
Sep 18 20:06:34.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:06:34.381: INFO: namespace webhook-5374 deletion completed in 12.102289207s
STEP: Destroying namespace "webhook-5374-markers" for this suite.
Sep 18 20:06:40.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:06:40.504: INFO: namespace webhook-5374-markers deletion completed in 6.123685656s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:23.963 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:06:40.529: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6193
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 20:06:40.695: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-4c188ed1-98e2-441a-a422-e5508194c508" in namespace "security-context-test-6193" to be "success or failure"
Sep 18 20:06:40.706: INFO: Pod "busybox-readonly-false-4c188ed1-98e2-441a-a422-e5508194c508": Phase="Pending", Reason="", readiness=false. Elapsed: 11.104708ms
Sep 18 20:06:42.710: INFO: Pod "busybox-readonly-false-4c188ed1-98e2-441a-a422-e5508194c508": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015035216s
Sep 18 20:06:44.714: INFO: Pod "busybox-readonly-false-4c188ed1-98e2-441a-a422-e5508194c508": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018784147s
Sep 18 20:06:44.714: INFO: Pod "busybox-readonly-false-4c188ed1-98e2-441a-a422-e5508194c508" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:06:44.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6193" for this suite.
Sep 18 20:06:50.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:06:50.831: INFO: namespace security-context-test-6193 deletion completed in 6.111342599s

â€¢ [SLOW TEST:10.302 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:06:50.832: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8374
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-366a7f2e-e96a-4e26-8c3f-1a02f2c09521
STEP: Creating a pod to test consume configMaps
Sep 18 20:06:51.002: INFO: Waiting up to 5m0s for pod "pod-configmaps-d885a6c4-2102-4806-a755-3fb1bea0b6c2" in namespace "configmap-8374" to be "success or failure"
Sep 18 20:06:51.005: INFO: Pod "pod-configmaps-d885a6c4-2102-4806-a755-3fb1bea0b6c2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.331472ms
Sep 18 20:06:53.014: INFO: Pod "pod-configmaps-d885a6c4-2102-4806-a755-3fb1bea0b6c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011813354s
Sep 18 20:06:55.018: INFO: Pod "pod-configmaps-d885a6c4-2102-4806-a755-3fb1bea0b6c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015871493s
STEP: Saw pod success
Sep 18 20:06:55.018: INFO: Pod "pod-configmaps-d885a6c4-2102-4806-a755-3fb1bea0b6c2" satisfied condition "success or failure"
Sep 18 20:06:55.021: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod pod-configmaps-d885a6c4-2102-4806-a755-3fb1bea0b6c2 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 18 20:06:55.045: INFO: Waiting for pod pod-configmaps-d885a6c4-2102-4806-a755-3fb1bea0b6c2 to disappear
Sep 18 20:06:55.048: INFO: Pod pod-configmaps-d885a6c4-2102-4806-a755-3fb1bea0b6c2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:06:55.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8374" for this suite.
Sep 18 20:07:01.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:07:01.159: INFO: namespace configmap-8374 deletion completed in 6.106295875s

â€¢ [SLOW TEST:10.327 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:07:01.160: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5128
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5128.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5128.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5128.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5128.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5128.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5128.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 18 20:07:05.384: INFO: DNS probes using dns-5128/dns-test-65e3e221-f1e2-44df-a165-f68b9679b74c succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:07:05.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5128" for this suite.
Sep 18 20:07:11.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:07:11.517: INFO: namespace dns-5128 deletion completed in 6.107273896s

â€¢ [SLOW TEST:10.357 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:07:11.518: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8938
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8938.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8938.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8938.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8938.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8938.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8938.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8938.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8938.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8938.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8938.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8938.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8938.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8938.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8938.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8938.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8938.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8938.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8938.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 18 20:07:17.743: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8938.svc.cluster.local from pod dns-8938/dns-test-18efbb63-5b9e-425f-a9e8-e2174dddc1dc: the server could not find the requested resource (get pods dns-test-18efbb63-5b9e-425f-a9e8-e2174dddc1dc)
Sep 18 20:07:17.747: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8938.svc.cluster.local from pod dns-8938/dns-test-18efbb63-5b9e-425f-a9e8-e2174dddc1dc: the server could not find the requested resource (get pods dns-test-18efbb63-5b9e-425f-a9e8-e2174dddc1dc)
Sep 18 20:07:17.751: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8938.svc.cluster.local from pod dns-8938/dns-test-18efbb63-5b9e-425f-a9e8-e2174dddc1dc: the server could not find the requested resource (get pods dns-test-18efbb63-5b9e-425f-a9e8-e2174dddc1dc)
Sep 18 20:07:17.754: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8938.svc.cluster.local from pod dns-8938/dns-test-18efbb63-5b9e-425f-a9e8-e2174dddc1dc: the server could not find the requested resource (get pods dns-test-18efbb63-5b9e-425f-a9e8-e2174dddc1dc)
Sep 18 20:07:17.770: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8938.svc.cluster.local from pod dns-8938/dns-test-18efbb63-5b9e-425f-a9e8-e2174dddc1dc: the server could not find the requested resource (get pods dns-test-18efbb63-5b9e-425f-a9e8-e2174dddc1dc)
Sep 18 20:07:17.775: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8938.svc.cluster.local from pod dns-8938/dns-test-18efbb63-5b9e-425f-a9e8-e2174dddc1dc: the server could not find the requested resource (get pods dns-test-18efbb63-5b9e-425f-a9e8-e2174dddc1dc)
Sep 18 20:07:17.778: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8938.svc.cluster.local from pod dns-8938/dns-test-18efbb63-5b9e-425f-a9e8-e2174dddc1dc: the server could not find the requested resource (get pods dns-test-18efbb63-5b9e-425f-a9e8-e2174dddc1dc)
Sep 18 20:07:17.782: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8938.svc.cluster.local from pod dns-8938/dns-test-18efbb63-5b9e-425f-a9e8-e2174dddc1dc: the server could not find the requested resource (get pods dns-test-18efbb63-5b9e-425f-a9e8-e2174dddc1dc)
Sep 18 20:07:17.790: INFO: Lookups using dns-8938/dns-test-18efbb63-5b9e-425f-a9e8-e2174dddc1dc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8938.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8938.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8938.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8938.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8938.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8938.svc.cluster.local jessie_udp@dns-test-service-2.dns-8938.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8938.svc.cluster.local]

Sep 18 20:07:22.832: INFO: DNS probes using dns-8938/dns-test-18efbb63-5b9e-425f-a9e8-e2174dddc1dc succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:07:22.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8938" for this suite.
Sep 18 20:07:28.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:07:29.043: INFO: namespace dns-8938 deletion completed in 6.108120131s

â€¢ [SLOW TEST:17.525 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:07:29.045: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9747
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 18 20:07:29.560: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 18 20:07:31.571: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704434049, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704434049, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704434049, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704434049, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 18 20:07:34.603: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:07:34.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9747" for this suite.
Sep 18 20:07:40.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:07:40.920: INFO: namespace webhook-9747 deletion completed in 6.096651878s
STEP: Destroying namespace "webhook-9747-markers" for this suite.
Sep 18 20:07:46.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:07:47.034: INFO: namespace webhook-9747-markers deletion completed in 6.114185519s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:18.011 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:07:47.058: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4055
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 18 20:07:47.233: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d51d1bb-f731-4001-8254-71b271f2ac66" in namespace "projected-4055" to be "success or failure"
Sep 18 20:07:47.246: INFO: Pod "downwardapi-volume-4d51d1bb-f731-4001-8254-71b271f2ac66": Phase="Pending", Reason="", readiness=false. Elapsed: 12.642899ms
Sep 18 20:07:49.250: INFO: Pod "downwardapi-volume-4d51d1bb-f731-4001-8254-71b271f2ac66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016983582s
Sep 18 20:07:51.255: INFO: Pod "downwardapi-volume-4d51d1bb-f731-4001-8254-71b271f2ac66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02168168s
STEP: Saw pod success
Sep 18 20:07:51.255: INFO: Pod "downwardapi-volume-4d51d1bb-f731-4001-8254-71b271f2ac66" satisfied condition "success or failure"
Sep 18 20:07:51.258: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod downwardapi-volume-4d51d1bb-f731-4001-8254-71b271f2ac66 container client-container: <nil>
STEP: delete the pod
Sep 18 20:07:51.283: INFO: Waiting for pod downwardapi-volume-4d51d1bb-f731-4001-8254-71b271f2ac66 to disappear
Sep 18 20:07:51.286: INFO: Pod downwardapi-volume-4d51d1bb-f731-4001-8254-71b271f2ac66 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:07:51.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4055" for this suite.
Sep 18 20:07:57.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:07:57.386: INFO: namespace projected-4055 deletion completed in 6.095479067s

â€¢ [SLOW TEST:10.328 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:07:57.386: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7446
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7446.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7446.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 18 20:08:03.658: INFO: DNS probes using dns-7446/dns-test-81573cba-3526-484c-944c-d27292101e1a succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:08:03.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7446" for this suite.
Sep 18 20:08:09.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:08:09.787: INFO: namespace dns-7446 deletion completed in 6.100994875s

â€¢ [SLOW TEST:12.401 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:08:09.787: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6317
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-9b7f38b3-8f40-4ee1-9a27-a9eed91f182f
STEP: Creating a pod to test consume secrets
Sep 18 20:08:09.957: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-738db8fa-785d-4908-89f8-02efa3ffdccd" in namespace "projected-6317" to be "success or failure"
Sep 18 20:08:09.962: INFO: Pod "pod-projected-secrets-738db8fa-785d-4908-89f8-02efa3ffdccd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.266666ms
Sep 18 20:08:11.966: INFO: Pod "pod-projected-secrets-738db8fa-785d-4908-89f8-02efa3ffdccd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008486561s
Sep 18 20:08:13.970: INFO: Pod "pod-projected-secrets-738db8fa-785d-4908-89f8-02efa3ffdccd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012557076s
STEP: Saw pod success
Sep 18 20:08:13.970: INFO: Pod "pod-projected-secrets-738db8fa-785d-4908-89f8-02efa3ffdccd" satisfied condition "success or failure"
Sep 18 20:08:13.972: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-projected-secrets-738db8fa-785d-4908-89f8-02efa3ffdccd container secret-volume-test: <nil>
STEP: delete the pod
Sep 18 20:08:14.022: INFO: Waiting for pod pod-projected-secrets-738db8fa-785d-4908-89f8-02efa3ffdccd to disappear
Sep 18 20:08:14.025: INFO: Pod pod-projected-secrets-738db8fa-785d-4908-89f8-02efa3ffdccd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:08:14.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6317" for this suite.
Sep 18 20:08:20.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:08:20.123: INFO: namespace projected-6317 deletion completed in 6.093219723s

â€¢ [SLOW TEST:10.335 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:08:20.125: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5345
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Sep 18 20:08:20.284: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 20:08:23.783: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:08:35.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5345" for this suite.
Sep 18 20:08:41.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:08:41.795: INFO: namespace crd-publish-openapi-5345 deletion completed in 6.113407048s

â€¢ [SLOW TEST:21.671 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:08:41.796: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5909
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Sep 18 20:08:46.512: INFO: Successfully updated pod "annotationupdate17003687-fec6-40d9-b82e-82aa83f82973"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:08:48.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5909" for this suite.
Sep 18 20:09:16.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:09:16.630: INFO: namespace downward-api-5909 deletion completed in 28.093085837s

â€¢ [SLOW TEST:34.834 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:09:16.631: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7322
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:09:21.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7322" for this suite.
Sep 18 20:09:49.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:09:49.931: INFO: namespace replication-controller-7322 deletion completed in 28.098394324s

â€¢ [SLOW TEST:33.300 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:09:49.932: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8702
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-8702
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 18 20:09:50.092: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 18 20:10:14.207: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.240.0.13 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8702 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 18 20:10:14.207: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 20:10:15.371: INFO: Found all expected endpoints: [netserver-0]
Sep 18 20:10:15.374: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.240.0.46 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8702 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 18 20:10:15.374: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 20:10:16.551: INFO: Found all expected endpoints: [netserver-1]
Sep 18 20:10:16.554: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.240.0.85 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8702 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 18 20:10:16.554: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 20:10:17.716: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:10:17.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8702" for this suite.
Sep 18 20:10:29.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:10:29.849: INFO: namespace pod-network-test-8702 deletion completed in 12.12701153s

â€¢ [SLOW TEST:39.917 seconds]
[sig-network] Networking
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:10:29.849: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-626
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-626
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 18 20:10:30.044: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 18 20:10:56.145: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.80:8080/dial?request=hostName&protocol=http&host=10.240.0.59&port=8080&tries=1'] Namespace:pod-network-test-626 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 18 20:10:56.145: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 20:10:56.297: INFO: Waiting for endpoints: map[]
Sep 18 20:10:56.300: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.80:8080/dial?request=hostName&protocol=http&host=10.240.0.9&port=8080&tries=1'] Namespace:pod-network-test-626 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 18 20:10:56.300: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 20:10:56.470: INFO: Waiting for endpoints: map[]
Sep 18 20:10:56.473: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.80:8080/dial?request=hostName&protocol=http&host=10.240.0.74&port=8080&tries=1'] Namespace:pod-network-test-626 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 18 20:10:56.473: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 20:10:56.615: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:10:56.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-626" for this suite.
Sep 18 20:11:08.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:11:08.722: INFO: namespace pod-network-test-626 deletion completed in 12.10106111s

â€¢ [SLOW TEST:38.873 seconds]
[sig-network] Networking
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:11:08.724: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9919
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 18 20:11:08.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-9919'
Sep 18 20:11:10.739: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 18 20:11:10.739: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Sep 18 20:11:10.773: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Sep 18 20:11:10.790: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Sep 18 20:11:10.803: INFO: scanned /root for discovery docs: <nil>
Sep 18 20:11:10.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-9919'
Sep 18 20:11:26.604: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep 18 20:11:26.604: INFO: stdout: "Created e2e-test-httpd-rc-9cac9e7f180c5d0f38c00aad32eb41ba\nScaling up e2e-test-httpd-rc-9cac9e7f180c5d0f38c00aad32eb41ba from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-9cac9e7f180c5d0f38c00aad32eb41ba up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-9cac9e7f180c5d0f38c00aad32eb41ba to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Sep 18 20:11:26.604: INFO: stdout: "Created e2e-test-httpd-rc-9cac9e7f180c5d0f38c00aad32eb41ba\nScaling up e2e-test-httpd-rc-9cac9e7f180c5d0f38c00aad32eb41ba from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-9cac9e7f180c5d0f38c00aad32eb41ba up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-9cac9e7f180c5d0f38c00aad32eb41ba to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Sep 18 20:11:26.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-9919'
Sep 18 20:11:26.693: INFO: stderr: ""
Sep 18 20:11:26.693: INFO: stdout: "e2e-test-httpd-rc-9cac9e7f180c5d0f38c00aad32eb41ba-r5m6q "
Sep 18 20:11:26.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods e2e-test-httpd-rc-9cac9e7f180c5d0f38c00aad32eb41ba-r5m6q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9919'
Sep 18 20:11:26.777: INFO: stderr: ""
Sep 18 20:11:26.777: INFO: stdout: "true"
Sep 18 20:11:26.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods e2e-test-httpd-rc-9cac9e7f180c5d0f38c00aad32eb41ba-r5m6q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9919'
Sep 18 20:11:26.861: INFO: stderr: ""
Sep 18 20:11:26.862: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Sep 18 20:11:26.862: INFO: e2e-test-httpd-rc-9cac9e7f180c5d0f38c00aad32eb41ba-r5m6q is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Sep 18 20:11:26.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 delete rc e2e-test-httpd-rc --namespace=kubectl-9919'
Sep 18 20:11:26.966: INFO: stderr: ""
Sep 18 20:11:26.966: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:11:26.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9919" for this suite.
Sep 18 20:11:54.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:11:55.068: INFO: namespace kubectl-9919 deletion completed in 28.094139579s

â€¢ [SLOW TEST:46.344 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:11:55.068: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-516
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-2badd125-5e0e-4e93-a370-21c3b762e030
STEP: Creating configMap with name cm-test-opt-upd-2247d11c-c45c-410c-877f-7fd910edcc7d
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-2badd125-5e0e-4e93-a370-21c3b762e030
STEP: Updating configmap cm-test-opt-upd-2247d11c-c45c-410c-877f-7fd910edcc7d
STEP: Creating configMap with name cm-test-opt-create-f61db745-1f11-4b63-934b-48926c3219b8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:12:01.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-516" for this suite.
Sep 18 20:12:13.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:12:13.484: INFO: namespace configmap-516 deletion completed in 12.098830521s

â€¢ [SLOW TEST:18.416 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:12:13.484: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7226
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 18 20:12:13.663: INFO: Waiting up to 5m0s for pod "downwardapi-volume-26ebe4f2-f082-4af8-bfcd-d98026b0ff0e" in namespace "downward-api-7226" to be "success or failure"
Sep 18 20:12:13.669: INFO: Pod "downwardapi-volume-26ebe4f2-f082-4af8-bfcd-d98026b0ff0e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.888359ms
Sep 18 20:12:15.675: INFO: Pod "downwardapi-volume-26ebe4f2-f082-4af8-bfcd-d98026b0ff0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012261807s
Sep 18 20:12:17.681: INFO: Pod "downwardapi-volume-26ebe4f2-f082-4af8-bfcd-d98026b0ff0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018060271s
STEP: Saw pod success
Sep 18 20:12:17.681: INFO: Pod "downwardapi-volume-26ebe4f2-f082-4af8-bfcd-d98026b0ff0e" satisfied condition "success or failure"
Sep 18 20:12:17.683: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod downwardapi-volume-26ebe4f2-f082-4af8-bfcd-d98026b0ff0e container client-container: <nil>
STEP: delete the pod
Sep 18 20:12:17.702: INFO: Waiting for pod downwardapi-volume-26ebe4f2-f082-4af8-bfcd-d98026b0ff0e to disappear
Sep 18 20:12:17.707: INFO: Pod downwardapi-volume-26ebe4f2-f082-4af8-bfcd-d98026b0ff0e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:12:17.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7226" for this suite.
Sep 18 20:12:23.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:12:23.830: INFO: namespace downward-api-7226 deletion completed in 6.118813057s

â€¢ [SLOW TEST:10.345 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:12:23.831: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4831
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 18 20:12:24.007: INFO: Waiting up to 5m0s for pod "pod-1e977268-251a-4349-aad5-6623b5124b5c" in namespace "emptydir-4831" to be "success or failure"
Sep 18 20:12:24.011: INFO: Pod "pod-1e977268-251a-4349-aad5-6623b5124b5c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.545876ms
Sep 18 20:12:26.015: INFO: Pod "pod-1e977268-251a-4349-aad5-6623b5124b5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007625198s
Sep 18 20:12:28.019: INFO: Pod "pod-1e977268-251a-4349-aad5-6623b5124b5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011236035s
STEP: Saw pod success
Sep 18 20:12:28.019: INFO: Pod "pod-1e977268-251a-4349-aad5-6623b5124b5c" satisfied condition "success or failure"
Sep 18 20:12:28.022: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-1e977268-251a-4349-aad5-6623b5124b5c container test-container: <nil>
STEP: delete the pod
Sep 18 20:12:28.079: INFO: Waiting for pod pod-1e977268-251a-4349-aad5-6623b5124b5c to disappear
Sep 18 20:12:28.089: INFO: Pod pod-1e977268-251a-4349-aad5-6623b5124b5c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:12:28.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4831" for this suite.
Sep 18 20:12:34.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:12:34.219: INFO: namespace emptydir-4831 deletion completed in 6.122831405s

â€¢ [SLOW TEST:10.389 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:12:34.223: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-1100
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep 18 20:12:42.482: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1100 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 18 20:12:42.482: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 20:12:42.644: INFO: Exec stderr: ""
Sep 18 20:12:42.644: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1100 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 18 20:12:42.644: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 20:12:42.777: INFO: Exec stderr: ""
Sep 18 20:12:42.777: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1100 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 18 20:12:42.777: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 20:12:42.945: INFO: Exec stderr: ""
Sep 18 20:12:42.945: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1100 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 18 20:12:42.945: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 20:12:43.114: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep 18 20:12:43.114: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1100 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 18 20:12:43.114: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 20:12:43.282: INFO: Exec stderr: ""
Sep 18 20:12:43.282: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1100 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 18 20:12:43.282: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 20:12:43.433: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep 18 20:12:43.433: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1100 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 18 20:12:43.433: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 20:12:43.596: INFO: Exec stderr: ""
Sep 18 20:12:43.596: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1100 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 18 20:12:43.596: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 20:12:43.754: INFO: Exec stderr: ""
Sep 18 20:12:43.754: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1100 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 18 20:12:43.756: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 20:12:43.919: INFO: Exec stderr: ""
Sep 18 20:12:43.919: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1100 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 18 20:12:43.920: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 20:12:44.060: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:12:44.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1100" for this suite.
Sep 18 20:13:28.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:13:28.212: INFO: namespace e2e-kubelet-etc-hosts-1100 deletion completed in 44.146107319s

â€¢ [SLOW TEST:53.990 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:13:28.217: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-6122
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep 18 20:13:28.611: INFO: Pod name wrapped-volume-race-1eba538d-f260-43f2-ab64-e4f7b9bd6884: Found 0 pods out of 5
Sep 18 20:13:33.619: INFO: Pod name wrapped-volume-race-1eba538d-f260-43f2-ab64-e4f7b9bd6884: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1eba538d-f260-43f2-ab64-e4f7b9bd6884 in namespace emptydir-wrapper-6122, will wait for the garbage collector to delete the pods
I0918 20:13:47.649252      14 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 20:13:47.649281      14 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 18 20:13:47.708: INFO: Deleting ReplicationController wrapped-volume-race-1eba538d-f260-43f2-ab64-e4f7b9bd6884 took: 8.616542ms
I0918 20:13:48.008240      14 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-6122/wrapped-volume-race-1eba538d-f260-43f2-ab64-e4f7b9bd6884-77jwl in state Running, deletion time 2019-09-18 20:14:17 +0000 UTC
I0918 20:13:48.008281      14 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-6122/wrapped-volume-race-1eba538d-f260-43f2-ab64-e4f7b9bd6884-d2jqn in state Running, deletion time 2019-09-18 20:14:17 +0000 UTC
I0918 20:13:48.008291      14 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-6122/wrapped-volume-race-1eba538d-f260-43f2-ab64-e4f7b9bd6884-cx25p in state Running, deletion time 2019-09-18 20:14:17 +0000 UTC
I0918 20:13:48.008298      14 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-6122/wrapped-volume-race-1eba538d-f260-43f2-ab64-e4f7b9bd6884-f8ttw in state Running, deletion time 2019-09-18 20:14:17 +0000 UTC
Sep 18 20:13:48.008: INFO: Terminating ReplicationController wrapped-volume-race-1eba538d-f260-43f2-ab64-e4f7b9bd6884 pods took: 300.398783ms
I0918 20:13:48.008385      14 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-6122/wrapped-volume-race-1eba538d-f260-43f2-ab64-e4f7b9bd6884-zt8qx in state Running, deletion time 2019-09-18 20:14:17 +0000 UTC
STEP: Creating RC which spawns configmap-volume pods
Sep 18 20:14:25.031: INFO: Pod name wrapped-volume-race-71eb618c-6009-4206-8597-c80af6db6db4: Found 0 pods out of 5
Sep 18 20:14:30.038: INFO: Pod name wrapped-volume-race-71eb618c-6009-4206-8597-c80af6db6db4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-71eb618c-6009-4206-8597-c80af6db6db4 in namespace emptydir-wrapper-6122, will wait for the garbage collector to delete the pods
I0918 20:14:44.065258      14 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 20:14:44.065565      14 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 18 20:14:44.125: INFO: Deleting ReplicationController wrapped-volume-race-71eb618c-6009-4206-8597-c80af6db6db4 took: 10.119733ms
I0918 20:14:44.425799      14 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-6122/wrapped-volume-race-71eb618c-6009-4206-8597-c80af6db6db4-dtmqq in state Running, deletion time 2019-09-18 20:15:14 +0000 UTC
I0918 20:14:44.425847      14 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-6122/wrapped-volume-race-71eb618c-6009-4206-8597-c80af6db6db4-rssnd in state Running, deletion time 2019-09-18 20:15:14 +0000 UTC
I0918 20:14:44.425861      14 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-6122/wrapped-volume-race-71eb618c-6009-4206-8597-c80af6db6db4-kjtc6 in state Running, deletion time 2019-09-18 20:15:14 +0000 UTC
I0918 20:14:44.525829      14 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-6122/wrapped-volume-race-71eb618c-6009-4206-8597-c80af6db6db4-7gnv6 in state Running, deletion time 2019-09-18 20:15:14 +0000 UTC
I0918 20:14:44.525888      14 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-6122/wrapped-volume-race-71eb618c-6009-4206-8597-c80af6db6db4-rssnd in state Running, deletion time 2019-09-18 20:15:14 +0000 UTC
I0918 20:14:44.525901      14 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-6122/wrapped-volume-race-71eb618c-6009-4206-8597-c80af6db6db4-kjtc6 in state Running, deletion time 2019-09-18 20:15:14 +0000 UTC
I0918 20:14:44.525910      14 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-6122/wrapped-volume-race-71eb618c-6009-4206-8597-c80af6db6db4-lwz9s in state Running, deletion time 2019-09-18 20:15:14 +0000 UTC
I0918 20:14:44.525917      14 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-6122/wrapped-volume-race-71eb618c-6009-4206-8597-c80af6db6db4-dtmqq in state Running, deletion time 2019-09-18 20:15:14 +0000 UTC
Sep 18 20:14:44.525: INFO: Terminating ReplicationController wrapped-volume-race-71eb618c-6009-4206-8597-c80af6db6db4 pods took: 400.466262ms
STEP: Creating RC which spawns configmap-volume pods
Sep 18 20:15:22.047: INFO: Pod name wrapped-volume-race-f734b8a5-1361-4bd0-a4f9-b43453582c0c: Found 0 pods out of 5
Sep 18 20:15:27.064: INFO: Pod name wrapped-volume-race-f734b8a5-1361-4bd0-a4f9-b43453582c0c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f734b8a5-1361-4bd0-a4f9-b43453582c0c in namespace emptydir-wrapper-6122, will wait for the garbage collector to delete the pods
I0918 20:15:41.101767      14 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 20:15:41.101890      14 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 18 20:15:41.161: INFO: Deleting ReplicationController wrapped-volume-race-f734b8a5-1361-4bd0-a4f9-b43453582c0c took: 9.157141ms
I0918 20:15:41.461245      14 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-6122/wrapped-volume-race-f734b8a5-1361-4bd0-a4f9-b43453582c0c-g5xst in state Running, deletion time 2019-09-18 20:16:11 +0000 UTC
I0918 20:15:41.461419      14 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-6122/wrapped-volume-race-f734b8a5-1361-4bd0-a4f9-b43453582c0c-gllr5 in state Running, deletion time 2019-09-18 20:16:11 +0000 UTC
I0918 20:15:41.461441      14 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-6122/wrapped-volume-race-f734b8a5-1361-4bd0-a4f9-b43453582c0c-xc2g2 in state Running, deletion time 2019-09-18 20:16:11 +0000 UTC
I0918 20:15:41.461463      14 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-6122/wrapped-volume-race-f734b8a5-1361-4bd0-a4f9-b43453582c0c-zxf6f in state Running, deletion time 2019-09-18 20:16:11 +0000 UTC
I0918 20:15:41.561264      14 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-6122/wrapped-volume-race-f734b8a5-1361-4bd0-a4f9-b43453582c0c-g5xst in state Running, deletion time 2019-09-18 20:16:11 +0000 UTC
I0918 20:15:41.561319      14 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-6122/wrapped-volume-race-f734b8a5-1361-4bd0-a4f9-b43453582c0c-jf5lh in state Running, deletion time 2019-09-18 20:16:11 +0000 UTC
I0918 20:15:41.561330      14 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-6122/wrapped-volume-race-f734b8a5-1361-4bd0-a4f9-b43453582c0c-gllr5 in state Running, deletion time 2019-09-18 20:16:11 +0000 UTC
I0918 20:15:41.561339      14 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-6122/wrapped-volume-race-f734b8a5-1361-4bd0-a4f9-b43453582c0c-xc2g2 in state Running, deletion time 2019-09-18 20:16:11 +0000 UTC
I0918 20:15:41.561346      14 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-6122/wrapped-volume-race-f734b8a5-1361-4bd0-a4f9-b43453582c0c-zxf6f in state Running, deletion time 2019-09-18 20:16:11 +0000 UTC
Sep 18 20:15:41.561: INFO: Terminating ReplicationController wrapped-volume-race-f734b8a5-1361-4bd0-a4f9-b43453582c0c pods took: 400.322708ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:16:22.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6122" for this suite.
Sep 18 20:16:30.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:16:30.463: INFO: namespace emptydir-wrapper-6122 deletion completed in 8.116115024s

â€¢ [SLOW TEST:182.246 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:16:30.463: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2218
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Sep 18 20:16:30.627: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 18 20:16:30.639: INFO: Waiting for terminating namespaces to be deleted...
Sep 18 20:16:30.642: INFO: 
Logging pods the kubelet thinks is on node k8s-agentpool1-40209065-vmss000000 before test
Sep 18 20:16:30.685: INFO: keyvault-flexvolume-n9xxt from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 20:16:30.685: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
Sep 18 20:16:30.685: INFO: kube-proxy-rhg4h from kube-system started at 2019-09-18 19:32:35 +0000 UTC (1 container statuses recorded)
Sep 18 20:16:30.685: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 18 20:16:30.685: INFO: blobfuse-flexvol-installer-6ghd6 from kube-system started at 2019-09-18 19:32:36 +0000 UTC (1 container statuses recorded)
Sep 18 20:16:30.685: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
Sep 18 20:16:30.685: INFO: azure-ip-masq-agent-l2c2n from kube-system started at 2019-09-18 19:32:37 +0000 UTC (1 container statuses recorded)
Sep 18 20:16:30.686: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Sep 18 20:16:30.686: INFO: sonobuoy-systemd-logs-daemon-set-13153e5f2c4542ef-86g6v from sonobuoy started at 2019-09-18 19:33:36 +0000 UTC (2 container statuses recorded)
Sep 18 20:16:30.686: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 18 20:16:30.686: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 18 20:16:30.686: INFO: azure-cni-networkmonitor-9x9wr from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 20:16:30.686: INFO: 	Container azure-cnms ready: true, restart count 0
Sep 18 20:16:30.686: INFO: kubernetes-dashboard-65966766b9-86cqc from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 20:16:30.686: INFO: 	Container kubernetes-dashboard ready: true, restart count 1
Sep 18 20:16:30.686: INFO: 
Logging pods the kubelet thinks is on node k8s-agentpool1-40209065-vmss000001 before test
Sep 18 20:16:30.723: INFO: azure-cni-networkmonitor-ppx7z from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 20:16:30.723: INFO: 	Container azure-cnms ready: true, restart count 0
Sep 18 20:16:30.723: INFO: blobfuse-flexvol-installer-bbdb6 from kube-system started at 2019-09-18 19:32:36 +0000 UTC (1 container statuses recorded)
Sep 18 20:16:30.723: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
Sep 18 20:16:30.723: INFO: azure-ip-masq-agent-62ddt from kube-system started at 2019-09-18 19:32:37 +0000 UTC (1 container statuses recorded)
Sep 18 20:16:30.723: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Sep 18 20:16:30.723: INFO: sonobuoy-systemd-logs-daemon-set-13153e5f2c4542ef-9l9xz from sonobuoy started at 2019-09-18 19:33:36 +0000 UTC (2 container statuses recorded)
Sep 18 20:16:30.723: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 18 20:16:30.723: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 18 20:16:30.723: INFO: keyvault-flexvolume-428z4 from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 20:16:30.723: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
Sep 18 20:16:30.723: INFO: kube-proxy-cp8gq from kube-system started at 2019-09-18 19:32:35 +0000 UTC (1 container statuses recorded)
Sep 18 20:16:30.723: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 18 20:16:30.723: INFO: sonobuoy-e2e-job-326e453a1b3e45d6 from sonobuoy started at 2019-09-18 19:33:36 +0000 UTC (2 container statuses recorded)
Sep 18 20:16:30.723: INFO: 	Container e2e ready: true, restart count 0
Sep 18 20:16:30.723: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 18 20:16:30.723: INFO: 
Logging pods the kubelet thinks is on node k8s-agentpool1-40209065-vmss000002 before test
Sep 18 20:16:30.756: INFO: azure-cni-networkmonitor-86wjx from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 20:16:30.756: INFO: 	Container azure-cnms ready: true, restart count 0
Sep 18 20:16:30.756: INFO: blobfuse-flexvol-installer-n7wtj from kube-system started at 2019-09-18 19:32:36 +0000 UTC (1 container statuses recorded)
Sep 18 20:16:30.756: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
Sep 18 20:16:30.756: INFO: metrics-server-855b565c8f-rrds2 from kube-system started at 2019-09-18 19:32:37 +0000 UTC (1 container statuses recorded)
Sep 18 20:16:30.756: INFO: 	Container metrics-server ready: true, restart count 0
Sep 18 20:16:30.756: INFO: keyvault-flexvolume-f9jq8 from kube-system started at 2019-09-18 19:32:35 +0000 UTC (1 container statuses recorded)
Sep 18 20:16:30.756: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
Sep 18 20:16:30.756: INFO: kube-proxy-vnxpl from kube-system started at 2019-09-18 19:32:35 +0000 UTC (1 container statuses recorded)
Sep 18 20:16:30.756: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 18 20:16:30.756: INFO: azure-ip-masq-agent-q7dfv from kube-system started at 2019-09-18 19:32:37 +0000 UTC (1 container statuses recorded)
Sep 18 20:16:30.756: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Sep 18 20:16:30.756: INFO: sonobuoy from sonobuoy started at 2019-09-18 19:33:29 +0000 UTC (1 container statuses recorded)
Sep 18 20:16:30.756: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 18 20:16:30.756: INFO: sonobuoy-systemd-logs-daemon-set-13153e5f2c4542ef-djfs2 from sonobuoy started at 2019-09-18 19:33:36 +0000 UTC (2 container statuses recorded)
Sep 18 20:16:30.756: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 18 20:16:30.756: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node k8s-agentpool1-40209065-vmss000000
STEP: verifying the node has the label node k8s-agentpool1-40209065-vmss000001
STEP: verifying the node has the label node k8s-agentpool1-40209065-vmss000002
Sep 18 20:16:30.825: INFO: Pod azure-cni-networkmonitor-86wjx requesting resource cpu=0m on Node k8s-agentpool1-40209065-vmss000002
Sep 18 20:16:30.825: INFO: Pod azure-cni-networkmonitor-9x9wr requesting resource cpu=0m on Node k8s-agentpool1-40209065-vmss000000
Sep 18 20:16:30.825: INFO: Pod azure-cni-networkmonitor-ppx7z requesting resource cpu=0m on Node k8s-agentpool1-40209065-vmss000001
Sep 18 20:16:30.825: INFO: Pod azure-ip-masq-agent-62ddt requesting resource cpu=50m on Node k8s-agentpool1-40209065-vmss000001
Sep 18 20:16:30.825: INFO: Pod azure-ip-masq-agent-l2c2n requesting resource cpu=50m on Node k8s-agentpool1-40209065-vmss000000
Sep 18 20:16:30.825: INFO: Pod azure-ip-masq-agent-q7dfv requesting resource cpu=50m on Node k8s-agentpool1-40209065-vmss000002
Sep 18 20:16:30.825: INFO: Pod blobfuse-flexvol-installer-6ghd6 requesting resource cpu=50m on Node k8s-agentpool1-40209065-vmss000000
Sep 18 20:16:30.825: INFO: Pod blobfuse-flexvol-installer-bbdb6 requesting resource cpu=50m on Node k8s-agentpool1-40209065-vmss000001
Sep 18 20:16:30.825: INFO: Pod blobfuse-flexvol-installer-n7wtj requesting resource cpu=50m on Node k8s-agentpool1-40209065-vmss000002
Sep 18 20:16:30.825: INFO: Pod keyvault-flexvolume-428z4 requesting resource cpu=50m on Node k8s-agentpool1-40209065-vmss000001
Sep 18 20:16:30.825: INFO: Pod keyvault-flexvolume-f9jq8 requesting resource cpu=50m on Node k8s-agentpool1-40209065-vmss000002
Sep 18 20:16:30.825: INFO: Pod keyvault-flexvolume-n9xxt requesting resource cpu=50m on Node k8s-agentpool1-40209065-vmss000000
Sep 18 20:16:30.825: INFO: Pod kube-proxy-cp8gq requesting resource cpu=100m on Node k8s-agentpool1-40209065-vmss000001
Sep 18 20:16:30.825: INFO: Pod kube-proxy-rhg4h requesting resource cpu=100m on Node k8s-agentpool1-40209065-vmss000000
Sep 18 20:16:30.825: INFO: Pod kube-proxy-vnxpl requesting resource cpu=100m on Node k8s-agentpool1-40209065-vmss000002
Sep 18 20:16:30.825: INFO: Pod kubernetes-dashboard-65966766b9-86cqc requesting resource cpu=300m on Node k8s-agentpool1-40209065-vmss000000
Sep 18 20:16:30.825: INFO: Pod metrics-server-855b565c8f-rrds2 requesting resource cpu=0m on Node k8s-agentpool1-40209065-vmss000002
Sep 18 20:16:30.825: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-agentpool1-40209065-vmss000002
Sep 18 20:16:30.825: INFO: Pod sonobuoy-e2e-job-326e453a1b3e45d6 requesting resource cpu=0m on Node k8s-agentpool1-40209065-vmss000001
Sep 18 20:16:30.825: INFO: Pod sonobuoy-systemd-logs-daemon-set-13153e5f2c4542ef-86g6v requesting resource cpu=0m on Node k8s-agentpool1-40209065-vmss000000
Sep 18 20:16:30.825: INFO: Pod sonobuoy-systemd-logs-daemon-set-13153e5f2c4542ef-9l9xz requesting resource cpu=0m on Node k8s-agentpool1-40209065-vmss000001
Sep 18 20:16:30.825: INFO: Pod sonobuoy-systemd-logs-daemon-set-13153e5f2c4542ef-djfs2 requesting resource cpu=0m on Node k8s-agentpool1-40209065-vmss000002
STEP: Starting Pods to consume most of the cluster CPU.
Sep 18 20:16:30.825: INFO: Creating a pod which consumes cpu=1225m on Node k8s-agentpool1-40209065-vmss000001
Sep 18 20:16:30.833: INFO: Creating a pod which consumes cpu=1225m on Node k8s-agentpool1-40209065-vmss000002
Sep 18 20:16:30.849: INFO: Creating a pod which consumes cpu=1015m on Node k8s-agentpool1-40209065-vmss000000
STEP: Creating another pod that requires unavailable amount of CPU.
I0918 20:16:36.899097      14 reflector.go:120] Starting reflector *v1.Event (0s) from k8s.io/kubernetes/test/e2e/common/events.go:136
I0918 20:16:36.899222      14 reflector.go:158] Listing and watching *v1.Event from k8s.io/kubernetes/test/e2e/common/events.go:136
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ff4edb0-6808-48be-8926-ca7aefb9f76c.15c5a1a8a2150c40], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2218/filler-pod-5ff4edb0-6808-48be-8926-ca7aefb9f76c to k8s-agentpool1-40209065-vmss000002]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ff4edb0-6808-48be-8926-ca7aefb9f76c.15c5a1a8fc20ab71], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ff4edb0-6808-48be-8926-ca7aefb9f76c.15c5a1a90e356a62], Reason = [Created], Message = [Created container filler-pod-5ff4edb0-6808-48be-8926-ca7aefb9f76c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ff4edb0-6808-48be-8926-ca7aefb9f76c.15c5a1a91ffd11b9], Reason = [Started], Message = [Started container filler-pod-5ff4edb0-6808-48be-8926-ca7aefb9f76c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7125307d-7740-4b47-b702-667cac27233b.15c5a1a8a1290214], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2218/filler-pod-7125307d-7740-4b47-b702-667cac27233b to k8s-agentpool1-40209065-vmss000001]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7125307d-7740-4b47-b702-667cac27233b.15c5a1a8fd2e236d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7125307d-7740-4b47-b702-667cac27233b.15c5a1a9172290f2], Reason = [Created], Message = [Created container filler-pod-7125307d-7740-4b47-b702-667cac27233b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7125307d-7740-4b47-b702-667cac27233b.15c5a1a92bef0d34], Reason = [Started], Message = [Started container filler-pod-7125307d-7740-4b47-b702-667cac27233b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e44f1e7-5fa5-4143-a13b-b5ad3fbf37fc.15c5a1a8a38d0320], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2218/filler-pod-7e44f1e7-5fa5-4143-a13b-b5ad3fbf37fc to k8s-agentpool1-40209065-vmss000000]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e44f1e7-5fa5-4143-a13b-b5ad3fbf37fc.15c5a1a9182ff31e], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e44f1e7-5fa5-4143-a13b-b5ad3fbf37fc.15c5a1a93890d682], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e44f1e7-5fa5-4143-a13b-b5ad3fbf37fc.15c5a1a9476923ea], Reason = [Created], Message = [Created container filler-pod-7e44f1e7-5fa5-4143-a13b-b5ad3fbf37fc]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e44f1e7-5fa5-4143-a13b-b5ad3fbf37fc.15c5a1a95c200956], Reason = [Started], Message = [Started container filler-pod-7e44f1e7-5fa5-4143-a13b-b5ad3fbf37fc]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15c5a1aa0b22be70], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 3 Insufficient cpu.]
STEP: removing the label node off the node k8s-agentpool1-40209065-vmss000000
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-agentpool1-40209065-vmss000001
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-agentpool1-40209065-vmss000002
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:16:38.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2218" for this suite.
Sep 18 20:16:44.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:16:44.101: INFO: namespace sched-pred-2218 deletion completed in 6.097084482s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:13.638 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
I0918 20:16:44.101508      14 request.go:706] Error in request: resource name may not be empty
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:16:44.102: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8194
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-8194
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-8194
STEP: Creating statefulset with conflicting port in namespace statefulset-8194
STEP: Waiting until pod test-pod will start running in namespace statefulset-8194
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8194
Sep 18 20:16:48.300: INFO: Observed stateful pod in namespace: statefulset-8194, name: ss-0, uid: 1d08676b-dfe3-4379-b796-509772b05dec, status phase: Pending. Waiting for statefulset controller to delete.
Sep 18 20:16:48.487: INFO: Observed stateful pod in namespace: statefulset-8194, name: ss-0, uid: 1d08676b-dfe3-4379-b796-509772b05dec, status phase: Failed. Waiting for statefulset controller to delete.
Sep 18 20:16:48.493: INFO: Observed stateful pod in namespace: statefulset-8194, name: ss-0, uid: 1d08676b-dfe3-4379-b796-509772b05dec, status phase: Failed. Waiting for statefulset controller to delete.
Sep 18 20:16:48.543: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8194
STEP: Removing pod with conflicting port in namespace statefulset-8194
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8194 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Sep 18 20:16:52.581: INFO: Deleting all statefulset in ns statefulset-8194
Sep 18 20:16:52.584: INFO: Scaling statefulset ss to 0
Sep 18 20:17:02.599: INFO: Waiting for statefulset status.replicas updated to 0
Sep 18 20:17:02.601: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:17:02.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8194" for this suite.
Sep 18 20:17:08.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:17:08.747: INFO: namespace statefulset-8194 deletion completed in 6.121254675s

â€¢ [SLOW TEST:24.646 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:17:08.747: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3182
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:17:08.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3182" for this suite.
Sep 18 20:17:32.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:17:33.044: INFO: namespace kubelet-test-3182 deletion completed in 24.104059489s

â€¢ [SLOW TEST:24.296 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:17:33.045: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7973
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-c803fdc3-dbde-4ce7-9997-97298b121f34 in namespace container-probe-7973
Sep 18 20:17:37.238: INFO: Started pod test-webserver-c803fdc3-dbde-4ce7-9997-97298b121f34 in namespace container-probe-7973
STEP: checking the pod's current state and verifying that restartCount is present
Sep 18 20:17:37.242: INFO: Initial restart count of pod test-webserver-c803fdc3-dbde-4ce7-9997-97298b121f34 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:21:37.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7973" for this suite.
Sep 18 20:21:43.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:21:43.906: INFO: namespace container-probe-7973 deletion completed in 6.112506311s

â€¢ [SLOW TEST:250.861 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:21:43.907: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-971
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-b92e205f-02a1-4922-a0b6-0f037a6dd021
STEP: Creating a pod to test consume configMaps
Sep 18 20:21:44.092: INFO: Waiting up to 5m0s for pod "pod-configmaps-1a8a12c1-719e-4209-9c9d-bb03af923d52" in namespace "configmap-971" to be "success or failure"
Sep 18 20:21:44.115: INFO: Pod "pod-configmaps-1a8a12c1-719e-4209-9c9d-bb03af923d52": Phase="Pending", Reason="", readiness=false. Elapsed: 23.324561ms
Sep 18 20:21:46.119: INFO: Pod "pod-configmaps-1a8a12c1-719e-4209-9c9d-bb03af923d52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027574072s
Sep 18 20:21:48.124: INFO: Pod "pod-configmaps-1a8a12c1-719e-4209-9c9d-bb03af923d52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031924786s
STEP: Saw pod success
Sep 18 20:21:48.124: INFO: Pod "pod-configmaps-1a8a12c1-719e-4209-9c9d-bb03af923d52" satisfied condition "success or failure"
Sep 18 20:21:48.127: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-configmaps-1a8a12c1-719e-4209-9c9d-bb03af923d52 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 18 20:21:48.187: INFO: Waiting for pod pod-configmaps-1a8a12c1-719e-4209-9c9d-bb03af923d52 to disappear
Sep 18 20:21:48.190: INFO: Pod pod-configmaps-1a8a12c1-719e-4209-9c9d-bb03af923d52 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:21:48.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-971" for this suite.
Sep 18 20:21:54.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:21:54.288: INFO: namespace configmap-971 deletion completed in 6.093261485s

â€¢ [SLOW TEST:10.382 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:21:54.289: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9364
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 18 20:21:54.467: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92ffe005-75fb-42ca-8e0a-134f29a77e95" in namespace "downward-api-9364" to be "success or failure"
Sep 18 20:21:54.473: INFO: Pod "downwardapi-volume-92ffe005-75fb-42ca-8e0a-134f29a77e95": Phase="Pending", Reason="", readiness=false. Elapsed: 6.428061ms
Sep 18 20:21:56.477: INFO: Pod "downwardapi-volume-92ffe005-75fb-42ca-8e0a-134f29a77e95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010286093s
Sep 18 20:21:58.481: INFO: Pod "downwardapi-volume-92ffe005-75fb-42ca-8e0a-134f29a77e95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01400763s
STEP: Saw pod success
Sep 18 20:21:58.481: INFO: Pod "downwardapi-volume-92ffe005-75fb-42ca-8e0a-134f29a77e95" satisfied condition "success or failure"
Sep 18 20:21:58.484: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod downwardapi-volume-92ffe005-75fb-42ca-8e0a-134f29a77e95 container client-container: <nil>
STEP: delete the pod
Sep 18 20:21:58.503: INFO: Waiting for pod downwardapi-volume-92ffe005-75fb-42ca-8e0a-134f29a77e95 to disappear
Sep 18 20:21:58.506: INFO: Pod downwardapi-volume-92ffe005-75fb-42ca-8e0a-134f29a77e95 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:21:58.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9364" for this suite.
Sep 18 20:22:04.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:22:04.622: INFO: namespace downward-api-9364 deletion completed in 6.105865667s

â€¢ [SLOW TEST:10.333 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:22:04.623: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-458
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 20:22:04.839: INFO: (0) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 46.844321ms)
Sep 18 20:22:04.847: INFO: (1) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 7.208557ms)
Sep 18 20:22:04.851: INFO: (2) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.754572ms)
Sep 18 20:22:04.856: INFO: (3) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.88167ms)
Sep 18 20:22:04.861: INFO: (4) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.380474ms)
Sep 18 20:22:04.865: INFO: (5) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.067375ms)
Sep 18 20:22:04.869: INFO: (6) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.449374ms)
Sep 18 20:22:04.873: INFO: (7) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.038675ms)
Sep 18 20:22:04.877: INFO: (8) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.751878ms)
Sep 18 20:22:04.882: INFO: (9) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.422873ms)
Sep 18 20:22:04.886: INFO: (10) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.173675ms)
Sep 18 20:22:04.891: INFO: (11) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.680072ms)
Sep 18 20:22:04.895: INFO: (12) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.248774ms)
Sep 18 20:22:04.900: INFO: (13) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.881171ms)
Sep 18 20:22:04.905: INFO: (14) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.895271ms)
Sep 18 20:22:04.910: INFO: (15) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 5.179969ms)
Sep 18 20:22:04.914: INFO: (16) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.969076ms)
Sep 18 20:22:04.918: INFO: (17) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.110076ms)
Sep 18 20:22:04.922: INFO: (18) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.065975ms)
Sep 18 20:22:04.926: INFO: (19) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.158575ms)
[AfterEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:22:04.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-458" for this suite.
Sep 18 20:22:10.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:22:11.028: INFO: namespace proxy-458 deletion completed in 6.097518752s

â€¢ [SLOW TEST:6.405 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:22:11.029: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-266
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 18 20:22:11.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-266'
Sep 18 20:22:13.171: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 18 20:22:13.171: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Sep 18 20:22:17.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 delete deployment e2e-test-httpd-deployment --namespace=kubectl-266'
Sep 18 20:22:17.286: INFO: stderr: ""
Sep 18 20:22:17.286: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:22:17.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-266" for this suite.
Sep 18 20:22:45.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:22:45.390: INFO: namespace kubectl-266 deletion completed in 28.096754577s

â€¢ [SLOW TEST:34.361 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:22:45.391: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8635
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:22:56.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8635" for this suite.
Sep 18 20:23:02.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:23:02.709: INFO: namespace resourcequota-8635 deletion completed in 6.097277518s

â€¢ [SLOW TEST:17.319 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:23:02.713: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9034
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-5f81ae84-c9e2-4196-96c3-21bc604d6189
STEP: Creating a pod to test consume configMaps
Sep 18 20:23:02.891: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e81035e0-cbbd-4e2e-9ee2-b25509db8684" in namespace "projected-9034" to be "success or failure"
Sep 18 20:23:02.904: INFO: Pod "pod-projected-configmaps-e81035e0-cbbd-4e2e-9ee2-b25509db8684": Phase="Pending", Reason="", readiness=false. Elapsed: 12.667425ms
Sep 18 20:23:04.908: INFO: Pod "pod-projected-configmaps-e81035e0-cbbd-4e2e-9ee2-b25509db8684": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016920572s
Sep 18 20:23:06.913: INFO: Pod "pod-projected-configmaps-e81035e0-cbbd-4e2e-9ee2-b25509db8684": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021711818s
STEP: Saw pod success
Sep 18 20:23:06.913: INFO: Pod "pod-projected-configmaps-e81035e0-cbbd-4e2e-9ee2-b25509db8684" satisfied condition "success or failure"
Sep 18 20:23:06.915: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod pod-projected-configmaps-e81035e0-cbbd-4e2e-9ee2-b25509db8684 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 18 20:23:06.973: INFO: Waiting for pod pod-projected-configmaps-e81035e0-cbbd-4e2e-9ee2-b25509db8684 to disappear
Sep 18 20:23:06.977: INFO: Pod pod-projected-configmaps-e81035e0-cbbd-4e2e-9ee2-b25509db8684 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:23:06.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9034" for this suite.
Sep 18 20:23:12.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:23:13.105: INFO: namespace projected-9034 deletion completed in 6.121741724s

â€¢ [SLOW TEST:10.392 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:23:13.105: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3874
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0918 20:23:14.312513      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 18 20:23:14.312: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:23:14.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3874" for this suite.
Sep 18 20:23:20.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:23:20.405: INFO: namespace gc-3874 deletion completed in 6.087637761s

â€¢ [SLOW TEST:7.300 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:23:20.407: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7385
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:23:24.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7385" for this suite.
Sep 18 20:23:30.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:23:30.684: INFO: namespace kubelet-test-7385 deletion completed in 6.090475592s

â€¢ [SLOW TEST:10.278 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:23:30.685: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-233
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-233
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-233
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-233
Sep 18 20:23:30.865: INFO: Found 0 stateful pods, waiting for 1
Sep 18 20:23:40.868: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep 18 20:23:40.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 18 20:23:41.162: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 18 20:23:41.162: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 18 20:23:41.162: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 18 20:23:41.166: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 18 20:23:51.180: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 18 20:23:51.180: INFO: Waiting for statefulset status.replicas updated to 0
Sep 18 20:23:51.195: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Sep 18 20:23:51.195: INFO: ss-0  k8s-agentpool1-40209065-vmss000001  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:30 +0000 UTC  }]
Sep 18 20:23:51.195: INFO: 
Sep 18 20:23:51.195: INFO: StatefulSet ss has not reached scale 3, at 1
Sep 18 20:23:52.200: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994719709s
Sep 18 20:23:53.204: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989419218s
Sep 18 20:23:54.209: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984917721s
Sep 18 20:23:55.213: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980425723s
Sep 18 20:23:56.218: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.975729526s
Sep 18 20:23:57.223: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.970877229s
Sep 18 20:23:58.231: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.96623003s
Sep 18 20:23:59.235: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.957887851s
Sep 18 20:24:00.241: INFO: Verifying statefulset ss doesn't scale past 3 for another 953.852247ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-233
Sep 18 20:24:01.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:24:01.493: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 18 20:24:01.493: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 18 20:24:01.493: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 18 20:24:01.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:24:01.793: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 18 20:24:01.793: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 18 20:24:01.793: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 18 20:24:01.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:24:02.072: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 18 20:24:02.072: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 18 20:24:02.072: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 18 20:24:02.077: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 18 20:24:02.077: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 18 20:24:02.077: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep 18 20:24:02.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 18 20:24:02.336: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 18 20:24:02.336: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 18 20:24:02.336: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 18 20:24:02.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 18 20:24:02.654: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 18 20:24:02.654: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 18 20:24:02.654: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 18 20:24:02.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 18 20:24:02.965: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 18 20:24:02.965: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 18 20:24:02.965: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 18 20:24:02.965: INFO: Waiting for statefulset status.replicas updated to 0
Sep 18 20:24:02.970: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep 18 20:24:12.977: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 18 20:24:12.977: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 18 20:24:12.977: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 18 20:24:12.990: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Sep 18 20:24:12.990: INFO: ss-0  k8s-agentpool1-40209065-vmss000001  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:30 +0000 UTC  }]
Sep 18 20:24:12.991: INFO: ss-1  k8s-agentpool1-40209065-vmss000002  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  }]
Sep 18 20:24:12.991: INFO: ss-2  k8s-agentpool1-40209065-vmss000000  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  }]
Sep 18 20:24:12.991: INFO: 
Sep 18 20:24:12.991: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 18 20:24:13.995: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Sep 18 20:24:13.995: INFO: ss-0  k8s-agentpool1-40209065-vmss000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:30 +0000 UTC  }]
Sep 18 20:24:13.995: INFO: ss-1  k8s-agentpool1-40209065-vmss000002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  }]
Sep 18 20:24:13.995: INFO: ss-2  k8s-agentpool1-40209065-vmss000000  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  }]
Sep 18 20:24:13.995: INFO: 
Sep 18 20:24:13.995: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 18 20:24:14.999: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Sep 18 20:24:14.999: INFO: ss-0  k8s-agentpool1-40209065-vmss000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:30 +0000 UTC  }]
Sep 18 20:24:14.999: INFO: ss-1  k8s-agentpool1-40209065-vmss000002  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  }]
Sep 18 20:24:15.000: INFO: ss-2  k8s-agentpool1-40209065-vmss000000  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  }]
Sep 18 20:24:15.000: INFO: 
Sep 18 20:24:15.000: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 18 20:24:16.004: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Sep 18 20:24:16.004: INFO: ss-0  k8s-agentpool1-40209065-vmss000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:30 +0000 UTC  }]
Sep 18 20:24:16.004: INFO: ss-2  k8s-agentpool1-40209065-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  }]
Sep 18 20:24:16.004: INFO: 
Sep 18 20:24:16.004: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 18 20:24:17.008: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Sep 18 20:24:17.008: INFO: ss-0  k8s-agentpool1-40209065-vmss000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:30 +0000 UTC  }]
Sep 18 20:24:17.008: INFO: ss-2  k8s-agentpool1-40209065-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  }]
Sep 18 20:24:17.008: INFO: 
Sep 18 20:24:17.008: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 18 20:24:18.013: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Sep 18 20:24:18.013: INFO: ss-0  k8s-agentpool1-40209065-vmss000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:30 +0000 UTC  }]
Sep 18 20:24:18.013: INFO: ss-2  k8s-agentpool1-40209065-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  }]
Sep 18 20:24:18.013: INFO: 
Sep 18 20:24:18.013: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 18 20:24:19.017: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Sep 18 20:24:19.017: INFO: ss-0  k8s-agentpool1-40209065-vmss000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:30 +0000 UTC  }]
Sep 18 20:24:19.017: INFO: ss-2  k8s-agentpool1-40209065-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  }]
Sep 18 20:24:19.017: INFO: 
Sep 18 20:24:19.017: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 18 20:24:20.022: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Sep 18 20:24:20.022: INFO: ss-0  k8s-agentpool1-40209065-vmss000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:30 +0000 UTC  }]
Sep 18 20:24:20.022: INFO: ss-2  k8s-agentpool1-40209065-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  }]
Sep 18 20:24:20.022: INFO: 
Sep 18 20:24:20.022: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 18 20:24:21.026: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Sep 18 20:24:21.026: INFO: ss-0  k8s-agentpool1-40209065-vmss000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:30 +0000 UTC  }]
Sep 18 20:24:21.026: INFO: ss-2  k8s-agentpool1-40209065-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:51 +0000 UTC  }]
Sep 18 20:24:21.026: INFO: 
Sep 18 20:24:21.026: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 18 20:24:22.030: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Sep 18 20:24:22.030: INFO: ss-0  k8s-agentpool1-40209065-vmss000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:24:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-18 20:23:30 +0000 UTC  }]
Sep 18 20:24:22.030: INFO: 
Sep 18 20:24:22.030: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-233
Sep 18 20:24:23.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:24:23.221: INFO: rc: 1
Sep 18 20:24:23.221: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  error: unable to upgrade connection: container not found ("webserver")
 [] <nil> 0xc002b2c060 exit status 1 <nil> <nil> true [0xc003587e30 0xc003587e68 0xc003587ea8] [0xc003587e30 0xc003587e68 0xc003587ea8] [0xc003587e50 0xc003587ea0] [0x10ef310 0x10ef310] 0xc0035b9620 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Sep 18 20:24:33.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:24:33.328: INFO: rc: 1
Sep 18 20:24:33.328: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002b2c3f0 exit status 1 <nil> <nil> true [0xc003587eb0 0xc003587ec8 0xc003587f20] [0xc003587eb0 0xc003587ec8 0xc003587f20] [0xc003587ec0 0xc003587f00] [0x10ef310 0x10ef310] 0xc0035b9980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:24:43.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:24:43.421: INFO: rc: 1
Sep 18 20:24:43.421: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc005f14360 exit status 1 <nil> <nil> true [0xc000234240 0xc000234750 0xc000234880] [0xc000234240 0xc000234750 0xc000234880] [0xc0002346c0 0xc000234860] [0x10ef310 0x10ef310] 0xc00507c360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:24:53.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:24:53.508: INFO: rc: 1
Sep 18 20:24:53.508: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00462c390 exit status 1 <nil> <nil> true [0xc00385a000 0xc00385a028 0xc00385a040] [0xc00385a000 0xc00385a028 0xc00385a040] [0xc00385a020 0xc00385a038] [0x10ef310 0x10ef310] 0xc0037742a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:25:03.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:25:03.613: INFO: rc: 1
Sep 18 20:25:03.613: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00462c780 exit status 1 <nil> <nil> true [0xc00385a048 0xc00385a078 0xc00385a098] [0xc00385a048 0xc00385a078 0xc00385a098] [0xc00385a060 0xc00385a090] [0x10ef310 0x10ef310] 0xc003774660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:25:13.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:25:13.697: INFO: rc: 1
Sep 18 20:25:13.698: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00462cb40 exit status 1 <nil> <nil> true [0xc00385a0a0 0xc00385a0b8 0xc00385a0d0] [0xc00385a0a0 0xc00385a0b8 0xc00385a0d0] [0xc00385a0b0 0xc00385a0c8] [0x10ef310 0x10ef310] 0xc0037749c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:25:23.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:25:23.794: INFO: rc: 1
Sep 18 20:25:23.794: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00462cf30 exit status 1 <nil> <nil> true [0xc00385a0d8 0xc00385a0f0 0xc00385a108] [0xc00385a0d8 0xc00385a0f0 0xc00385a108] [0xc00385a0e8 0xc00385a100] [0x10ef310 0x10ef310] 0xc003774d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:25:33.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:25:33.905: INFO: rc: 1
Sep 18 20:25:33.905: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00462d2f0 exit status 1 <nil> <nil> true [0xc00385a110 0xc00385a128 0xc00385a140] [0xc00385a110 0xc00385a128 0xc00385a140] [0xc00385a120 0xc00385a138] [0x10ef310 0x10ef310] 0xc003775080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:25:43.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:25:44.017: INFO: rc: 1
Sep 18 20:25:44.017: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00462d890 exit status 1 <nil> <nil> true [0xc00385a148 0xc00385a160 0xc00385a178] [0xc00385a148 0xc00385a160 0xc00385a178] [0xc00385a158 0xc00385a170] [0x10ef310 0x10ef310] 0xc0037753e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:25:54.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:25:54.109: INFO: rc: 1
Sep 18 20:25:54.109: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc005f14780 exit status 1 <nil> <nil> true [0xc000234890 0xc000234be0 0xc000234eb0] [0xc000234890 0xc000234be0 0xc000234eb0] [0xc000234a40 0xc000234e70] [0x10ef310 0x10ef310] 0xc00507c720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:26:04.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:26:04.204: INFO: rc: 1
Sep 18 20:26:04.205: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00462dc50 exit status 1 <nil> <nil> true [0xc00385a180 0xc00385a198 0xc00385a1b0] [0xc00385a180 0xc00385a198 0xc00385a1b0] [0xc00385a190 0xc00385a1a8] [0x10ef310 0x10ef310] 0xc0037757a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:26:14.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:26:14.305: INFO: rc: 1
Sep 18 20:26:14.305: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002308030 exit status 1 <nil> <nil> true [0xc00385a1b8 0xc00385a1d0 0xc00385a1e8] [0xc00385a1b8 0xc00385a1d0 0xc00385a1e8] [0xc00385a1c8 0xc00385a1e0] [0x10ef310 0x10ef310] 0xc003775b00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:26:24.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:26:24.414: INFO: rc: 1
Sep 18 20:26:24.414: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc005f14ba0 exit status 1 <nil> <nil> true [0xc000235068 0xc000235148 0xc0002351e8] [0xc000235068 0xc000235148 0xc0002351e8] [0xc0002350f0 0xc0002351d8] [0x10ef310 0x10ef310] 0xc00507cba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:26:34.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:26:34.513: INFO: rc: 1
Sep 18 20:26:34.514: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0023083c0 exit status 1 <nil> <nil> true [0xc00385a1f0 0xc00385a208 0xc00385a220] [0xc00385a1f0 0xc00385a208 0xc00385a220] [0xc00385a200 0xc00385a218] [0x10ef310 0x10ef310] 0xc003775e60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:26:44.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:26:44.624: INFO: rc: 1
Sep 18 20:26:44.624: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00462c3c0 exit status 1 <nil> <nil> true [0xc00385a018 0xc00385a030 0xc00385a048] [0xc00385a018 0xc00385a030 0xc00385a048] [0xc00385a028 0xc00385a040] [0x10ef310 0x10ef310] 0xc0037742a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:26:54.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:26:54.723: INFO: rc: 1
Sep 18 20:26:54.723: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002308390 exit status 1 <nil> <nil> true [0xc000234080 0xc0002346c0 0xc000234860] [0xc000234080 0xc0002346c0 0xc000234860] [0xc000234480 0xc0002347e0] [0x10ef310 0x10ef310] 0xc00507c360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:27:04.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:27:04.813: INFO: rc: 1
Sep 18 20:27:04.814: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0023087b0 exit status 1 <nil> <nil> true [0xc000234880 0xc000234a40 0xc000234e70] [0xc000234880 0xc000234a40 0xc000234e70] [0xc000234910 0xc000234d18] [0x10ef310 0x10ef310] 0xc00507c720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:27:14.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:27:14.913: INFO: rc: 1
Sep 18 20:27:14.913: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00462c7e0 exit status 1 <nil> <nil> true [0xc00385a058 0xc00385a088 0xc00385a0a0] [0xc00385a058 0xc00385a088 0xc00385a0a0] [0xc00385a078 0xc00385a098] [0x10ef310 0x10ef310] 0xc003774660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:27:24.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:27:25.003: INFO: rc: 1
Sep 18 20:27:25.003: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00462cc00 exit status 1 <nil> <nil> true [0xc00385a0a8 0xc00385a0c0 0xc00385a0d8] [0xc00385a0a8 0xc00385a0c0 0xc00385a0d8] [0xc00385a0b8 0xc00385a0d0] [0x10ef310 0x10ef310] 0xc0037749c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:27:35.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:27:35.102: INFO: rc: 1
Sep 18 20:27:35.103: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002308c00 exit status 1 <nil> <nil> true [0xc000234eb0 0xc0002350f0 0xc0002351d8] [0xc000234eb0 0xc0002350f0 0xc0002351d8] [0xc0002350a0 0xc0002351c0] [0x10ef310 0x10ef310] 0xc00507cba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:27:45.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:27:45.189: INFO: rc: 1
Sep 18 20:27:45.190: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002308fc0 exit status 1 <nil> <nil> true [0xc0002351e8 0xc0002352b0 0xc000235350] [0xc0002351e8 0xc0002352b0 0xc000235350] [0xc0002352a8 0xc000235308] [0x10ef310 0x10ef310] 0xc00507cf00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:27:55.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:27:55.277: INFO: rc: 1
Sep 18 20:27:55.278: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002309350 exit status 1 <nil> <nil> true [0xc000235378 0xc000235400 0xc0002354b0] [0xc000235378 0xc000235400 0xc0002354b0] [0xc0002353c8 0xc000235490] [0x10ef310 0x10ef310] 0xc00507d3e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:28:05.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:28:05.364: INFO: rc: 1
Sep 18 20:28:05.364: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002309740 exit status 1 <nil> <nil> true [0xc000235578 0xc0002355d8 0xc000235610] [0xc000235578 0xc0002355d8 0xc000235610] [0xc0002355c8 0xc0002355f8] [0x10ef310 0x10ef310] 0xc00507d740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:28:15.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:28:15.459: INFO: rc: 1
Sep 18 20:28:15.459: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00462d050 exit status 1 <nil> <nil> true [0xc00385a0e0 0xc00385a0f8 0xc00385a110] [0xc00385a0e0 0xc00385a0f8 0xc00385a110] [0xc00385a0f0 0xc00385a108] [0x10ef310 0x10ef310] 0xc003774d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:28:25.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:28:25.568: INFO: rc: 1
Sep 18 20:28:25.568: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002309c50 exit status 1 <nil> <nil> true [0xc000235650 0xc0002356d0 0xc000235718] [0xc000235650 0xc0002356d0 0xc000235718] [0xc0002356b8 0xc000235708] [0x10ef310 0x10ef310] 0xc00507dc80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:28:35.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:28:35.667: INFO: rc: 1
Sep 18 20:28:35.667: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00462d620 exit status 1 <nil> <nil> true [0xc00385a118 0xc00385a130 0xc00385a148] [0xc00385a118 0xc00385a130 0xc00385a148] [0xc00385a128 0xc00385a140] [0x10ef310 0x10ef310] 0xc003775080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:28:45.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:28:45.771: INFO: rc: 1
Sep 18 20:28:45.771: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00462da40 exit status 1 <nil> <nil> true [0xc00385a158 0xc00385a170 0xc00385a188] [0xc00385a158 0xc00385a170 0xc00385a188] [0xc00385a168 0xc00385a180] [0x10ef310 0x10ef310] 0xc0037753e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:28:55.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:28:55.859: INFO: rc: 1
Sep 18 20:28:55.859: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00462c390 exit status 1 <nil> <nil> true [0xc000234080 0xc0002346c0 0xc000234860] [0xc000234080 0xc0002346c0 0xc000234860] [0xc000234480 0xc0002347e0] [0x10ef310 0x10ef310] 0xc00507c360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:29:05.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:29:05.949: INFO: rc: 1
Sep 18 20:29:05.950: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00462c780 exit status 1 <nil> <nil> true [0xc000234880 0xc000234a40 0xc000234e70] [0xc000234880 0xc000234a40 0xc000234e70] [0xc000234910 0xc000234d18] [0x10ef310 0x10ef310] 0xc00507c720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:29:15.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:29:16.036: INFO: rc: 1
Sep 18 20:29:16.036: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00462cb70 exit status 1 <nil> <nil> true [0xc000234eb0 0xc0002350f0 0xc0002351d8] [0xc000234eb0 0xc0002350f0 0xc0002351d8] [0xc0002350a0 0xc0002351c0] [0x10ef310 0x10ef310] 0xc00507cba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 18 20:29:26.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-233 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 20:29:26.130: INFO: rc: 1
Sep 18 20:29:26.130: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Sep 18 20:29:26.131: INFO: Scaling statefulset ss to 0
Sep 18 20:29:26.139: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Sep 18 20:29:26.142: INFO: Deleting all statefulset in ns statefulset-233
Sep 18 20:29:26.145: INFO: Scaling statefulset ss to 0
Sep 18 20:29:26.153: INFO: Waiting for statefulset status.replicas updated to 0
Sep 18 20:29:26.155: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:29:26.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-233" for this suite.
Sep 18 20:29:32.198: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:29:32.290: INFO: namespace statefulset-233 deletion completed in 6.103976612s

â€¢ [SLOW TEST:361.606 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:29:32.291: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-196
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 20:29:32.452: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 18 20:29:35.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 --namespace=crd-publish-openapi-196 create -f -'
Sep 18 20:29:37.156: INFO: stderr: ""
Sep 18 20:29:37.156: INFO: stdout: "e2e-test-crd-publish-openapi-9627-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 18 20:29:37.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 --namespace=crd-publish-openapi-196 delete e2e-test-crd-publish-openapi-9627-crds test-cr'
Sep 18 20:29:37.249: INFO: stderr: ""
Sep 18 20:29:37.249: INFO: stdout: "e2e-test-crd-publish-openapi-9627-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep 18 20:29:37.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 --namespace=crd-publish-openapi-196 apply -f -'
Sep 18 20:29:37.817: INFO: stderr: ""
Sep 18 20:29:37.817: INFO: stdout: "e2e-test-crd-publish-openapi-9627-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 18 20:29:37.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 --namespace=crd-publish-openapi-196 delete e2e-test-crd-publish-openapi-9627-crds test-cr'
Sep 18 20:29:37.910: INFO: stderr: ""
Sep 18 20:29:37.910: INFO: stdout: "e2e-test-crd-publish-openapi-9627-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Sep 18 20:29:37.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 explain e2e-test-crd-publish-openapi-9627-crds'
Sep 18 20:29:38.158: INFO: stderr: ""
Sep 18 20:29:38.158: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9627-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:29:41.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-196" for this suite.
Sep 18 20:29:47.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:29:47.710: INFO: namespace crd-publish-openapi-196 deletion completed in 6.093494107s

â€¢ [SLOW TEST:15.419 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:29:47.710: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6491
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 20:29:47.916: INFO: (0) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 35.939495ms)
Sep 18 20:29:47.922: INFO: (1) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 5.795767ms)
Sep 18 20:29:47.926: INFO: (2) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.178877ms)
Sep 18 20:29:47.931: INFO: (3) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.428775ms)
Sep 18 20:29:47.935: INFO: (4) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.789773ms)
Sep 18 20:29:47.940: INFO: (5) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.684074ms)
Sep 18 20:29:47.949: INFO: (6) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 8.884449ms)
Sep 18 20:29:47.954: INFO: (7) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.596773ms)
Sep 18 20:29:47.958: INFO: (8) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.100977ms)
Sep 18 20:29:47.962: INFO: (9) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.141677ms)
Sep 18 20:29:47.967: INFO: (10) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.278176ms)
Sep 18 20:29:47.971: INFO: (11) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.422675ms)
Sep 18 20:29:47.976: INFO: (12) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.291875ms)
Sep 18 20:29:47.980: INFO: (13) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.305475ms)
Sep 18 20:29:47.985: INFO: (14) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.272976ms)
Sep 18 20:29:47.989: INFO: (15) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.069976ms)
Sep 18 20:29:47.993: INFO: (16) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.191476ms)
Sep 18 20:29:47.997: INFO: (17) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.998377ms)
Sep 18 20:29:48.001: INFO: (18) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.127876ms)
Sep 18 20:29:48.005: INFO: (19) /api/v1/nodes/k8s-agentpool1-40209065-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.098677ms)
[AfterEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:29:48.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6491" for this suite.
Sep 18 20:29:54.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:29:54.097: INFO: namespace proxy-6491 deletion completed in 6.087896552s

â€¢ [SLOW TEST:6.387 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:29:54.098: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1075
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-4d4fd896-c218-4097-9ae9-141fab15c173
STEP: Creating a pod to test consume configMaps
Sep 18 20:29:54.271: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7554345c-f2ec-487e-b0b4-d7389b4d5a78" in namespace "projected-1075" to be "success or failure"
Sep 18 20:29:54.283: INFO: Pod "pod-projected-configmaps-7554345c-f2ec-487e-b0b4-d7389b4d5a78": Phase="Pending", Reason="", readiness=false. Elapsed: 11.841632ms
Sep 18 20:29:56.287: INFO: Pod "pod-projected-configmaps-7554345c-f2ec-487e-b0b4-d7389b4d5a78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016084129s
Sep 18 20:29:58.291: INFO: Pod "pod-projected-configmaps-7554345c-f2ec-487e-b0b4-d7389b4d5a78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020067428s
STEP: Saw pod success
Sep 18 20:29:58.291: INFO: Pod "pod-projected-configmaps-7554345c-f2ec-487e-b0b4-d7389b4d5a78" satisfied condition "success or failure"
Sep 18 20:29:58.293: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-projected-configmaps-7554345c-f2ec-487e-b0b4-d7389b4d5a78 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 18 20:29:58.359: INFO: Waiting for pod pod-projected-configmaps-7554345c-f2ec-487e-b0b4-d7389b4d5a78 to disappear
Sep 18 20:29:58.363: INFO: Pod pod-projected-configmaps-7554345c-f2ec-487e-b0b4-d7389b4d5a78 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:29:58.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1075" for this suite.
Sep 18 20:30:04.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:30:04.479: INFO: namespace projected-1075 deletion completed in 6.110254947s

â€¢ [SLOW TEST:10.381 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:30:04.481: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1387
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-aa1d1686-b45f-459c-8963-47c165b348e2 in namespace container-probe-1387
Sep 18 20:30:08.663: INFO: Started pod busybox-aa1d1686-b45f-459c-8963-47c165b348e2 in namespace container-probe-1387
STEP: checking the pod's current state and verifying that restartCount is present
Sep 18 20:30:08.666: INFO: Initial restart count of pod busybox-aa1d1686-b45f-459c-8963-47c165b348e2 is 0
Sep 18 20:31:00.777: INFO: Restart count of pod container-probe-1387/busybox-aa1d1686-b45f-459c-8963-47c165b348e2 is now 1 (52.110718202s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:31:00.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1387" for this suite.
Sep 18 20:31:06.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:31:06.898: INFO: namespace container-probe-1387 deletion completed in 6.105017803s

â€¢ [SLOW TEST:62.417 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:31:06.898: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5097
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 18 20:31:07.065: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6f0d566f-6e40-4634-a3cd-1443ad541e51" in namespace "projected-5097" to be "success or failure"
Sep 18 20:31:07.075: INFO: Pod "downwardapi-volume-6f0d566f-6e40-4634-a3cd-1443ad541e51": Phase="Pending", Reason="", readiness=false. Elapsed: 10.119143ms
Sep 18 20:31:09.083: INFO: Pod "downwardapi-volume-6f0d566f-6e40-4634-a3cd-1443ad541e51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01745847s
Sep 18 20:31:11.087: INFO: Pod "downwardapi-volume-6f0d566f-6e40-4634-a3cd-1443ad541e51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021402918s
STEP: Saw pod success
Sep 18 20:31:11.087: INFO: Pod "downwardapi-volume-6f0d566f-6e40-4634-a3cd-1443ad541e51" satisfied condition "success or failure"
Sep 18 20:31:11.090: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod downwardapi-volume-6f0d566f-6e40-4634-a3cd-1443ad541e51 container client-container: <nil>
STEP: delete the pod
Sep 18 20:31:11.120: INFO: Waiting for pod downwardapi-volume-6f0d566f-6e40-4634-a3cd-1443ad541e51 to disappear
Sep 18 20:31:11.122: INFO: Pod downwardapi-volume-6f0d566f-6e40-4634-a3cd-1443ad541e51 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:31:11.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5097" for this suite.
Sep 18 20:31:17.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:31:17.217: INFO: namespace projected-5097 deletion completed in 6.090389006s

â€¢ [SLOW TEST:10.319 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:31:17.217: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6310
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 18 20:31:21.916: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b8fa524e-50a7-4a6d-bf2b-ea11384b9b66"
Sep 18 20:31:21.916: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b8fa524e-50a7-4a6d-bf2b-ea11384b9b66" in namespace "pods-6310" to be "terminated due to deadline exceeded"
Sep 18 20:31:21.926: INFO: Pod "pod-update-activedeadlineseconds-b8fa524e-50a7-4a6d-bf2b-ea11384b9b66": Phase="Running", Reason="", readiness=true. Elapsed: 9.650645ms
Sep 18 20:31:23.930: INFO: Pod "pod-update-activedeadlineseconds-b8fa524e-50a7-4a6d-bf2b-ea11384b9b66": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.014361096s
Sep 18 20:31:23.931: INFO: Pod "pod-update-activedeadlineseconds-b8fa524e-50a7-4a6d-bf2b-ea11384b9b66" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:31:23.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6310" for this suite.
Sep 18 20:31:29.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:31:30.044: INFO: namespace pods-6310 deletion completed in 6.108742725s

â€¢ [SLOW TEST:12.827 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:31:30.045: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1609
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:31:40.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1609" for this suite.
Sep 18 20:31:46.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:31:46.319: INFO: namespace job-1609 deletion completed in 6.094805433s

â€¢ [SLOW TEST:16.274 seconds]
[sig-apps] Job
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:31:46.321: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8504
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 20:31:46.490: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep 18 20:31:51.495: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 18 20:31:51.495: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Sep 18 20:31:51.518: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8504 /apis/apps/v1/namespaces/deployment-8504/deployments/test-cleanup-deployment 8548b90f-9d4d-4ab3-a8a8-90c9f08acb24 13084 1 2019-09-18 20:31:51 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00053cec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Sep 18 20:31:51.532: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-8504 /apis/apps/v1/namespaces/deployment-8504/replicasets/test-cleanup-deployment-65db99849b 68b4deaa-aef9-4001-9ef4-65e469796484 13086 1 2019-09-18 20:31:51 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 8548b90f-9d4d-4ab3-a8a8-90c9f08acb24 0xc000db3937 0xc000db3938}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000db3a28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 18 20:31:51.532: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep 18 20:31:51.532: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8504 /apis/apps/v1/namespaces/deployment-8504/replicasets/test-cleanup-controller 30fa4379-5098-4b2c-a4ac-3d00d01d978c 13085 1 2019-09-18 20:31:46 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 8548b90f-9d4d-4ab3-a8a8-90c9f08acb24 0xc000db3647 0xc000db3648}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000db3788 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 18 20:31:51.543: INFO: Pod "test-cleanup-controller-x2kxp" is available:
&Pod{ObjectMeta:{test-cleanup-controller-x2kxp test-cleanup-controller- deployment-8504 /api/v1/namespaces/deployment-8504/pods/test-cleanup-controller-x2kxp 5133edb6-c7ce-413b-a1b4-ef1f664bfcee 13080 0 2019-09-18 20:31:46 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-controller 30fa4379-5098-4b2c-a4ac-3d00d01d978c 0xc000e5a3b7 0xc000e5a3b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-x5mgl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-x5mgl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-x5mgl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 20:31:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 20:31:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 20:31:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 20:31:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.35,PodIP:10.240.0.37,StartTime:2019-09-18 20:31:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-18 20:31:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://433a7500a7b7bbcc12b9fddc605f793b2b02d4290de37efcb6462765152fdc06,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.37,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 20:31:51.544: INFO: Pod "test-cleanup-deployment-65db99849b-bxl5w" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-65db99849b-bxl5w test-cleanup-deployment-65db99849b- deployment-8504 /api/v1/namespaces/deployment-8504/pods/test-cleanup-deployment-65db99849b-bxl5w ed1c709b-fc0a-421d-9b98-546bb4dc4bd3 13092 0 2019-09-18 20:31:51 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-65db99849b 68b4deaa-aef9-4001-9ef4-65e469796484 0xc000e5a5d7 0xc000e5a5d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-x5mgl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-x5mgl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-x5mgl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 20:31:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:31:51.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8504" for this suite.
Sep 18 20:31:57.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:31:57.671: INFO: namespace deployment-8504 deletion completed in 6.121539302s

â€¢ [SLOW TEST:11.350 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:31:57.671: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3716
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-3716
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3716 to expose endpoints map[]
Sep 18 20:31:57.864: INFO: Get endpoints failed (15.408413ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Sep 18 20:31:58.959: INFO: successfully validated that service multi-endpoint-test in namespace services-3716 exposes endpoints map[] (1.110042128s elapsed)
STEP: Creating pod pod1 in namespace services-3716
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3716 to expose endpoints map[pod1:[100]]
Sep 18 20:32:02.006: INFO: successfully validated that service multi-endpoint-test in namespace services-3716 exposes endpoints map[pod1:[100]] (3.038983429s elapsed)
STEP: Creating pod pod2 in namespace services-3716
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3716 to expose endpoints map[pod1:[100] pod2:[101]]
Sep 18 20:32:05.085: INFO: successfully validated that service multi-endpoint-test in namespace services-3716 exposes endpoints map[pod1:[100] pod2:[101]] (3.074682029s elapsed)
STEP: Deleting pod pod1 in namespace services-3716
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3716 to expose endpoints map[pod2:[101]]
Sep 18 20:32:06.124: INFO: successfully validated that service multi-endpoint-test in namespace services-3716 exposes endpoints map[pod2:[101]] (1.030176681s elapsed)
STEP: Deleting pod pod2 in namespace services-3716
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3716 to expose endpoints map[]
Sep 18 20:32:06.143: INFO: successfully validated that service multi-endpoint-test in namespace services-3716 exposes endpoints map[] (12.453329ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:32:06.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3716" for this suite.
Sep 18 20:32:18.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:32:18.332: INFO: namespace services-3716 deletion completed in 12.1301863s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:20.661 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:32:18.333: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2185
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 18 20:32:18.986: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 18 20:32:20.994: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704435538, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704435538, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704435539, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704435538, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 18 20:32:23.002: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704435538, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704435538, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704435539, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704435538, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 18 20:32:26.015: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 20:32:26.019: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2406-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:32:27.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2185" for this suite.
Sep 18 20:32:33.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:32:33.426: INFO: namespace webhook-2185 deletion completed in 6.122469355s
STEP: Destroying namespace "webhook-2185-markers" for this suite.
Sep 18 20:32:39.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:32:39.532: INFO: namespace webhook-2185-markers deletion completed in 6.105867759s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:21.214 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:32:39.547: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1402
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Sep 18 20:32:39.709: INFO: Waiting up to 5m0s for pod "var-expansion-db5e3fa1-2466-4637-ba1a-c3593f07782c" in namespace "var-expansion-1402" to be "success or failure"
Sep 18 20:32:39.719: INFO: Pod "var-expansion-db5e3fa1-2466-4637-ba1a-c3593f07782c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.382942ms
Sep 18 20:32:41.723: INFO: Pod "var-expansion-db5e3fa1-2466-4637-ba1a-c3593f07782c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014475139s
Sep 18 20:32:43.727: INFO: Pod "var-expansion-db5e3fa1-2466-4637-ba1a-c3593f07782c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018589638s
STEP: Saw pod success
Sep 18 20:32:43.727: INFO: Pod "var-expansion-db5e3fa1-2466-4637-ba1a-c3593f07782c" satisfied condition "success or failure"
Sep 18 20:32:43.731: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod var-expansion-db5e3fa1-2466-4637-ba1a-c3593f07782c container dapi-container: <nil>
STEP: delete the pod
Sep 18 20:32:43.782: INFO: Waiting for pod var-expansion-db5e3fa1-2466-4637-ba1a-c3593f07782c to disappear
Sep 18 20:32:43.788: INFO: Pod var-expansion-db5e3fa1-2466-4637-ba1a-c3593f07782c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:32:43.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1402" for this suite.
Sep 18 20:32:49.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:32:49.886: INFO: namespace var-expansion-1402 deletion completed in 6.093221547s

â€¢ [SLOW TEST:10.339 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:32:49.886: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9730
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-340af4dd-9259-44fe-aec5-bf2672c04e69
STEP: Creating secret with name s-test-opt-upd-18656c6d-5ca7-4490-a879-40aa551dfd28
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-340af4dd-9259-44fe-aec5-bf2672c04e69
STEP: Updating secret s-test-opt-upd-18656c6d-5ca7-4490-a879-40aa551dfd28
STEP: Creating secret with name s-test-opt-create-1ceac2e8-ddec-4b02-b242-a98450351a7c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:32:58.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9730" for this suite.
Sep 18 20:33:10.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:33:10.252: INFO: namespace secrets-9730 deletion completed in 12.094488264s

â€¢ [SLOW TEST:20.367 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:33:10.253: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6630
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6340
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5813
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:33:41.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6630" for this suite.
Sep 18 20:33:47.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:33:47.904: INFO: namespace namespaces-6630 deletion completed in 6.091240042s
STEP: Destroying namespace "nsdeletetest-6340" for this suite.
Sep 18 20:33:47.907: INFO: Namespace nsdeletetest-6340 was already deleted
STEP: Destroying namespace "nsdeletetest-5813" for this suite.
Sep 18 20:33:53.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:33:54.004: INFO: namespace nsdeletetest-5813 deletion completed in 6.097307217s

â€¢ [SLOW TEST:43.752 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:33:54.006: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9061
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:34:01.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9061" for this suite.
Sep 18 20:34:07.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:34:07.329: INFO: namespace resourcequota-9061 deletion completed in 6.123001891s

â€¢ [SLOW TEST:13.324 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:34:07.332: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3815
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 18 20:34:07.502: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6c62fdc0-3784-4fbc-80d0-d1eb6488d61a" in namespace "projected-3815" to be "success or failure"
Sep 18 20:34:07.516: INFO: Pod "downwardapi-volume-6c62fdc0-3784-4fbc-80d0-d1eb6488d61a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.165126ms
Sep 18 20:34:09.519: INFO: Pod "downwardapi-volume-6c62fdc0-3784-4fbc-80d0-d1eb6488d61a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016967967s
Sep 18 20:34:11.523: INFO: Pod "downwardapi-volume-6c62fdc0-3784-4fbc-80d0-d1eb6488d61a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020795809s
STEP: Saw pod success
Sep 18 20:34:11.523: INFO: Pod "downwardapi-volume-6c62fdc0-3784-4fbc-80d0-d1eb6488d61a" satisfied condition "success or failure"
Sep 18 20:34:11.526: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod downwardapi-volume-6c62fdc0-3784-4fbc-80d0-d1eb6488d61a container client-container: <nil>
STEP: delete the pod
Sep 18 20:34:11.553: INFO: Waiting for pod downwardapi-volume-6c62fdc0-3784-4fbc-80d0-d1eb6488d61a to disappear
Sep 18 20:34:11.556: INFO: Pod downwardapi-volume-6c62fdc0-3784-4fbc-80d0-d1eb6488d61a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:34:11.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3815" for this suite.
Sep 18 20:34:17.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:34:17.661: INFO: namespace projected-3815 deletion completed in 6.101318626s

â€¢ [SLOW TEST:10.330 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:34:17.662: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4210
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 18 20:34:17.836: INFO: Waiting up to 5m0s for pod "pod-b1671621-ddc0-44f8-ad01-961d7254eaae" in namespace "emptydir-4210" to be "success or failure"
Sep 18 20:34:17.840: INFO: Pod "pod-b1671621-ddc0-44f8-ad01-961d7254eaae": Phase="Pending", Reason="", readiness=false. Elapsed: 3.775579ms
Sep 18 20:34:19.844: INFO: Pod "pod-b1671621-ddc0-44f8-ad01-961d7254eaae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007377826s
Sep 18 20:34:21.848: INFO: Pod "pod-b1671621-ddc0-44f8-ad01-961d7254eaae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011351871s
STEP: Saw pod success
Sep 18 20:34:21.848: INFO: Pod "pod-b1671621-ddc0-44f8-ad01-961d7254eaae" satisfied condition "success or failure"
Sep 18 20:34:21.850: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod pod-b1671621-ddc0-44f8-ad01-961d7254eaae container test-container: <nil>
STEP: delete the pod
Sep 18 20:34:21.877: INFO: Waiting for pod pod-b1671621-ddc0-44f8-ad01-961d7254eaae to disappear
Sep 18 20:34:21.879: INFO: Pod pod-b1671621-ddc0-44f8-ad01-961d7254eaae no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:34:21.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4210" for this suite.
Sep 18 20:34:27.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:34:28.021: INFO: namespace emptydir-4210 deletion completed in 6.136485042s

â€¢ [SLOW TEST:10.359 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:34:28.021: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1728
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 20:34:52.239: INFO: Container started at 2019-09-18 20:34:30 +0000 UTC, pod became ready at 2019-09-18 20:34:51 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:34:52.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1728" for this suite.
Sep 18 20:35:20.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:35:20.346: INFO: namespace container-probe-1728 deletion completed in 28.101572436s

â€¢ [SLOW TEST:52.325 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:35:20.348: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6738
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 18 20:35:21.032: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 18 20:35:23.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704435721, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704435721, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704435721, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704435721, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 18 20:35:26.076: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:35:26.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6738" for this suite.
Sep 18 20:35:32.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:35:32.475: INFO: namespace webhook-6738 deletion completed in 6.101343817s
STEP: Destroying namespace "webhook-6738-markers" for this suite.
Sep 18 20:35:38.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:35:38.566: INFO: namespace webhook-6738-markers deletion completed in 6.091106581s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:18.231 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:35:38.579: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7437
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 20:35:42.795: INFO: Waiting up to 5m0s for pod "client-envvars-35351e4d-6471-4d03-8f89-b3f8ec7769ea" in namespace "pods-7437" to be "success or failure"
Sep 18 20:35:42.805: INFO: Pod "client-envvars-35351e4d-6471-4d03-8f89-b3f8ec7769ea": Phase="Pending", Reason="", readiness=false. Elapsed: 10.839439ms
Sep 18 20:35:44.814: INFO: Pod "client-envvars-35351e4d-6471-4d03-8f89-b3f8ec7769ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019260892s
Sep 18 20:35:46.818: INFO: Pod "client-envvars-35351e4d-6471-4d03-8f89-b3f8ec7769ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023743668s
STEP: Saw pod success
Sep 18 20:35:46.818: INFO: Pod "client-envvars-35351e4d-6471-4d03-8f89-b3f8ec7769ea" satisfied condition "success or failure"
Sep 18 20:35:46.821: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod client-envvars-35351e4d-6471-4d03-8f89-b3f8ec7769ea container env3cont: <nil>
STEP: delete the pod
Sep 18 20:35:46.842: INFO: Waiting for pod client-envvars-35351e4d-6471-4d03-8f89-b3f8ec7769ea to disappear
Sep 18 20:35:46.857: INFO: Pod client-envvars-35351e4d-6471-4d03-8f89-b3f8ec7769ea no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:35:46.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7437" for this suite.
Sep 18 20:35:58.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:35:58.954: INFO: namespace pods-7437 deletion completed in 12.091953905s

â€¢ [SLOW TEST:20.375 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:35:58.956: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-443
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Sep 18 20:36:05.149: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:36:05.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0918 20:36:05.149492      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-443" for this suite.
Sep 18 20:36:11.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:36:11.251: INFO: namespace gc-443 deletion completed in 6.099048572s

â€¢ [SLOW TEST:12.296 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:36:11.252: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-2790
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:36:11.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-2790" for this suite.
Sep 18 20:36:17.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:36:17.627: INFO: namespace tables-2790 deletion completed in 6.124869434s

â€¢ [SLOW TEST:6.375 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:36:17.628: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4987
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Sep 18 20:36:17.796: INFO: Waiting up to 5m0s for pod "downward-api-222b9a94-5817-49c1-9d39-edfbd27de680" in namespace "downward-api-4987" to be "success or failure"
Sep 18 20:36:17.806: INFO: Pod "downward-api-222b9a94-5817-49c1-9d39-edfbd27de680": Phase="Pending", Reason="", readiness=false. Elapsed: 10.276143ms
Sep 18 20:36:19.810: INFO: Pod "downward-api-222b9a94-5817-49c1-9d39-edfbd27de680": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014127534s
Sep 18 20:36:21.814: INFO: Pod "downward-api-222b9a94-5817-49c1-9d39-edfbd27de680": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018218324s
STEP: Saw pod success
Sep 18 20:36:21.814: INFO: Pod "downward-api-222b9a94-5817-49c1-9d39-edfbd27de680" satisfied condition "success or failure"
Sep 18 20:36:21.818: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod downward-api-222b9a94-5817-49c1-9d39-edfbd27de680 container dapi-container: <nil>
STEP: delete the pod
Sep 18 20:36:21.865: INFO: Waiting for pod downward-api-222b9a94-5817-49c1-9d39-edfbd27de680 to disappear
Sep 18 20:36:21.868: INFO: Pod downward-api-222b9a94-5817-49c1-9d39-edfbd27de680 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:36:21.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4987" for this suite.
Sep 18 20:36:27.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:36:28.002: INFO: namespace downward-api-4987 deletion completed in 6.128635823s

â€¢ [SLOW TEST:10.374 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:36:28.004: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3117
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Sep 18 20:36:28.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 api-versions'
Sep 18 20:36:28.374: INFO: stderr: ""
Sep 18 20:36:28.374: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:36:28.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3117" for this suite.
Sep 18 20:36:34.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:36:34.485: INFO: namespace kubectl-3117 deletion completed in 6.104242766s

â€¢ [SLOW TEST:6.482 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:36:34.487: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1516
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 18 20:36:37.683: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:36:37.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1516" for this suite.
Sep 18 20:36:43.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:36:43.808: INFO: namespace container-runtime-1516 deletion completed in 6.101048693s

â€¢ [SLOW TEST:9.321 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:36:43.809: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1477
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:36:48.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1477" for this suite.
Sep 18 20:36:54.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:36:54.902: INFO: namespace watch-1477 deletion completed in 6.17703848s

â€¢ [SLOW TEST:11.093 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:36:54.902: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5049
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:36:59.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5049" for this suite.
Sep 18 20:37:45.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:37:45.212: INFO: namespace kubelet-test-5049 deletion completed in 46.099283559s

â€¢ [SLOW TEST:50.311 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:37:45.213: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2172
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 18 20:37:45.814: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 18 20:37:47.823: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704435865, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704435865, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704435865, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704435865, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 18 20:37:50.844: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:37:50.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2172" for this suite.
Sep 18 20:37:56.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:37:57.015: INFO: namespace webhook-2172 deletion completed in 6.098073604s
STEP: Destroying namespace "webhook-2172-markers" for this suite.
Sep 18 20:38:03.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:38:03.109: INFO: namespace webhook-2172-markers deletion completed in 6.094232496s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:17.908 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:38:03.121: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-6052
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 20:38:03.286: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Creating first CR 
Sep 18 20:38:03.421: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-09-18T20:38:03Z generation:1 name:name1 resourceVersion:14779 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:cff8b64c-c421-404c-adc3-44dec3cd4eae] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Sep 18 20:38:13.427: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-09-18T20:38:13Z generation:1 name:name2 resourceVersion:14795 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:24f5f64a-6345-43f6-b0f5-4153cf141315] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Sep 18 20:38:23.432: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-09-18T20:38:03Z generation:2 name:name1 resourceVersion:14811 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:cff8b64c-c421-404c-adc3-44dec3cd4eae] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Sep 18 20:38:33.437: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-09-18T20:38:13Z generation:2 name:name2 resourceVersion:14829 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:24f5f64a-6345-43f6-b0f5-4153cf141315] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Sep 18 20:38:43.445: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-09-18T20:38:03Z generation:2 name:name1 resourceVersion:14845 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:cff8b64c-c421-404c-adc3-44dec3cd4eae] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Sep 18 20:38:53.451: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-09-18T20:38:13Z generation:2 name:name2 resourceVersion:14863 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:24f5f64a-6345-43f6-b0f5-4153cf141315] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:39:03.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6052" for this suite.
Sep 18 20:39:09.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:39:10.085: INFO: namespace crd-watch-6052 deletion completed in 6.112610731s

â€¢ [SLOW TEST:66.964 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:39:10.092: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1002
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-9990
STEP: Creating secret with name secret-test-17bc2d23-a7f9-40a8-a61f-b714a3a41f4b
STEP: Creating a pod to test consume secrets
Sep 18 20:39:10.424: INFO: Waiting up to 5m0s for pod "pod-secrets-d2cd945a-52c3-451d-bab2-c5b64c53248a" in namespace "secrets-1002" to be "success or failure"
Sep 18 20:39:10.428: INFO: Pod "pod-secrets-d2cd945a-52c3-451d-bab2-c5b64c53248a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.645045ms
Sep 18 20:39:12.432: INFO: Pod "pod-secrets-d2cd945a-52c3-451d-bab2-c5b64c53248a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008332877s
Sep 18 20:39:14.436: INFO: Pod "pod-secrets-d2cd945a-52c3-451d-bab2-c5b64c53248a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012509644s
STEP: Saw pod success
Sep 18 20:39:14.436: INFO: Pod "pod-secrets-d2cd945a-52c3-451d-bab2-c5b64c53248a" satisfied condition "success or failure"
Sep 18 20:39:14.439: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod pod-secrets-d2cd945a-52c3-451d-bab2-c5b64c53248a container secret-volume-test: <nil>
STEP: delete the pod
Sep 18 20:39:14.498: INFO: Waiting for pod pod-secrets-d2cd945a-52c3-451d-bab2-c5b64c53248a to disappear
Sep 18 20:39:14.501: INFO: Pod pod-secrets-d2cd945a-52c3-451d-bab2-c5b64c53248a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:39:14.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1002" for this suite.
Sep 18 20:39:20.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:39:20.603: INFO: namespace secrets-1002 deletion completed in 6.097302073s
STEP: Destroying namespace "secret-namespace-9990" for this suite.
Sep 18 20:39:26.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:39:26.692: INFO: namespace secret-namespace-9990 deletion completed in 6.089582939s

â€¢ [SLOW TEST:16.602 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:39:26.693: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7334
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Sep 18 20:39:30.903: INFO: Pod pod-hostip-e9d57877-b175-4868-a23a-b3806d74fa1a has hostIP: 10.240.0.35
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:39:30.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7334" for this suite.
Sep 18 20:39:58.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:39:59.007: INFO: namespace pods-7334 deletion completed in 28.099383606s

â€¢ [SLOW TEST:32.315 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:39:59.008: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8975
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 20:39:59.188: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep 18 20:39:59.197: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:39:59.203: INFO: Number of nodes with available pods: 0
Sep 18 20:39:59.203: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 20:40:00.209: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:00.213: INFO: Number of nodes with available pods: 0
Sep 18 20:40:00.213: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 20:40:01.212: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:01.218: INFO: Number of nodes with available pods: 0
Sep 18 20:40:01.218: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 20:40:02.209: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:02.212: INFO: Number of nodes with available pods: 2
Sep 18 20:40:02.212: INFO: Node k8s-agentpool1-40209065-vmss000001 is running more than one daemon pod
Sep 18 20:40:03.209: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:03.212: INFO: Number of nodes with available pods: 3
Sep 18 20:40:03.212: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep 18 20:40:03.237: INFO: Wrong image for pod: daemon-set-4g8vb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:03.237: INFO: Wrong image for pod: daemon-set-8jjd2. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:03.237: INFO: Wrong image for pod: daemon-set-hfk57. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:03.252: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:04.257: INFO: Wrong image for pod: daemon-set-4g8vb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:04.257: INFO: Wrong image for pod: daemon-set-8jjd2. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:04.257: INFO: Wrong image for pod: daemon-set-hfk57. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:04.261: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:05.257: INFO: Wrong image for pod: daemon-set-4g8vb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:05.257: INFO: Wrong image for pod: daemon-set-8jjd2. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:05.257: INFO: Pod daemon-set-8jjd2 is not available
Sep 18 20:40:05.257: INFO: Wrong image for pod: daemon-set-hfk57. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:05.261: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:06.257: INFO: Wrong image for pod: daemon-set-4g8vb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:06.257: INFO: Wrong image for pod: daemon-set-8jjd2. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:06.257: INFO: Pod daemon-set-8jjd2 is not available
Sep 18 20:40:06.257: INFO: Wrong image for pod: daemon-set-hfk57. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:06.261: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:07.256: INFO: Wrong image for pod: daemon-set-4g8vb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:07.256: INFO: Wrong image for pod: daemon-set-8jjd2. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:07.256: INFO: Pod daemon-set-8jjd2 is not available
Sep 18 20:40:07.256: INFO: Wrong image for pod: daemon-set-hfk57. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:07.263: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:08.256: INFO: Wrong image for pod: daemon-set-4g8vb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:08.256: INFO: Wrong image for pod: daemon-set-8jjd2. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:08.256: INFO: Pod daemon-set-8jjd2 is not available
Sep 18 20:40:08.256: INFO: Wrong image for pod: daemon-set-hfk57. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:08.261: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:09.257: INFO: Wrong image for pod: daemon-set-4g8vb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:09.257: INFO: Wrong image for pod: daemon-set-8jjd2. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:09.257: INFO: Pod daemon-set-8jjd2 is not available
Sep 18 20:40:09.257: INFO: Wrong image for pod: daemon-set-hfk57. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:09.262: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:10.257: INFO: Wrong image for pod: daemon-set-4g8vb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:10.257: INFO: Pod daemon-set-fm8fw is not available
Sep 18 20:40:10.257: INFO: Wrong image for pod: daemon-set-hfk57. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:10.261: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:11.257: INFO: Wrong image for pod: daemon-set-4g8vb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:11.257: INFO: Pod daemon-set-fm8fw is not available
Sep 18 20:40:11.257: INFO: Wrong image for pod: daemon-set-hfk57. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:11.262: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:12.257: INFO: Wrong image for pod: daemon-set-4g8vb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:12.257: INFO: Pod daemon-set-fm8fw is not available
Sep 18 20:40:12.257: INFO: Wrong image for pod: daemon-set-hfk57. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:12.264: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:13.257: INFO: Wrong image for pod: daemon-set-4g8vb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:13.257: INFO: Wrong image for pod: daemon-set-hfk57. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:13.261: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:14.258: INFO: Wrong image for pod: daemon-set-4g8vb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:14.258: INFO: Wrong image for pod: daemon-set-hfk57. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:14.258: INFO: Pod daemon-set-hfk57 is not available
Sep 18 20:40:14.266: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:15.257: INFO: Wrong image for pod: daemon-set-4g8vb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:15.257: INFO: Pod daemon-set-vt5ms is not available
Sep 18 20:40:15.262: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:16.261: INFO: Wrong image for pod: daemon-set-4g8vb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:16.261: INFO: Pod daemon-set-vt5ms is not available
Sep 18 20:40:16.267: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:17.256: INFO: Wrong image for pod: daemon-set-4g8vb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:17.256: INFO: Pod daemon-set-vt5ms is not available
Sep 18 20:40:17.272: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:18.256: INFO: Wrong image for pod: daemon-set-4g8vb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:18.260: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:19.257: INFO: Wrong image for pod: daemon-set-4g8vb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:19.257: INFO: Pod daemon-set-4g8vb is not available
Sep 18 20:40:19.262: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:20.259: INFO: Wrong image for pod: daemon-set-4g8vb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:20.259: INFO: Pod daemon-set-4g8vb is not available
Sep 18 20:40:20.265: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:21.257: INFO: Wrong image for pod: daemon-set-4g8vb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 18 20:40:21.257: INFO: Pod daemon-set-4g8vb is not available
Sep 18 20:40:21.262: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:22.257: INFO: Pod daemon-set-jfvfn is not available
Sep 18 20:40:22.261: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Sep 18 20:40:22.265: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:22.268: INFO: Number of nodes with available pods: 2
Sep 18 20:40:22.268: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 20:40:23.273: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:23.277: INFO: Number of nodes with available pods: 2
Sep 18 20:40:23.277: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 20:40:24.274: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:24.278: INFO: Number of nodes with available pods: 2
Sep 18 20:40:24.278: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 20:40:25.273: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:25.277: INFO: Number of nodes with available pods: 2
Sep 18 20:40:25.277: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 20:40:26.274: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:26.277: INFO: Number of nodes with available pods: 2
Sep 18 20:40:26.277: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 20:40:27.273: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:27.275: INFO: Number of nodes with available pods: 2
Sep 18 20:40:27.275: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 20:40:28.273: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:40:28.276: INFO: Number of nodes with available pods: 3
Sep 18 20:40:28.276: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8975, will wait for the garbage collector to delete the pods
I0918 20:40:28.295573      14 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 20:40:28.295708      14 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 18 20:40:28.351: INFO: Deleting DaemonSet.extensions daemon-set took: 5.550739ms
I0918 20:40:28.651447      14 controller_utils.go:810] Ignoring inactive pod daemonsets-8975/daemon-set-fm8fw in state Running, deletion time 2019-09-18 20:40:58 +0000 UTC
I0918 20:40:28.651500      14 controller_utils.go:810] Ignoring inactive pod daemonsets-8975/daemon-set-vt5ms in state Running, deletion time 2019-09-18 20:40:58 +0000 UTC
I0918 20:40:28.651511      14 controller_utils.go:810] Ignoring inactive pod daemonsets-8975/daemon-set-jfvfn in state Running, deletion time 2019-09-18 20:40:58 +0000 UTC
Sep 18 20:40:28.651: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.362619ms
Sep 18 20:40:41.955: INFO: Number of nodes with available pods: 0
Sep 18 20:40:41.955: INFO: Number of running nodes: 0, number of available pods: 0
Sep 18 20:40:41.958: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8975/daemonsets","resourceVersion":"15234"},"items":null}

Sep 18 20:40:41.962: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8975/pods","resourceVersion":"15234"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:40:41.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8975" for this suite.
Sep 18 20:40:48.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:40:48.100: INFO: namespace daemonsets-8975 deletion completed in 6.117755667s

â€¢ [SLOW TEST:49.092 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:40:48.100: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9743
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9743.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9743.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9743.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9743.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9743.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9743.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9743.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9743.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9743.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9743.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9743.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9743.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9743.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 92.104.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.104.92_udp@PTR;check="$$(dig +tcp +noall +answer +search 92.104.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.104.92_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9743.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9743.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9743.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9743.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9743.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9743.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9743.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9743.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9743.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9743.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9743.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9743.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9743.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 92.104.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.104.92_udp@PTR;check="$$(dig +tcp +noall +answer +search 92.104.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.104.92_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 18 20:40:54.345: INFO: Unable to read wheezy_udp@dns-test-service.dns-9743.svc.cluster.local from pod dns-9743/dns-test-fa3497e0-aef8-4785-b440-6f3d26238211: the server could not find the requested resource (get pods dns-test-fa3497e0-aef8-4785-b440-6f3d26238211)
Sep 18 20:40:54.348: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9743.svc.cluster.local from pod dns-9743/dns-test-fa3497e0-aef8-4785-b440-6f3d26238211: the server could not find the requested resource (get pods dns-test-fa3497e0-aef8-4785-b440-6f3d26238211)
Sep 18 20:40:54.352: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9743.svc.cluster.local from pod dns-9743/dns-test-fa3497e0-aef8-4785-b440-6f3d26238211: the server could not find the requested resource (get pods dns-test-fa3497e0-aef8-4785-b440-6f3d26238211)
Sep 18 20:40:54.355: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9743.svc.cluster.local from pod dns-9743/dns-test-fa3497e0-aef8-4785-b440-6f3d26238211: the server could not find the requested resource (get pods dns-test-fa3497e0-aef8-4785-b440-6f3d26238211)
Sep 18 20:40:54.380: INFO: Unable to read jessie_udp@dns-test-service.dns-9743.svc.cluster.local from pod dns-9743/dns-test-fa3497e0-aef8-4785-b440-6f3d26238211: the server could not find the requested resource (get pods dns-test-fa3497e0-aef8-4785-b440-6f3d26238211)
Sep 18 20:40:54.384: INFO: Unable to read jessie_tcp@dns-test-service.dns-9743.svc.cluster.local from pod dns-9743/dns-test-fa3497e0-aef8-4785-b440-6f3d26238211: the server could not find the requested resource (get pods dns-test-fa3497e0-aef8-4785-b440-6f3d26238211)
Sep 18 20:40:54.394: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9743.svc.cluster.local from pod dns-9743/dns-test-fa3497e0-aef8-4785-b440-6f3d26238211: the server could not find the requested resource (get pods dns-test-fa3497e0-aef8-4785-b440-6f3d26238211)
Sep 18 20:40:54.398: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9743.svc.cluster.local from pod dns-9743/dns-test-fa3497e0-aef8-4785-b440-6f3d26238211: the server could not find the requested resource (get pods dns-test-fa3497e0-aef8-4785-b440-6f3d26238211)
Sep 18 20:40:54.423: INFO: Lookups using dns-9743/dns-test-fa3497e0-aef8-4785-b440-6f3d26238211 failed for: [wheezy_udp@dns-test-service.dns-9743.svc.cluster.local wheezy_tcp@dns-test-service.dns-9743.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9743.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9743.svc.cluster.local jessie_udp@dns-test-service.dns-9743.svc.cluster.local jessie_tcp@dns-test-service.dns-9743.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9743.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9743.svc.cluster.local]

Sep 18 20:40:59.514: INFO: DNS probes using dns-9743/dns-test-fa3497e0-aef8-4785-b440-6f3d26238211 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:40:59.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9743" for this suite.
Sep 18 20:41:05.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:41:05.774: INFO: namespace dns-9743 deletion completed in 6.111125753s

â€¢ [SLOW TEST:17.674 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:41:05.774: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8077
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0918 20:41:16.013011      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 18 20:41:16.013: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:41:16.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8077" for this suite.
Sep 18 20:41:22.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:41:22.109: INFO: namespace gc-8077 deletion completed in 6.092954262s

â€¢ [SLOW TEST:16.335 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:41:22.110: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-672
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Sep 18 20:41:22.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 create -f - --namespace=kubectl-672'
Sep 18 20:41:24.904: INFO: stderr: ""
Sep 18 20:41:24.904: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 18 20:41:24.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-672'
Sep 18 20:41:25.022: INFO: stderr: ""
Sep 18 20:41:25.022: INFO: stdout: "update-demo-nautilus-f5b95 update-demo-nautilus-p4g4x "
Sep 18 20:41:25.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-f5b95 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-672'
Sep 18 20:41:25.138: INFO: stderr: ""
Sep 18 20:41:25.138: INFO: stdout: ""
Sep 18 20:41:25.138: INFO: update-demo-nautilus-f5b95 is created but not running
Sep 18 20:41:30.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-672'
Sep 18 20:41:30.235: INFO: stderr: ""
Sep 18 20:41:30.235: INFO: stdout: "update-demo-nautilus-f5b95 update-demo-nautilus-p4g4x "
Sep 18 20:41:30.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-f5b95 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-672'
Sep 18 20:41:30.335: INFO: stderr: ""
Sep 18 20:41:30.335: INFO: stdout: "true"
Sep 18 20:41:30.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-f5b95 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-672'
Sep 18 20:41:30.426: INFO: stderr: ""
Sep 18 20:41:30.426: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 18 20:41:30.426: INFO: validating pod update-demo-nautilus-f5b95
Sep 18 20:41:30.432: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 18 20:41:30.432: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 18 20:41:30.432: INFO: update-demo-nautilus-f5b95 is verified up and running
Sep 18 20:41:30.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-p4g4x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-672'
Sep 18 20:41:30.522: INFO: stderr: ""
Sep 18 20:41:30.522: INFO: stdout: "true"
Sep 18 20:41:30.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods update-demo-nautilus-p4g4x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-672'
Sep 18 20:41:30.620: INFO: stderr: ""
Sep 18 20:41:30.620: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 18 20:41:30.620: INFO: validating pod update-demo-nautilus-p4g4x
Sep 18 20:41:30.627: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 18 20:41:30.627: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 18 20:41:30.627: INFO: update-demo-nautilus-p4g4x is verified up and running
STEP: using delete to clean up resources
Sep 18 20:41:30.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 delete --grace-period=0 --force -f - --namespace=kubectl-672'
Sep 18 20:41:30.731: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 18 20:41:30.731: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 18 20:41:30.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-672'
Sep 18 20:41:30.856: INFO: stderr: "No resources found in kubectl-672 namespace.\n"
Sep 18 20:41:30.856: INFO: stdout: ""
Sep 18 20:41:30.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods -l name=update-demo --namespace=kubectl-672 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 18 20:41:30.974: INFO: stderr: ""
Sep 18 20:41:30.974: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:41:30.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-672" for this suite.
Sep 18 20:41:42.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:41:43.088: INFO: namespace kubectl-672 deletion completed in 12.109985045s

â€¢ [SLOW TEST:20.978 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:41:43.089: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4566
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4566
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4566
STEP: creating replication controller externalsvc in namespace services-4566
I0918 20:41:43.397713      14 runners.go:184] Created replication controller with name: externalsvc, namespace: services-4566, replica count: 2
I0918 20:41:43.398728      14 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 20:41:43.398755      14 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 20:41:46.452904      14 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Sep 18 20:41:46.478: INFO: Creating new exec pod
Sep 18 20:41:50.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=services-4566 execpodj8dl2 -- /bin/sh -x -c nslookup clusterip-service'
Sep 18 20:41:50.750: INFO: stderr: "+ nslookup clusterip-service\n"
Sep 18 20:41:50.750: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nclusterip-service.services-4566.svc.cluster.local\tcanonical name = externalsvc.services-4566.svc.cluster.local.\nName:\texternalsvc.services-4566.svc.cluster.local\nAddress: 10.0.37.190\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4566, will wait for the garbage collector to delete the pods
I0918 20:41:50.754799      14 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 20:41:50.754829      14 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 18 20:41:50.813: INFO: Deleting ReplicationController externalsvc took: 8.245416ms
Sep 18 20:41:51.113: INFO: Terminating ReplicationController externalsvc pods took: 300.339623ms
I0918 20:41:51.113386      14 controller_utils.go:810] Ignoring inactive pod services-4566/externalsvc-4zrnn in state Running, deletion time 2019-09-18 20:41:52 +0000 UTC
I0918 20:41:51.113441      14 controller_utils.go:810] Ignoring inactive pod services-4566/externalsvc-kcblb in state Running, deletion time 2019-09-18 20:41:52 +0000 UTC
Sep 18 20:42:03.655: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:42:03.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4566" for this suite.
Sep 18 20:42:09.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:42:09.819: INFO: namespace services-4566 deletion completed in 6.108957137s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:26.730 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:42:09.819: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9398
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 18 20:42:09.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-9398'
Sep 18 20:42:10.093: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 18 20:42:10.093: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Sep 18 20:42:10.112: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-74wgr]
Sep 18 20:42:10.112: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-74wgr" in namespace "kubectl-9398" to be "running and ready"
Sep 18 20:42:10.114: INFO: Pod "e2e-test-httpd-rc-74wgr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.596874ms
Sep 18 20:42:12.119: INFO: Pod "e2e-test-httpd-rc-74wgr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007403536s
Sep 18 20:42:14.123: INFO: Pod "e2e-test-httpd-rc-74wgr": Phase="Running", Reason="", readiness=true. Elapsed: 4.011296037s
Sep 18 20:42:14.123: INFO: Pod "e2e-test-httpd-rc-74wgr" satisfied condition "running and ready"
Sep 18 20:42:14.123: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-74wgr]
Sep 18 20:42:14.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 logs rc/e2e-test-httpd-rc --namespace=kubectl-9398'
Sep 18 20:42:14.269: INFO: stderr: ""
Sep 18 20:42:14.269: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.240.0.48. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.240.0.48. Set the 'ServerName' directive globally to suppress this message\n[Wed Sep 18 20:42:12.396101 2019] [mpm_event:notice] [pid 1:tid 140078655630184] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Wed Sep 18 20:42:12.396164 2019] [core:notice] [pid 1:tid 140078655630184] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Sep 18 20:42:14.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 delete rc e2e-test-httpd-rc --namespace=kubectl-9398'
Sep 18 20:42:14.378: INFO: stderr: ""
Sep 18 20:42:14.378: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:42:14.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9398" for this suite.
Sep 18 20:42:26.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:42:26.497: INFO: namespace kubectl-9398 deletion completed in 12.10870908s

â€¢ [SLOW TEST:16.677 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:42:26.497: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4224
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-7xsh
STEP: Creating a pod to test atomic-volume-subpath
Sep 18 20:42:26.679: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-7xsh" in namespace "subpath-4224" to be "success or failure"
Sep 18 20:42:26.682: INFO: Pod "pod-subpath-test-configmap-7xsh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.708073ms
Sep 18 20:42:28.686: INFO: Pod "pod-subpath-test-configmap-7xsh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006886279s
Sep 18 20:42:30.690: INFO: Pod "pod-subpath-test-configmap-7xsh": Phase="Running", Reason="", readiness=true. Elapsed: 4.010372721s
Sep 18 20:42:32.694: INFO: Pod "pod-subpath-test-configmap-7xsh": Phase="Running", Reason="", readiness=true. Elapsed: 6.014466084s
Sep 18 20:42:34.698: INFO: Pod "pod-subpath-test-configmap-7xsh": Phase="Running", Reason="", readiness=true. Elapsed: 8.018586775s
Sep 18 20:42:36.702: INFO: Pod "pod-subpath-test-configmap-7xsh": Phase="Running", Reason="", readiness=true. Elapsed: 10.0222216s
Sep 18 20:42:38.705: INFO: Pod "pod-subpath-test-configmap-7xsh": Phase="Running", Reason="", readiness=true. Elapsed: 12.026080249s
Sep 18 20:42:40.709: INFO: Pod "pod-subpath-test-configmap-7xsh": Phase="Running", Reason="", readiness=true. Elapsed: 14.029994427s
Sep 18 20:42:42.713: INFO: Pod "pod-subpath-test-configmap-7xsh": Phase="Running", Reason="", readiness=true. Elapsed: 16.034169729s
Sep 18 20:42:44.718: INFO: Pod "pod-subpath-test-configmap-7xsh": Phase="Running", Reason="", readiness=true. Elapsed: 18.038398058s
Sep 18 20:42:46.721: INFO: Pod "pod-subpath-test-configmap-7xsh": Phase="Running", Reason="", readiness=true. Elapsed: 20.04203302s
Sep 18 20:42:48.725: INFO: Pod "pod-subpath-test-configmap-7xsh": Phase="Running", Reason="", readiness=true. Elapsed: 22.046110005s
Sep 18 20:42:50.729: INFO: Pod "pod-subpath-test-configmap-7xsh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.049652023s
STEP: Saw pod success
Sep 18 20:42:50.729: INFO: Pod "pod-subpath-test-configmap-7xsh" satisfied condition "success or failure"
Sep 18 20:42:50.731: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-subpath-test-configmap-7xsh container test-container-subpath-configmap-7xsh: <nil>
STEP: delete the pod
Sep 18 20:42:50.779: INFO: Waiting for pod pod-subpath-test-configmap-7xsh to disappear
Sep 18 20:42:50.782: INFO: Pod pod-subpath-test-configmap-7xsh no longer exists
STEP: Deleting pod pod-subpath-test-configmap-7xsh
Sep 18 20:42:50.782: INFO: Deleting pod "pod-subpath-test-configmap-7xsh" in namespace "subpath-4224"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:42:50.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4224" for this suite.
Sep 18 20:42:56.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:42:56.899: INFO: namespace subpath-4224 deletion completed in 6.11058484s

â€¢ [SLOW TEST:30.402 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:42:56.900: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4628
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-f91ced9f-3f0f-4b41-82d0-96e2d68d1c40
Sep 18 20:42:57.089: INFO: Pod name my-hostname-basic-f91ced9f-3f0f-4b41-82d0-96e2d68d1c40: Found 0 pods out of 1
Sep 18 20:43:02.093: INFO: Pod name my-hostname-basic-f91ced9f-3f0f-4b41-82d0-96e2d68d1c40: Found 1 pods out of 1
Sep 18 20:43:02.093: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-f91ced9f-3f0f-4b41-82d0-96e2d68d1c40" are running
Sep 18 20:43:02.096: INFO: Pod "my-hostname-basic-f91ced9f-3f0f-4b41-82d0-96e2d68d1c40-mn2hg" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-18 20:42:57 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-18 20:42:59 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-18 20:42:59 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-18 20:42:57 +0000 UTC Reason: Message:}])
Sep 18 20:43:02.096: INFO: Trying to dial the pod
Sep 18 20:43:07.108: INFO: Controller my-hostname-basic-f91ced9f-3f0f-4b41-82d0-96e2d68d1c40: Got expected result from replica 1 [my-hostname-basic-f91ced9f-3f0f-4b41-82d0-96e2d68d1c40-mn2hg]: "my-hostname-basic-f91ced9f-3f0f-4b41-82d0-96e2d68d1c40-mn2hg", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:43:07.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4628" for this suite.
Sep 18 20:43:13.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:43:13.215: INFO: namespace replication-controller-4628 deletion completed in 6.102144379s

â€¢ [SLOW TEST:16.315 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:43:13.216: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1477
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep 18 20:43:13.389: INFO: Pod name pod-release: Found 0 pods out of 1
Sep 18 20:43:18.392: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:43:19.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1477" for this suite.
Sep 18 20:43:25.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:43:25.511: INFO: namespace replication-controller-1477 deletion completed in 6.098998691s

â€¢ [SLOW TEST:12.296 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:43:25.513: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7266
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 18 20:43:28.708: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:43:28.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7266" for this suite.
Sep 18 20:43:34.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:43:34.827: INFO: namespace container-runtime-7266 deletion completed in 6.0934593s

â€¢ [SLOW TEST:9.314 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:43:34.827: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8092
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 18 20:43:34.995: INFO: Waiting up to 5m0s for pod "downwardapi-volume-89e325e1-ab2d-4e70-920f-285da6681956" in namespace "downward-api-8092" to be "success or failure"
Sep 18 20:43:35.012: INFO: Pod "downwardapi-volume-89e325e1-ab2d-4e70-920f-285da6681956": Phase="Pending", Reason="", readiness=false. Elapsed: 17.093637ms
Sep 18 20:43:37.017: INFO: Pod "downwardapi-volume-89e325e1-ab2d-4e70-920f-285da6681956": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021473546s
Sep 18 20:43:39.021: INFO: Pod "downwardapi-volume-89e325e1-ab2d-4e70-920f-285da6681956": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025443983s
STEP: Saw pod success
Sep 18 20:43:39.021: INFO: Pod "downwardapi-volume-89e325e1-ab2d-4e70-920f-285da6681956" satisfied condition "success or failure"
Sep 18 20:43:39.023: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod downwardapi-volume-89e325e1-ab2d-4e70-920f-285da6681956 container client-container: <nil>
STEP: delete the pod
Sep 18 20:43:39.049: INFO: Waiting for pod downwardapi-volume-89e325e1-ab2d-4e70-920f-285da6681956 to disappear
Sep 18 20:43:39.052: INFO: Pod downwardapi-volume-89e325e1-ab2d-4e70-920f-285da6681956 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:43:39.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8092" for this suite.
Sep 18 20:43:45.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:43:45.166: INFO: namespace downward-api-8092 deletion completed in 6.108561845s

â€¢ [SLOW TEST:10.339 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:43:45.166: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7890
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-2c65cb53-4968-4c2b-ab52-1fd1b831d6a5 in namespace container-probe-7890
Sep 18 20:43:49.364: INFO: Started pod liveness-2c65cb53-4968-4c2b-ab52-1fd1b831d6a5 in namespace container-probe-7890
STEP: checking the pod's current state and verifying that restartCount is present
Sep 18 20:43:49.367: INFO: Initial restart count of pod liveness-2c65cb53-4968-4c2b-ab52-1fd1b831d6a5 is 0
Sep 18 20:44:09.429: INFO: Restart count of pod container-probe-7890/liveness-2c65cb53-4968-4c2b-ab52-1fd1b831d6a5 is now 1 (20.062025835s elapsed)
Sep 18 20:44:29.471: INFO: Restart count of pod container-probe-7890/liveness-2c65cb53-4968-4c2b-ab52-1fd1b831d6a5 is now 2 (40.103633285s elapsed)
Sep 18 20:44:49.519: INFO: Restart count of pod container-probe-7890/liveness-2c65cb53-4968-4c2b-ab52-1fd1b831d6a5 is now 3 (1m0.15150891s elapsed)
Sep 18 20:45:09.562: INFO: Restart count of pod container-probe-7890/liveness-2c65cb53-4968-4c2b-ab52-1fd1b831d6a5 is now 4 (1m20.194884224s elapsed)
Sep 18 20:46:19.730: INFO: Restart count of pod container-probe-7890/liveness-2c65cb53-4968-4c2b-ab52-1fd1b831d6a5 is now 5 (2m30.362886676s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:46:19.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7890" for this suite.
Sep 18 20:46:25.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:46:25.848: INFO: namespace container-probe-7890 deletion completed in 6.09587829s

â€¢ [SLOW TEST:160.682 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:46:25.848: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6490
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 18 20:46:26.014: INFO: Waiting up to 5m0s for pod "pod-97b2eaee-870e-4d32-8891-88dc3bd7af67" in namespace "emptydir-6490" to be "success or failure"
Sep 18 20:46:26.018: INFO: Pod "pod-97b2eaee-870e-4d32-8891-88dc3bd7af67": Phase="Pending", Reason="", readiness=false. Elapsed: 3.891566ms
Sep 18 20:46:28.021: INFO: Pod "pod-97b2eaee-870e-4d32-8891-88dc3bd7af67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007539178s
Sep 18 20:46:30.025: INFO: Pod "pod-97b2eaee-870e-4d32-8891-88dc3bd7af67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011475905s
STEP: Saw pod success
Sep 18 20:46:30.025: INFO: Pod "pod-97b2eaee-870e-4d32-8891-88dc3bd7af67" satisfied condition "success or failure"
Sep 18 20:46:30.029: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-97b2eaee-870e-4d32-8891-88dc3bd7af67 container test-container: <nil>
STEP: delete the pod
Sep 18 20:46:30.074: INFO: Waiting for pod pod-97b2eaee-870e-4d32-8891-88dc3bd7af67 to disappear
Sep 18 20:46:30.083: INFO: Pod pod-97b2eaee-870e-4d32-8891-88dc3bd7af67 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:46:30.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6490" for this suite.
Sep 18 20:46:36.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:46:36.204: INFO: namespace emptydir-6490 deletion completed in 6.112207728s

â€¢ [SLOW TEST:10.356 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:46:36.206: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6029
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 20:46:36.369: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 18 20:46:38.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 --namespace=crd-publish-openapi-6029 create -f -'
Sep 18 20:46:38.924: INFO: stderr: ""
Sep 18 20:46:38.924: INFO: stdout: "e2e-test-crd-publish-openapi-1787-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 18 20:46:38.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 --namespace=crd-publish-openapi-6029 delete e2e-test-crd-publish-openapi-1787-crds test-cr'
Sep 18 20:46:39.027: INFO: stderr: ""
Sep 18 20:46:39.027: INFO: stdout: "e2e-test-crd-publish-openapi-1787-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep 18 20:46:39.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 --namespace=crd-publish-openapi-6029 apply -f -'
Sep 18 20:46:39.294: INFO: stderr: ""
Sep 18 20:46:39.294: INFO: stdout: "e2e-test-crd-publish-openapi-1787-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 18 20:46:39.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 --namespace=crd-publish-openapi-6029 delete e2e-test-crd-publish-openapi-1787-crds test-cr'
Sep 18 20:46:39.388: INFO: stderr: ""
Sep 18 20:46:39.388: INFO: stdout: "e2e-test-crd-publish-openapi-1787-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 18 20:46:39.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 explain e2e-test-crd-publish-openapi-1787-crds'
Sep 18 20:46:39.637: INFO: stderr: ""
Sep 18 20:46:39.637: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1787-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:46:42.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6029" for this suite.
Sep 18 20:46:48.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:46:48.666: INFO: namespace crd-publish-openapi-6029 deletion completed in 6.100989352s

â€¢ [SLOW TEST:12.460 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:46:48.667: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8466
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 18 20:46:56.930: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 18 20:46:56.952: INFO: Pod pod-with-poststart-http-hook still exists
Sep 18 20:46:58.953: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 18 20:46:58.956: INFO: Pod pod-with-poststart-http-hook still exists
Sep 18 20:47:00.953: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 18 20:47:00.959: INFO: Pod pod-with-poststart-http-hook still exists
Sep 18 20:47:02.953: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 18 20:47:02.958: INFO: Pod pod-with-poststart-http-hook still exists
Sep 18 20:47:04.953: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 18 20:47:04.957: INFO: Pod pod-with-poststart-http-hook still exists
Sep 18 20:47:06.953: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 18 20:47:06.956: INFO: Pod pod-with-poststart-http-hook still exists
Sep 18 20:47:08.953: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 18 20:47:08.957: INFO: Pod pod-with-poststart-http-hook still exists
Sep 18 20:47:10.953: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 18 20:47:10.956: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:47:10.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8466" for this suite.
Sep 18 20:47:32.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:47:33.075: INFO: namespace container-lifecycle-hook-8466 deletion completed in 22.115685972s

â€¢ [SLOW TEST:44.408 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:47:33.076: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2560
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 18 20:47:33.258: INFO: Waiting up to 5m0s for pod "downwardapi-volume-35e614e4-0055-4d1b-baf8-5afdaa64a193" in namespace "projected-2560" to be "success or failure"
Sep 18 20:47:33.268: INFO: Pod "downwardapi-volume-35e614e4-0055-4d1b-baf8-5afdaa64a193": Phase="Pending", Reason="", readiness=false. Elapsed: 9.886017ms
Sep 18 20:47:35.271: INFO: Pod "downwardapi-volume-35e614e4-0055-4d1b-baf8-5afdaa64a193": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013421888s
Sep 18 20:47:37.275: INFO: Pod "downwardapi-volume-35e614e4-0055-4d1b-baf8-5afdaa64a193": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01744477s
STEP: Saw pod success
Sep 18 20:47:37.275: INFO: Pod "downwardapi-volume-35e614e4-0055-4d1b-baf8-5afdaa64a193" satisfied condition "success or failure"
Sep 18 20:47:37.279: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod downwardapi-volume-35e614e4-0055-4d1b-baf8-5afdaa64a193 container client-container: <nil>
STEP: delete the pod
Sep 18 20:47:37.308: INFO: Waiting for pod downwardapi-volume-35e614e4-0055-4d1b-baf8-5afdaa64a193 to disappear
Sep 18 20:47:37.311: INFO: Pod downwardapi-volume-35e614e4-0055-4d1b-baf8-5afdaa64a193 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:47:37.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2560" for this suite.
Sep 18 20:47:43.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:47:43.433: INFO: namespace projected-2560 deletion completed in 6.118051059s

â€¢ [SLOW TEST:10.358 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:47:43.434: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4234
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Sep 18 20:47:43.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 create -f - --namespace=kubectl-4234'
Sep 18 20:47:43.932: INFO: stderr: ""
Sep 18 20:47:43.932: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep 18 20:47:44.936: INFO: Selector matched 1 pods for map[app:redis]
Sep 18 20:47:44.936: INFO: Found 0 / 1
Sep 18 20:47:45.944: INFO: Selector matched 1 pods for map[app:redis]
Sep 18 20:47:45.944: INFO: Found 0 / 1
Sep 18 20:47:46.936: INFO: Selector matched 1 pods for map[app:redis]
Sep 18 20:47:46.936: INFO: Found 1 / 1
Sep 18 20:47:46.936: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep 18 20:47:46.940: INFO: Selector matched 1 pods for map[app:redis]
Sep 18 20:47:46.940: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 18 20:47:46.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 patch pod redis-master-dr4zp --namespace=kubectl-4234 -p {"metadata":{"annotations":{"x":"y"}}}'
Sep 18 20:47:47.042: INFO: stderr: ""
Sep 18 20:47:47.043: INFO: stdout: "pod/redis-master-dr4zp patched\n"
STEP: checking annotations
Sep 18 20:47:47.046: INFO: Selector matched 1 pods for map[app:redis]
Sep 18 20:47:47.046: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:47:47.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4234" for this suite.
Sep 18 20:48:15.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:48:15.158: INFO: namespace kubectl-4234 deletion completed in 28.107025048s

â€¢ [SLOW TEST:31.724 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:48:15.163: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1538
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 18 20:48:15.329: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3ca454a0-8f4b-4929-bf7d-d68c57c0d576" in namespace "downward-api-1538" to be "success or failure"
Sep 18 20:48:15.335: INFO: Pod "downwardapi-volume-3ca454a0-8f4b-4929-bf7d-d68c57c0d576": Phase="Pending", Reason="", readiness=false. Elapsed: 6.13575ms
Sep 18 20:48:17.339: INFO: Pod "downwardapi-volume-3ca454a0-8f4b-4929-bf7d-d68c57c0d576": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01010133s
Sep 18 20:48:19.344: INFO: Pod "downwardapi-volume-3ca454a0-8f4b-4929-bf7d-d68c57c0d576": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01467952s
STEP: Saw pod success
Sep 18 20:48:19.344: INFO: Pod "downwardapi-volume-3ca454a0-8f4b-4929-bf7d-d68c57c0d576" satisfied condition "success or failure"
Sep 18 20:48:19.346: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod downwardapi-volume-3ca454a0-8f4b-4929-bf7d-d68c57c0d576 container client-container: <nil>
STEP: delete the pod
Sep 18 20:48:19.383: INFO: Waiting for pod downwardapi-volume-3ca454a0-8f4b-4929-bf7d-d68c57c0d576 to disappear
Sep 18 20:48:19.386: INFO: Pod downwardapi-volume-3ca454a0-8f4b-4929-bf7d-d68c57c0d576 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:48:19.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1538" for this suite.
Sep 18 20:48:25.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:48:25.497: INFO: namespace downward-api-1538 deletion completed in 6.106865996s

â€¢ [SLOW TEST:10.335 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:48:25.500: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3531
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:48:25.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3531" for this suite.
Sep 18 20:48:31.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:48:31.807: INFO: namespace resourcequota-3531 deletion completed in 6.099643289s

â€¢ [SLOW TEST:6.307 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:48:31.807: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-608
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 18 20:48:31.992: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d0e36de5-36c0-4cd4-bd31-6b8281186684" in namespace "projected-608" to be "success or failure"
Sep 18 20:48:32.005: INFO: Pod "downwardapi-volume-d0e36de5-36c0-4cd4-bd31-6b8281186684": Phase="Pending", Reason="", readiness=false. Elapsed: 12.743596ms
Sep 18 20:48:34.009: INFO: Pod "downwardapi-volume-d0e36de5-36c0-4cd4-bd31-6b8281186684": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016796593s
Sep 18 20:48:36.012: INFO: Pod "downwardapi-volume-d0e36de5-36c0-4cd4-bd31-6b8281186684": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020279208s
STEP: Saw pod success
Sep 18 20:48:36.012: INFO: Pod "downwardapi-volume-d0e36de5-36c0-4cd4-bd31-6b8281186684" satisfied condition "success or failure"
Sep 18 20:48:36.015: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod downwardapi-volume-d0e36de5-36c0-4cd4-bd31-6b8281186684 container client-container: <nil>
STEP: delete the pod
Sep 18 20:48:36.077: INFO: Waiting for pod downwardapi-volume-d0e36de5-36c0-4cd4-bd31-6b8281186684 to disappear
Sep 18 20:48:36.079: INFO: Pod downwardapi-volume-d0e36de5-36c0-4cd4-bd31-6b8281186684 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:48:36.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-608" for this suite.
Sep 18 20:48:42.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:48:42.181: INFO: namespace projected-608 deletion completed in 6.097031629s

â€¢ [SLOW TEST:10.374 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:48:42.182: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3664
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
I0918 20:48:42.363728      14 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/client-go/tools/watch/informerwatcher.go:146
I0918 20:48:42.363824      14 reflector.go:158] Listing and watching *v1.Pod from k8s.io/client-go/tools/watch/informerwatcher.go:146
Sep 18 20:48:42.366: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:48:53.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3664" for this suite.
Sep 18 20:48:59.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:48:59.671: INFO: namespace pods-3664 deletion completed in 6.102637541s

â€¢ [SLOW TEST:17.489 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:48:59.671: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-503
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Sep 18 20:49:09.859: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0918 20:49:09.859339      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:49:09.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-503" for this suite.
Sep 18 20:49:15.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:49:15.952: INFO: namespace gc-503 deletion completed in 6.088843874s

â€¢ [SLOW TEST:16.281 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:49:15.952: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4118
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Sep 18 20:49:16.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 create -f - --namespace=kubectl-4118'
Sep 18 20:49:16.433: INFO: stderr: ""
Sep 18 20:49:16.433: INFO: stdout: "pod/pause created\n"
Sep 18 20:49:16.433: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep 18 20:49:16.433: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4118" to be "running and ready"
Sep 18 20:49:16.477: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 44.037548ms
Sep 18 20:49:18.482: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049390729s
Sep 18 20:49:20.486: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.053481033s
Sep 18 20:49:20.486: INFO: Pod "pause" satisfied condition "running and ready"
Sep 18 20:49:20.486: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Sep 18 20:49:20.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 label pods pause testing-label=testing-label-value --namespace=kubectl-4118'
Sep 18 20:49:20.581: INFO: stderr: ""
Sep 18 20:49:20.581: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep 18 20:49:20.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pod pause -L testing-label --namespace=kubectl-4118'
Sep 18 20:49:20.666: INFO: stderr: ""
Sep 18 20:49:20.666: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep 18 20:49:20.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 label pods pause testing-label- --namespace=kubectl-4118'
Sep 18 20:49:20.772: INFO: stderr: ""
Sep 18 20:49:20.772: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep 18 20:49:20.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pod pause -L testing-label --namespace=kubectl-4118'
Sep 18 20:49:20.859: INFO: stderr: ""
Sep 18 20:49:20.859: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Sep 18 20:49:20.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 delete --grace-period=0 --force -f - --namespace=kubectl-4118'
Sep 18 20:49:20.954: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 18 20:49:20.954: INFO: stdout: "pod \"pause\" force deleted\n"
Sep 18 20:49:20.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get rc,svc -l name=pause --no-headers --namespace=kubectl-4118'
Sep 18 20:49:21.052: INFO: stderr: "No resources found in kubectl-4118 namespace.\n"
Sep 18 20:49:21.053: INFO: stdout: ""
Sep 18 20:49:21.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pods -l name=pause --namespace=kubectl-4118 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 18 20:49:21.155: INFO: stderr: ""
Sep 18 20:49:21.155: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:49:21.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4118" for this suite.
Sep 18 20:49:27.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:49:27.258: INFO: namespace kubectl-4118 deletion completed in 6.097066525s

â€¢ [SLOW TEST:11.306 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:49:27.258: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9825
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 18 20:49:27.455: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7c34f7f0-e675-43ac-bc68-178d3c3f58f1" in namespace "downward-api-9825" to be "success or failure"
Sep 18 20:49:27.458: INFO: Pod "downwardapi-volume-7c34f7f0-e675-43ac-bc68-178d3c3f58f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.845277ms
Sep 18 20:49:29.462: INFO: Pod "downwardapi-volume-7c34f7f0-e675-43ac-bc68-178d3c3f58f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006676139s
Sep 18 20:49:31.466: INFO: Pod "downwardapi-volume-7c34f7f0-e675-43ac-bc68-178d3c3f58f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011079809s
STEP: Saw pod success
Sep 18 20:49:31.466: INFO: Pod "downwardapi-volume-7c34f7f0-e675-43ac-bc68-178d3c3f58f1" satisfied condition "success or failure"
Sep 18 20:49:31.470: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod downwardapi-volume-7c34f7f0-e675-43ac-bc68-178d3c3f58f1 container client-container: <nil>
STEP: delete the pod
Sep 18 20:49:31.496: INFO: Waiting for pod downwardapi-volume-7c34f7f0-e675-43ac-bc68-178d3c3f58f1 to disappear
Sep 18 20:49:31.499: INFO: Pod downwardapi-volume-7c34f7f0-e675-43ac-bc68-178d3c3f58f1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:49:31.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9825" for this suite.
Sep 18 20:49:37.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:49:37.629: INFO: namespace downward-api-9825 deletion completed in 6.123226514s

â€¢ [SLOW TEST:10.370 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:49:37.630: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4640
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Sep 18 20:49:37.786: INFO: PodSpec: initContainers in spec.initContainers
Sep 18 20:50:26.401: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-b2ec8803-682e-4422-ac32-4bc76d11699d", GenerateName:"", Namespace:"init-container-4640", SelfLink:"/api/v1/namespaces/init-container-4640/pods/pod-init-b2ec8803-682e-4422-ac32-4bc76d11699d", UID:"7ea07889-80ea-475c-9ecc-3e9cc2cd8fe0", ResourceVersion:"17271", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63704436577, loc:(*time.Location)(0x84be2c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"786400168"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-g4p2l", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc003840040), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-g4p2l", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-g4p2l", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-g4p2l", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc005900638), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-agentpool1-40209065-vmss000001", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00202a4e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0059006b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0059006d0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0059006d8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0059006dc), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436577, loc:(*time.Location)(0x84be2c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436577, loc:(*time.Location)(0x84be2c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436577, loc:(*time.Location)(0x84be2c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436577, loc:(*time.Location)(0x84be2c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.240.0.35", PodIP:"10.240.0.37", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.240.0.37"}}, StartTime:(*v1.Time)(0xc002f3bb60), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00094afc0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00094b0a0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://d53d3723c4eeef15596b69238c49ad457cadb99b6ada2672be4ed00fadad461c", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f3bba0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f3bb80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc00590075f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:50:26.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4640" for this suite.
Sep 18 20:50:54.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:50:54.532: INFO: namespace init-container-4640 deletion completed in 28.09808906s

â€¢ [SLOW TEST:76.902 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:50:54.533: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6377
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Sep 18 20:51:24.765: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0918 20:51:24.765589      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:51:24.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6377" for this suite.
Sep 18 20:51:30.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:51:30.876: INFO: namespace gc-6377 deletion completed in 6.105328053s

â€¢ [SLOW TEST:36.343 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:51:30.876: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2431
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-28960ee5-841f-49f2-bb05-47d175c83b9f
STEP: Creating secret with name secret-projected-all-test-volume-98974e30-6b48-47a6-9ef0-3e7e06a85867
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep 18 20:51:31.049: INFO: Waiting up to 5m0s for pod "projected-volume-7a545cf8-c170-44e8-bbbb-2e3d15933984" in namespace "projected-2431" to be "success or failure"
Sep 18 20:51:31.061: INFO: Pod "projected-volume-7a545cf8-c170-44e8-bbbb-2e3d15933984": Phase="Pending", Reason="", readiness=false. Elapsed: 11.918509ms
Sep 18 20:51:33.065: INFO: Pod "projected-volume-7a545cf8-c170-44e8-bbbb-2e3d15933984": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01608915s
Sep 18 20:51:35.074: INFO: Pod "projected-volume-7a545cf8-c170-44e8-bbbb-2e3d15933984": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024790467s
STEP: Saw pod success
Sep 18 20:51:35.074: INFO: Pod "projected-volume-7a545cf8-c170-44e8-bbbb-2e3d15933984" satisfied condition "success or failure"
Sep 18 20:51:35.091: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod projected-volume-7a545cf8-c170-44e8-bbbb-2e3d15933984 container projected-all-volume-test: <nil>
STEP: delete the pod
Sep 18 20:51:35.149: INFO: Waiting for pod projected-volume-7a545cf8-c170-44e8-bbbb-2e3d15933984 to disappear
Sep 18 20:51:35.153: INFO: Pod projected-volume-7a545cf8-c170-44e8-bbbb-2e3d15933984 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:51:35.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2431" for this suite.
Sep 18 20:51:41.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:51:41.256: INFO: namespace projected-2431 deletion completed in 6.09836476s

â€¢ [SLOW TEST:10.380 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:51:41.258: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6760
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 20:51:41.431: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep 18 20:51:46.436: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 18 20:51:46.436: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep 18 20:51:48.440: INFO: Creating deployment "test-rollover-deployment"
Sep 18 20:51:48.447: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep 18 20:51:50.459: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep 18 20:51:50.465: INFO: Ensure that both replica sets have 1 created replica
Sep 18 20:51:50.471: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep 18 20:51:50.484: INFO: Updating deployment test-rollover-deployment
Sep 18 20:51:50.484: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep 18 20:51:52.499: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep 18 20:51:52.504: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep 18 20:51:52.510: INFO: all replica sets need to contain the pod-template-hash label
Sep 18 20:51:52.510: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436708, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436708, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436710, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436708, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 18 20:51:54.517: INFO: all replica sets need to contain the pod-template-hash label
Sep 18 20:51:54.518: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436708, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436708, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436713, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436708, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 18 20:51:56.518: INFO: all replica sets need to contain the pod-template-hash label
Sep 18 20:51:56.518: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436708, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436708, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436713, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436708, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 18 20:51:58.519: INFO: all replica sets need to contain the pod-template-hash label
Sep 18 20:51:58.519: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436708, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436708, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436713, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436708, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 18 20:52:00.517: INFO: all replica sets need to contain the pod-template-hash label
Sep 18 20:52:00.517: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436708, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436708, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436713, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436708, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 18 20:52:02.517: INFO: all replica sets need to contain the pod-template-hash label
Sep 18 20:52:02.517: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436708, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436708, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436713, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704436708, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 18 20:52:04.517: INFO: 
Sep 18 20:52:04.517: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Sep 18 20:52:04.527: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6760 /apis/apps/v1/namespaces/deployment-6760/deployments/test-rollover-deployment 553e16bb-043e-4df1-aa50-09ea7705d37c 17608 2 2019-09-18 20:51:48 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0037505e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-09-18 20:51:48 +0000 UTC,LastTransitionTime:2019-09-18 20:51:48 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2019-09-18 20:52:03 +0000 UTC,LastTransitionTime:2019-09-18 20:51:48 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 18 20:52:04.530: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-6760 /apis/apps/v1/namespaces/deployment-6760/replicasets/test-rollover-deployment-7d7dc6548c 59a79030-1ccb-480f-b635-609d1ef47b04 17597 2 2019-09-18 20:51:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 553e16bb-043e-4df1-aa50-09ea7705d37c 0xc003750aa7 0xc003750aa8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003750b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 18 20:52:04.530: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep 18 20:52:04.530: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6760 /apis/apps/v1/namespaces/deployment-6760/replicasets/test-rollover-controller f373c2c5-de70-4e2e-823e-73ec44363955 17606 2 2019-09-18 20:51:41 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 553e16bb-043e-4df1-aa50-09ea7705d37c 0xc0037509d7 0xc0037509d8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003750a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 18 20:52:04.530: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-6760 /apis/apps/v1/namespaces/deployment-6760/replicasets/test-rollover-deployment-f6c94f66c 1ad9b551-7336-411a-b716-1399c5866a45 17569 2 2019-09-18 20:51:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 553e16bb-043e-4df1-aa50-09ea7705d37c 0xc003750b70 0xc003750b71}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003750be8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 18 20:52:04.534: INFO: Pod "test-rollover-deployment-7d7dc6548c-77mvg" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-77mvg test-rollover-deployment-7d7dc6548c- deployment-6760 /api/v1/namespaces/deployment-6760/pods/test-rollover-deployment-7d7dc6548c-77mvg e39e829e-595f-4b9c-a54c-a7f458e5a534 17579 0 2019-09-18 20:51:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c 59a79030-1ccb-480f-b635-609d1ef47b04 0xc00340d3c7 0xc00340d3c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-66pbk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-66pbk,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-66pbk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 20:51:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 20:51:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 20:51:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 20:51:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.35,PodIP:10.240.0.65,StartTime:2019-09-18 20:51:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-18 20:51:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://1cdc10f5a8b22ff4735900ebc22decacc37c0f3cb0f88bb8d996806e65fb01ed,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.65,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:52:04.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6760" for this suite.
Sep 18 20:52:10.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:52:10.632: INFO: namespace deployment-6760 deletion completed in 6.094044612s

â€¢ [SLOW TEST:29.375 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:52:10.635: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-836
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-d6c84b0f-3116-4a39-a258-f07294c3a553 in namespace container-probe-836
Sep 18 20:52:14.815: INFO: Started pod busybox-d6c84b0f-3116-4a39-a258-f07294c3a553 in namespace container-probe-836
STEP: checking the pod's current state and verifying that restartCount is present
Sep 18 20:52:14.818: INFO: Initial restart count of pod busybox-d6c84b0f-3116-4a39-a258-f07294c3a553 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:56:15.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-836" for this suite.
Sep 18 20:56:21.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:56:21.516: INFO: namespace container-probe-836 deletion completed in 6.09311106s

â€¢ [SLOW TEST:250.882 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:56:21.517: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3693
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Sep 18 20:56:21.708: INFO: Waiting up to 5m0s for pod "client-containers-ffce38b3-f569-42ef-8df4-95c60abc9b0a" in namespace "containers-3693" to be "success or failure"
Sep 18 20:56:21.720: INFO: Pod "client-containers-ffce38b3-f569-42ef-8df4-95c60abc9b0a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.666018ms
Sep 18 20:56:23.724: INFO: Pod "client-containers-ffce38b3-f569-42ef-8df4-95c60abc9b0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015476842s
Sep 18 20:56:25.727: INFO: Pod "client-containers-ffce38b3-f569-42ef-8df4-95c60abc9b0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019225673s
STEP: Saw pod success
Sep 18 20:56:25.727: INFO: Pod "client-containers-ffce38b3-f569-42ef-8df4-95c60abc9b0a" satisfied condition "success or failure"
Sep 18 20:56:25.730: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod client-containers-ffce38b3-f569-42ef-8df4-95c60abc9b0a container test-container: <nil>
STEP: delete the pod
Sep 18 20:56:25.789: INFO: Waiting for pod client-containers-ffce38b3-f569-42ef-8df4-95c60abc9b0a to disappear
Sep 18 20:56:25.792: INFO: Pod client-containers-ffce38b3-f569-42ef-8df4-95c60abc9b0a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:56:25.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3693" for this suite.
Sep 18 20:56:31.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:56:31.905: INFO: namespace containers-3693 deletion completed in 6.108511939s

â€¢ [SLOW TEST:10.388 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:56:31.906: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9024
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Sep 18 20:56:32.071: INFO: Waiting up to 5m0s for pod "client-containers-72e0e5fa-b362-494a-bcb3-e6e966fbac2f" in namespace "containers-9024" to be "success or failure"
Sep 18 20:56:32.085: INFO: Pod "client-containers-72e0e5fa-b362-494a-bcb3-e6e966fbac2f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.510698ms
Sep 18 20:56:34.092: INFO: Pod "client-containers-72e0e5fa-b362-494a-bcb3-e6e966fbac2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021161731s
Sep 18 20:56:36.097: INFO: Pod "client-containers-72e0e5fa-b362-494a-bcb3-e6e966fbac2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025730284s
STEP: Saw pod success
Sep 18 20:56:36.097: INFO: Pod "client-containers-72e0e5fa-b362-494a-bcb3-e6e966fbac2f" satisfied condition "success or failure"
Sep 18 20:56:36.100: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod client-containers-72e0e5fa-b362-494a-bcb3-e6e966fbac2f container test-container: <nil>
STEP: delete the pod
Sep 18 20:56:36.154: INFO: Waiting for pod client-containers-72e0e5fa-b362-494a-bcb3-e6e966fbac2f to disappear
Sep 18 20:56:36.158: INFO: Pod client-containers-72e0e5fa-b362-494a-bcb3-e6e966fbac2f no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:56:36.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9024" for this suite.
Sep 18 20:56:42.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:56:42.294: INFO: namespace containers-9024 deletion completed in 6.131266863s

â€¢ [SLOW TEST:10.388 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:56:42.295: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1368
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 20:56:42.487: INFO: Create a RollingUpdate DaemonSet
Sep 18 20:56:42.490: INFO: Check that daemon pods launch on every node of the cluster
Sep 18 20:56:42.499: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:56:42.505: INFO: Number of nodes with available pods: 0
Sep 18 20:56:42.506: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 20:56:43.516: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:56:43.520: INFO: Number of nodes with available pods: 0
Sep 18 20:56:43.520: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 20:56:44.511: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:56:44.515: INFO: Number of nodes with available pods: 0
Sep 18 20:56:44.515: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 20:56:45.511: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:56:45.514: INFO: Number of nodes with available pods: 3
Sep 18 20:56:45.514: INFO: Number of running nodes: 3, number of available pods: 3
Sep 18 20:56:45.514: INFO: Update the DaemonSet to trigger a rollout
Sep 18 20:56:45.522: INFO: Updating DaemonSet daemon-set
Sep 18 20:56:52.540: INFO: Roll back the DaemonSet before rollout is complete
Sep 18 20:56:52.546: INFO: Updating DaemonSet daemon-set
Sep 18 20:56:52.546: INFO: Make sure DaemonSet rollback is complete
Sep 18 20:56:52.553: INFO: Wrong image for pod: daemon-set-zf4zx. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 18 20:56:52.553: INFO: Pod daemon-set-zf4zx is not available
Sep 18 20:56:52.562: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:56:53.566: INFO: Wrong image for pod: daemon-set-zf4zx. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 18 20:56:53.566: INFO: Pod daemon-set-zf4zx is not available
Sep 18 20:56:53.572: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:56:54.567: INFO: Wrong image for pod: daemon-set-zf4zx. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 18 20:56:54.567: INFO: Pod daemon-set-zf4zx is not available
Sep 18 20:56:54.571: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:56:55.567: INFO: Wrong image for pod: daemon-set-zf4zx. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 18 20:56:55.567: INFO: Pod daemon-set-zf4zx is not available
Sep 18 20:56:55.573: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:56:56.566: INFO: Wrong image for pod: daemon-set-zf4zx. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 18 20:56:56.566: INFO: Pod daemon-set-zf4zx is not available
Sep 18 20:56:56.571: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:56:57.567: INFO: Wrong image for pod: daemon-set-zf4zx. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 18 20:56:57.567: INFO: Pod daemon-set-zf4zx is not available
Sep 18 20:56:57.572: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:56:58.566: INFO: Wrong image for pod: daemon-set-zf4zx. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 18 20:56:58.566: INFO: Pod daemon-set-zf4zx is not available
Sep 18 20:56:58.570: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:56:59.566: INFO: Wrong image for pod: daemon-set-zf4zx. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 18 20:56:59.566: INFO: Pod daemon-set-zf4zx is not available
Sep 18 20:56:59.571: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:57:00.566: INFO: Wrong image for pod: daemon-set-zf4zx. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 18 20:57:00.566: INFO: Pod daemon-set-zf4zx is not available
Sep 18 20:57:00.571: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:57:01.567: INFO: Wrong image for pod: daemon-set-zf4zx. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 18 20:57:01.567: INFO: Pod daemon-set-zf4zx is not available
Sep 18 20:57:01.571: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 18 20:57:02.566: INFO: Pod daemon-set-gcvff is not available
Sep 18 20:57:02.570: INFO: DaemonSet pods can't tolerate node k8s-master-40209065-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1368, will wait for the garbage collector to delete the pods
I0918 20:57:02.578981      14 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 20:57:02.579020      14 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 18 20:57:02.635: INFO: Deleting DaemonSet.extensions daemon-set took: 6.066957ms
I0918 20:57:02.935448      14 controller_utils.go:810] Ignoring inactive pod daemonsets-1368/daemon-set-cxnn2 in state Running, deletion time 2019-09-18 20:57:32 +0000 UTC
Sep 18 20:57:03.035: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.387989ms
I0918 20:57:03.035471      14 controller_utils.go:810] Ignoring inactive pod daemonsets-1368/daemon-set-cxnn2 in state Running, deletion time 2019-09-18 20:57:32 +0000 UTC
I0918 20:57:03.035515      14 controller_utils.go:810] Ignoring inactive pod daemonsets-1368/daemon-set-kh9st in state Running, deletion time 2019-09-18 20:57:32 +0000 UTC
I0918 20:57:03.035526      14 controller_utils.go:810] Ignoring inactive pod daemonsets-1368/daemon-set-gcvff in state Pending, deletion time 2019-09-18 20:57:32 +0000 UTC
Sep 18 20:57:13.638: INFO: Number of nodes with available pods: 0
Sep 18 20:57:13.638: INFO: Number of running nodes: 0, number of available pods: 0
Sep 18 20:57:13.647: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1368/daemonsets","resourceVersion":"18303"},"items":null}

Sep 18 20:57:13.651: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1368/pods","resourceVersion":"18303"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:57:13.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1368" for this suite.
Sep 18 20:57:19.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:57:19.776: INFO: namespace daemonsets-1368 deletion completed in 6.101328069s

â€¢ [SLOW TEST:37.481 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:57:19.776: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9030
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-9030
STEP: creating replication controller nodeport-test in namespace services-9030
I0918 20:57:19.971430      14 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-9030, replica count: 2
I0918 20:57:19.971525      14 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 20:57:19.971545      14 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 18 20:57:23.021: INFO: Creating new exec pod
I0918 20:57:23.021860      14 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0918 20:57:27.040033      14 reflector.go:120] Starting reflector *v1.Endpoints (0s) from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
I0918 20:57:27.040205      14 reflector.go:158] Listing and watching *v1.Endpoints from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
Sep 18 20:57:28.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=services-9030 execpodx527v -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Sep 18 20:57:29.974: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep 18 20:57:29.974: INFO: stdout: ""
Sep 18 20:57:29.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=services-9030 execpodx527v -- /bin/sh -x -c nc -zv -t -w 2 10.0.16.68 80'
Sep 18 20:57:30.318: INFO: stderr: "+ nc -zv -t -w 2 10.0.16.68 80\nConnection to 10.0.16.68 80 port [tcp/http] succeeded!\n"
Sep 18 20:57:30.318: INFO: stdout: ""
Sep 18 20:57:30.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=services-9030 execpodx527v -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.4 32206'
Sep 18 20:57:30.650: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.4 32206\nConnection to 10.240.0.4 32206 port [tcp/32206] succeeded!\n"
Sep 18 20:57:30.650: INFO: stdout: ""
Sep 18 20:57:30.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=services-9030 execpodx527v -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.35 32206'
Sep 18 20:57:30.989: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.35 32206\nConnection to 10.240.0.35 32206 port [tcp/32206] succeeded!\n"
Sep 18 20:57:30.989: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:57:30.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9030" for this suite.
Sep 18 20:57:37.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:57:37.101: INFO: namespace services-9030 deletion completed in 6.104771274s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:17.325 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:57:37.102: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-736
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 18 20:57:38.130: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 18 20:57:40.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704437058, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704437058, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704437058, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704437058, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 18 20:57:43.170: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:57:43.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-736" for this suite.
Sep 18 20:57:49.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:57:49.337: INFO: namespace webhook-736 deletion completed in 6.092713246s
STEP: Destroying namespace "webhook-736-markers" for this suite.
Sep 18 20:57:55.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:57:55.451: INFO: namespace webhook-736-markers deletion completed in 6.113730043s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:18.363 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:57:55.465: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8483
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 18 20:58:00.162: INFO: Successfully updated pod "pod-update-dd629d5b-6a70-4c1b-8e82-4f8dc457a17c"
STEP: verifying the updated pod is in kubernetes
Sep 18 20:58:00.175: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:58:00.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8483" for this suite.
Sep 18 20:58:28.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:58:28.318: INFO: namespace pods-8483 deletion completed in 28.138029629s

â€¢ [SLOW TEST:32.853 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:58:28.319: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3723
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Sep 18 20:58:32.514: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-815280101 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Sep 18 20:58:42.606: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:58:42.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3723" for this suite.
Sep 18 20:58:48.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:58:48.728: INFO: namespace pods-3723 deletion completed in 6.115056592s

â€¢ [SLOW TEST:20.410 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:58:48.729: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3977
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-d756c33f-b4bb-4a0e-8154-6fb0fe57b4c4
STEP: Creating a pod to test consume secrets
Sep 18 20:58:48.914: INFO: Waiting up to 5m0s for pod "pod-secrets-8698dbd3-a20a-4b98-a9fc-7b2a5fe5db57" in namespace "secrets-3977" to be "success or failure"
Sep 18 20:58:48.917: INFO: Pod "pod-secrets-8698dbd3-a20a-4b98-a9fc-7b2a5fe5db57": Phase="Pending", Reason="", readiness=false. Elapsed: 3.117779ms
Sep 18 20:58:50.921: INFO: Pod "pod-secrets-8698dbd3-a20a-4b98-a9fc-7b2a5fe5db57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007087956s
Sep 18 20:58:52.925: INFO: Pod "pod-secrets-8698dbd3-a20a-4b98-a9fc-7b2a5fe5db57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010643839s
STEP: Saw pod success
Sep 18 20:58:52.925: INFO: Pod "pod-secrets-8698dbd3-a20a-4b98-a9fc-7b2a5fe5db57" satisfied condition "success or failure"
Sep 18 20:58:52.927: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-secrets-8698dbd3-a20a-4b98-a9fc-7b2a5fe5db57 container secret-volume-test: <nil>
STEP: delete the pod
Sep 18 20:58:52.952: INFO: Waiting for pod pod-secrets-8698dbd3-a20a-4b98-a9fc-7b2a5fe5db57 to disappear
Sep 18 20:58:52.955: INFO: Pod pod-secrets-8698dbd3-a20a-4b98-a9fc-7b2a5fe5db57 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:58:52.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3977" for this suite.
Sep 18 20:58:58.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:58:59.054: INFO: namespace secrets-3977 deletion completed in 6.094420899s

â€¢ [SLOW TEST:10.325 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:58:59.054: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1085
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 20:58:59.247: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-9ef838e2-5020-4eaa-8b49-3b03fee1666a" in namespace "security-context-test-1085" to be "success or failure"
Sep 18 20:58:59.250: INFO: Pod "alpine-nnp-false-9ef838e2-5020-4eaa-8b49-3b03fee1666a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.191578ms
Sep 18 20:59:01.254: INFO: Pod "alpine-nnp-false-9ef838e2-5020-4eaa-8b49-3b03fee1666a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006829478s
Sep 18 20:59:03.258: INFO: Pod "alpine-nnp-false-9ef838e2-5020-4eaa-8b49-3b03fee1666a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010957279s
Sep 18 20:59:03.258: INFO: Pod "alpine-nnp-false-9ef838e2-5020-4eaa-8b49-3b03fee1666a" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:59:03.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1085" for this suite.
Sep 18 20:59:09.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:59:09.376: INFO: namespace security-context-test-1085 deletion completed in 6.10494729s

â€¢ [SLOW TEST:10.321 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:59:09.376: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4629
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Sep 18 20:59:09.534: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 18 20:59:09.549: INFO: Waiting for terminating namespaces to be deleted...
Sep 18 20:59:09.552: INFO: 
Logging pods the kubelet thinks is on node k8s-agentpool1-40209065-vmss000000 before test
Sep 18 20:59:09.589: INFO: azure-cni-networkmonitor-9x9wr from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 20:59:09.589: INFO: 	Container azure-cnms ready: true, restart count 0
Sep 18 20:59:09.589: INFO: kubernetes-dashboard-65966766b9-86cqc from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 20:59:09.589: INFO: 	Container kubernetes-dashboard ready: true, restart count 1
Sep 18 20:59:09.589: INFO: keyvault-flexvolume-n9xxt from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 20:59:09.589: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
Sep 18 20:59:09.589: INFO: kube-proxy-rhg4h from kube-system started at 2019-09-18 19:32:35 +0000 UTC (1 container statuses recorded)
Sep 18 20:59:09.589: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 18 20:59:09.589: INFO: blobfuse-flexvol-installer-6ghd6 from kube-system started at 2019-09-18 19:32:36 +0000 UTC (1 container statuses recorded)
Sep 18 20:59:09.589: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
Sep 18 20:59:09.589: INFO: azure-ip-masq-agent-l2c2n from kube-system started at 2019-09-18 19:32:37 +0000 UTC (1 container statuses recorded)
Sep 18 20:59:09.589: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Sep 18 20:59:09.589: INFO: sonobuoy-systemd-logs-daemon-set-13153e5f2c4542ef-86g6v from sonobuoy started at 2019-09-18 19:33:36 +0000 UTC (2 container statuses recorded)
Sep 18 20:59:09.589: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 18 20:59:09.589: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 18 20:59:09.589: INFO: 
Logging pods the kubelet thinks is on node k8s-agentpool1-40209065-vmss000001 before test
Sep 18 20:59:09.624: INFO: keyvault-flexvolume-428z4 from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 20:59:09.624: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
Sep 18 20:59:09.624: INFO: kube-proxy-cp8gq from kube-system started at 2019-09-18 19:32:35 +0000 UTC (1 container statuses recorded)
Sep 18 20:59:09.624: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 18 20:59:09.624: INFO: sonobuoy-e2e-job-326e453a1b3e45d6 from sonobuoy started at 2019-09-18 19:33:36 +0000 UTC (2 container statuses recorded)
Sep 18 20:59:09.624: INFO: 	Container e2e ready: true, restart count 0
Sep 18 20:59:09.624: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 18 20:59:09.624: INFO: azure-cni-networkmonitor-ppx7z from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 20:59:09.624: INFO: 	Container azure-cnms ready: true, restart count 0
Sep 18 20:59:09.624: INFO: blobfuse-flexvol-installer-bbdb6 from kube-system started at 2019-09-18 19:32:36 +0000 UTC (1 container statuses recorded)
Sep 18 20:59:09.624: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
Sep 18 20:59:09.624: INFO: azure-ip-masq-agent-62ddt from kube-system started at 2019-09-18 19:32:37 +0000 UTC (1 container statuses recorded)
Sep 18 20:59:09.624: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Sep 18 20:59:09.624: INFO: sonobuoy-systemd-logs-daemon-set-13153e5f2c4542ef-9l9xz from sonobuoy started at 2019-09-18 19:33:36 +0000 UTC (2 container statuses recorded)
Sep 18 20:59:09.624: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 18 20:59:09.624: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 18 20:59:09.624: INFO: 
Logging pods the kubelet thinks is on node k8s-agentpool1-40209065-vmss000002 before test
Sep 18 20:59:09.632: INFO: azure-cni-networkmonitor-86wjx from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 20:59:09.632: INFO: 	Container azure-cnms ready: true, restart count 0
Sep 18 20:59:09.632: INFO: blobfuse-flexvol-installer-n7wtj from kube-system started at 2019-09-18 19:32:36 +0000 UTC (1 container statuses recorded)
Sep 18 20:59:09.632: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
Sep 18 20:59:09.632: INFO: metrics-server-855b565c8f-rrds2 from kube-system started at 2019-09-18 19:32:37 +0000 UTC (1 container statuses recorded)
Sep 18 20:59:09.632: INFO: 	Container metrics-server ready: true, restart count 0
Sep 18 20:59:09.632: INFO: sonobuoy from sonobuoy started at 2019-09-18 19:33:29 +0000 UTC (1 container statuses recorded)
Sep 18 20:59:09.632: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 18 20:59:09.632: INFO: sonobuoy-systemd-logs-daemon-set-13153e5f2c4542ef-djfs2 from sonobuoy started at 2019-09-18 19:33:36 +0000 UTC (2 container statuses recorded)
Sep 18 20:59:09.632: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 18 20:59:09.632: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 18 20:59:09.632: INFO: keyvault-flexvolume-f9jq8 from kube-system started at 2019-09-18 19:32:35 +0000 UTC (1 container statuses recorded)
Sep 18 20:59:09.632: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
Sep 18 20:59:09.632: INFO: kube-proxy-vnxpl from kube-system started at 2019-09-18 19:32:35 +0000 UTC (1 container statuses recorded)
Sep 18 20:59:09.632: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 18 20:59:09.632: INFO: azure-ip-masq-agent-q7dfv from kube-system started at 2019-09-18 19:32:37 +0000 UTC (1 container statuses recorded)
Sep 18 20:59:09.632: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-915d5458-efa9-44b8-b3e3-d4c14052729d 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-915d5458-efa9-44b8-b3e3-d4c14052729d off the node k8s-agentpool1-40209065-vmss000002
STEP: verifying the node doesn't have the label kubernetes.io/e2e-915d5458-efa9-44b8-b3e3-d4c14052729d
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:59:17.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4629" for this suite.
Sep 18 20:59:35.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 20:59:35.836: INFO: namespace sched-pred-4629 deletion completed in 18.099030118s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
I0918 20:59:35.836890      14 request.go:706] Error in request: resource name may not be empty

â€¢ [SLOW TEST:26.461 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 20:59:35.842: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-4364
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4364
I0918 20:59:36.023485      14 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4364, replica count: 1
I0918 20:59:36.023596      14 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 20:59:36.023617      14 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 20:59:37.074085      14 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0918 20:59:38.074446      14 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0918 20:59:39.074684      14 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0918 20:59:39.074995      14 reflector.go:120] Starting reflector *v1.Endpoints (0s) from k8s.io/kubernetes/test/e2e/network/service_latency.go:323
I0918 20:59:39.075021      14 reflector.go:158] Listing and watching *v1.Endpoints from k8s.io/kubernetes/test/e2e/network/service_latency.go:323
Sep 18 20:59:39.195: INFO: Created: latency-svc-9dxbm
Sep 18 20:59:39.204: INFO: Got endpoints: latency-svc-9dxbm [28.995901ms]
Sep 18 20:59:39.236: INFO: Created: latency-svc-m2vwz
Sep 18 20:59:39.239: INFO: Got endpoints: latency-svc-m2vwz [34.890361ms]
Sep 18 20:59:39.243: INFO: Created: latency-svc-fzbq6
Sep 18 20:59:39.255: INFO: Got endpoints: latency-svc-fzbq6 [51.579847ms]
Sep 18 20:59:39.264: INFO: Created: latency-svc-bpjxf
Sep 18 20:59:39.277: INFO: Got endpoints: latency-svc-bpjxf [72.707002ms]
Sep 18 20:59:39.285: INFO: Created: latency-svc-ph665
Sep 18 20:59:39.293: INFO: Got endpoints: latency-svc-ph665 [87.305002ms]
Sep 18 20:59:39.309: INFO: Created: latency-svc-xrbdr
Sep 18 20:59:39.313: INFO: Got endpoints: latency-svc-xrbdr [107.666463ms]
Sep 18 20:59:39.326: INFO: Created: latency-svc-jtfh9
Sep 18 20:59:39.332: INFO: Got endpoints: latency-svc-jtfh9 [125.780239ms]
Sep 18 20:59:39.345: INFO: Created: latency-svc-kjkbx
Sep 18 20:59:39.352: INFO: Got endpoints: latency-svc-kjkbx [146.266799ms]
Sep 18 20:59:39.371: INFO: Created: latency-svc-blv49
Sep 18 20:59:39.390: INFO: Got endpoints: latency-svc-blv49 [183.707941ms]
Sep 18 20:59:39.394: INFO: Created: latency-svc-q5jfk
Sep 18 20:59:39.412: INFO: Got endpoints: latency-svc-q5jfk [206.300187ms]
Sep 18 20:59:39.413: INFO: Created: latency-svc-hq69b
Sep 18 20:59:39.422: INFO: Got endpoints: latency-svc-hq69b [216.166419ms]
Sep 18 20:59:39.428: INFO: Created: latency-svc-vb92c
Sep 18 20:59:39.442: INFO: Created: latency-svc-pp5d9
Sep 18 20:59:39.457: INFO: Got endpoints: latency-svc-vb92c [65.247553ms]
Sep 18 20:59:39.465: INFO: Got endpoints: latency-svc-pp5d9 [258.755028ms]
Sep 18 20:59:39.469: INFO: Created: latency-svc-lxjqz
Sep 18 20:59:39.484: INFO: Created: latency-svc-85f97
Sep 18 20:59:39.488: INFO: Got endpoints: latency-svc-lxjqz [281.85587ms]
Sep 18 20:59:39.509: INFO: Got endpoints: latency-svc-85f97 [302.684927ms]
Sep 18 20:59:39.524: INFO: Created: latency-svc-zhh47
Sep 18 20:59:39.524: INFO: Got endpoints: latency-svc-zhh47 [318.34462ms]
Sep 18 20:59:39.541: INFO: Created: latency-svc-tzlcd
Sep 18 20:59:39.553: INFO: Created: latency-svc-qnhgq
Sep 18 20:59:39.555: INFO: Got endpoints: latency-svc-tzlcd [348.95481ms]
Sep 18 20:59:39.566: INFO: Got endpoints: latency-svc-qnhgq [324.776076ms]
Sep 18 20:59:39.569: INFO: Created: latency-svc-f6xxh
Sep 18 20:59:39.590: INFO: Created: latency-svc-v46qk
Sep 18 20:59:39.590: INFO: Got endpoints: latency-svc-f6xxh [331.297831ms]
Sep 18 20:59:39.595: INFO: Got endpoints: latency-svc-v46qk [316.576531ms]
Sep 18 20:59:39.605: INFO: Created: latency-svc-x6t4p
Sep 18 20:59:39.621: INFO: Got endpoints: latency-svc-x6t4p [324.704476ms]
Sep 18 20:59:39.633: INFO: Created: latency-svc-4t9fh
Sep 18 20:59:39.637: INFO: Created: latency-svc-qd6rz
Sep 18 20:59:39.643: INFO: Got endpoints: latency-svc-4t9fh [330.034139ms]
Sep 18 20:59:39.650: INFO: Got endpoints: latency-svc-qd6rz [315.44234ms]
Sep 18 20:59:39.656: INFO: Created: latency-svc-t28bd
Sep 18 20:59:39.667: INFO: Created: latency-svc-lb6m2
Sep 18 20:59:39.675: INFO: Got endpoints: latency-svc-t28bd [322.835489ms]
Sep 18 20:59:39.683: INFO: Created: latency-svc-cxczd
Sep 18 20:59:39.696: INFO: Got endpoints: latency-svc-lb6m2 [284.066055ms]
Sep 18 20:59:39.707: INFO: Got endpoints: latency-svc-cxczd [280.652178ms]
Sep 18 20:59:39.708: INFO: Created: latency-svc-5xctn
Sep 18 20:59:39.717: INFO: Got endpoints: latency-svc-5xctn [259.272924ms]
Sep 18 20:59:39.742: INFO: Created: latency-svc-ktqtk
Sep 18 20:59:39.761: INFO: Created: latency-svc-t9g64
Sep 18 20:59:39.765: INFO: Got endpoints: latency-svc-ktqtk [299.080251ms]
Sep 18 20:59:39.786: INFO: Created: latency-svc-hdzvm
Sep 18 20:59:39.790: INFO: Got endpoints: latency-svc-t9g64 [300.874439ms]
Sep 18 20:59:39.798: INFO: Got endpoints: latency-svc-hdzvm [289.075721ms]
Sep 18 20:59:39.807: INFO: Created: latency-svc-2l794
Sep 18 20:59:39.819: INFO: Got endpoints: latency-svc-2l794 [294.93718ms]
Sep 18 20:59:39.823: INFO: Created: latency-svc-sc5z6
Sep 18 20:59:39.845: INFO: Created: latency-svc-h5892
Sep 18 20:59:39.849: INFO: Got endpoints: latency-svc-sc5z6 [292.116799ms]
Sep 18 20:59:39.865: INFO: Created: latency-svc-tr4x7
Sep 18 20:59:39.865: INFO: Got endpoints: latency-svc-h5892 [296.228471ms]
Sep 18 20:59:39.873: INFO: Got endpoints: latency-svc-tr4x7 [278.670991ms]
Sep 18 20:59:39.889: INFO: Created: latency-svc-nztl4
Sep 18 20:59:39.904: INFO: Got endpoints: latency-svc-nztl4 [313.560252ms]
Sep 18 20:59:39.911: INFO: Created: latency-svc-wkmlz
Sep 18 20:59:39.921: INFO: Got endpoints: latency-svc-wkmlz [300.368443ms]
Sep 18 20:59:39.945: INFO: Created: latency-svc-55r8f
Sep 18 20:59:39.966: INFO: Got endpoints: latency-svc-55r8f [322.159294ms]
Sep 18 20:59:39.971: INFO: Created: latency-svc-7dgx7
Sep 18 20:59:39.975: INFO: Got endpoints: latency-svc-7dgx7 [325.227072ms]
Sep 18 20:59:39.994: INFO: Created: latency-svc-pjk9q
Sep 18 20:59:40.017: INFO: Created: latency-svc-27w52
Sep 18 20:59:40.027: INFO: Got endpoints: latency-svc-27w52 [330.612035ms]
Sep 18 20:59:40.029: INFO: Got endpoints: latency-svc-pjk9q [351.532292ms]
Sep 18 20:59:40.036: INFO: Created: latency-svc-2pbtf
Sep 18 20:59:40.052: INFO: Got endpoints: latency-svc-2pbtf [344.52674ms]
Sep 18 20:59:40.065: INFO: Created: latency-svc-thjzz
Sep 18 20:59:40.076: INFO: Got endpoints: latency-svc-thjzz [359.18734ms]
Sep 18 20:59:40.077: INFO: Created: latency-svc-bvsjd
Sep 18 20:59:40.091: INFO: Created: latency-svc-7v8cs
Sep 18 20:59:40.096: INFO: Got endpoints: latency-svc-bvsjd [330.860134ms]
Sep 18 20:59:40.121: INFO: Created: latency-svc-qz47m
Sep 18 20:59:40.121: INFO: Got endpoints: latency-svc-7v8cs [331.273231ms]
Sep 18 20:59:40.130: INFO: Created: latency-svc-ckf85
Sep 18 20:59:40.130: INFO: Got endpoints: latency-svc-qz47m [332.097426ms]
Sep 18 20:59:40.149: INFO: Got endpoints: latency-svc-ckf85 [329.399045ms]
Sep 18 20:59:40.161: INFO: Created: latency-svc-nn6xv
Sep 18 20:59:40.184: INFO: Created: latency-svc-vdcp9
Sep 18 20:59:40.189: INFO: Got endpoints: latency-svc-nn6xv [339.629074ms]
Sep 18 20:59:40.202: INFO: Got endpoints: latency-svc-vdcp9 [336.998092ms]
Sep 18 20:59:40.208: INFO: Created: latency-svc-gj4bl
Sep 18 20:59:40.218: INFO: Created: latency-svc-cd7k2
Sep 18 20:59:40.225: INFO: Got endpoints: latency-svc-gj4bl [352.172588ms]
Sep 18 20:59:40.243: INFO: Got endpoints: latency-svc-cd7k2 [339.265976ms]
Sep 18 20:59:40.267: INFO: Created: latency-svc-wsw5v
Sep 18 20:59:40.272: INFO: Created: latency-svc-gkt4k
Sep 18 20:59:40.272: INFO: Created: latency-svc-z89ts
Sep 18 20:59:40.273: INFO: Got endpoints: latency-svc-wsw5v [351.151596ms]
Sep 18 20:59:40.300: INFO: Created: latency-svc-rnv27
Sep 18 20:59:40.305: INFO: Got endpoints: latency-svc-z89ts [339.249677ms]
Sep 18 20:59:40.306: INFO: Created: latency-svc-wkn9p
Sep 18 20:59:40.321: INFO: Created: latency-svc-bd2hw
Sep 18 20:59:40.342: INFO: Created: latency-svc-69g49
Sep 18 20:59:40.367: INFO: Got endpoints: latency-svc-gkt4k [391.28072ms]
Sep 18 20:59:40.370: INFO: Created: latency-svc-lmqgz
Sep 18 20:59:40.386: INFO: Created: latency-svc-fvbbx
Sep 18 20:59:40.405: INFO: Got endpoints: latency-svc-rnv27 [376.439822ms]
Sep 18 20:59:40.408: INFO: Created: latency-svc-9dnnd
Sep 18 20:59:40.441: INFO: Created: latency-svc-l4jd2
Sep 18 20:59:40.451: INFO: Created: latency-svc-66r44
Sep 18 20:59:40.465: INFO: Got endpoints: latency-svc-wkn9p [430.644251ms]
Sep 18 20:59:40.469: INFO: Created: latency-svc-xp7nq
Sep 18 20:59:40.492: INFO: Created: latency-svc-zmpcg
Sep 18 20:59:40.519: INFO: Got endpoints: latency-svc-bd2hw [466.783604ms]
Sep 18 20:59:40.522: INFO: Created: latency-svc-tcmzr
Sep 18 20:59:40.594: INFO: Got endpoints: latency-svc-69g49 [518.073052ms]
Sep 18 20:59:40.598: INFO: Created: latency-svc-xbsll
Sep 18 20:59:40.615: INFO: Got endpoints: latency-svc-lmqgz [519.062245ms]
Sep 18 20:59:40.618: INFO: Created: latency-svc-kvvtq
Sep 18 20:59:40.654: INFO: Got endpoints: latency-svc-fvbbx [532.921751ms]
Sep 18 20:59:40.658: INFO: Created: latency-svc-lzb9m
Sep 18 20:59:40.677: INFO: Created: latency-svc-hp7t8
Sep 18 20:59:40.689: INFO: Created: latency-svc-2mlvz
Sep 18 20:59:40.703: INFO: Created: latency-svc-97x8r
Sep 18 20:59:40.706: INFO: Got endpoints: latency-svc-9dnnd [575.279861ms]
Sep 18 20:59:40.724: INFO: Created: latency-svc-8plxp
Sep 18 20:59:40.747: INFO: Created: latency-svc-9vrzb
Sep 18 20:59:40.766: INFO: Got endpoints: latency-svc-l4jd2 [616.788177ms]
Sep 18 20:59:40.773: INFO: Created: latency-svc-zxmtr
Sep 18 20:59:40.781: INFO: Created: latency-svc-8lrpg
Sep 18 20:59:40.796: INFO: Created: latency-svc-5jljd
Sep 18 20:59:40.802: INFO: Got endpoints: latency-svc-66r44 [613.2235ms]
Sep 18 20:59:40.821: INFO: Created: latency-svc-nr8bm
Sep 18 20:59:40.853: INFO: Got endpoints: latency-svc-xp7nq [651.488039ms]
Sep 18 20:59:40.875: INFO: Created: latency-svc-6xwr8
Sep 18 20:59:40.903: INFO: Got endpoints: latency-svc-zmpcg [677.361161ms]
Sep 18 20:59:40.927: INFO: Created: latency-svc-dtmlf
Sep 18 20:59:40.954: INFO: Got endpoints: latency-svc-tcmzr [711.427428ms]
Sep 18 20:59:40.978: INFO: Created: latency-svc-mhfh8
Sep 18 20:59:41.001: INFO: Got endpoints: latency-svc-xbsll [728.74151ms]
Sep 18 20:59:41.027: INFO: Created: latency-svc-gg9cs
Sep 18 20:59:41.056: INFO: Got endpoints: latency-svc-kvvtq [750.412561ms]
Sep 18 20:59:41.096: INFO: Created: latency-svc-pdk6n
Sep 18 20:59:41.104: INFO: Got endpoints: latency-svc-lzb9m [732.184986ms]
Sep 18 20:59:41.133: INFO: Created: latency-svc-z6h85
Sep 18 20:59:41.151: INFO: Got endpoints: latency-svc-hp7t8 [745.890192ms]
Sep 18 20:59:41.173: INFO: Created: latency-svc-jsk6v
Sep 18 20:59:41.204: INFO: Got endpoints: latency-svc-2mlvz [738.780941ms]
Sep 18 20:59:41.225: INFO: Created: latency-svc-q6png
Sep 18 20:59:41.253: INFO: Got endpoints: latency-svc-97x8r [730.526997ms]
Sep 18 20:59:41.277: INFO: Created: latency-svc-tvj6z
Sep 18 20:59:41.318: INFO: Got endpoints: latency-svc-8plxp [722.545852ms]
Sep 18 20:59:41.349: INFO: Created: latency-svc-hln6l
Sep 18 20:59:41.356: INFO: Got endpoints: latency-svc-9vrzb [740.888027ms]
Sep 18 20:59:41.374: INFO: Created: latency-svc-rmjk9
Sep 18 20:59:41.404: INFO: Got endpoints: latency-svc-zxmtr [749.668767ms]
Sep 18 20:59:41.426: INFO: Created: latency-svc-457wt
Sep 18 20:59:41.452: INFO: Got endpoints: latency-svc-8lrpg [745.910693ms]
Sep 18 20:59:41.476: INFO: Created: latency-svc-qq5fk
Sep 18 20:59:41.513: INFO: Got endpoints: latency-svc-5jljd [747.316083ms]
Sep 18 20:59:41.543: INFO: Created: latency-svc-rjfjq
Sep 18 20:59:41.550: INFO: Got endpoints: latency-svc-nr8bm [748.035878ms]
Sep 18 20:59:41.581: INFO: Created: latency-svc-ldkz7
Sep 18 20:59:41.601: INFO: Got endpoints: latency-svc-6xwr8 [747.310883ms]
Sep 18 20:59:41.626: INFO: Created: latency-svc-fkcs4
Sep 18 20:59:41.651: INFO: Got endpoints: latency-svc-dtmlf [744.626602ms]
Sep 18 20:59:41.677: INFO: Created: latency-svc-vhzpv
Sep 18 20:59:41.705: INFO: Got endpoints: latency-svc-mhfh8 [750.959158ms]
Sep 18 20:59:41.736: INFO: Created: latency-svc-cbzxg
Sep 18 20:59:41.751: INFO: Got endpoints: latency-svc-gg9cs [744.385203ms]
Sep 18 20:59:41.774: INFO: Created: latency-svc-dz7qx
Sep 18 20:59:41.802: INFO: Got endpoints: latency-svc-pdk6n [746.167591ms]
Sep 18 20:59:41.825: INFO: Created: latency-svc-4wvsl
Sep 18 20:59:41.852: INFO: Got endpoints: latency-svc-z6h85 [748.341576ms]
Sep 18 20:59:41.873: INFO: Created: latency-svc-cc72x
Sep 18 20:59:41.920: INFO: Got endpoints: latency-svc-jsk6v [763.693771ms]
Sep 18 20:59:41.938: INFO: Created: latency-svc-hmm7l
Sep 18 20:59:41.953: INFO: Got endpoints: latency-svc-q6png [744.7802ms]
Sep 18 20:59:41.977: INFO: Created: latency-svc-jgk2h
Sep 18 20:59:42.004: INFO: Got endpoints: latency-svc-tvj6z [751.354155ms]
Sep 18 20:59:42.026: INFO: Created: latency-svc-r7hxq
Sep 18 20:59:42.050: INFO: Got endpoints: latency-svc-hln6l [731.865889ms]
Sep 18 20:59:42.077: INFO: Created: latency-svc-qktq2
Sep 18 20:59:42.101: INFO: Got endpoints: latency-svc-rmjk9 [745.316597ms]
Sep 18 20:59:42.124: INFO: Created: latency-svc-2xvc6
Sep 18 20:59:42.153: INFO: Got endpoints: latency-svc-457wt [748.971772ms]
Sep 18 20:59:42.176: INFO: Created: latency-svc-cznfv
Sep 18 20:59:42.201: INFO: Got endpoints: latency-svc-qq5fk [741.813721ms]
Sep 18 20:59:42.223: INFO: Created: latency-svc-hqrmk
Sep 18 20:59:42.252: INFO: Got endpoints: latency-svc-rjfjq [738.551443ms]
Sep 18 20:59:42.275: INFO: Created: latency-svc-k88cv
Sep 18 20:59:42.302: INFO: Got endpoints: latency-svc-ldkz7 [751.266756ms]
Sep 18 20:59:42.330: INFO: Created: latency-svc-s577h
Sep 18 20:59:42.352: INFO: Got endpoints: latency-svc-fkcs4 [747.69978ms]
Sep 18 20:59:42.374: INFO: Created: latency-svc-sz5lt
Sep 18 20:59:42.403: INFO: Got endpoints: latency-svc-vhzpv [751.578054ms]
Sep 18 20:59:42.427: INFO: Created: latency-svc-zqjvl
Sep 18 20:59:42.451: INFO: Got endpoints: latency-svc-cbzxg [746.121691ms]
Sep 18 20:59:42.476: INFO: Created: latency-svc-khfwg
Sep 18 20:59:42.501: INFO: Got endpoints: latency-svc-dz7qx [746.666488ms]
Sep 18 20:59:42.526: INFO: Created: latency-svc-6phbs
Sep 18 20:59:42.551: INFO: Got endpoints: latency-svc-4wvsl [746.143892ms]
Sep 18 20:59:42.569: INFO: Created: latency-svc-8hkxc
Sep 18 20:59:42.608: INFO: Got endpoints: latency-svc-cc72x [754.189136ms]
Sep 18 20:59:42.626: INFO: Created: latency-svc-k2bjf
Sep 18 20:59:42.652: INFO: Got endpoints: latency-svc-hmm7l [732.118987ms]
Sep 18 20:59:42.670: INFO: Created: latency-svc-j49cc
Sep 18 20:59:42.708: INFO: Got endpoints: latency-svc-jgk2h [750.020266ms]
Sep 18 20:59:42.756: INFO: Created: latency-svc-s9rbh
Sep 18 20:59:42.761: INFO: Got endpoints: latency-svc-r7hxq [752.397149ms]
Sep 18 20:59:42.788: INFO: Created: latency-svc-fdd74
Sep 18 20:59:42.803: INFO: Got endpoints: latency-svc-qktq2 [750.138865ms]
Sep 18 20:59:42.824: INFO: Created: latency-svc-d2kxm
Sep 18 20:59:42.852: INFO: Got endpoints: latency-svc-2xvc6 [751.298956ms]
Sep 18 20:59:42.881: INFO: Created: latency-svc-4hplr
Sep 18 20:59:42.902: INFO: Got endpoints: latency-svc-cznfv [749.25027ms]
Sep 18 20:59:42.935: INFO: Created: latency-svc-d5p9p
Sep 18 20:59:42.952: INFO: Got endpoints: latency-svc-hqrmk [750.153465ms]
Sep 18 20:59:42.972: INFO: Created: latency-svc-2wkg6
Sep 18 20:59:43.001: INFO: Got endpoints: latency-svc-k88cv [744.918601ms]
Sep 18 20:59:43.029: INFO: Created: latency-svc-lvh2g
Sep 18 20:59:43.054: INFO: Got endpoints: latency-svc-s577h [747.216584ms]
Sep 18 20:59:43.073: INFO: Created: latency-svc-5b46j
Sep 18 20:59:43.117: INFO: Got endpoints: latency-svc-sz5lt [760.460794ms]
Sep 18 20:59:43.142: INFO: Created: latency-svc-vmsdc
Sep 18 20:59:43.159: INFO: Got endpoints: latency-svc-zqjvl [750.137264ms]
Sep 18 20:59:43.190: INFO: Created: latency-svc-zq2fc
Sep 18 20:59:43.199: INFO: Got endpoints: latency-svc-khfwg [743.813908ms]
Sep 18 20:59:43.237: INFO: Created: latency-svc-4wpkr
Sep 18 20:59:43.251: INFO: Got endpoints: latency-svc-6phbs [748.413577ms]
Sep 18 20:59:43.271: INFO: Created: latency-svc-xxmtv
Sep 18 20:59:43.304: INFO: Got endpoints: latency-svc-8hkxc [751.855053ms]
Sep 18 20:59:43.325: INFO: Created: latency-svc-l5ltv
Sep 18 20:59:43.353: INFO: Got endpoints: latency-svc-k2bjf [745.496997ms]
Sep 18 20:59:43.387: INFO: Created: latency-svc-97djk
Sep 18 20:59:43.406: INFO: Got endpoints: latency-svc-j49cc [752.936246ms]
Sep 18 20:59:43.462: INFO: Got endpoints: latency-svc-s9rbh [753.410542ms]
Sep 18 20:59:43.477: INFO: Created: latency-svc-djm6f
Sep 18 20:59:43.495: INFO: Created: latency-svc-w4884
Sep 18 20:59:43.501: INFO: Got endpoints: latency-svc-fdd74 [739.786235ms]
Sep 18 20:59:43.522: INFO: Created: latency-svc-nb7s5
Sep 18 20:59:43.552: INFO: Got endpoints: latency-svc-d2kxm [744.902501ms]
Sep 18 20:59:43.597: INFO: Created: latency-svc-lxqh6
Sep 18 20:59:43.605: INFO: Got endpoints: latency-svc-4hplr [746.813487ms]
Sep 18 20:59:43.624: INFO: Created: latency-svc-fkm88
Sep 18 20:59:43.650: INFO: Got endpoints: latency-svc-d5p9p [744.583703ms]
Sep 18 20:59:43.673: INFO: Created: latency-svc-z76vl
Sep 18 20:59:43.701: INFO: Got endpoints: latency-svc-2wkg6 [749.226072ms]
Sep 18 20:59:43.729: INFO: Created: latency-svc-x46ds
Sep 18 20:59:43.752: INFO: Got endpoints: latency-svc-lvh2g [750.86926ms]
Sep 18 20:59:43.773: INFO: Created: latency-svc-sh65q
Sep 18 20:59:43.800: INFO: Got endpoints: latency-svc-5b46j [746.121293ms]
Sep 18 20:59:43.824: INFO: Created: latency-svc-mr84h
Sep 18 20:59:43.853: INFO: Got endpoints: latency-svc-vmsdc [732.020889ms]
Sep 18 20:59:43.876: INFO: Created: latency-svc-s6d2h
Sep 18 20:59:43.902: INFO: Got endpoints: latency-svc-zq2fc [740.435531ms]
Sep 18 20:59:43.929: INFO: Created: latency-svc-k2jbs
Sep 18 20:59:43.953: INFO: Got endpoints: latency-svc-4wpkr [740.355032ms]
Sep 18 20:59:43.973: INFO: Created: latency-svc-k6hwk
Sep 18 20:59:44.001: INFO: Got endpoints: latency-svc-xxmtv [749.152871ms]
Sep 18 20:59:44.020: INFO: Created: latency-svc-2qf68
Sep 18 20:59:44.055: INFO: Got endpoints: latency-svc-l5ltv [750.703561ms]
Sep 18 20:59:44.074: INFO: Created: latency-svc-8lbql
Sep 18 20:59:44.100: INFO: Got endpoints: latency-svc-97djk [742.369118ms]
Sep 18 20:59:44.124: INFO: Created: latency-svc-phpcw
Sep 18 20:59:44.153: INFO: Got endpoints: latency-svc-djm6f [746.46429ms]
Sep 18 20:59:44.178: INFO: Created: latency-svc-kkzt4
Sep 18 20:59:44.200: INFO: Got endpoints: latency-svc-w4884 [735.333667ms]
Sep 18 20:59:44.223: INFO: Created: latency-svc-z9q57
Sep 18 20:59:44.260: INFO: Got endpoints: latency-svc-nb7s5 [755.26303ms]
Sep 18 20:59:44.277: INFO: Created: latency-svc-nxz7p
Sep 18 20:59:44.300: INFO: Got endpoints: latency-svc-lxqh6 [745.676996ms]
Sep 18 20:59:44.317: INFO: Created: latency-svc-5kc2t
Sep 18 20:59:44.351: INFO: Got endpoints: latency-svc-fkm88 [745.286399ms]
Sep 18 20:59:44.374: INFO: Created: latency-svc-j8mjh
Sep 18 20:59:44.404: INFO: Got endpoints: latency-svc-z76vl [754.095438ms]
Sep 18 20:59:44.490: INFO: Created: latency-svc-hl6c8
Sep 18 20:59:44.498: INFO: Got endpoints: latency-svc-x46ds [797.063245ms]
Sep 18 20:59:44.526: INFO: Got endpoints: latency-svc-sh65q [772.612812ms]
Sep 18 20:59:44.529: INFO: Created: latency-svc-plfbq
Sep 18 20:59:44.547: INFO: Created: latency-svc-lkhvv
Sep 18 20:59:44.554: INFO: Got endpoints: latency-svc-mr84h [753.093145ms]
Sep 18 20:59:44.574: INFO: Created: latency-svc-pd5lc
Sep 18 20:59:44.604: INFO: Got endpoints: latency-svc-s6d2h [746.686689ms]
Sep 18 20:59:44.621: INFO: Created: latency-svc-5chrz
Sep 18 20:59:44.652: INFO: Got endpoints: latency-svc-k2jbs [749.672869ms]
Sep 18 20:59:44.676: INFO: Created: latency-svc-lxbgl
Sep 18 20:59:44.701: INFO: Got endpoints: latency-svc-k6hwk [747.621783ms]
Sep 18 20:59:44.719: INFO: Created: latency-svc-j6s5j
Sep 18 20:59:44.752: INFO: Got endpoints: latency-svc-2qf68 [750.95016ms]
Sep 18 20:59:44.774: INFO: Created: latency-svc-p7vtf
Sep 18 20:59:44.854: INFO: Got endpoints: latency-svc-8lbql [799.481428ms]
Sep 18 20:59:44.875: INFO: Got endpoints: latency-svc-phpcw [774.062402ms]
Sep 18 20:59:44.883: INFO: Created: latency-svc-mdvq5
Sep 18 20:59:44.892: INFO: Created: latency-svc-ccb5f
Sep 18 20:59:44.902: INFO: Got endpoints: latency-svc-kkzt4 [749.465071ms]
Sep 18 20:59:44.934: INFO: Created: latency-svc-grb2z
Sep 18 20:59:44.953: INFO: Got endpoints: latency-svc-z9q57 [752.873047ms]
Sep 18 20:59:44.973: INFO: Created: latency-svc-pffzc
Sep 18 20:59:45.002: INFO: Got endpoints: latency-svc-nxz7p [741.949922ms]
Sep 18 20:59:45.022: INFO: Created: latency-svc-xfgnc
Sep 18 20:59:45.053: INFO: Got endpoints: latency-svc-5kc2t [752.858447ms]
Sep 18 20:59:45.074: INFO: Created: latency-svc-l2htj
Sep 18 20:59:45.102: INFO: Got endpoints: latency-svc-j8mjh [751.597555ms]
Sep 18 20:59:45.120: INFO: Created: latency-svc-plfkd
Sep 18 20:59:45.155: INFO: Got endpoints: latency-svc-hl6c8 [750.215565ms]
Sep 18 20:59:45.195: INFO: Created: latency-svc-xbbkv
Sep 18 20:59:45.229: INFO: Got endpoints: latency-svc-plfbq [730.668299ms]
Sep 18 20:59:45.267: INFO: Created: latency-svc-lk5qh
Sep 18 20:59:45.272: INFO: Got endpoints: latency-svc-lkhvv [745.245099ms]
Sep 18 20:59:45.305: INFO: Created: latency-svc-n2pz4
Sep 18 20:59:45.310: INFO: Got endpoints: latency-svc-pd5lc [755.821627ms]
Sep 18 20:59:45.343: INFO: Created: latency-svc-swcz5
Sep 18 20:59:45.352: INFO: Got endpoints: latency-svc-5chrz [748.13078ms]
Sep 18 20:59:45.369: INFO: Created: latency-svc-gqq8m
Sep 18 20:59:45.411: INFO: Got endpoints: latency-svc-lxbgl [759.360103ms]
Sep 18 20:59:45.434: INFO: Created: latency-svc-gffgc
Sep 18 20:59:45.461: INFO: Got endpoints: latency-svc-j6s5j [760.303696ms]
Sep 18 20:59:45.482: INFO: Created: latency-svc-8cb2f
Sep 18 20:59:45.534: INFO: Got endpoints: latency-svc-p7vtf [782.386145ms]
Sep 18 20:59:45.553: INFO: Created: latency-svc-f58ct
Sep 18 20:59:45.561: INFO: Got endpoints: latency-svc-mdvq5 [706.200567ms]
Sep 18 20:59:45.585: INFO: Created: latency-svc-496ld
Sep 18 20:59:45.608: INFO: Got endpoints: latency-svc-ccb5f [733.108183ms]
Sep 18 20:59:45.629: INFO: Created: latency-svc-87tzg
Sep 18 20:59:45.656: INFO: Got endpoints: latency-svc-grb2z [754.020639ms]
Sep 18 20:59:45.690: INFO: Created: latency-svc-w7x8q
Sep 18 20:59:45.715: INFO: Got endpoints: latency-svc-pffzc [759.119005ms]
Sep 18 20:59:45.733: INFO: Created: latency-svc-47r2x
Sep 18 20:59:45.750: INFO: Got endpoints: latency-svc-xfgnc [748.723877ms]
Sep 18 20:59:45.771: INFO: Created: latency-svc-d28fv
Sep 18 20:59:45.807: INFO: Got endpoints: latency-svc-l2htj [753.718941ms]
Sep 18 20:59:45.826: INFO: Created: latency-svc-c56p9
Sep 18 20:59:45.853: INFO: Got endpoints: latency-svc-plfkd [750.731962ms]
Sep 18 20:59:45.871: INFO: Created: latency-svc-76bw2
Sep 18 20:59:45.904: INFO: Got endpoints: latency-svc-xbbkv [748.759276ms]
Sep 18 20:59:45.927: INFO: Created: latency-svc-czb62
Sep 18 20:59:45.958: INFO: Got endpoints: latency-svc-lk5qh [729.520508ms]
Sep 18 20:59:45.979: INFO: Created: latency-svc-kv2qv
Sep 18 20:59:46.002: INFO: Got endpoints: latency-svc-n2pz4 [729.614207ms]
Sep 18 20:59:46.025: INFO: Created: latency-svc-2d2pt
Sep 18 20:59:46.054: INFO: Got endpoints: latency-svc-swcz5 [744.105208ms]
Sep 18 20:59:46.074: INFO: Created: latency-svc-sc8nh
Sep 18 20:59:46.099: INFO: Got endpoints: latency-svc-gqq8m [747.374486ms]
Sep 18 20:59:46.120: INFO: Created: latency-svc-pq9d5
Sep 18 20:59:46.155: INFO: Got endpoints: latency-svc-gffgc [743.69671ms]
Sep 18 20:59:46.173: INFO: Created: latency-svc-5zkn2
Sep 18 20:59:46.211: INFO: Got endpoints: latency-svc-8cb2f [749.379072ms]
Sep 18 20:59:46.252: INFO: Created: latency-svc-6tktn
Sep 18 20:59:46.259: INFO: Got endpoints: latency-svc-f58ct [724.85704ms]
Sep 18 20:59:46.284: INFO: Created: latency-svc-nlzqm
Sep 18 20:59:46.299: INFO: Got endpoints: latency-svc-496ld [738.416347ms]
Sep 18 20:59:46.318: INFO: Created: latency-svc-4zlfg
Sep 18 20:59:46.351: INFO: Got endpoints: latency-svc-87tzg [743.069615ms]
Sep 18 20:59:46.375: INFO: Created: latency-svc-rjg6j
Sep 18 20:59:46.403: INFO: Got endpoints: latency-svc-w7x8q [747.211287ms]
Sep 18 20:59:46.425: INFO: Created: latency-svc-q9g45
Sep 18 20:59:46.452: INFO: Got endpoints: latency-svc-47r2x [736.923057ms]
Sep 18 20:59:46.479: INFO: Created: latency-svc-gnsn4
Sep 18 20:59:46.502: INFO: Got endpoints: latency-svc-d28fv [751.498058ms]
Sep 18 20:59:46.525: INFO: Created: latency-svc-47ck4
Sep 18 20:59:46.556: INFO: Got endpoints: latency-svc-c56p9 [748.683777ms]
Sep 18 20:59:46.579: INFO: Created: latency-svc-w9znc
Sep 18 20:59:46.600: INFO: Got endpoints: latency-svc-76bw2 [746.79479ms]
Sep 18 20:59:46.625: INFO: Created: latency-svc-6q9d2
Sep 18 20:59:46.657: INFO: Got endpoints: latency-svc-czb62 [753.358245ms]
Sep 18 20:59:46.681: INFO: Created: latency-svc-dn688
Sep 18 20:59:46.712: INFO: Got endpoints: latency-svc-kv2qv [753.496644ms]
Sep 18 20:59:46.738: INFO: Created: latency-svc-rsvbd
Sep 18 20:59:46.757: INFO: Got endpoints: latency-svc-2d2pt [755.044833ms]
Sep 18 20:59:46.774: INFO: Created: latency-svc-vszl4
Sep 18 20:59:46.803: INFO: Got endpoints: latency-svc-sc8nh [748.992175ms]
Sep 18 20:59:46.826: INFO: Created: latency-svc-dnb28
Sep 18 20:59:46.853: INFO: Got endpoints: latency-svc-pq9d5 [753.188747ms]
Sep 18 20:59:46.885: INFO: Created: latency-svc-7ks67
Sep 18 20:59:46.902: INFO: Got endpoints: latency-svc-5zkn2 [746.982588ms]
Sep 18 20:59:46.947: INFO: Created: latency-svc-s2j8p
Sep 18 20:59:46.966: INFO: Got endpoints: latency-svc-6tktn [751.298559ms]
Sep 18 20:59:46.989: INFO: Created: latency-svc-8mgbp
Sep 18 20:59:47.001: INFO: Got endpoints: latency-svc-nlzqm [741.728025ms]
Sep 18 20:59:47.025: INFO: Created: latency-svc-kn8bx
Sep 18 20:59:47.057: INFO: Got endpoints: latency-svc-4zlfg [757.391618ms]
Sep 18 20:59:47.125: INFO: Got endpoints: latency-svc-rjg6j [774.168403ms]
Sep 18 20:59:47.151: INFO: Got endpoints: latency-svc-q9g45 [745.433ms]
Sep 18 20:59:47.202: INFO: Got endpoints: latency-svc-gnsn4 [749.844469ms]
Sep 18 20:59:47.254: INFO: Got endpoints: latency-svc-47ck4 [748.656977ms]
Sep 18 20:59:47.301: INFO: Got endpoints: latency-svc-w9znc [743.090415ms]
Sep 18 20:59:47.353: INFO: Got endpoints: latency-svc-6q9d2 [747.963683ms]
Sep 18 20:59:47.403: INFO: Got endpoints: latency-svc-dn688 [744.599206ms]
Sep 18 20:59:47.450: INFO: Got endpoints: latency-svc-rsvbd [733.909579ms]
Sep 18 20:59:47.503: INFO: Got endpoints: latency-svc-vszl4 [746.331494ms]
Sep 18 20:59:47.552: INFO: Got endpoints: latency-svc-dnb28 [747.947482ms]
Sep 18 20:59:47.601: INFO: Got endpoints: latency-svc-7ks67 [743.545113ms]
Sep 18 20:59:47.655: INFO: Got endpoints: latency-svc-s2j8p [745.560399ms]
Sep 18 20:59:47.709: INFO: Got endpoints: latency-svc-8mgbp [738.21005ms]
Sep 18 20:59:47.752: INFO: Got endpoints: latency-svc-kn8bx [746.609192ms]
Sep 18 20:59:47.756: INFO: Latencies: [34.890361ms 51.579847ms 65.247553ms 72.707002ms 87.305002ms 107.666463ms 125.780239ms 146.266799ms 183.707941ms 206.300187ms 216.166419ms 258.755028ms 259.272924ms 278.670991ms 280.652178ms 281.85587ms 284.066055ms 289.075721ms 292.116799ms 294.93718ms 296.228471ms 299.080251ms 300.368443ms 300.874439ms 302.684927ms 313.560252ms 315.44234ms 316.576531ms 318.34462ms 322.159294ms 322.835489ms 324.704476ms 324.776076ms 325.227072ms 329.399045ms 330.034139ms 330.612035ms 330.860134ms 331.273231ms 331.297831ms 332.097426ms 336.998092ms 339.249677ms 339.265976ms 339.629074ms 344.52674ms 348.95481ms 351.151596ms 351.532292ms 352.172588ms 359.18734ms 376.439822ms 391.28072ms 430.644251ms 466.783604ms 518.073052ms 519.062245ms 532.921751ms 575.279861ms 613.2235ms 616.788177ms 651.488039ms 677.361161ms 706.200567ms 711.427428ms 722.545852ms 724.85704ms 728.74151ms 729.520508ms 729.614207ms 730.526997ms 730.668299ms 731.865889ms 732.020889ms 732.118987ms 732.184986ms 733.108183ms 733.909579ms 735.333667ms 736.923057ms 738.21005ms 738.416347ms 738.551443ms 738.780941ms 739.786235ms 740.355032ms 740.435531ms 740.888027ms 741.728025ms 741.813721ms 741.949922ms 742.369118ms 743.069615ms 743.090415ms 743.545113ms 743.69671ms 743.813908ms 744.105208ms 744.385203ms 744.583703ms 744.599206ms 744.626602ms 744.7802ms 744.902501ms 744.918601ms 745.245099ms 745.286399ms 745.316597ms 745.433ms 745.496997ms 745.560399ms 745.676996ms 745.890192ms 745.910693ms 746.121293ms 746.121691ms 746.143892ms 746.167591ms 746.331494ms 746.46429ms 746.609192ms 746.666488ms 746.686689ms 746.79479ms 746.813487ms 746.982588ms 747.211287ms 747.216584ms 747.310883ms 747.316083ms 747.374486ms 747.621783ms 747.69978ms 747.947482ms 747.963683ms 748.035878ms 748.13078ms 748.341576ms 748.413577ms 748.656977ms 748.683777ms 748.723877ms 748.759276ms 748.971772ms 748.992175ms 749.152871ms 749.226072ms 749.25027ms 749.379072ms 749.465071ms 749.668767ms 749.672869ms 749.844469ms 750.020266ms 750.137264ms 750.138865ms 750.153465ms 750.215565ms 750.412561ms 750.703561ms 750.731962ms 750.86926ms 750.95016ms 750.959158ms 751.266756ms 751.298559ms 751.298956ms 751.354155ms 751.498058ms 751.578054ms 751.597555ms 751.855053ms 752.397149ms 752.858447ms 752.873047ms 752.936246ms 753.093145ms 753.188747ms 753.358245ms 753.410542ms 753.496644ms 753.718941ms 754.020639ms 754.095438ms 754.189136ms 755.044833ms 755.26303ms 755.821627ms 757.391618ms 759.119005ms 759.360103ms 760.303696ms 760.460794ms 763.693771ms 772.612812ms 774.062402ms 774.168403ms 782.386145ms 797.063245ms 799.481428ms]
Sep 18 20:59:47.757: INFO: 50 %ile: 744.599206ms
Sep 18 20:59:47.757: INFO: 90 %ile: 753.496644ms
Sep 18 20:59:47.758: INFO: 99 %ile: 797.063245ms
Sep 18 20:59:47.758: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 20:59:47.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4364" for this suite.
Sep 18 21:00:01.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:00:01.869: INFO: namespace svc-latency-4364 deletion completed in 14.10095132s

â€¢ [SLOW TEST:26.029 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:00:01.869: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9793
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 18 21:00:02.317: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 18 21:00:04.329: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704437202, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704437202, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704437202, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704437202, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 18 21:00:07.356: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Sep 18 21:00:08.356: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Sep 18 21:00:09.356: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Sep 18 21:00:10.356: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Sep 18 21:00:11.356: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:00:21.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9793" for this suite.
Sep 18 21:00:27.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:00:27.633: INFO: namespace webhook-9793 deletion completed in 6.126110389s
STEP: Destroying namespace "webhook-9793-markers" for this suite.
Sep 18 21:00:33.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:00:33.740: INFO: namespace webhook-9793-markers deletion completed in 6.106291456s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:31.890 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:00:33.760: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9118
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 21:00:33.921: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:00:37.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9118" for this suite.
Sep 18 21:01:26.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:01:26.103: INFO: namespace pods-9118 deletion completed in 48.121037948s

â€¢ [SLOW TEST:52.343 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:01:26.107: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5948
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 18 21:01:26.744: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 18 21:01:28.753: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704437286, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704437286, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704437286, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704437286, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 18 21:01:31.777: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:01:31.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5948" for this suite.
Sep 18 21:01:37.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:01:37.945: INFO: namespace webhook-5948 deletion completed in 6.122210762s
STEP: Destroying namespace "webhook-5948-markers" for this suite.
Sep 18 21:01:43.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:01:44.045: INFO: namespace webhook-5948-markers deletion completed in 6.099795841s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:17.954 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:01:44.068: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-955
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-blh9
STEP: Creating a pod to test atomic-volume-subpath
Sep 18 21:01:44.247: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-blh9" in namespace "subpath-955" to be "success or failure"
Sep 18 21:01:44.250: INFO: Pod "pod-subpath-test-downwardapi-blh9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.173178ms
Sep 18 21:01:46.255: INFO: Pod "pod-subpath-test-downwardapi-blh9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008034056s
Sep 18 21:01:48.259: INFO: Pod "pod-subpath-test-downwardapi-blh9": Phase="Running", Reason="", readiness=true. Elapsed: 4.011950944s
Sep 18 21:01:50.263: INFO: Pod "pod-subpath-test-downwardapi-blh9": Phase="Running", Reason="", readiness=true. Elapsed: 6.016256732s
Sep 18 21:01:52.267: INFO: Pod "pod-subpath-test-downwardapi-blh9": Phase="Running", Reason="", readiness=true. Elapsed: 8.020141925s
Sep 18 21:01:54.271: INFO: Pod "pod-subpath-test-downwardapi-blh9": Phase="Running", Reason="", readiness=true. Elapsed: 10.024639417s
Sep 18 21:01:56.276: INFO: Pod "pod-subpath-test-downwardapi-blh9": Phase="Running", Reason="", readiness=true. Elapsed: 12.028969114s
Sep 18 21:01:58.279: INFO: Pod "pod-subpath-test-downwardapi-blh9": Phase="Running", Reason="", readiness=true. Elapsed: 14.032137521s
Sep 18 21:02:00.283: INFO: Pod "pod-subpath-test-downwardapi-blh9": Phase="Running", Reason="", readiness=true. Elapsed: 16.036580022s
Sep 18 21:02:02.287: INFO: Pod "pod-subpath-test-downwardapi-blh9": Phase="Running", Reason="", readiness=true. Elapsed: 18.040620129s
Sep 18 21:02:04.291: INFO: Pod "pod-subpath-test-downwardapi-blh9": Phase="Running", Reason="", readiness=true. Elapsed: 20.044865037s
Sep 18 21:02:06.295: INFO: Pod "pod-subpath-test-downwardapi-blh9": Phase="Running", Reason="", readiness=true. Elapsed: 22.048678451s
Sep 18 21:02:08.302: INFO: Pod "pod-subpath-test-downwardapi-blh9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.054993451s
STEP: Saw pod success
Sep 18 21:02:08.302: INFO: Pod "pod-subpath-test-downwardapi-blh9" satisfied condition "success or failure"
Sep 18 21:02:08.307: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-subpath-test-downwardapi-blh9 container test-container-subpath-downwardapi-blh9: <nil>
STEP: delete the pod
Sep 18 21:02:08.363: INFO: Waiting for pod pod-subpath-test-downwardapi-blh9 to disappear
Sep 18 21:02:08.365: INFO: Pod pod-subpath-test-downwardapi-blh9 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-blh9
Sep 18 21:02:08.365: INFO: Deleting pod "pod-subpath-test-downwardapi-blh9" in namespace "subpath-955"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:02:08.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-955" for this suite.
Sep 18 21:02:14.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:02:14.478: INFO: namespace subpath-955 deletion completed in 6.105086138s

â€¢ [SLOW TEST:30.410 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:02:14.478: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1629
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 21:02:14.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 create -f - --namespace=kubectl-1629'
Sep 18 21:02:18.519: INFO: stderr: ""
Sep 18 21:02:18.519: INFO: stdout: "replicationcontroller/redis-master created\n"
Sep 18 21:02:18.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 create -f - --namespace=kubectl-1629'
Sep 18 21:02:19.722: INFO: stderr: ""
Sep 18 21:02:19.722: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep 18 21:02:20.726: INFO: Selector matched 1 pods for map[app:redis]
Sep 18 21:02:20.726: INFO: Found 0 / 1
Sep 18 21:02:21.727: INFO: Selector matched 1 pods for map[app:redis]
Sep 18 21:02:21.727: INFO: Found 1 / 1
Sep 18 21:02:21.727: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 18 21:02:21.730: INFO: Selector matched 1 pods for map[app:redis]
Sep 18 21:02:21.730: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 18 21:02:21.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 describe pod redis-master-xlv86 --namespace=kubectl-1629'
Sep 18 21:02:21.844: INFO: stderr: ""
Sep 18 21:02:21.844: INFO: stdout: "Name:         redis-master-xlv86\nNamespace:    kubectl-1629\nPriority:     0\nNode:         k8s-agentpool1-40209065-vmss000002/10.240.0.66\nStart Time:   Wed, 18 Sep 2019 21:02:18 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           10.240.0.69\nIPs:\n  IP:           10.240.0.69\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://c7569338f778bf90958d44cb8bfdf63b357828ff2bcb35141dcf81ab7d1b79d4\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 18 Sep 2019 21:02:20 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jxfmc (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-jxfmc:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-jxfmc\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                                         Message\n  ----    ------     ----       ----                                         -------\n  Normal  Scheduled  <unknown>  default-scheduler                            Successfully assigned kubectl-1629/redis-master-xlv86 to k8s-agentpool1-40209065-vmss000002\n  Normal  Pulled     2s         kubelet, k8s-agentpool1-40209065-vmss000002  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    1s         kubelet, k8s-agentpool1-40209065-vmss000002  Created container redis-master\n  Normal  Started    1s         kubelet, k8s-agentpool1-40209065-vmss000002  Started container redis-master\n"
Sep 18 21:02:21.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 describe rc redis-master --namespace=kubectl-1629'
Sep 18 21:02:21.973: INFO: stderr: ""
Sep 18 21:02:21.973: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-1629\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-xlv86\n"
Sep 18 21:02:21.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 describe service redis-master --namespace=kubectl-1629'
Sep 18 21:02:22.086: INFO: stderr: ""
Sep 18 21:02:22.086: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-1629\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.0.196.183\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.240.0.69:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep 18 21:02:22.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 describe node k8s-agentpool1-40209065-vmss000000'
Sep 18 21:02:22.220: INFO: stderr: ""
Sep 18 21:02:22.220: INFO: stdout: "Name:               k8s-agentpool1-40209065-vmss000000\nRoles:              agent\nLabels:             agentpool=agentpool1\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=Standard_D2_v3\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eastus\n                    failure-domain.beta.kubernetes.io/zone=0\n                    foo=bar\n                    kubernetes.azure.com/cluster=kubernetes-eastus-91643\n                    kubernetes.azure.com/role=agent\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-agentpool1-40209065-vmss000000\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=agent\n                    node-role.kubernetes.io/agent=\n                    storageprofile=managed\n                    storagetier=Standard_LRS\nAnnotations:        foo: bar\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 18 Sep 2019 19:32:15 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 18 Sep 2019 21:01:31 +0000   Wed, 18 Sep 2019 19:32:06 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 18 Sep 2019 21:01:31 +0000   Wed, 18 Sep 2019 19:32:06 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 18 Sep 2019 21:01:31 +0000   Wed, 18 Sep 2019 19:32:06 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 18 Sep 2019 21:01:31 +0000   Wed, 18 Sep 2019 19:32:06 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  Hostname:    k8s-agentpool1-40209065-vmss000000\n  InternalIP:  10.240.0.4\nCapacity:\n attachable-volumes-azure-disk:  4\n cpu:                            2\n ephemeral-storage:              30428648Ki\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         8145856Ki\n pods:                           30\nAllocatable:\n attachable-volumes-azure-disk:  4\n cpu:                            2\n ephemeral-storage:              28043041951\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         7377856Ki\n pods:                           30\nSystem Info:\n Machine ID:                 c747e38d0506492e8513d428a115842d\n System UUID:                60B8A3C8-AF3B-534E-80CA-F840105DEA0C\n Boot ID:                    3853da53-d76e-4a17-927a-c0bf7e168817\n Kernel Version:             4.15.0-1057-azure\n OS Image:                   Ubuntu 16.04.6 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://3.0.6\n Kubelet Version:            v1.16.0\n Kube-Proxy Version:         v1.16.0\nProviderID:                  azure:///subscriptions/3014546b-7d1c-4f80-8523-f24a9976fe6a/resourceGroups/kubernetes-eastus-91643/providers/Microsoft.Compute/virtualMachineScaleSets/k8s-agentpool1-40209065-vmss/virtualMachines/0\nNon-terminated Pods:         (7 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                azure-cni-networkmonitor-9x9wr                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         89m\n  kube-system                azure-ip-masq-agent-l2c2n                                  50m (2%)      50m (2%)    50Mi (0%)        250Mi (3%)     89m\n  kube-system                blobfuse-flexvol-installer-6ghd6                           50m (2%)      50m (2%)    100Mi (1%)       100Mi (1%)     89m\n  kube-system                keyvault-flexvolume-n9xxt                                  50m (2%)      50m (2%)    100Mi (1%)       100Mi (1%)     89m\n  kube-system                kube-proxy-rhg4h                                           100m (5%)     0 (0%)      0 (0%)           0 (0%)         89m\n  kube-system                kubernetes-dashboard-65966766b9-86cqc                      300m (15%)    300m (15%)  150Mi (2%)       150Mi (2%)     89m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-13153e5f2c4542ef-86g6v    0 (0%)        0 (0%)      0 (0%)           0 (0%)         88m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                       Requests    Limits\n  --------                       --------    ------\n  cpu                            550m (27%)  450m (22%)\n  memory                         400Mi (5%)  600Mi (8%)\n  ephemeral-storage              0 (0%)      0 (0%)\n  attachable-volumes-azure-disk  0           0\nEvents:                          <none>\n"
Sep 18 21:02:22.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 describe namespace kubectl-1629'
Sep 18 21:02:22.339: INFO: stderr: ""
Sep 18 21:02:22.339: INFO: stdout: "Name:         kubectl-1629\nLabels:       e2e-framework=kubectl\n              e2e-run=30115aef-52cd-4ac4-beca-5d5a9b55b49c\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:02:22.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1629" for this suite.
Sep 18 21:02:50.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:02:50.441: INFO: namespace kubectl-1629 deletion completed in 28.096272806s

â€¢ [SLOW TEST:35.963 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:02:50.442: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2217
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 18 21:02:50.615: INFO: Waiting up to 5m0s for pod "downwardapi-volume-18ea6a3a-9e54-4997-b5b4-a591feea6750" in namespace "downward-api-2217" to be "success or failure"
Sep 18 21:02:50.627: INFO: Pod "downwardapi-volume-18ea6a3a-9e54-4997-b5b4-a591feea6750": Phase="Pending", Reason="", readiness=false. Elapsed: 12.017119ms
Sep 18 21:02:52.632: INFO: Pod "downwardapi-volume-18ea6a3a-9e54-4997-b5b4-a591feea6750": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016320493s
Sep 18 21:02:54.636: INFO: Pod "downwardapi-volume-18ea6a3a-9e54-4997-b5b4-a591feea6750": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020628868s
Sep 18 21:02:56.641: INFO: Pod "downwardapi-volume-18ea6a3a-9e54-4997-b5b4-a591feea6750": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025646542s
Sep 18 21:02:58.645: INFO: Pod "downwardapi-volume-18ea6a3a-9e54-4997-b5b4-a591feea6750": Phase="Pending", Reason="", readiness=false. Elapsed: 8.029424027s
Sep 18 21:03:00.649: INFO: Pod "downwardapi-volume-18ea6a3a-9e54-4997-b5b4-a591feea6750": Phase="Pending", Reason="", readiness=false. Elapsed: 10.033531012s
Sep 18 21:03:02.653: INFO: Pod "downwardapi-volume-18ea6a3a-9e54-4997-b5b4-a591feea6750": Phase="Pending", Reason="", readiness=false. Elapsed: 12.037462301s
Sep 18 21:03:04.657: INFO: Pod "downwardapi-volume-18ea6a3a-9e54-4997-b5b4-a591feea6750": Phase="Pending", Reason="", readiness=false. Elapsed: 14.041514291s
Sep 18 21:03:06.660: INFO: Pod "downwardapi-volume-18ea6a3a-9e54-4997-b5b4-a591feea6750": Phase="Pending", Reason="", readiness=false. Elapsed: 16.045160387s
Sep 18 21:03:08.665: INFO: Pod "downwardapi-volume-18ea6a3a-9e54-4997-b5b4-a591feea6750": Phase="Pending", Reason="", readiness=false. Elapsed: 18.049317081s
Sep 18 21:03:10.668: INFO: Pod "downwardapi-volume-18ea6a3a-9e54-4997-b5b4-a591feea6750": Phase="Pending", Reason="", readiness=false. Elapsed: 20.052863883s
Sep 18 21:03:12.672: INFO: Pod "downwardapi-volume-18ea6a3a-9e54-4997-b5b4-a591feea6750": Phase="Pending", Reason="", readiness=false. Elapsed: 22.056719285s
Sep 18 21:03:14.676: INFO: Pod "downwardapi-volume-18ea6a3a-9e54-4997-b5b4-a591feea6750": Phase="Pending", Reason="", readiness=false. Elapsed: 24.060687988s
Sep 18 21:03:16.680: INFO: Pod "downwardapi-volume-18ea6a3a-9e54-4997-b5b4-a591feea6750": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.064883193s
STEP: Saw pod success
Sep 18 21:03:16.680: INFO: Pod "downwardapi-volume-18ea6a3a-9e54-4997-b5b4-a591feea6750" satisfied condition "success or failure"
Sep 18 21:03:16.683: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod downwardapi-volume-18ea6a3a-9e54-4997-b5b4-a591feea6750 container client-container: <nil>
STEP: delete the pod
Sep 18 21:03:16.732: INFO: Waiting for pod downwardapi-volume-18ea6a3a-9e54-4997-b5b4-a591feea6750 to disappear
Sep 18 21:03:16.742: INFO: Pod downwardapi-volume-18ea6a3a-9e54-4997-b5b4-a591feea6750 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:03:16.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2217" for this suite.
Sep 18 21:03:22.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:03:22.850: INFO: namespace downward-api-2217 deletion completed in 6.102462428s

â€¢ [SLOW TEST:32.408 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:03:22.850: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2910
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2910.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2910.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2910.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2910.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2910.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2910.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 18 21:03:27.154: INFO: DNS probes using dns-2910/dns-test-c1dde131-9c6f-4056-8438-a1a2651c9386 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:03:27.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2910" for this suite.
Sep 18 21:03:33.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:03:33.322: INFO: namespace dns-2910 deletion completed in 6.112224402s

â€¢ [SLOW TEST:10.472 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:03:33.323: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6531
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0918 21:04:13.517266      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 18 21:04:13.517: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:04:13.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6531" for this suite.
Sep 18 21:04:19.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:04:19.634: INFO: namespace gc-6531 deletion completed in 6.112810559s

â€¢ [SLOW TEST:46.311 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:04:19.634: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9496
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 21:04:19.791: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 18 21:04:22.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 --namespace=crd-publish-openapi-9496 create -f -'
Sep 18 21:04:24.989: INFO: stderr: ""
Sep 18 21:04:24.989: INFO: stdout: "e2e-test-crd-publish-openapi-5669-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 18 21:04:24.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 --namespace=crd-publish-openapi-9496 delete e2e-test-crd-publish-openapi-5669-crds test-cr'
Sep 18 21:04:25.110: INFO: stderr: ""
Sep 18 21:04:25.110: INFO: stdout: "e2e-test-crd-publish-openapi-5669-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep 18 21:04:25.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 --namespace=crd-publish-openapi-9496 apply -f -'
Sep 18 21:04:25.365: INFO: stderr: ""
Sep 18 21:04:25.365: INFO: stdout: "e2e-test-crd-publish-openapi-5669-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 18 21:04:25.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 --namespace=crd-publish-openapi-9496 delete e2e-test-crd-publish-openapi-5669-crds test-cr'
Sep 18 21:04:25.461: INFO: stderr: ""
Sep 18 21:04:25.461: INFO: stdout: "e2e-test-crd-publish-openapi-5669-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 18 21:04:25.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 explain e2e-test-crd-publish-openapi-5669-crds'
Sep 18 21:04:25.707: INFO: stderr: ""
Sep 18 21:04:25.707: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5669-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:04:29.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9496" for this suite.
Sep 18 21:04:35.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:04:35.275: INFO: namespace crd-publish-openapi-9496 deletion completed in 6.095572625s

â€¢ [SLOW TEST:15.641 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:04:35.275: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4484
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Sep 18 21:04:35.438: INFO: Waiting up to 5m0s for pod "pod-8fec9eb0-3482-4f31-94a0-5873a3ddd9f8" in namespace "emptydir-4484" to be "success or failure"
Sep 18 21:04:35.457: INFO: Pod "pod-8fec9eb0-3482-4f31-94a0-5873a3ddd9f8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.933975ms
Sep 18 21:04:37.460: INFO: Pod "pod-8fec9eb0-3482-4f31-94a0-5873a3ddd9f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022188477s
Sep 18 21:04:39.464: INFO: Pod "pod-8fec9eb0-3482-4f31-94a0-5873a3ddd9f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025919379s
STEP: Saw pod success
Sep 18 21:04:39.464: INFO: Pod "pod-8fec9eb0-3482-4f31-94a0-5873a3ddd9f8" satisfied condition "success or failure"
Sep 18 21:04:39.467: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-8fec9eb0-3482-4f31-94a0-5873a3ddd9f8 container test-container: <nil>
STEP: delete the pod
Sep 18 21:04:39.514: INFO: Waiting for pod pod-8fec9eb0-3482-4f31-94a0-5873a3ddd9f8 to disappear
Sep 18 21:04:39.523: INFO: Pod pod-8fec9eb0-3482-4f31-94a0-5873a3ddd9f8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:04:39.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4484" for this suite.
Sep 18 21:04:45.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:04:45.650: INFO: namespace emptydir-4484 deletion completed in 6.119597499s

â€¢ [SLOW TEST:10.374 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:04:45.651: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7685
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Sep 18 21:04:45.849: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:05:02.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7685" for this suite.
Sep 18 21:05:08.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:05:08.664: INFO: namespace crd-publish-openapi-7685 deletion completed in 6.144044008s

â€¢ [SLOW TEST:23.013 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:05:08.664: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7449
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-9dae329e-ebed-49dc-b7d1-0c9ee832e089
STEP: Creating a pod to test consume configMaps
Sep 18 21:05:08.833: INFO: Waiting up to 5m0s for pod "pod-configmaps-9f919464-dd5d-46ab-a111-d705ccded1ce" in namespace "configmap-7449" to be "success or failure"
Sep 18 21:05:08.845: INFO: Pod "pod-configmaps-9f919464-dd5d-46ab-a111-d705ccded1ce": Phase="Pending", Reason="", readiness=false. Elapsed: 12.22672ms
Sep 18 21:05:10.849: INFO: Pod "pod-configmaps-9f919464-dd5d-46ab-a111-d705ccded1ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015777254s
Sep 18 21:05:12.852: INFO: Pod "pod-configmaps-9f919464-dd5d-46ab-a111-d705ccded1ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01941939s
STEP: Saw pod success
Sep 18 21:05:12.852: INFO: Pod "pod-configmaps-9f919464-dd5d-46ab-a111-d705ccded1ce" satisfied condition "success or failure"
Sep 18 21:05:12.855: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-configmaps-9f919464-dd5d-46ab-a111-d705ccded1ce container configmap-volume-test: <nil>
STEP: delete the pod
Sep 18 21:05:12.924: INFO: Waiting for pod pod-configmaps-9f919464-dd5d-46ab-a111-d705ccded1ce to disappear
Sep 18 21:05:12.934: INFO: Pod pod-configmaps-9f919464-dd5d-46ab-a111-d705ccded1ce no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:05:12.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7449" for this suite.
Sep 18 21:05:18.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:05:19.041: INFO: namespace configmap-7449 deletion completed in 6.100449528s

â€¢ [SLOW TEST:10.377 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:05:19.041: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3362
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-17527f6e-2a6a-486c-bd2d-ee7de95edbc8
STEP: Creating a pod to test consume configMaps
Sep 18 21:05:19.243: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1badd200-2847-494d-9c8e-395e013aca0e" in namespace "projected-3362" to be "success or failure"
Sep 18 21:05:19.250: INFO: Pod "pod-projected-configmaps-1badd200-2847-494d-9c8e-395e013aca0e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.485157ms
Sep 18 21:05:21.254: INFO: Pod "pod-projected-configmaps-1badd200-2847-494d-9c8e-395e013aca0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010771097s
Sep 18 21:05:23.258: INFO: Pod "pod-projected-configmaps-1badd200-2847-494d-9c8e-395e013aca0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014507143s
STEP: Saw pod success
Sep 18 21:05:23.258: INFO: Pod "pod-projected-configmaps-1badd200-2847-494d-9c8e-395e013aca0e" satisfied condition "success or failure"
Sep 18 21:05:23.261: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod pod-projected-configmaps-1badd200-2847-494d-9c8e-395e013aca0e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 18 21:05:23.332: INFO: Waiting for pod pod-projected-configmaps-1badd200-2847-494d-9c8e-395e013aca0e to disappear
Sep 18 21:05:23.335: INFO: Pod pod-projected-configmaps-1badd200-2847-494d-9c8e-395e013aca0e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:05:23.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3362" for this suite.
Sep 18 21:05:29.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:05:29.441: INFO: namespace projected-3362 deletion completed in 6.099536664s

â€¢ [SLOW TEST:10.400 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:05:29.442: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4067
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:05:42.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4067" for this suite.
Sep 18 21:05:48.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:05:48.786: INFO: namespace resourcequota-4067 deletion completed in 6.100876311s

â€¢ [SLOW TEST:19.345 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:05:48.787: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2681
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-b4cc91c0-e240-4d75-87e2-4a88affce3e3 in namespace container-probe-2681
Sep 18 21:05:52.967: INFO: Started pod liveness-b4cc91c0-e240-4d75-87e2-4a88affce3e3 in namespace container-probe-2681
STEP: checking the pod's current state and verifying that restartCount is present
Sep 18 21:05:52.970: INFO: Initial restart count of pod liveness-b4cc91c0-e240-4d75-87e2-4a88affce3e3 is 0
Sep 18 21:06:13.013: INFO: Restart count of pod container-probe-2681/liveness-b4cc91c0-e240-4d75-87e2-4a88affce3e3 is now 1 (20.042826097s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:06:13.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2681" for this suite.
Sep 18 21:06:19.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:06:19.151: INFO: namespace container-probe-2681 deletion completed in 6.123114547s

â€¢ [SLOW TEST:30.364 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:06:19.152: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9889
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Sep 18 21:06:19.313: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 18 21:06:19.325: INFO: Waiting for terminating namespaces to be deleted...
Sep 18 21:06:19.328: INFO: 
Logging pods the kubelet thinks is on node k8s-agentpool1-40209065-vmss000000 before test
Sep 18 21:06:19.363: INFO: azure-cni-networkmonitor-9x9wr from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 21:06:19.363: INFO: 	Container azure-cnms ready: true, restart count 0
Sep 18 21:06:19.363: INFO: kubernetes-dashboard-65966766b9-86cqc from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 21:06:19.363: INFO: 	Container kubernetes-dashboard ready: true, restart count 1
Sep 18 21:06:19.363: INFO: keyvault-flexvolume-n9xxt from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 21:06:19.364: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
Sep 18 21:06:19.364: INFO: kube-proxy-rhg4h from kube-system started at 2019-09-18 19:32:35 +0000 UTC (1 container statuses recorded)
Sep 18 21:06:19.364: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 18 21:06:19.364: INFO: blobfuse-flexvol-installer-6ghd6 from kube-system started at 2019-09-18 19:32:36 +0000 UTC (1 container statuses recorded)
Sep 18 21:06:19.364: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
Sep 18 21:06:19.364: INFO: azure-ip-masq-agent-l2c2n from kube-system started at 2019-09-18 19:32:37 +0000 UTC (1 container statuses recorded)
Sep 18 21:06:19.364: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Sep 18 21:06:19.364: INFO: sonobuoy-systemd-logs-daemon-set-13153e5f2c4542ef-86g6v from sonobuoy started at 2019-09-18 19:33:36 +0000 UTC (2 container statuses recorded)
Sep 18 21:06:19.364: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 18 21:06:19.364: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 18 21:06:19.364: INFO: 
Logging pods the kubelet thinks is on node k8s-agentpool1-40209065-vmss000001 before test
Sep 18 21:06:19.371: INFO: kube-proxy-cp8gq from kube-system started at 2019-09-18 19:32:35 +0000 UTC (1 container statuses recorded)
Sep 18 21:06:19.371: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 18 21:06:19.371: INFO: sonobuoy-e2e-job-326e453a1b3e45d6 from sonobuoy started at 2019-09-18 19:33:36 +0000 UTC (2 container statuses recorded)
Sep 18 21:06:19.371: INFO: 	Container e2e ready: true, restart count 0
Sep 18 21:06:19.371: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 18 21:06:19.371: INFO: keyvault-flexvolume-428z4 from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 21:06:19.371: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
Sep 18 21:06:19.371: INFO: blobfuse-flexvol-installer-bbdb6 from kube-system started at 2019-09-18 19:32:36 +0000 UTC (1 container statuses recorded)
Sep 18 21:06:19.371: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
Sep 18 21:06:19.372: INFO: azure-ip-masq-agent-62ddt from kube-system started at 2019-09-18 19:32:37 +0000 UTC (1 container statuses recorded)
Sep 18 21:06:19.372: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Sep 18 21:06:19.372: INFO: sonobuoy-systemd-logs-daemon-set-13153e5f2c4542ef-9l9xz from sonobuoy started at 2019-09-18 19:33:36 +0000 UTC (2 container statuses recorded)
Sep 18 21:06:19.372: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 18 21:06:19.372: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 18 21:06:19.372: INFO: azure-cni-networkmonitor-ppx7z from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 21:06:19.372: INFO: 	Container azure-cnms ready: true, restart count 0
Sep 18 21:06:19.372: INFO: 
Logging pods the kubelet thinks is on node k8s-agentpool1-40209065-vmss000002 before test
Sep 18 21:06:19.379: INFO: kube-proxy-vnxpl from kube-system started at 2019-09-18 19:32:35 +0000 UTC (1 container statuses recorded)
Sep 18 21:06:19.379: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 18 21:06:19.379: INFO: azure-ip-masq-agent-q7dfv from kube-system started at 2019-09-18 19:32:37 +0000 UTC (1 container statuses recorded)
Sep 18 21:06:19.379: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Sep 18 21:06:19.379: INFO: sonobuoy from sonobuoy started at 2019-09-18 19:33:29 +0000 UTC (1 container statuses recorded)
Sep 18 21:06:19.379: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 18 21:06:19.379: INFO: sonobuoy-systemd-logs-daemon-set-13153e5f2c4542ef-djfs2 from sonobuoy started at 2019-09-18 19:33:36 +0000 UTC (2 container statuses recorded)
Sep 18 21:06:19.379: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 18 21:06:19.379: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 18 21:06:19.379: INFO: keyvault-flexvolume-f9jq8 from kube-system started at 2019-09-18 19:32:35 +0000 UTC (1 container statuses recorded)
Sep 18 21:06:19.379: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
Sep 18 21:06:19.379: INFO: blobfuse-flexvol-installer-n7wtj from kube-system started at 2019-09-18 19:32:36 +0000 UTC (1 container statuses recorded)
Sep 18 21:06:19.379: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
Sep 18 21:06:19.379: INFO: metrics-server-855b565c8f-rrds2 from kube-system started at 2019-09-18 19:32:37 +0000 UTC (1 container statuses recorded)
Sep 18 21:06:19.379: INFO: 	Container metrics-server ready: true, restart count 0
Sep 18 21:06:19.379: INFO: azure-cni-networkmonitor-86wjx from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 21:06:19.379: INFO: 	Container azure-cnms ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
I0918 21:06:19.392844      14 reflector.go:120] Starting reflector *v1.Event (0s) from k8s.io/kubernetes/test/e2e/common/events.go:136
I0918 21:06:19.392896      14 reflector.go:158] Listing and watching *v1.Event from k8s.io/kubernetes/test/e2e/common/events.go:136
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15c5a460757c644e], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15c5a460768c188a], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:06:20.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9889" for this suite.
Sep 18 21:06:26.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:06:26.588: INFO: namespace sched-pred-9889 deletion completed in 6.174537328s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
I0918 21:06:26.588941      14 request.go:706] Error in request: resource name may not be empty

â€¢ [SLOW TEST:7.436 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:06:26.589: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1612
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 21:06:26.743: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:06:30.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1612" for this suite.
Sep 18 21:07:14.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:07:15.052: INFO: namespace pods-1612 deletion completed in 44.090903503s

â€¢ [SLOW TEST:48.464 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:07:15.053: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8654
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-8654
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8654 to expose endpoints map[]
Sep 18 21:07:15.278: INFO: Get endpoints failed (3.367578ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Sep 18 21:07:16.281: INFO: successfully validated that service endpoint-test2 in namespace services-8654 exposes endpoints map[] (1.00672139s elapsed)
STEP: Creating pod pod1 in namespace services-8654
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8654 to expose endpoints map[pod1:[80]]
Sep 18 21:07:19.328: INFO: successfully validated that service endpoint-test2 in namespace services-8654 exposes endpoints map[pod1:[80]] (3.038910849s elapsed)
STEP: Creating pod pod2 in namespace services-8654
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8654 to expose endpoints map[pod1:[80] pod2:[80]]
Sep 18 21:07:22.375: INFO: successfully validated that service endpoint-test2 in namespace services-8654 exposes endpoints map[pod1:[80] pod2:[80]] (3.038602955s elapsed)
STEP: Deleting pod pod1 in namespace services-8654
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8654 to expose endpoints map[pod2:[80]]
Sep 18 21:07:22.410: INFO: successfully validated that service endpoint-test2 in namespace services-8654 exposes endpoints map[pod2:[80]] (26.281227ms elapsed)
STEP: Deleting pod pod2 in namespace services-8654
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8654 to expose endpoints map[]
Sep 18 21:07:22.425: INFO: successfully validated that service endpoint-test2 in namespace services-8654 exposes endpoints map[] (8.567443ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:07:22.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8654" for this suite.
Sep 18 21:07:50.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:07:50.581: INFO: namespace services-8654 deletion completed in 28.123308262s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:35.529 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:07:50.582: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2974
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-kcc2
STEP: Creating a pod to test atomic-volume-subpath
Sep 18 21:07:50.762: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-kcc2" in namespace "subpath-2974" to be "success or failure"
Sep 18 21:07:50.792: INFO: Pod "pod-subpath-test-secret-kcc2": Phase="Pending", Reason="", readiness=false. Elapsed: 29.170209ms
Sep 18 21:07:52.796: INFO: Pod "pod-subpath-test-secret-kcc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033150977s
Sep 18 21:07:54.800: INFO: Pod "pod-subpath-test-secret-kcc2": Phase="Running", Reason="", readiness=true. Elapsed: 4.037368846s
Sep 18 21:07:56.804: INFO: Pod "pod-subpath-test-secret-kcc2": Phase="Running", Reason="", readiness=true. Elapsed: 6.041694415s
Sep 18 21:07:58.809: INFO: Pod "pod-subpath-test-secret-kcc2": Phase="Running", Reason="", readiness=true. Elapsed: 8.046121085s
Sep 18 21:08:00.813: INFO: Pod "pod-subpath-test-secret-kcc2": Phase="Running", Reason="", readiness=true. Elapsed: 10.050524157s
Sep 18 21:08:02.817: INFO: Pod "pod-subpath-test-secret-kcc2": Phase="Running", Reason="", readiness=true. Elapsed: 12.05489573s
Sep 18 21:08:04.822: INFO: Pod "pod-subpath-test-secret-kcc2": Phase="Running", Reason="", readiness=true. Elapsed: 14.059420804s
Sep 18 21:08:06.826: INFO: Pod "pod-subpath-test-secret-kcc2": Phase="Running", Reason="", readiness=true. Elapsed: 16.063610681s
Sep 18 21:08:08.831: INFO: Pod "pod-subpath-test-secret-kcc2": Phase="Running", Reason="", readiness=true. Elapsed: 18.068083658s
Sep 18 21:08:10.835: INFO: Pod "pod-subpath-test-secret-kcc2": Phase="Running", Reason="", readiness=true. Elapsed: 20.072166939s
Sep 18 21:08:12.839: INFO: Pod "pod-subpath-test-secret-kcc2": Phase="Running", Reason="", readiness=true. Elapsed: 22.076099322s
Sep 18 21:08:14.843: INFO: Pod "pod-subpath-test-secret-kcc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.080116906s
STEP: Saw pod success
Sep 18 21:08:14.843: INFO: Pod "pod-subpath-test-secret-kcc2" satisfied condition "success or failure"
Sep 18 21:08:14.845: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod pod-subpath-test-secret-kcc2 container test-container-subpath-secret-kcc2: <nil>
STEP: delete the pod
Sep 18 21:08:14.901: INFO: Waiting for pod pod-subpath-test-secret-kcc2 to disappear
Sep 18 21:08:14.904: INFO: Pod pod-subpath-test-secret-kcc2 no longer exists
STEP: Deleting pod pod-subpath-test-secret-kcc2
Sep 18 21:08:14.904: INFO: Deleting pod "pod-subpath-test-secret-kcc2" in namespace "subpath-2974"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:08:14.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2974" for this suite.
Sep 18 21:08:20.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:08:21.003: INFO: namespace subpath-2974 deletion completed in 6.092009737s

â€¢ [SLOW TEST:30.422 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:08:21.013: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8844
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 21:08:21.176: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:08:21.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8844" for this suite.
Sep 18 21:08:27.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:08:27.836: INFO: namespace custom-resource-definition-8844 deletion completed in 6.112771916s

â€¢ [SLOW TEST:6.824 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:08:27.837: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3369
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:08:45.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3369" for this suite.
Sep 18 21:08:51.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:08:51.149: INFO: namespace resourcequota-3369 deletion completed in 6.108234092s

â€¢ [SLOW TEST:23.313 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:08:51.150: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4367
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 18 21:08:51.320: INFO: Waiting up to 5m0s for pod "pod-894cea9b-1089-459f-ab12-212e8f8e8449" in namespace "emptydir-4367" to be "success or failure"
Sep 18 21:08:51.323: INFO: Pod "pod-894cea9b-1089-459f-ab12-212e8f8e8449": Phase="Pending", Reason="", readiness=false. Elapsed: 2.98078ms
Sep 18 21:08:53.327: INFO: Pod "pod-894cea9b-1089-459f-ab12-212e8f8e8449": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007207388s
Sep 18 21:08:55.331: INFO: Pod "pod-894cea9b-1089-459f-ab12-212e8f8e8449": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0110338s
STEP: Saw pod success
Sep 18 21:08:55.331: INFO: Pod "pod-894cea9b-1089-459f-ab12-212e8f8e8449" satisfied condition "success or failure"
Sep 18 21:08:55.335: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-894cea9b-1089-459f-ab12-212e8f8e8449 container test-container: <nil>
STEP: delete the pod
Sep 18 21:08:55.391: INFO: Waiting for pod pod-894cea9b-1089-459f-ab12-212e8f8e8449 to disappear
Sep 18 21:08:55.394: INFO: Pod pod-894cea9b-1089-459f-ab12-212e8f8e8449 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:08:55.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4367" for this suite.
Sep 18 21:09:01.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:09:01.490: INFO: namespace emptydir-4367 deletion completed in 6.091790119s

â€¢ [SLOW TEST:10.341 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:09:01.491: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2468
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-2468
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-2468
Sep 18 21:09:01.686: INFO: Found 0 stateful pods, waiting for 1
Sep 18 21:09:11.690: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Sep 18 21:09:11.752: INFO: Deleting all statefulset in ns statefulset-2468
Sep 18 21:09:11.756: INFO: Scaling statefulset ss to 0
Sep 18 21:09:31.784: INFO: Waiting for statefulset status.replicas updated to 0
Sep 18 21:09:31.793: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:09:31.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2468" for this suite.
Sep 18 21:09:37.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:09:37.952: INFO: namespace statefulset-2468 deletion completed in 6.119424506s

â€¢ [SLOW TEST:36.461 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:09:37.953: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6711
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-09a7d7d0-1219-481f-92db-f464e392956b
STEP: Creating a pod to test consume secrets
Sep 18 21:09:38.128: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-95365302-9947-45c5-89e6-77f399a91097" in namespace "projected-6711" to be "success or failure"
Sep 18 21:09:38.132: INFO: Pod "pod-projected-secrets-95365302-9947-45c5-89e6-77f399a91097": Phase="Pending", Reason="", readiness=false. Elapsed: 3.887975ms
Sep 18 21:09:40.136: INFO: Pod "pod-projected-secrets-95365302-9947-45c5-89e6-77f399a91097": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007668215s
Sep 18 21:09:42.140: INFO: Pod "pod-projected-secrets-95365302-9947-45c5-89e6-77f399a91097": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011823053s
STEP: Saw pod success
Sep 18 21:09:42.140: INFO: Pod "pod-projected-secrets-95365302-9947-45c5-89e6-77f399a91097" satisfied condition "success or failure"
Sep 18 21:09:42.143: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod pod-projected-secrets-95365302-9947-45c5-89e6-77f399a91097 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 18 21:09:42.165: INFO: Waiting for pod pod-projected-secrets-95365302-9947-45c5-89e6-77f399a91097 to disappear
Sep 18 21:09:42.168: INFO: Pod pod-projected-secrets-95365302-9947-45c5-89e6-77f399a91097 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:09:42.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6711" for this suite.
Sep 18 21:09:48.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:09:48.282: INFO: namespace projected-6711 deletion completed in 6.1078744s

â€¢ [SLOW TEST:10.329 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:09:48.282: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4333
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 18 21:09:48.457: INFO: Waiting up to 5m0s for pod "pod-53733a1e-b805-4587-9b4f-ef2208ba121b" in namespace "emptydir-4333" to be "success or failure"
Sep 18 21:09:48.463: INFO: Pod "pod-53733a1e-b805-4587-9b4f-ef2208ba121b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.760962ms
Sep 18 21:09:50.467: INFO: Pod "pod-53733a1e-b805-4587-9b4f-ef2208ba121b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009826206s
Sep 18 21:09:52.471: INFO: Pod "pod-53733a1e-b805-4587-9b4f-ef2208ba121b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013516453s
STEP: Saw pod success
Sep 18 21:09:52.471: INFO: Pod "pod-53733a1e-b805-4587-9b4f-ef2208ba121b" satisfied condition "success or failure"
Sep 18 21:09:52.474: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-53733a1e-b805-4587-9b4f-ef2208ba121b container test-container: <nil>
STEP: delete the pod
Sep 18 21:09:52.500: INFO: Waiting for pod pod-53733a1e-b805-4587-9b4f-ef2208ba121b to disappear
Sep 18 21:09:52.503: INFO: Pod pod-53733a1e-b805-4587-9b4f-ef2208ba121b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:09:52.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4333" for this suite.
Sep 18 21:09:58.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:09:58.599: INFO: namespace emptydir-4333 deletion completed in 6.091417025s

â€¢ [SLOW TEST:10.316 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:09:58.600: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5364
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 18 21:09:59.294: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 18 21:10:01.302: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704437799, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704437799, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704437799, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704437799, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 18 21:10:04.330: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 21:10:04.336: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2468-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:10:05.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5364" for this suite.
Sep 18 21:10:11.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:10:11.894: INFO: namespace webhook-5364 deletion completed in 6.131904784s
STEP: Destroying namespace "webhook-5364-markers" for this suite.
Sep 18 21:10:17.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:10:18.008: INFO: namespace webhook-5364-markers deletion completed in 6.113940011s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:19.420 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:10:18.021: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8331
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-8331
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8331
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8331
Sep 18 21:10:18.207: INFO: Found 0 stateful pods, waiting for 1
Sep 18 21:10:28.212: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep 18 21:10:28.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-8331 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 18 21:10:28.489: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 18 21:10:28.489: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 18 21:10:28.489: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 18 21:10:28.494: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 18 21:10:38.498: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 18 21:10:38.498: INFO: Waiting for statefulset status.replicas updated to 0
Sep 18 21:10:38.513: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999998s
Sep 18 21:10:39.517: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995271632s
Sep 18 21:10:40.521: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990999961s
Sep 18 21:10:41.525: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.986877189s
Sep 18 21:10:42.529: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982591118s
Sep 18 21:10:43.537: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.978649644s
Sep 18 21:10:44.555: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.971406891s
Sep 18 21:10:45.560: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.952106117s
Sep 18 21:10:46.571: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.948041643s
Sep 18 21:10:47.582: INFO: Verifying statefulset ss doesn't scale past 1 for another 936.713216ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8331
Sep 18 21:10:48.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-8331 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 21:10:48.871: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 18 21:10:48.871: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 18 21:10:48.871: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 18 21:10:48.875: INFO: Found 1 stateful pods, waiting for 3
Sep 18 21:10:58.879: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 18 21:10:58.879: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 18 21:10:58.879: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep 18 21:10:58.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-8331 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 18 21:10:59.151: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 18 21:10:59.151: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 18 21:10:59.151: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 18 21:10:59.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-8331 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 18 21:10:59.458: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 18 21:10:59.458: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 18 21:10:59.458: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 18 21:10:59.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-8331 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 18 21:10:59.758: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 18 21:10:59.758: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 18 21:10:59.758: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 18 21:10:59.758: INFO: Waiting for statefulset status.replicas updated to 0
Sep 18 21:10:59.762: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep 18 21:11:09.770: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 18 21:11:09.770: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 18 21:11:09.770: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 18 21:11:09.791: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999998s
Sep 18 21:11:10.796: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.987372744s
Sep 18 21:11:11.801: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982122698s
Sep 18 21:11:12.806: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.977348939s
Sep 18 21:11:13.810: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.97276897s
Sep 18 21:11:14.815: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.968017195s
Sep 18 21:11:15.819: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963411511s
Sep 18 21:11:16.894: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.958805619s
Sep 18 21:11:17.898: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.884712487s
Sep 18 21:11:18.912: INFO: Verifying statefulset ss doesn't scale past 3 for another 880.446275ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8331
Sep 18 21:11:19.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-8331 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 21:11:20.160: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 18 21:11:20.160: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 18 21:11:20.160: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 18 21:11:20.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-8331 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 21:11:20.458: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 18 21:11:20.458: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 18 21:11:20.458: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 18 21:11:20.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=statefulset-8331 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 18 21:11:20.716: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 18 21:11:20.716: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 18 21:11:20.716: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 18 21:11:20.716: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Sep 18 21:12:00.733: INFO: Deleting all statefulset in ns statefulset-8331
Sep 18 21:12:00.737: INFO: Scaling statefulset ss to 0
Sep 18 21:12:00.772: INFO: Waiting for statefulset status.replicas updated to 0
Sep 18 21:12:00.774: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:12:00.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8331" for this suite.
Sep 18 21:12:06.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:12:06.890: INFO: namespace statefulset-8331 deletion completed in 6.099388088s

â€¢ [SLOW TEST:108.870 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:12:06.891: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6632
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-fa16cddc-0593-4924-895e-c8a5c0411173
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:12:07.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6632" for this suite.
Sep 18 21:12:13.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:12:13.162: INFO: namespace secrets-6632 deletion completed in 6.097660671s

â€¢ [SLOW TEST:6.271 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:12:13.162: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6896
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep 18 21:12:17.873: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6896 pod-service-account-72da1409-4eac-4743-9cee-00123aaab854 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep 18 21:12:18.126: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6896 pod-service-account-72da1409-4eac-4743-9cee-00123aaab854 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep 18 21:12:18.370: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6896 pod-service-account-72da1409-4eac-4743-9cee-00123aaab854 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:12:18.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6896" for this suite.
Sep 18 21:12:24.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:12:24.740: INFO: namespace svcaccounts-6896 deletion completed in 6.102660901s

â€¢ [SLOW TEST:11.577 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:12:24.740: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1671
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Sep 18 21:12:24.914: INFO: Waiting up to 5m0s for pod "var-expansion-7a2ee2e7-76bc-44e8-bf6c-4e905712023f" in namespace "var-expansion-1671" to be "success or failure"
Sep 18 21:12:24.916: INFO: Pod "var-expansion-7a2ee2e7-76bc-44e8-bf6c-4e905712023f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.691971ms
Sep 18 21:12:26.920: INFO: Pod "var-expansion-7a2ee2e7-76bc-44e8-bf6c-4e905712023f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006290619s
Sep 18 21:12:28.925: INFO: Pod "var-expansion-7a2ee2e7-76bc-44e8-bf6c-4e905712023f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011492177s
STEP: Saw pod success
Sep 18 21:12:28.925: INFO: Pod "var-expansion-7a2ee2e7-76bc-44e8-bf6c-4e905712023f" satisfied condition "success or failure"
Sep 18 21:12:28.929: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod var-expansion-7a2ee2e7-76bc-44e8-bf6c-4e905712023f container dapi-container: <nil>
STEP: delete the pod
Sep 18 21:12:28.975: INFO: Waiting for pod var-expansion-7a2ee2e7-76bc-44e8-bf6c-4e905712023f to disappear
Sep 18 21:12:28.978: INFO: Pod var-expansion-7a2ee2e7-76bc-44e8-bf6c-4e905712023f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:12:28.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1671" for this suite.
Sep 18 21:12:35.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:12:35.097: INFO: namespace var-expansion-1671 deletion completed in 6.106435185s

â€¢ [SLOW TEST:10.356 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:12:35.098: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7677
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 21:12:35.313: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"5e8a2324-2c4d-4b6a-aa77-1e12501b0f00", Controller:(*bool)(0xc0056b587e), BlockOwnerDeletion:(*bool)(0xc0056b587f)}}
Sep 18 21:12:35.324: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"12159e61-df59-47e0-bac0-55e8b630e4ba", Controller:(*bool)(0xc003955cbe), BlockOwnerDeletion:(*bool)(0xc003955cbf)}}
Sep 18 21:12:35.337: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"de301804-08ee-4073-9372-4a9ebdf8e149", Controller:(*bool)(0xc0056b5a3e), BlockOwnerDeletion:(*bool)(0xc0056b5a3f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:12:40.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7677" for this suite.
Sep 18 21:12:46.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:12:46.465: INFO: namespace gc-7677 deletion completed in 6.105270352s

â€¢ [SLOW TEST:11.367 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:12:46.465: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6559
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Sep 18 21:12:46.623: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 18 21:12:46.637: INFO: Waiting for terminating namespaces to be deleted...
Sep 18 21:12:46.640: INFO: 
Logging pods the kubelet thinks is on node k8s-agentpool1-40209065-vmss000000 before test
Sep 18 21:12:46.677: INFO: keyvault-flexvolume-n9xxt from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 21:12:46.677: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
Sep 18 21:12:46.677: INFO: kube-proxy-rhg4h from kube-system started at 2019-09-18 19:32:35 +0000 UTC (1 container statuses recorded)
Sep 18 21:12:46.677: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 18 21:12:46.677: INFO: blobfuse-flexvol-installer-6ghd6 from kube-system started at 2019-09-18 19:32:36 +0000 UTC (1 container statuses recorded)
Sep 18 21:12:46.677: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
Sep 18 21:12:46.677: INFO: azure-ip-masq-agent-l2c2n from kube-system started at 2019-09-18 19:32:37 +0000 UTC (1 container statuses recorded)
Sep 18 21:12:46.677: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Sep 18 21:12:46.677: INFO: sonobuoy-systemd-logs-daemon-set-13153e5f2c4542ef-86g6v from sonobuoy started at 2019-09-18 19:33:36 +0000 UTC (2 container statuses recorded)
Sep 18 21:12:46.677: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 18 21:12:46.677: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 18 21:12:46.677: INFO: azure-cni-networkmonitor-9x9wr from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 21:12:46.677: INFO: 	Container azure-cnms ready: true, restart count 0
Sep 18 21:12:46.677: INFO: kubernetes-dashboard-65966766b9-86cqc from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 21:12:46.677: INFO: 	Container kubernetes-dashboard ready: true, restart count 1
Sep 18 21:12:46.677: INFO: 
Logging pods the kubelet thinks is on node k8s-agentpool1-40209065-vmss000001 before test
Sep 18 21:12:46.717: INFO: sonobuoy-systemd-logs-daemon-set-13153e5f2c4542ef-9l9xz from sonobuoy started at 2019-09-18 19:33:36 +0000 UTC (2 container statuses recorded)
Sep 18 21:12:46.717: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 18 21:12:46.717: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 18 21:12:46.717: INFO: azure-cni-networkmonitor-ppx7z from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 21:12:46.717: INFO: 	Container azure-cnms ready: true, restart count 0
Sep 18 21:12:46.717: INFO: blobfuse-flexvol-installer-bbdb6 from kube-system started at 2019-09-18 19:32:36 +0000 UTC (1 container statuses recorded)
Sep 18 21:12:46.717: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
Sep 18 21:12:46.717: INFO: azure-ip-masq-agent-62ddt from kube-system started at 2019-09-18 19:32:37 +0000 UTC (1 container statuses recorded)
Sep 18 21:12:46.717: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Sep 18 21:12:46.717: INFO: keyvault-flexvolume-428z4 from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 21:12:46.717: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
Sep 18 21:12:46.717: INFO: kube-proxy-cp8gq from kube-system started at 2019-09-18 19:32:35 +0000 UTC (1 container statuses recorded)
Sep 18 21:12:46.717: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 18 21:12:46.717: INFO: sonobuoy-e2e-job-326e453a1b3e45d6 from sonobuoy started at 2019-09-18 19:33:36 +0000 UTC (2 container statuses recorded)
Sep 18 21:12:46.717: INFO: 	Container e2e ready: true, restart count 0
Sep 18 21:12:46.717: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 18 21:12:46.717: INFO: 
Logging pods the kubelet thinks is on node k8s-agentpool1-40209065-vmss000002 before test
Sep 18 21:12:46.725: INFO: azure-cni-networkmonitor-86wjx from kube-system started at 2019-09-18 19:32:34 +0000 UTC (1 container statuses recorded)
Sep 18 21:12:46.725: INFO: 	Container azure-cnms ready: true, restart count 0
Sep 18 21:12:46.725: INFO: blobfuse-flexvol-installer-n7wtj from kube-system started at 2019-09-18 19:32:36 +0000 UTC (1 container statuses recorded)
Sep 18 21:12:46.725: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
Sep 18 21:12:46.725: INFO: metrics-server-855b565c8f-rrds2 from kube-system started at 2019-09-18 19:32:37 +0000 UTC (1 container statuses recorded)
Sep 18 21:12:46.725: INFO: 	Container metrics-server ready: true, restart count 0
Sep 18 21:12:46.725: INFO: keyvault-flexvolume-f9jq8 from kube-system started at 2019-09-18 19:32:35 +0000 UTC (1 container statuses recorded)
Sep 18 21:12:46.725: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
Sep 18 21:12:46.725: INFO: kube-proxy-vnxpl from kube-system started at 2019-09-18 19:32:35 +0000 UTC (1 container statuses recorded)
Sep 18 21:12:46.725: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 18 21:12:46.725: INFO: azure-ip-masq-agent-q7dfv from kube-system started at 2019-09-18 19:32:37 +0000 UTC (1 container statuses recorded)
Sep 18 21:12:46.725: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Sep 18 21:12:46.725: INFO: sonobuoy from sonobuoy started at 2019-09-18 19:33:29 +0000 UTC (1 container statuses recorded)
Sep 18 21:12:46.725: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 18 21:12:46.725: INFO: sonobuoy-systemd-logs-daemon-set-13153e5f2c4542ef-djfs2 from sonobuoy started at 2019-09-18 19:33:36 +0000 UTC (2 container statuses recorded)
Sep 18 21:12:46.725: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 18 21:12:46.725: INFO: 	Container systemd-logs ready: true, restart count 1
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-37f3cdf5-c67b-4fcd-a1d4-b030270805bd 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-37f3cdf5-c67b-4fcd-a1d4-b030270805bd off the node k8s-agentpool1-40209065-vmss000001
STEP: verifying the node doesn't have the label kubernetes.io/e2e-37f3cdf5-c67b-4fcd-a1d4-b030270805bd
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:17:54.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6559" for this suite.
Sep 18 21:18:22.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:18:22.986: INFO: namespace sched-pred-6559 deletion completed in 28.113858171s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
I0918 21:18:22.986503      14 request.go:706] Error in request: resource name may not be empty

â€¢ [SLOW TEST:336.522 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:18:22.988: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2530
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 21:18:23.150: INFO: Creating ReplicaSet my-hostname-basic-37ef3feb-672a-4f1b-a446-1e457e1871e3
Sep 18 21:18:23.157: INFO: Pod name my-hostname-basic-37ef3feb-672a-4f1b-a446-1e457e1871e3: Found 0 pods out of 1
Sep 18 21:18:28.161: INFO: Pod name my-hostname-basic-37ef3feb-672a-4f1b-a446-1e457e1871e3: Found 1 pods out of 1
Sep 18 21:18:28.161: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-37ef3feb-672a-4f1b-a446-1e457e1871e3" is running
Sep 18 21:18:28.164: INFO: Pod "my-hostname-basic-37ef3feb-672a-4f1b-a446-1e457e1871e3-2924t" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-18 21:18:23 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-18 21:18:25 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-18 21:18:25 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-18 21:18:23 +0000 UTC Reason: Message:}])
Sep 18 21:18:28.164: INFO: Trying to dial the pod
Sep 18 21:18:33.177: INFO: Controller my-hostname-basic-37ef3feb-672a-4f1b-a446-1e457e1871e3: Got expected result from replica 1 [my-hostname-basic-37ef3feb-672a-4f1b-a446-1e457e1871e3-2924t]: "my-hostname-basic-37ef3feb-672a-4f1b-a446-1e457e1871e3-2924t", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:18:33.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2530" for this suite.
Sep 18 21:18:39.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:18:39.280: INFO: namespace replicaset-2530 deletion completed in 6.097853413s

â€¢ [SLOW TEST:16.291 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:18:39.280: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8561
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-d046f51d-c10f-440b-b9ae-85cc7554a447
STEP: Creating a pod to test consume secrets
Sep 18 21:18:39.454: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-935eeb4c-0f69-41f8-927a-22b70c248c69" in namespace "projected-8561" to be "success or failure"
Sep 18 21:18:39.459: INFO: Pod "pod-projected-secrets-935eeb4c-0f69-41f8-927a-22b70c248c69": Phase="Pending", Reason="", readiness=false. Elapsed: 4.778558ms
Sep 18 21:18:41.463: INFO: Pod "pod-projected-secrets-935eeb4c-0f69-41f8-927a-22b70c248c69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008331879s
Sep 18 21:18:43.467: INFO: Pod "pod-projected-secrets-935eeb4c-0f69-41f8-927a-22b70c248c69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012489007s
STEP: Saw pod success
Sep 18 21:18:43.467: INFO: Pod "pod-projected-secrets-935eeb4c-0f69-41f8-927a-22b70c248c69" satisfied condition "success or failure"
Sep 18 21:18:43.469: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-projected-secrets-935eeb4c-0f69-41f8-927a-22b70c248c69 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 18 21:18:43.524: INFO: Waiting for pod pod-projected-secrets-935eeb4c-0f69-41f8-927a-22b70c248c69 to disappear
Sep 18 21:18:43.527: INFO: Pod pod-projected-secrets-935eeb4c-0f69-41f8-927a-22b70c248c69 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:18:43.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8561" for this suite.
Sep 18 21:18:49.543: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:18:49.624: INFO: namespace projected-8561 deletion completed in 6.092576163s

â€¢ [SLOW TEST:10.344 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:18:49.625: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6275
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 18 21:18:57.867: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 18 21:18:57.923: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 18 21:18:59.923: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 18 21:18:59.927: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 18 21:19:01.923: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 18 21:19:01.927: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:19:01.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6275" for this suite.
Sep 18 21:19:29.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:19:30.029: INFO: namespace container-lifecycle-hook-6275 deletion completed in 28.097249283s

â€¢ [SLOW TEST:40.404 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:19:30.030: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1850
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-b9f4a2b0-7899-4468-9233-c06fd31455bc
STEP: Creating a pod to test consume configMaps
Sep 18 21:19:30.198: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-eac8647c-c58b-4a9b-90f9-44a57dc3875d" in namespace "projected-1850" to be "success or failure"
Sep 18 21:19:30.212: INFO: Pod "pod-projected-configmaps-eac8647c-c58b-4a9b-90f9-44a57dc3875d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.389085ms
Sep 18 21:19:32.219: INFO: Pod "pod-projected-configmaps-eac8647c-c58b-4a9b-90f9-44a57dc3875d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020317689s
Sep 18 21:19:34.222: INFO: Pod "pod-projected-configmaps-eac8647c-c58b-4a9b-90f9-44a57dc3875d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024124132s
STEP: Saw pod success
Sep 18 21:19:34.222: INFO: Pod "pod-projected-configmaps-eac8647c-c58b-4a9b-90f9-44a57dc3875d" satisfied condition "success or failure"
Sep 18 21:19:34.228: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod pod-projected-configmaps-eac8647c-c58b-4a9b-90f9-44a57dc3875d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 18 21:19:34.267: INFO: Waiting for pod pod-projected-configmaps-eac8647c-c58b-4a9b-90f9-44a57dc3875d to disappear
Sep 18 21:19:34.273: INFO: Pod pod-projected-configmaps-eac8647c-c58b-4a9b-90f9-44a57dc3875d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:19:34.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1850" for this suite.
Sep 18 21:19:40.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:19:40.382: INFO: namespace projected-1850 deletion completed in 6.104902096s

â€¢ [SLOW TEST:10.352 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:19:40.383: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7440
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-405fa0cb-08b5-42d0-a566-170dcf653ac3
STEP: Creating a pod to test consume configMaps
Sep 18 21:19:40.561: INFO: Waiting up to 5m0s for pod "pod-configmaps-53968108-c4df-424f-bb63-795d0d03b954" in namespace "configmap-7440" to be "success or failure"
Sep 18 21:19:40.569: INFO: Pod "pod-configmaps-53968108-c4df-424f-bb63-795d0d03b954": Phase="Pending", Reason="", readiness=false. Elapsed: 7.656534ms
Sep 18 21:19:42.572: INFO: Pod "pod-configmaps-53968108-c4df-424f-bb63-795d0d03b954": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011542925s
Sep 18 21:19:44.577: INFO: Pod "pod-configmaps-53968108-c4df-424f-bb63-795d0d03b954": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015812824s
STEP: Saw pod success
Sep 18 21:19:44.577: INFO: Pod "pod-configmaps-53968108-c4df-424f-bb63-795d0d03b954" satisfied condition "success or failure"
Sep 18 21:19:44.580: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod pod-configmaps-53968108-c4df-424f-bb63-795d0d03b954 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 18 21:19:44.609: INFO: Waiting for pod pod-configmaps-53968108-c4df-424f-bb63-795d0d03b954 to disappear
Sep 18 21:19:44.613: INFO: Pod pod-configmaps-53968108-c4df-424f-bb63-795d0d03b954 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:19:44.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7440" for this suite.
Sep 18 21:19:50.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:19:50.743: INFO: namespace configmap-7440 deletion completed in 6.124645408s

â€¢ [SLOW TEST:10.360 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:19:50.743: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4321
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:20:50.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4321" for this suite.
Sep 18 21:21:18.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:21:19.049: INFO: namespace container-probe-4321 deletion completed in 28.126328248s

â€¢ [SLOW TEST:88.306 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:21:19.051: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-414
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Sep 18 21:21:19.239: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-414" to be "success or failure"
Sep 18 21:21:19.255: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 15.166874ms
Sep 18 21:21:21.258: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018637783s
Sep 18 21:21:23.262: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022946095s
STEP: Saw pod success
Sep 18 21:21:23.262: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Sep 18 21:21:23.265: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Sep 18 21:21:23.318: INFO: Waiting for pod pod-host-path-test to disappear
Sep 18 21:21:23.321: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:21:23.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-414" for this suite.
Sep 18 21:21:29.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:21:29.440: INFO: namespace hostpath-414 deletion completed in 6.112994061s

â€¢ [SLOW TEST:10.389 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:21:29.440: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 21:21:29.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 version'
Sep 18 21:21:29.774: INFO: stderr: ""
Sep 18 21:21:29.774: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.0\", GitCommit:\"2bd9643cee5b3b3a5ecbd3af49d09018f0773c77\", GitTreeState:\"clean\", BuildDate:\"2019-09-18T14:36:53Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.0\", GitCommit:\"2bd9643cee5b3b3a5ecbd3af49d09018f0773c77\", GitTreeState:\"clean\", BuildDate:\"2019-09-18T14:27:17Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:21:29.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4112" for this suite.
Sep 18 21:21:35.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:21:35.880: INFO: namespace kubectl-4112 deletion completed in 6.099379965s

â€¢ [SLOW TEST:6.440 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:21:35.881: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8171
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8171
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8171
STEP: creating replication controller externalsvc in namespace services-8171
I0918 21:21:36.101136      14 runners.go:184] Created replication controller with name: externalsvc, namespace: services-8171, replica count: 2
I0918 21:21:36.101242      14 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 21:21:36.101265      14 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 21:21:39.151480      14 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0918 21:21:42.151705      14 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Sep 18 21:21:42.186: INFO: Creating new exec pod
Sep 18 21:21:46.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=services-8171 execpodv2lh6 -- /bin/sh -x -c nslookup nodeport-service'
Sep 18 21:21:48.847: INFO: stderr: "+ nslookup nodeport-service\n"
Sep 18 21:21:48.847: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nnodeport-service.services-8171.svc.cluster.local\tcanonical name = externalsvc.services-8171.svc.cluster.local.\nName:\texternalsvc.services-8171.svc.cluster.local\nAddress: 10.0.175.65\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8171, will wait for the garbage collector to delete the pods
I0918 21:21:48.850764      14 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 21:21:48.850813      14 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 18 21:21:48.907: INFO: Deleting ReplicationController externalsvc took: 6.246548ms
Sep 18 21:21:51.007: INFO: Terminating ReplicationController externalsvc pods took: 2.100755542s
I0918 21:21:51.007742      14 controller_utils.go:810] Ignoring inactive pod services-8171/externalsvc-ktb8p in state Running, deletion time 2019-09-18 21:21:51 +0000 UTC
I0918 21:21:51.007806      14 controller_utils.go:810] Ignoring inactive pod services-8171/externalsvc-zg5xx in state Running, deletion time 2019-09-18 21:21:51 +0000 UTC
Sep 18 21:22:03.639: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:22:03.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8171" for this suite.
Sep 18 21:22:09.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:22:09.772: INFO: namespace services-8171 deletion completed in 6.102266602s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:33.891 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:22:09.774: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1365
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:22:20.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1365" for this suite.
Sep 18 21:22:26.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:22:27.089: INFO: namespace resourcequota-1365 deletion completed in 6.111745549s

â€¢ [SLOW TEST:17.316 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:22:27.090: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2545
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-2545
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 18 21:22:27.255: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 18 21:22:49.363: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.73:8080/dial?request=hostName&protocol=udp&host=10.240.0.67&port=8081&tries=1'] Namespace:pod-network-test-2545 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 18 21:22:49.363: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 21:22:49.516: INFO: Waiting for endpoints: map[]
Sep 18 21:22:49.519: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.73:8080/dial?request=hostName&protocol=udp&host=10.240.0.22&port=8081&tries=1'] Namespace:pod-network-test-2545 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 18 21:22:49.519: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 21:22:49.679: INFO: Waiting for endpoints: map[]
Sep 18 21:22:49.682: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.73:8080/dial?request=hostName&protocol=udp&host=10.240.0.56&port=8081&tries=1'] Namespace:pod-network-test-2545 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 18 21:22:49.682: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 21:22:49.845: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:22:49.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2545" for this suite.
Sep 18 21:23:01.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:23:01.941: INFO: namespace pod-network-test-2545 deletion completed in 12.090351967s

â€¢ [SLOW TEST:34.852 seconds]
[sig-network] Networking
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:23:01.943: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6021
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 18 21:23:02.919: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 18 21:23:04.929: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704438582, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704438582, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704438582, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704438582, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 18 21:23:07.950: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:23:08.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6021" for this suite.
Sep 18 21:23:14.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:23:14.123: INFO: namespace webhook-6021 deletion completed in 6.099136824s
STEP: Destroying namespace "webhook-6021-markers" for this suite.
Sep 18 21:23:20.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:23:20.233: INFO: namespace webhook-6021-markers deletion completed in 6.109642209s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:18.303 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:23:20.246: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2797
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-36acb14e-ce2f-4c06-8f94-6fc5a961d544
STEP: Creating a pod to test consume secrets
Sep 18 21:23:20.439: INFO: Waiting up to 5m0s for pod "pod-secrets-f1644213-386e-4a2c-ab8f-9af39460634c" in namespace "secrets-2797" to be "success or failure"
Sep 18 21:23:20.448: INFO: Pod "pod-secrets-f1644213-386e-4a2c-ab8f-9af39460634c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.882828ms
Sep 18 21:23:22.452: INFO: Pod "pod-secrets-f1644213-386e-4a2c-ab8f-9af39460634c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012421448s
Sep 18 21:23:24.456: INFO: Pod "pod-secrets-f1644213-386e-4a2c-ab8f-9af39460634c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016230872s
STEP: Saw pod success
Sep 18 21:23:24.456: INFO: Pod "pod-secrets-f1644213-386e-4a2c-ab8f-9af39460634c" satisfied condition "success or failure"
Sep 18 21:23:24.458: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod pod-secrets-f1644213-386e-4a2c-ab8f-9af39460634c container secret-volume-test: <nil>
STEP: delete the pod
Sep 18 21:23:24.503: INFO: Waiting for pod pod-secrets-f1644213-386e-4a2c-ab8f-9af39460634c to disappear
Sep 18 21:23:24.505: INFO: Pod pod-secrets-f1644213-386e-4a2c-ab8f-9af39460634c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:23:24.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2797" for this suite.
Sep 18 21:23:30.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:23:30.608: INFO: namespace secrets-2797 deletion completed in 6.097657724s

â€¢ [SLOW TEST:10.362 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:23:30.613: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4225
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4225, will wait for the garbage collector to delete the pods
I0918 21:23:34.783289      14 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 21:23:34.783428      14 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 18 21:23:34.839: INFO: Deleting Job.batch foo took: 6.450548ms
I0918 21:23:35.140212      14 controller_utils.go:810] Ignoring inactive pod job-4225/foo-kzv5m in state Running, deletion time 2019-09-18 21:24:05 +0000 UTC
I0918 21:23:35.140277      14 controller_utils.go:810] Ignoring inactive pod job-4225/foo-79zgq in state Running, deletion time 2019-09-18 21:24:05 +0000 UTC
Sep 18 21:23:35.140: INFO: Terminating Job.batch foo pods took: 300.347582ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:24:13.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4225" for this suite.
Sep 18 21:24:19.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:24:19.747: INFO: namespace job-4225 deletion completed in 6.094894072s

â€¢ [SLOW TEST:49.134 seconds]
[sig-apps] Job
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:24:19.748: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3531
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Sep 18 21:24:19.924: INFO: Waiting up to 5m0s for pod "downward-api-0de3609e-dbd0-43de-bb66-b3e3dd75589d" in namespace "downward-api-3531" to be "success or failure"
Sep 18 21:24:19.927: INFO: Pod "downward-api-0de3609e-dbd0-43de-bb66-b3e3dd75589d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.292874ms
Sep 18 21:24:21.931: INFO: Pod "downward-api-0de3609e-dbd0-43de-bb66-b3e3dd75589d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007125s
Sep 18 21:24:23.934: INFO: Pod "downward-api-0de3609e-dbd0-43de-bb66-b3e3dd75589d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010775335s
STEP: Saw pod success
Sep 18 21:24:23.934: INFO: Pod "downward-api-0de3609e-dbd0-43de-bb66-b3e3dd75589d" satisfied condition "success or failure"
Sep 18 21:24:23.937: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod downward-api-0de3609e-dbd0-43de-bb66-b3e3dd75589d container dapi-container: <nil>
STEP: delete the pod
Sep 18 21:24:23.961: INFO: Waiting for pod downward-api-0de3609e-dbd0-43de-bb66-b3e3dd75589d to disappear
Sep 18 21:24:23.964: INFO: Pod downward-api-0de3609e-dbd0-43de-bb66-b3e3dd75589d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:24:23.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3531" for this suite.
Sep 18 21:24:30.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:24:30.131: INFO: namespace downward-api-3531 deletion completed in 6.098076352s

â€¢ [SLOW TEST:10.383 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:24:30.131: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3082
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 21:24:30.308: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6a8080ed-1a7f-4ee7-9f9c-bdca5c2f4236" in namespace "security-context-test-3082" to be "success or failure"
Sep 18 21:24:30.321: INFO: Pod "busybox-privileged-false-6a8080ed-1a7f-4ee7-9f9c-bdca5c2f4236": Phase="Pending", Reason="", readiness=false. Elapsed: 12.684099ms
Sep 18 21:24:32.325: INFO: Pod "busybox-privileged-false-6a8080ed-1a7f-4ee7-9f9c-bdca5c2f4236": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01652406s
Sep 18 21:24:34.328: INFO: Pod "busybox-privileged-false-6a8080ed-1a7f-4ee7-9f9c-bdca5c2f4236": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020317727s
Sep 18 21:24:34.328: INFO: Pod "busybox-privileged-false-6a8080ed-1a7f-4ee7-9f9c-bdca5c2f4236" satisfied condition "success or failure"
Sep 18 21:24:34.367: INFO: Got logs for pod "busybox-privileged-false-6a8080ed-1a7f-4ee7-9f9c-bdca5c2f4236": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:24:34.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3082" for this suite.
Sep 18 21:24:40.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:24:40.470: INFO: namespace security-context-test-3082 deletion completed in 6.098861948s

â€¢ [SLOW TEST:10.338 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:24:40.471: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2957
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 21:24:40.695: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep 18 21:24:40.707: INFO: Number of nodes with available pods: 0
Sep 18 21:24:40.707: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep 18 21:24:40.737: INFO: Number of nodes with available pods: 0
Sep 18 21:24:40.737: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 21:24:41.741: INFO: Number of nodes with available pods: 0
Sep 18 21:24:41.741: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 21:24:42.741: INFO: Number of nodes with available pods: 0
Sep 18 21:24:42.741: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 21:24:43.741: INFO: Number of nodes with available pods: 1
Sep 18 21:24:43.741: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep 18 21:24:43.765: INFO: Number of nodes with available pods: 1
Sep 18 21:24:43.765: INFO: Number of running nodes: 0, number of available pods: 1
Sep 18 21:24:44.769: INFO: Number of nodes with available pods: 0
Sep 18 21:24:44.769: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep 18 21:24:44.781: INFO: Number of nodes with available pods: 0
Sep 18 21:24:44.781: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 21:24:45.785: INFO: Number of nodes with available pods: 0
Sep 18 21:24:45.785: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 21:24:46.785: INFO: Number of nodes with available pods: 0
Sep 18 21:24:46.785: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 21:24:47.785: INFO: Number of nodes with available pods: 0
Sep 18 21:24:47.785: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 21:24:48.785: INFO: Number of nodes with available pods: 0
Sep 18 21:24:48.785: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 21:24:49.788: INFO: Number of nodes with available pods: 0
Sep 18 21:24:49.788: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 21:24:50.785: INFO: Number of nodes with available pods: 0
Sep 18 21:24:50.785: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 21:24:51.785: INFO: Number of nodes with available pods: 0
Sep 18 21:24:51.785: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 21:24:52.785: INFO: Number of nodes with available pods: 0
Sep 18 21:24:52.785: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 21:24:53.784: INFO: Number of nodes with available pods: 0
Sep 18 21:24:53.785: INFO: Node k8s-agentpool1-40209065-vmss000000 is running more than one daemon pod
Sep 18 21:24:54.785: INFO: Number of nodes with available pods: 1
Sep 18 21:24:54.785: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2957, will wait for the garbage collector to delete the pods
I0918 21:24:54.794899      14 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 21:24:54.794927      14 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 18 21:24:54.850: INFO: Deleting DaemonSet.extensions daemon-set took: 5.511856ms
Sep 18 21:24:55.150: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.328523ms
I0918 21:24:55.150832      14 controller_utils.go:810] Ignoring inactive pod daemonsets-2957/daemon-set-hg8fq in state Running, deletion time 2019-09-18 21:25:25 +0000 UTC
Sep 18 21:25:01.953: INFO: Number of nodes with available pods: 0
Sep 18 21:25:01.953: INFO: Number of running nodes: 0, number of available pods: 0
Sep 18 21:25:01.959: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2957/daemonsets","resourceVersion":"24835"},"items":null}

Sep 18 21:25:01.962: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2957/pods","resourceVersion":"24835"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:25:01.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2957" for this suite.
Sep 18 21:25:08.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:25:08.099: INFO: namespace daemonsets-2957 deletion completed in 6.106143053s

â€¢ [SLOW TEST:27.628 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:25:08.101: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7417
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 18 21:25:16.320: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 18 21:25:16.323: INFO: Pod pod-with-prestop-http-hook still exists
Sep 18 21:25:18.323: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 18 21:25:18.327: INFO: Pod pod-with-prestop-http-hook still exists
Sep 18 21:25:20.323: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 18 21:25:20.327: INFO: Pod pod-with-prestop-http-hook still exists
Sep 18 21:25:22.323: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 18 21:25:22.333: INFO: Pod pod-with-prestop-http-hook still exists
Sep 18 21:25:24.323: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 18 21:25:24.327: INFO: Pod pod-with-prestop-http-hook still exists
Sep 18 21:25:26.323: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 18 21:25:26.327: INFO: Pod pod-with-prestop-http-hook still exists
Sep 18 21:25:28.323: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 18 21:25:28.329: INFO: Pod pod-with-prestop-http-hook still exists
Sep 18 21:25:30.323: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 18 21:25:30.328: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:25:30.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7417" for this suite.
Sep 18 21:25:42.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:25:42.456: INFO: namespace container-lifecycle-hook-7417 deletion completed in 12.116651324s

â€¢ [SLOW TEST:34.355 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:25:42.456: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8496
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 18 21:25:42.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-8496'
Sep 18 21:25:42.736: INFO: stderr: ""
Sep 18 21:25:42.736: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
I0918 21:25:42.736416      14 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 21:25:42.736463      14 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
STEP: verifying the pod e2e-test-httpd-pod was created
Sep 18 21:25:47.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 get pod e2e-test-httpd-pod --namespace=kubectl-8496 -o json'
Sep 18 21:25:47.873: INFO: stderr: ""
Sep 18 21:25:47.873: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-09-18T21:25:42Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8496\",\n        \"resourceVersion\": \"24981\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-8496/pods/e2e-test-httpd-pod\",\n        \"uid\": \"df089ad3-4435-43e7-ba37-e2d3cd59f081\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-xhktk\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-agentpool1-40209065-vmss000001\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-xhktk\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-xhktk\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-18T21:25:42Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-18T21:25:45Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-18T21:25:45Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-18T21:25:42Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://00c91ef45b1888e51f2bc23c2e115f5630a800044f337be6219f363e4a78419a\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-09-18T21:25:45Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.240.0.35\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.240.0.49\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.240.0.49\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-09-18T21:25:42Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep 18 21:25:47.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 replace -f - --namespace=kubectl-8496'
Sep 18 21:25:48.384: INFO: stderr: ""
Sep 18 21:25:48.384: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Sep 18 21:25:48.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 delete pods e2e-test-httpd-pod --namespace=kubectl-8496'
Sep 18 21:25:51.719: INFO: stderr: ""
Sep 18 21:25:51.719: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:25:51.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8496" for this suite.
Sep 18 21:25:57.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:25:57.834: INFO: namespace kubectl-8496 deletion completed in 6.109704767s

â€¢ [SLOW TEST:15.379 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:25:57.835: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1389
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 21:25:58.010: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep 18 21:25:58.028: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 18 21:26:02.054: INFO: Creating deployment "test-rolling-update-deployment"
Sep 18 21:26:02.058: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep 18 21:26:02.076: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep 18 21:26:04.083: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep 18 21:26:04.085: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704438762, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704438762, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704438762, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704438762, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 18 21:26:06.089: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Sep 18 21:26:06.098: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1389 /apis/apps/v1/namespaces/deployment-1389/deployments/test-rolling-update-deployment 37be9d90-7549-4040-afc9-1d74f8354eec 25082 1 2019-09-18 21:26:02 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005615fe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-09-18 21:26:02 +0000 UTC,LastTransitionTime:2019-09-18 21:26:02 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2019-09-18 21:26:04 +0000 UTC,LastTransitionTime:2019-09-18 21:26:02 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 18 21:26:06.101: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-1389 /apis/apps/v1/namespaces/deployment-1389/replicasets/test-rolling-update-deployment-55d946486 409029cd-dc56-41eb-8fed-3f5f5bc31e3f 25070 1 2019-09-18 21:26:02 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 37be9d90-7549-4040-afc9-1d74f8354eec 0xc006e4e4e0 0xc006e4e4e1}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006e4e548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 18 21:26:06.101: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep 18 21:26:06.101: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1389 /apis/apps/v1/namespaces/deployment-1389/replicasets/test-rolling-update-controller 845bb6b7-bec3-40f4-833a-3f73ccea129a 25081 2 2019-09-18 21:25:58 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 37be9d90-7549-4040-afc9-1d74f8354eec 0xc006e4e417 0xc006e4e418}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006e4e478 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 18 21:26:06.105: INFO: Pod "test-rolling-update-deployment-55d946486-sh47x" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-sh47x test-rolling-update-deployment-55d946486- deployment-1389 /api/v1/namespaces/deployment-1389/pods/test-rolling-update-deployment-55d946486-sh47x 1036deef-740d-4cab-9648-88c7923f2c0d 25069 0 2019-09-18 21:26:02 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 409029cd-dc56-41eb-8fed-3f5f5bc31e3f 0xc0070f9e70 0xc0070f9e71}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nkqmj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nkqmj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nkqmj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:26:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:26:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:26:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:26:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.35,PodIP:10.240.0.41,StartTime:2019-09-18 21:26:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-18 21:26:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://14d15f14111b203a3e1aa78800687abae544091b678b08864803816adf74566c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.41,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:26:06.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1389" for this suite.
Sep 18 21:26:12.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:26:12.228: INFO: namespace deployment-1389 deletion completed in 6.118837815s

â€¢ [SLOW TEST:14.393 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:26:12.228: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2627
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:26:16.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2627" for this suite.
Sep 18 21:27:00.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:27:00.564: INFO: namespace kubelet-test-2627 deletion completed in 44.091927943s

â€¢ [SLOW TEST:48.336 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:27:00.569: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6270
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6270
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-6270
I0918 21:27:00.786060      14 runners.go:184] Created replication controller with name: externalname-service, namespace: services-6270, replica count: 2
I0918 21:27:00.788409      14 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 21:27:00.788585      14 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 21:27:03.839893      14 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 18 21:27:06.840: INFO: Creating new exec pod
I0918 21:27:06.840121      14 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0918 21:27:10.859600      14 reflector.go:120] Starting reflector *v1.Endpoints (0s) from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
I0918 21:27:10.859649      14 reflector.go:158] Listing and watching *v1.Endpoints from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
Sep 18 21:27:11.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=services-6270 execpodf7p6d -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep 18 21:27:12.115: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 18 21:27:12.116: INFO: stdout: ""
Sep 18 21:27:12.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=services-6270 execpodf7p6d -- /bin/sh -x -c nc -zv -t -w 2 10.0.118.42 80'
Sep 18 21:27:12.370: INFO: stderr: "+ nc -zv -t -w 2 10.0.118.42 80\nConnection to 10.0.118.42 80 port [tcp/http] succeeded!\n"
Sep 18 21:27:12.370: INFO: stdout: ""
Sep 18 21:27:12.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=services-6270 execpodf7p6d -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.4 31502'
Sep 18 21:27:12.627: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.4 31502\nConnection to 10.240.0.4 31502 port [tcp/31502] succeeded!\n"
Sep 18 21:27:12.627: INFO: stdout: ""
Sep 18 21:27:12.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 exec --namespace=services-6270 execpodf7p6d -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.35 31502'
Sep 18 21:27:12.925: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.35 31502\nConnection to 10.240.0.35 31502 port [tcp/31502] succeeded!\n"
Sep 18 21:27:12.925: INFO: stdout: ""
Sep 18 21:27:12.926: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:27:12.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6270" for this suite.
Sep 18 21:27:19.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:27:19.140: INFO: namespace services-6270 deletion completed in 6.138385379s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:18.572 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:27:19.141: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7407
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-269427cc-82fd-4a89-9692-0e51edad5fa7
STEP: Creating a pod to test consume secrets
Sep 18 21:27:19.438: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-41ece613-775d-4f11-97b7-3a8d38b79704" in namespace "projected-7407" to be "success or failure"
Sep 18 21:27:19.441: INFO: Pod "pod-projected-secrets-41ece613-775d-4f11-97b7-3a8d38b79704": Phase="Pending", Reason="", readiness=false. Elapsed: 2.917678ms
Sep 18 21:27:21.445: INFO: Pod "pod-projected-secrets-41ece613-775d-4f11-97b7-3a8d38b79704": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006991206s
Sep 18 21:27:23.449: INFO: Pod "pod-projected-secrets-41ece613-775d-4f11-97b7-3a8d38b79704": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01086074s
STEP: Saw pod success
Sep 18 21:27:23.449: INFO: Pod "pod-projected-secrets-41ece613-775d-4f11-97b7-3a8d38b79704" satisfied condition "success or failure"
Sep 18 21:27:23.452: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-projected-secrets-41ece613-775d-4f11-97b7-3a8d38b79704 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 18 21:27:23.471: INFO: Waiting for pod pod-projected-secrets-41ece613-775d-4f11-97b7-3a8d38b79704 to disappear
Sep 18 21:27:23.474: INFO: Pod pod-projected-secrets-41ece613-775d-4f11-97b7-3a8d38b79704 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:27:23.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7407" for this suite.
Sep 18 21:27:29.488: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:27:29.584: INFO: namespace projected-7407 deletion completed in 6.105908604s

â€¢ [SLOW TEST:10.443 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:27:29.586: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-858
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 18 21:27:29.752: INFO: Waiting up to 5m0s for pod "pod-a89e723d-532d-479b-92b6-71905c3ee30e" in namespace "emptydir-858" to be "success or failure"
Sep 18 21:27:29.757: INFO: Pod "pod-a89e723d-532d-479b-92b6-71905c3ee30e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.438058ms
Sep 18 21:27:31.762: INFO: Pod "pod-a89e723d-532d-479b-92b6-71905c3ee30e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009999207s
Sep 18 21:27:33.765: INFO: Pod "pod-a89e723d-532d-479b-92b6-71905c3ee30e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013638167s
STEP: Saw pod success
Sep 18 21:27:33.765: INFO: Pod "pod-a89e723d-532d-479b-92b6-71905c3ee30e" satisfied condition "success or failure"
Sep 18 21:27:33.770: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod pod-a89e723d-532d-479b-92b6-71905c3ee30e container test-container: <nil>
STEP: delete the pod
Sep 18 21:27:33.814: INFO: Waiting for pod pod-a89e723d-532d-479b-92b6-71905c3ee30e to disappear
Sep 18 21:27:33.818: INFO: Pod pod-a89e723d-532d-479b-92b6-71905c3ee30e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:27:33.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-858" for this suite.
Sep 18 21:27:39.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:27:39.935: INFO: namespace emptydir-858 deletion completed in 6.112749524s

â€¢ [SLOW TEST:10.349 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:27:39.936: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8020
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 18 21:27:40.389: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 18 21:27:42.399: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704438860, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704438860, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704438860, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704438860, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 18 21:27:45.420: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:27:57.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8020" for this suite.
Sep 18 21:28:03.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:28:03.709: INFO: namespace webhook-8020 deletion completed in 6.160378819s
STEP: Destroying namespace "webhook-8020-markers" for this suite.
Sep 18 21:28:09.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:28:09.808: INFO: namespace webhook-8020-markers deletion completed in 6.098493834s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:29.889 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:28:09.826: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8670
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-d067638a-32d8-4f06-85f4-303d7b5eef76
STEP: Creating a pod to test consume secrets
Sep 18 21:28:09.997: INFO: Waiting up to 5m0s for pod "pod-secrets-29023663-9dd0-426c-adb4-5a661bef1d00" in namespace "secrets-8670" to be "success or failure"
Sep 18 21:28:10.004: INFO: Pod "pod-secrets-29023663-9dd0-426c-adb4-5a661bef1d00": Phase="Pending", Reason="", readiness=false. Elapsed: 7.218145ms
Sep 18 21:28:12.008: INFO: Pod "pod-secrets-29023663-9dd0-426c-adb4-5a661bef1d00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010983888s
Sep 18 21:28:14.011: INFO: Pod "pod-secrets-29023663-9dd0-426c-adb4-5a661bef1d00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014428838s
STEP: Saw pod success
Sep 18 21:28:14.011: INFO: Pod "pod-secrets-29023663-9dd0-426c-adb4-5a661bef1d00" satisfied condition "success or failure"
Sep 18 21:28:14.014: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-secrets-29023663-9dd0-426c-adb4-5a661bef1d00 container secret-volume-test: <nil>
STEP: delete the pod
Sep 18 21:28:14.033: INFO: Waiting for pod pod-secrets-29023663-9dd0-426c-adb4-5a661bef1d00 to disappear
Sep 18 21:28:14.036: INFO: Pod pod-secrets-29023663-9dd0-426c-adb4-5a661bef1d00 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:28:14.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8670" for this suite.
Sep 18 21:28:20.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:28:20.129: INFO: namespace secrets-8670 deletion completed in 6.089058473s

â€¢ [SLOW TEST:10.303 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:28:20.129: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4153
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-afc00946-372b-4dde-bef0-c35dab8e10c5
STEP: Creating a pod to test consume secrets
Sep 18 21:28:20.299: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-958d53ed-bcde-4488-bffa-338f317cb135" in namespace "projected-4153" to be "success or failure"
Sep 18 21:28:20.309: INFO: Pod "pod-projected-secrets-958d53ed-bcde-4488-bffa-338f317cb135": Phase="Pending", Reason="", readiness=false. Elapsed: 9.691926ms
Sep 18 21:28:22.313: INFO: Pod "pod-projected-secrets-958d53ed-bcde-4488-bffa-338f317cb135": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014119986s
Sep 18 21:28:24.318: INFO: Pod "pod-projected-secrets-958d53ed-bcde-4488-bffa-338f317cb135": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018376451s
STEP: Saw pod success
Sep 18 21:28:24.318: INFO: Pod "pod-projected-secrets-958d53ed-bcde-4488-bffa-338f317cb135" satisfied condition "success or failure"
Sep 18 21:28:24.321: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod pod-projected-secrets-958d53ed-bcde-4488-bffa-338f317cb135 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 18 21:28:24.338: INFO: Waiting for pod pod-projected-secrets-958d53ed-bcde-4488-bffa-338f317cb135 to disappear
Sep 18 21:28:24.341: INFO: Pod pod-projected-secrets-958d53ed-bcde-4488-bffa-338f317cb135 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:28:24.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4153" for this suite.
Sep 18 21:28:30.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:28:30.442: INFO: namespace projected-4153 deletion completed in 6.097640672s

â€¢ [SLOW TEST:10.313 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:28:30.443: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3307
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-46ac777b-266f-471e-8e83-b2806d2135cc
STEP: Creating a pod to test consume configMaps
Sep 18 21:28:30.693: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ca20dca8-fcc5-46fb-80a5-1b399d65bd69" in namespace "projected-3307" to be "success or failure"
Sep 18 21:28:30.697: INFO: Pod "pod-projected-configmaps-ca20dca8-fcc5-46fb-80a5-1b399d65bd69": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053069ms
Sep 18 21:28:32.701: INFO: Pod "pod-projected-configmaps-ca20dca8-fcc5-46fb-80a5-1b399d65bd69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007746456s
Sep 18 21:28:34.706: INFO: Pod "pod-projected-configmaps-ca20dca8-fcc5-46fb-80a5-1b399d65bd69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01228634s
STEP: Saw pod success
Sep 18 21:28:34.706: INFO: Pod "pod-projected-configmaps-ca20dca8-fcc5-46fb-80a5-1b399d65bd69" satisfied condition "success or failure"
Sep 18 21:28:34.708: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-projected-configmaps-ca20dca8-fcc5-46fb-80a5-1b399d65bd69 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 18 21:28:34.726: INFO: Waiting for pod pod-projected-configmaps-ca20dca8-fcc5-46fb-80a5-1b399d65bd69 to disappear
Sep 18 21:28:34.730: INFO: Pod pod-projected-configmaps-ca20dca8-fcc5-46fb-80a5-1b399d65bd69 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:28:34.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3307" for this suite.
Sep 18 21:28:40.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:28:40.828: INFO: namespace projected-3307 deletion completed in 6.093830466s

â€¢ [SLOW TEST:10.385 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:28:40.828: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7985
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 18 21:28:41.002: INFO: Waiting up to 5m0s for pod "pod-9c49262a-fb75-48dd-ae90-322b95948094" in namespace "emptydir-7985" to be "success or failure"
Sep 18 21:28:41.013: INFO: Pod "pod-9c49262a-fb75-48dd-ae90-322b95948094": Phase="Pending", Reason="", readiness=false. Elapsed: 10.648019ms
Sep 18 21:28:43.025: INFO: Pod "pod-9c49262a-fb75-48dd-ae90-322b95948094": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023392057s
Sep 18 21:28:45.030: INFO: Pod "pod-9c49262a-fb75-48dd-ae90-322b95948094": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027529365s
Sep 18 21:28:47.035: INFO: Pod "pod-9c49262a-fb75-48dd-ae90-322b95948094": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03261567s
STEP: Saw pod success
Sep 18 21:28:47.035: INFO: Pod "pod-9c49262a-fb75-48dd-ae90-322b95948094" satisfied condition "success or failure"
Sep 18 21:28:47.039: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod pod-9c49262a-fb75-48dd-ae90-322b95948094 container test-container: <nil>
STEP: delete the pod
Sep 18 21:28:47.072: INFO: Waiting for pod pod-9c49262a-fb75-48dd-ae90-322b95948094 to disappear
Sep 18 21:28:47.077: INFO: Pod pod-9c49262a-fb75-48dd-ae90-322b95948094 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:28:47.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7985" for this suite.
Sep 18 21:28:53.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:28:53.206: INFO: namespace emptydir-7985 deletion completed in 6.121459529s

â€¢ [SLOW TEST:12.378 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:28:53.206: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 18 21:28:53.921: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 18 21:28:55.931: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704438933, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704438933, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704438933, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704438933, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 18 21:28:58.954: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 21:28:58.957: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:29:00.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2" for this suite.
Sep 18 21:29:06.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:29:06.320: INFO: namespace webhook-2 deletion completed in 6.105581428s
STEP: Destroying namespace "webhook-2-markers" for this suite.
Sep 18 21:29:12.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:29:12.420: INFO: namespace webhook-2-markers deletion completed in 6.099842607s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:19.226 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:29:12.433: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2108
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Sep 18 21:29:17.143: INFO: Successfully updated pod "labelsupdate3887b96e-a1d5-4182-a524-647657ff852a"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:29:19.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2108" for this suite.
Sep 18 21:29:47.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:29:47.267: INFO: namespace projected-2108 deletion completed in 28.101619087s

â€¢ [SLOW TEST:34.833 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:29:47.267: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6873
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 21:29:47.435: INFO: Creating deployment "webserver-deployment"
Sep 18 21:29:47.439: INFO: Waiting for observed generation 1
Sep 18 21:29:49.459: INFO: Waiting for all required pods to come up
Sep 18 21:29:49.463: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep 18 21:29:53.491: INFO: Waiting for deployment "webserver-deployment" to complete
Sep 18 21:29:53.498: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep 18 21:29:53.517: INFO: Updating deployment webserver-deployment
Sep 18 21:29:53.517: INFO: Waiting for observed generation 2
Sep 18 21:29:55.528: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep 18 21:29:55.530: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep 18 21:29:55.533: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 18 21:29:55.547: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep 18 21:29:55.547: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep 18 21:29:55.549: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 18 21:29:55.554: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep 18 21:29:55.554: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep 18 21:29:55.565: INFO: Updating deployment webserver-deployment
Sep 18 21:29:55.566: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep 18 21:29:55.578: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep 18 21:29:57.593: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Sep 18 21:29:57.599: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6873 /apis/apps/v1/namespaces/deployment-6873/deployments/webserver-deployment e8910f52-fd7f-42b8-a691-44071dbb5506 26186 3 2019-09-18 21:29:47 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0039547a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-09-18 21:29:55 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2019-09-18 21:29:55 +0000 UTC,LastTransitionTime:2019-09-18 21:29:47 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Sep 18 21:29:57.602: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-6873 /apis/apps/v1/namespaces/deployment-6873/replicasets/webserver-deployment-c7997dcc8 9e5c3fbe-3877-4c0d-8099-2f544c53190a 26180 3 2019-09-18 21:29:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e8910f52-fd7f-42b8-a691-44071dbb5506 0xc003954d17 0xc003954d18}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003954d88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 18 21:29:57.602: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep 18 21:29:57.602: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-6873 /apis/apps/v1/namespaces/deployment-6873/replicasets/webserver-deployment-595b5b9587 32da21c9-eb1b-493b-9927-83d5c18fad3c 26177 3 2019-09-18 21:29:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e8910f52-fd7f-42b8-a691-44071dbb5506 0xc003954c57 0xc003954c58}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003954cb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Sep 18 21:29:57.615: INFO: Pod "webserver-deployment-595b5b9587-2fmg2" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2fmg2 webserver-deployment-595b5b9587- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-595b5b9587-2fmg2 69ef7b01-1acb-4075-b83e-aaf17ba102cf 26221 0 2019-09-18 21:29:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 32da21c9-eb1b-493b-9927-83d5c18fad3c 0xc00424f087 0xc00424f088}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.66,PodIP:,StartTime:2019-09-18 21:29:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.615: INFO: Pod "webserver-deployment-595b5b9587-2wmg5" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2wmg5 webserver-deployment-595b5b9587- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-595b5b9587-2wmg5 77b1c0b6-7a3a-4b5d-9318-8c4bdf2f74f4 26018 0 2019-09-18 21:29:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 32da21c9-eb1b-493b-9927-83d5c18fad3c 0xc00424f1e7 0xc00424f1e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.66,PodIP:10.240.0.73,StartTime:2019-09-18 21:29:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-18 21:29:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f7153aa46b4697e3c069bfc2442ec2ef1de3e40e1d5c2d48f4d8e4fa1beeb367,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.73,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.615: INFO: Pod "webserver-deployment-595b5b9587-2zvkb" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2zvkb webserver-deployment-595b5b9587- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-595b5b9587-2zvkb 505534f1-93f0-416a-85a7-77b10eac7b31 26205 0 2019-09-18 21:29:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 32da21c9-eb1b-493b-9927-83d5c18fad3c 0xc00424f350 0xc00424f351}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-09-18 21:29:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.615: INFO: Pod "webserver-deployment-595b5b9587-5nsvn" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5nsvn webserver-deployment-595b5b9587- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-595b5b9587-5nsvn 73d8cea7-362d-476e-9948-a7bd91b06623 26193 0 2019-09-18 21:29:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 32da21c9-eb1b-493b-9927-83d5c18fad3c 0xc00424f490 0xc00424f491}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.35,PodIP:,StartTime:2019-09-18 21:29:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.616: INFO: Pod "webserver-deployment-595b5b9587-5wjwl" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5wjwl webserver-deployment-595b5b9587- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-595b5b9587-5wjwl 60f98b02-758e-4374-914b-a12b8a83dcff 26207 0 2019-09-18 21:29:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 32da21c9-eb1b-493b-9927-83d5c18fad3c 0xc00424f5d7 0xc00424f5d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.35,PodIP:,StartTime:2019-09-18 21:29:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.616: INFO: Pod "webserver-deployment-595b5b9587-8lc8q" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-8lc8q webserver-deployment-595b5b9587- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-595b5b9587-8lc8q c2bb1fa7-853c-455b-9ef7-fa6c19efc5a7 26172 0 2019-09-18 21:29:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 32da21c9-eb1b-493b-9927-83d5c18fad3c 0xc00424f727 0xc00424f728}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.35,PodIP:,StartTime:2019-09-18 21:29:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.616: INFO: Pod "webserver-deployment-595b5b9587-ctc96" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-ctc96 webserver-deployment-595b5b9587- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-595b5b9587-ctc96 8dfd98cb-3c0d-412c-8c05-270bee540162 26133 0 2019-09-18 21:29:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 32da21c9-eb1b-493b-9927-83d5c18fad3c 0xc00424f877 0xc00424f878}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.66,PodIP:,StartTime:2019-09-18 21:29:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.616: INFO: Pod "webserver-deployment-595b5b9587-jfqbf" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jfqbf webserver-deployment-595b5b9587- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-595b5b9587-jfqbf 9edca547-55d5-4bf5-bd6d-4db73276e7f6 26222 0 2019-09-18 21:29:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 32da21c9-eb1b-493b-9927-83d5c18fad3c 0xc00424f9c7 0xc00424f9c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.35,PodIP:,StartTime:2019-09-18 21:29:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.616: INFO: Pod "webserver-deployment-595b5b9587-jfwbm" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jfwbm webserver-deployment-595b5b9587- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-595b5b9587-jfwbm e5fce1b0-9d44-426f-a7e7-737fc85cd2a6 26006 0 2019-09-18 21:29:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 32da21c9-eb1b-493b-9927-83d5c18fad3c 0xc00424fb17 0xc00424fb18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:10.240.0.30,StartTime:2019-09-18 21:29:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-18 21:29:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://cffccda4a3fdd7c911f5a447089cb45c6b433dc7cca0fa88a96b4681df421b50,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.30,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.616: INFO: Pod "webserver-deployment-595b5b9587-kf984" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-kf984 webserver-deployment-595b5b9587- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-595b5b9587-kf984 8b65f6ee-9531-4fad-b46d-9d38997832fe 26035 0 2019-09-18 21:29:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 32da21c9-eb1b-493b-9927-83d5c18fad3c 0xc00424fc90 0xc00424fc91}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.35,PodIP:10.240.0.58,StartTime:2019-09-18 21:29:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-18 21:29:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://da273af89291072f4d7ec078557321cb31ea84e57f5c041bbcf0b4c9e7e0f778,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.617: INFO: Pod "webserver-deployment-595b5b9587-ppc6f" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-ppc6f webserver-deployment-595b5b9587- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-595b5b9587-ppc6f 0ef502b4-584f-405c-93ff-85b6b342c3d6 26174 0 2019-09-18 21:29:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 32da21c9-eb1b-493b-9927-83d5c18fad3c 0xc00424fdf0 0xc00424fdf1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-09-18 21:29:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.617: INFO: Pod "webserver-deployment-595b5b9587-sqtxb" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-sqtxb webserver-deployment-595b5b9587- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-595b5b9587-sqtxb e3007f70-1619-4f1b-ab13-19980c1ac67f 26015 0 2019-09-18 21:29:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 32da21c9-eb1b-493b-9927-83d5c18fad3c 0xc00424ff30 0xc00424ff31}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.66,PodIP:10.240.0.67,StartTime:2019-09-18 21:29:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-18 21:29:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://abc876314d7f608a5d0cf737e35cc29cd2ceed8a16217332aa6818e8a1a91457,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.67,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.617: INFO: Pod "webserver-deployment-595b5b9587-tzqsg" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-tzqsg webserver-deployment-595b5b9587- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-595b5b9587-tzqsg c70a5898-10ab-43e4-ba59-3b31e786f531 26200 0 2019-09-18 21:29:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 32da21c9-eb1b-493b-9927-83d5c18fad3c 0xc0060be090 0xc0060be091}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.66,PodIP:,StartTime:2019-09-18 21:29:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.617: INFO: Pod "webserver-deployment-595b5b9587-vtxkn" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vtxkn webserver-deployment-595b5b9587- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-595b5b9587-vtxkn b89a5a69-e452-4503-bb0d-d256a0fb7c3e 26009 0 2019-09-18 21:29:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 32da21c9-eb1b-493b-9927-83d5c18fad3c 0xc0060be1d7 0xc0060be1d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:10.240.0.10,StartTime:2019-09-18 21:29:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-18 21:29:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://d3936101ff3234c5b413d9db65bd888e5cf704c0e54d8ef6d572e5e58d0f91b0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.617: INFO: Pod "webserver-deployment-595b5b9587-vxszf" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vxszf webserver-deployment-595b5b9587- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-595b5b9587-vxszf a4d074ea-b14b-4d2c-b11c-fa9534d36c81 26029 0 2019-09-18 21:29:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 32da21c9-eb1b-493b-9927-83d5c18fad3c 0xc0060be340 0xc0060be341}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.35,PodIP:10.240.0.45,StartTime:2019-09-18 21:29:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-18 21:29:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://457a4bae7c8a7a27a459de76b25733d0af82f04f804d55b15c73dc36a2e7cae7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.617: INFO: Pod "webserver-deployment-595b5b9587-wdf4z" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-wdf4z webserver-deployment-595b5b9587- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-595b5b9587-wdf4z 4240bc72-867e-40d0-98fc-3e9504ad06e8 26026 0 2019-09-18 21:29:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 32da21c9-eb1b-493b-9927-83d5c18fad3c 0xc0060be4b0 0xc0060be4b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.35,PodIP:10.240.0.61,StartTime:2019-09-18 21:29:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-18 21:29:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://37c56b6c8a6037ca552d2abec0f803cc57a281bffe65fc923dd9a16209114d15,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.618: INFO: Pod "webserver-deployment-595b5b9587-wmxwj" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-wmxwj webserver-deployment-595b5b9587- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-595b5b9587-wmxwj bddffc6c-c945-4c0b-bcde-1b32eeb57916 26190 0 2019-09-18 21:29:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 32da21c9-eb1b-493b-9927-83d5c18fad3c 0xc0060be610 0xc0060be611}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.66,PodIP:,StartTime:2019-09-18 21:29:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.618: INFO: Pod "webserver-deployment-595b5b9587-wsl96" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-wsl96 webserver-deployment-595b5b9587- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-595b5b9587-wsl96 4efd6609-67ff-4eb8-8d0e-377e89f08e21 26021 0 2019-09-18 21:29:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 32da21c9-eb1b-493b-9927-83d5c18fad3c 0xc0060be757 0xc0060be758}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.66,PodIP:10.240.0.70,StartTime:2019-09-18 21:29:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-18 21:29:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://7474e5925a65a81ac476900af2882e658c1865ce9c6fc8fc4c2091f3d147fe93,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.70,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.618: INFO: Pod "webserver-deployment-595b5b9587-x4gfs" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-x4gfs webserver-deployment-595b5b9587- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-595b5b9587-x4gfs 51e303a0-6bea-4730-a36f-6eb3e8a8542b 26192 0 2019-09-18 21:29:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 32da21c9-eb1b-493b-9927-83d5c18fad3c 0xc0060be8c0 0xc0060be8c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-09-18 21:29:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.618: INFO: Pod "webserver-deployment-595b5b9587-xdjww" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xdjww webserver-deployment-595b5b9587- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-595b5b9587-xdjww 1cd987ef-4e89-4b0c-b840-a2ac22846743 26139 0 2019-09-18 21:29:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 32da21c9-eb1b-493b-9927-83d5c18fad3c 0xc0060bea00 0xc0060bea01}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-09-18 21:29:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.618: INFO: Pod "webserver-deployment-c7997dcc8-72qz9" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-72qz9 webserver-deployment-c7997dcc8- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-c7997dcc8-72qz9 f1417b5e-22e8-448d-a13c-01d07157141d 26195 0 2019-09-18 21:29:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9e5c3fbe-3877-4c0d-8099-2f544c53190a 0xc0060beb40 0xc0060beb41}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.35,PodIP:,StartTime:2019-09-18 21:29:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.618: INFO: Pod "webserver-deployment-c7997dcc8-7tcv6" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-7tcv6 webserver-deployment-c7997dcc8- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-c7997dcc8-7tcv6 46c63757-fda2-420c-ae42-f6f57426ff38 26136 0 2019-09-18 21:29:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9e5c3fbe-3877-4c0d-8099-2f544c53190a 0xc0060beca0 0xc0060beca1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.35,PodIP:,StartTime:2019-09-18 21:29:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.619: INFO: Pod "webserver-deployment-c7997dcc8-9xhht" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-9xhht webserver-deployment-c7997dcc8- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-c7997dcc8-9xhht 7d8b2e2e-508a-4f86-af6d-09929b0d612c 26226 0 2019-09-18 21:29:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9e5c3fbe-3877-4c0d-8099-2f544c53190a 0xc0060bee00 0xc0060bee01}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.35,PodIP:10.240.0.44,StartTime:2019-09-18 21:29:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.619: INFO: Pod "webserver-deployment-c7997dcc8-b4xwn" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-b4xwn webserver-deployment-c7997dcc8- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-c7997dcc8-b4xwn 4e449c6e-b524-49ec-9c9a-457692836ca7 26188 0 2019-09-18 21:29:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9e5c3fbe-3877-4c0d-8099-2f544c53190a 0xc0060bef90 0xc0060bef91}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.35,PodIP:,StartTime:2019-09-18 21:29:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.619: INFO: Pod "webserver-deployment-c7997dcc8-bpng8" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-bpng8 webserver-deployment-c7997dcc8- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-c7997dcc8-bpng8 e8a570ff-10df-40d5-8cd1-401a595b1b8a 26198 0 2019-09-18 21:29:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9e5c3fbe-3877-4c0d-8099-2f544c53190a 0xc0060bf0f0 0xc0060bf0f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.66,PodIP:,StartTime:2019-09-18 21:29:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.619: INFO: Pod "webserver-deployment-c7997dcc8-g7n94" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-g7n94 webserver-deployment-c7997dcc8- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-c7997dcc8-g7n94 a46916b7-5986-4ca2-9014-a8341d8c708b 26194 0 2019-09-18 21:29:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9e5c3fbe-3877-4c0d-8099-2f544c53190a 0xc0060bf250 0xc0060bf251}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-09-18 21:29:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.619: INFO: Pod "webserver-deployment-c7997dcc8-gtsrt" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-gtsrt webserver-deployment-c7997dcc8- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-c7997dcc8-gtsrt 582c0b2d-3a9c-47e9-ab0f-a1f66309a91b 26087 0 2019-09-18 21:29:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9e5c3fbe-3877-4c0d-8099-2f544c53190a 0xc0060bf3b0 0xc0060bf3b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-09-18 21:29:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.620: INFO: Pod "webserver-deployment-c7997dcc8-hg274" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-hg274 webserver-deployment-c7997dcc8- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-c7997dcc8-hg274 75270f74-31cc-450e-a4b4-c25a61e16f69 26206 0 2019-09-18 21:29:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9e5c3fbe-3877-4c0d-8099-2f544c53190a 0xc0060bf510 0xc0060bf511}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.66,PodIP:,StartTime:2019-09-18 21:29:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.620: INFO: Pod "webserver-deployment-c7997dcc8-kb69t" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-kb69t webserver-deployment-c7997dcc8- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-c7997dcc8-kb69t 998a53ff-5114-4535-a71c-423bdf81a225 26169 0 2019-09-18 21:29:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9e5c3fbe-3877-4c0d-8099-2f544c53190a 0xc0060bf670 0xc0060bf671}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.66,PodIP:,StartTime:2019-09-18 21:29:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.620: INFO: Pod "webserver-deployment-c7997dcc8-sct5w" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-sct5w webserver-deployment-c7997dcc8- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-c7997dcc8-sct5w 5b932e09-24f6-400d-8e36-14c6560dad8a 26089 0 2019-09-18 21:29:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9e5c3fbe-3877-4c0d-8099-2f544c53190a 0xc0060bf7d0 0xc0060bf7d1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.66,PodIP:,StartTime:2019-09-18 21:29:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.620: INFO: Pod "webserver-deployment-c7997dcc8-sdjx7" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-sdjx7 webserver-deployment-c7997dcc8- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-c7997dcc8-sdjx7 df6e9153-381e-4616-b5f4-1168bfa55f11 26196 0 2019-09-18 21:29:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9e5c3fbe-3877-4c0d-8099-2f544c53190a 0xc0060bf930 0xc0060bf931}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-09-18 21:29:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.620: INFO: Pod "webserver-deployment-c7997dcc8-t6nn6" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-t6nn6 webserver-deployment-c7997dcc8- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-c7997dcc8-t6nn6 a577433b-c410-45cd-a440-026d5b3a0b11 26064 0 2019-09-18 21:29:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9e5c3fbe-3877-4c0d-8099-2f544c53190a 0xc0060bfa90 0xc0060bfa91}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-09-18 21:29:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 18 21:29:57.620: INFO: Pod "webserver-deployment-c7997dcc8-zl5mp" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-zl5mp webserver-deployment-c7997dcc8- deployment-6873 /api/v1/namespaces/deployment-6873/pods/webserver-deployment-c7997dcc8-zl5mp d97dd39a-acfb-43fc-bb87-ed8f21a89e65 26068 0 2019-09-18 21:29:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9e5c3fbe-3877-4c0d-8099-2f544c53190a 0xc0060bfbf0 0xc0060bfbf1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k5gq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k5gq8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k5gq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool1-40209065-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-18 21:29:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.66,PodIP:,StartTime:2019-09-18 21:29:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:29:57.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6873" for this suite.
Sep 18 21:30:05.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:30:05.748: INFO: namespace deployment-6873 deletion completed in 8.122466105s

â€¢ [SLOW TEST:18.481 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:30:05.749: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-384
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-2cf4f6b4-548a-4b7a-86f4-7dea94f40eca
STEP: Creating a pod to test consume secrets
Sep 18 21:30:05.930: INFO: Waiting up to 5m0s for pod "pod-secrets-8a8e5f64-6766-4997-8dbf-af479bd64462" in namespace "secrets-384" to be "success or failure"
Sep 18 21:30:05.945: INFO: Pod "pod-secrets-8a8e5f64-6766-4997-8dbf-af479bd64462": Phase="Pending", Reason="", readiness=false. Elapsed: 15.634582ms
Sep 18 21:30:07.950: INFO: Pod "pod-secrets-8a8e5f64-6766-4997-8dbf-af479bd64462": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020622036s
Sep 18 21:30:09.954: INFO: Pod "pod-secrets-8a8e5f64-6766-4997-8dbf-af479bd64462": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024364103s
Sep 18 21:30:11.958: INFO: Pod "pod-secrets-8a8e5f64-6766-4997-8dbf-af479bd64462": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02848437s
STEP: Saw pod success
Sep 18 21:30:11.958: INFO: Pod "pod-secrets-8a8e5f64-6766-4997-8dbf-af479bd64462" satisfied condition "success or failure"
Sep 18 21:30:11.961: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-secrets-8a8e5f64-6766-4997-8dbf-af479bd64462 container secret-env-test: <nil>
STEP: delete the pod
Sep 18 21:30:12.018: INFO: Waiting for pod pod-secrets-8a8e5f64-6766-4997-8dbf-af479bd64462 to disappear
Sep 18 21:30:12.020: INFO: Pod pod-secrets-8a8e5f64-6766-4997-8dbf-af479bd64462 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:30:12.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-384" for this suite.
Sep 18 21:30:18.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:30:18.120: INFO: namespace secrets-384 deletion completed in 6.093349811s

â€¢ [SLOW TEST:12.371 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:30:18.120: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5664
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:30:22.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5664" for this suite.
Sep 18 21:30:36.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:30:36.422: INFO: namespace containers-5664 deletion completed in 14.094681487s

â€¢ [SLOW TEST:18.302 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:30:36.423: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3133
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-3133/secret-test-48376180-e798-485e-bdc8-ce214a40664b
STEP: Creating a pod to test consume secrets
Sep 18 21:30:36.602: INFO: Waiting up to 5m0s for pod "pod-configmaps-db7d8c45-ef77-440b-bc3e-8d97f39a8008" in namespace "secrets-3133" to be "success or failure"
Sep 18 21:30:36.611: INFO: Pod "pod-configmaps-db7d8c45-ef77-440b-bc3e-8d97f39a8008": Phase="Pending", Reason="", readiness=false. Elapsed: 8.646834ms
Sep 18 21:30:38.618: INFO: Pod "pod-configmaps-db7d8c45-ef77-440b-bc3e-8d97f39a8008": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01617302s
Sep 18 21:30:40.623: INFO: Pod "pod-configmaps-db7d8c45-ef77-440b-bc3e-8d97f39a8008": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020508232s
STEP: Saw pod success
Sep 18 21:30:40.623: INFO: Pod "pod-configmaps-db7d8c45-ef77-440b-bc3e-8d97f39a8008" satisfied condition "success or failure"
Sep 18 21:30:40.630: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod pod-configmaps-db7d8c45-ef77-440b-bc3e-8d97f39a8008 container env-test: <nil>
STEP: delete the pod
Sep 18 21:30:40.654: INFO: Waiting for pod pod-configmaps-db7d8c45-ef77-440b-bc3e-8d97f39a8008 to disappear
Sep 18 21:30:40.657: INFO: Pod pod-configmaps-db7d8c45-ef77-440b-bc3e-8d97f39a8008 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:30:40.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3133" for this suite.
Sep 18 21:30:46.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:30:46.761: INFO: namespace secrets-3133 deletion completed in 6.099028409s

â€¢ [SLOW TEST:10.338 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:30:46.761: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-8381
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8293
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3155
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:30:53.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8381" for this suite.
Sep 18 21:30:59.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:30:59.389: INFO: namespace namespaces-8381 deletion completed in 6.105271922s
STEP: Destroying namespace "nsdeletetest-8293" for this suite.
Sep 18 21:30:59.397: INFO: Namespace nsdeletetest-8293 was already deleted
STEP: Destroying namespace "nsdeletetest-3155" for this suite.
Sep 18 21:31:05.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:31:05.495: INFO: namespace nsdeletetest-3155 deletion completed in 6.0986585s

â€¢ [SLOW TEST:18.734 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:31:05.497: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1003
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-c09a61dd-1707-415e-ab55-ae05a530f1c0
STEP: Creating a pod to test consume configMaps
Sep 18 21:31:05.676: INFO: Waiting up to 5m0s for pod "pod-configmaps-75f8c3ae-5dae-4b0f-aabd-d28e1b064593" in namespace "configmap-1003" to be "success or failure"
Sep 18 21:31:05.680: INFO: Pod "pod-configmaps-75f8c3ae-5dae-4b0f-aabd-d28e1b064593": Phase="Pending", Reason="", readiness=false. Elapsed: 4.374167ms
Sep 18 21:31:07.686: INFO: Pod "pod-configmaps-75f8c3ae-5dae-4b0f-aabd-d28e1b064593": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010179511s
Sep 18 21:31:09.690: INFO: Pod "pod-configmaps-75f8c3ae-5dae-4b0f-aabd-d28e1b064593": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014364869s
STEP: Saw pod success
Sep 18 21:31:09.690: INFO: Pod "pod-configmaps-75f8c3ae-5dae-4b0f-aabd-d28e1b064593" satisfied condition "success or failure"
Sep 18 21:31:09.694: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod pod-configmaps-75f8c3ae-5dae-4b0f-aabd-d28e1b064593 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 18 21:31:09.712: INFO: Waiting for pod pod-configmaps-75f8c3ae-5dae-4b0f-aabd-d28e1b064593 to disappear
Sep 18 21:31:09.715: INFO: Pod pod-configmaps-75f8c3ae-5dae-4b0f-aabd-d28e1b064593 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:31:09.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1003" for this suite.
Sep 18 21:31:15.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:31:15.822: INFO: namespace configmap-1003 deletion completed in 6.103068414s

â€¢ [SLOW TEST:10.325 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:31:15.827: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4442
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-ded485a6-7ca1-4dfc-bf3b-a7a35a17dac8
STEP: Creating a pod to test consume configMaps
Sep 18 21:31:16.008: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5e64c661-8a3b-47e9-b07a-76f6d4c573ae" in namespace "projected-4442" to be "success or failure"
Sep 18 21:31:16.018: INFO: Pod "pod-projected-configmaps-5e64c661-8a3b-47e9-b07a-76f6d4c573ae": Phase="Pending", Reason="", readiness=false. Elapsed: 9.646428ms
Sep 18 21:31:18.022: INFO: Pod "pod-projected-configmaps-5e64c661-8a3b-47e9-b07a-76f6d4c573ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0135763s
Sep 18 21:31:20.025: INFO: Pod "pod-projected-configmaps-5e64c661-8a3b-47e9-b07a-76f6d4c573ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017265478s
STEP: Saw pod success
Sep 18 21:31:20.025: INFO: Pod "pod-projected-configmaps-5e64c661-8a3b-47e9-b07a-76f6d4c573ae" satisfied condition "success or failure"
Sep 18 21:31:20.028: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod pod-projected-configmaps-5e64c661-8a3b-47e9-b07a-76f6d4c573ae container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 18 21:31:20.054: INFO: Waiting for pod pod-projected-configmaps-5e64c661-8a3b-47e9-b07a-76f6d4c573ae to disappear
Sep 18 21:31:20.057: INFO: Pod pod-projected-configmaps-5e64c661-8a3b-47e9-b07a-76f6d4c573ae no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:31:20.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4442" for this suite.
Sep 18 21:31:26.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:31:26.158: INFO: namespace projected-4442 deletion completed in 6.094249527s

â€¢ [SLOW TEST:10.331 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:31:26.158: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1637
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 18 21:31:26.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-1637'
Sep 18 21:31:26.431: INFO: stderr: ""
Sep 18 21:31:26.431: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Sep 18 21:31:26.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 delete pods e2e-test-httpd-pod --namespace=kubectl-1637'
Sep 18 21:31:31.074: INFO: stderr: ""
Sep 18 21:31:31.074: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:31:31.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1637" for this suite.
Sep 18 21:31:37.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:31:37.210: INFO: namespace kubectl-1637 deletion completed in 6.122771662s

â€¢ [SLOW TEST:11.052 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:31:37.211: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-4533
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:31:41.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4533" for this suite.
Sep 18 21:31:47.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:31:47.526: INFO: namespace emptydir-wrapper-4533 deletion completed in 6.094891414s

â€¢ [SLOW TEST:10.315 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:31:47.529: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5030
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-5030/configmap-test-684d0e87-e16c-4dc2-9465-316b9f5486a7
STEP: Creating a pod to test consume configMaps
Sep 18 21:31:47.765: INFO: Waiting up to 5m0s for pod "pod-configmaps-1420ec0f-a1a0-47ab-ab31-10d4be24b0d9" in namespace "configmap-5030" to be "success or failure"
Sep 18 21:31:47.773: INFO: Pod "pod-configmaps-1420ec0f-a1a0-47ab-ab31-10d4be24b0d9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.952841ms
Sep 18 21:31:49.777: INFO: Pod "pod-configmaps-1420ec0f-a1a0-47ab-ab31-10d4be24b0d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012139657s
Sep 18 21:31:51.781: INFO: Pod "pod-configmaps-1420ec0f-a1a0-47ab-ab31-10d4be24b0d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016467775s
STEP: Saw pod success
Sep 18 21:31:51.781: INFO: Pod "pod-configmaps-1420ec0f-a1a0-47ab-ab31-10d4be24b0d9" satisfied condition "success or failure"
Sep 18 21:31:51.785: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod pod-configmaps-1420ec0f-a1a0-47ab-ab31-10d4be24b0d9 container env-test: <nil>
STEP: delete the pod
Sep 18 21:31:51.811: INFO: Waiting for pod pod-configmaps-1420ec0f-a1a0-47ab-ab31-10d4be24b0d9 to disappear
Sep 18 21:31:51.815: INFO: Pod pod-configmaps-1420ec0f-a1a0-47ab-ab31-10d4be24b0d9 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:31:51.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5030" for this suite.
Sep 18 21:31:57.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:31:57.943: INFO: namespace configmap-5030 deletion completed in 6.123528644s

â€¢ [SLOW TEST:10.414 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:31:57.944: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-671
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 18 21:31:58.134: INFO: Waiting up to 5m0s for pod "downwardapi-volume-35669ad0-5728-46a9-ad53-582747181885" in namespace "projected-671" to be "success or failure"
Sep 18 21:31:58.145: INFO: Pod "downwardapi-volume-35669ad0-5728-46a9-ad53-582747181885": Phase="Pending", Reason="", readiness=false. Elapsed: 10.904919ms
Sep 18 21:32:00.149: INFO: Pod "downwardapi-volume-35669ad0-5728-46a9-ad53-582747181885": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014744952s
Sep 18 21:32:02.159: INFO: Pod "downwardapi-volume-35669ad0-5728-46a9-ad53-582747181885": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024565043s
STEP: Saw pod success
Sep 18 21:32:02.159: INFO: Pod "downwardapi-volume-35669ad0-5728-46a9-ad53-582747181885" satisfied condition "success or failure"
Sep 18 21:32:02.162: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod downwardapi-volume-35669ad0-5728-46a9-ad53-582747181885 container client-container: <nil>
STEP: delete the pod
Sep 18 21:32:02.181: INFO: Waiting for pod downwardapi-volume-35669ad0-5728-46a9-ad53-582747181885 to disappear
Sep 18 21:32:02.191: INFO: Pod downwardapi-volume-35669ad0-5728-46a9-ad53-582747181885 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:32:02.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-671" for this suite.
Sep 18 21:32:08.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:32:08.292: INFO: namespace projected-671 deletion completed in 6.097015985s

â€¢ [SLOW TEST:10.348 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:32:08.293: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4002
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-060fd2ed-0c1f-434f-93e7-74b979bda307
STEP: Creating a pod to test consume configMaps
Sep 18 21:32:08.489: INFO: Waiting up to 5m0s for pod "pod-configmaps-778e7382-57ab-453c-b6df-b2faa6423a50" in namespace "configmap-4002" to be "success or failure"
Sep 18 21:32:08.509: INFO: Pod "pod-configmaps-778e7382-57ab-453c-b6df-b2faa6423a50": Phase="Pending", Reason="", readiness=false. Elapsed: 20.477547ms
Sep 18 21:32:10.513: INFO: Pod "pod-configmaps-778e7382-57ab-453c-b6df-b2faa6423a50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024203695s
Sep 18 21:32:12.517: INFO: Pod "pod-configmaps-778e7382-57ab-453c-b6df-b2faa6423a50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028123144s
STEP: Saw pod success
Sep 18 21:32:12.517: INFO: Pod "pod-configmaps-778e7382-57ab-453c-b6df-b2faa6423a50" satisfied condition "success or failure"
Sep 18 21:32:12.520: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-configmaps-778e7382-57ab-453c-b6df-b2faa6423a50 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 18 21:32:12.566: INFO: Waiting for pod pod-configmaps-778e7382-57ab-453c-b6df-b2faa6423a50 to disappear
Sep 18 21:32:12.576: INFO: Pod pod-configmaps-778e7382-57ab-453c-b6df-b2faa6423a50 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:32:12.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4002" for this suite.
Sep 18 21:32:18.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:32:18.678: INFO: namespace configmap-4002 deletion completed in 6.097450123s

â€¢ [SLOW TEST:10.385 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:32:18.678: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5660
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep 18 21:32:23.901: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:32:23.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5660" for this suite.
Sep 18 21:32:41.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:32:42.049: INFO: namespace replicaset-5660 deletion completed in 18.105943664s

â€¢ [SLOW TEST:23.371 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:32:42.050: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-107
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Sep 18 21:32:42.207: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-815280101 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:32:42.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-107" for this suite.
Sep 18 21:32:48.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:32:48.390: INFO: namespace kubectl-107 deletion completed in 6.100212919s

â€¢ [SLOW TEST:6.340 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:32:48.391: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3522
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep 18 21:32:48.651: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3522 /api/v1/namespaces/watch-3522/configmaps/e2e-watch-test-watch-closed fdb449b6-c370-4b90-9c92-f5e90056e749 27127 0 2019-09-18 21:32:48 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 18 21:32:48.651: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3522 /api/v1/namespaces/watch-3522/configmaps/e2e-watch-test-watch-closed fdb449b6-c370-4b90-9c92-f5e90056e749 27128 0 2019-09-18 21:32:48 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep 18 21:32:48.664: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3522 /api/v1/namespaces/watch-3522/configmaps/e2e-watch-test-watch-closed fdb449b6-c370-4b90-9c92-f5e90056e749 27129 0 2019-09-18 21:32:48 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 18 21:32:48.664: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3522 /api/v1/namespaces/watch-3522/configmaps/e2e-watch-test-watch-closed fdb449b6-c370-4b90-9c92-f5e90056e749 27130 0 2019-09-18 21:32:48 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:32:48.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3522" for this suite.
Sep 18 21:32:54.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:32:54.778: INFO: namespace watch-3522 deletion completed in 6.109754572s

â€¢ [SLOW TEST:6.388 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:32:54.779: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5984
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Sep 18 21:32:54.947: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:33:10.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5984" for this suite.
Sep 18 21:33:16.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:33:16.476: INFO: namespace crd-publish-openapi-5984 deletion completed in 6.098040938s

â€¢ [SLOW TEST:21.697 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:33:16.477: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9136
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Sep 18 21:33:16.652: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Sep 18 21:33:16.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 create -f - --namespace=kubectl-9136'
Sep 18 21:33:19.113: INFO: stderr: ""
Sep 18 21:33:19.113: INFO: stdout: "service/redis-slave created\n"
Sep 18 21:33:19.113: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Sep 18 21:33:19.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 create -f - --namespace=kubectl-9136'
Sep 18 21:33:19.778: INFO: stderr: ""
Sep 18 21:33:19.778: INFO: stdout: "service/redis-master created\n"
Sep 18 21:33:19.779: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep 18 21:33:19.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 create -f - --namespace=kubectl-9136'
Sep 18 21:33:20.165: INFO: stderr: ""
Sep 18 21:33:20.165: INFO: stdout: "service/frontend created\n"
Sep 18 21:33:20.165: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Sep 18 21:33:20.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 create -f - --namespace=kubectl-9136'
Sep 18 21:33:20.427: INFO: stderr: ""
Sep 18 21:33:20.427: INFO: stdout: "deployment.apps/frontend created\n"
Sep 18 21:33:20.429: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 18 21:33:20.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 create -f - --namespace=kubectl-9136'
Sep 18 21:33:20.754: INFO: stderr: ""
Sep 18 21:33:20.754: INFO: stdout: "deployment.apps/redis-master created\n"
Sep 18 21:33:20.755: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Sep 18 21:33:20.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 create -f - --namespace=kubectl-9136'
Sep 18 21:33:21.154: INFO: stderr: ""
Sep 18 21:33:21.154: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Sep 18 21:33:21.154: INFO: Waiting for all frontend pods to be Running.
I0918 21:33:21.155048      14 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0918 21:33:21.155082      14 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 18 21:33:51.209: INFO: Waiting for frontend to serve content.
Sep 18 21:33:51.229: INFO: Trying to add a new entry to the guestbook.
Sep 18 21:33:51.247: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Sep 18 21:33:51.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 delete --grace-period=0 --force -f - --namespace=kubectl-9136'
Sep 18 21:33:51.435: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 18 21:33:51.435: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Sep 18 21:33:51.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 delete --grace-period=0 --force -f - --namespace=kubectl-9136'
Sep 18 21:33:51.569: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 18 21:33:51.570: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep 18 21:33:51.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 delete --grace-period=0 --force -f - --namespace=kubectl-9136'
Sep 18 21:33:51.708: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 18 21:33:51.709: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 18 21:33:51.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 delete --grace-period=0 --force -f - --namespace=kubectl-9136'
Sep 18 21:33:51.822: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 18 21:33:51.822: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 18 21:33:51.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 delete --grace-period=0 --force -f - --namespace=kubectl-9136'
Sep 18 21:33:51.932: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 18 21:33:51.932: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep 18 21:33:51.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 delete --grace-period=0 --force -f - --namespace=kubectl-9136'
Sep 18 21:33:52.041: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 18 21:33:52.041: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:33:52.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9136" for this suite.
Sep 18 21:34:04.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:34:04.149: INFO: namespace kubectl-9136 deletion completed in 12.102117276s

â€¢ [SLOW TEST:47.672 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:34:04.149: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6344
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 18 21:34:04.305: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Sep 18 21:34:07.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 --namespace=crd-publish-openapi-6344 create -f -'
Sep 18 21:34:09.425: INFO: stderr: ""
Sep 18 21:34:09.425: INFO: stdout: "e2e-test-crd-publish-openapi-8241-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 18 21:34:09.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 --namespace=crd-publish-openapi-6344 delete e2e-test-crd-publish-openapi-8241-crds test-foo'
Sep 18 21:34:09.539: INFO: stderr: ""
Sep 18 21:34:09.539: INFO: stdout: "e2e-test-crd-publish-openapi-8241-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep 18 21:34:09.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 --namespace=crd-publish-openapi-6344 apply -f -'
Sep 18 21:34:09.783: INFO: stderr: ""
Sep 18 21:34:09.783: INFO: stdout: "e2e-test-crd-publish-openapi-8241-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 18 21:34:09.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 --namespace=crd-publish-openapi-6344 delete e2e-test-crd-publish-openapi-8241-crds test-foo'
Sep 18 21:34:09.882: INFO: stderr: ""
Sep 18 21:34:09.882: INFO: stdout: "e2e-test-crd-publish-openapi-8241-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Sep 18 21:34:09.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 --namespace=crd-publish-openapi-6344 create -f -'
Sep 18 21:34:10.150: INFO: rc: 1
Sep 18 21:34:10.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 --namespace=crd-publish-openapi-6344 apply -f -'
Sep 18 21:34:10.393: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Sep 18 21:34:10.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 --namespace=crd-publish-openapi-6344 create -f -'
Sep 18 21:34:10.608: INFO: rc: 1
Sep 18 21:34:10.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 --namespace=crd-publish-openapi-6344 apply -f -'
Sep 18 21:34:10.826: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Sep 18 21:34:10.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 explain e2e-test-crd-publish-openapi-8241-crds'
Sep 18 21:34:11.078: INFO: stderr: ""
Sep 18 21:34:11.079: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8241-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Sep 18 21:34:11.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 explain e2e-test-crd-publish-openapi-8241-crds.metadata'
Sep 18 21:34:11.290: INFO: stderr: ""
Sep 18 21:34:11.290: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8241-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep 18 21:34:11.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 explain e2e-test-crd-publish-openapi-8241-crds.spec'
Sep 18 21:34:11.531: INFO: stderr: ""
Sep 18 21:34:11.531: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8241-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep 18 21:34:11.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 explain e2e-test-crd-publish-openapi-8241-crds.spec.bars'
Sep 18 21:34:11.780: INFO: stderr: ""
Sep 18 21:34:11.780: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8241-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Sep 18 21:34:11.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 explain e2e-test-crd-publish-openapi-8241-crds.spec.bars2'
Sep 18 21:34:12.030: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:34:15.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6344" for this suite.
Sep 18 21:34:21.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:34:21.664: INFO: namespace crd-publish-openapi-6344 deletion completed in 6.096266869s

â€¢ [SLOW TEST:17.515 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:34:21.667: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2422
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-bd0b5b0f-de77-47bf-b527-493f08da14eb
STEP: Creating a pod to test consume secrets
Sep 18 21:34:21.845: INFO: Waiting up to 5m0s for pod "pod-secrets-3376fbe7-d89f-44d4-aa23-e065bde9bcbe" in namespace "secrets-2422" to be "success or failure"
Sep 18 21:34:21.849: INFO: Pod "pod-secrets-3376fbe7-d89f-44d4-aa23-e065bde9bcbe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.443468ms
Sep 18 21:34:23.853: INFO: Pod "pod-secrets-3376fbe7-d89f-44d4-aa23-e065bde9bcbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008268971s
Sep 18 21:34:25.857: INFO: Pod "pod-secrets-3376fbe7-d89f-44d4-aa23-e065bde9bcbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011993676s
STEP: Saw pod success
Sep 18 21:34:25.857: INFO: Pod "pod-secrets-3376fbe7-d89f-44d4-aa23-e065bde9bcbe" satisfied condition "success or failure"
Sep 18 21:34:25.859: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000001 pod pod-secrets-3376fbe7-d89f-44d4-aa23-e065bde9bcbe container secret-volume-test: <nil>
STEP: delete the pod
Sep 18 21:34:25.913: INFO: Waiting for pod pod-secrets-3376fbe7-d89f-44d4-aa23-e065bde9bcbe to disappear
Sep 18 21:34:25.916: INFO: Pod pod-secrets-3376fbe7-d89f-44d4-aa23-e065bde9bcbe no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:34:25.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2422" for this suite.
Sep 18 21:34:31.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:34:32.017: INFO: namespace secrets-2422 deletion completed in 6.095417508s

â€¢ [SLOW TEST:10.349 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:34:32.021: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8162
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Sep 18 21:34:32.181: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Sep 18 21:34:44.768: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
Sep 18 21:34:47.814: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:34:59.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8162" for this suite.
Sep 18 21:35:05.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:35:05.501: INFO: namespace crd-publish-openapi-8162 deletion completed in 6.143660853s

â€¢ [SLOW TEST:33.480 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:35:05.502: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-223
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Sep 18 21:35:05.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815280101 cluster-info'
Sep 18 21:35:05.813: INFO: stderr: ""
Sep 18 21:35:05.813: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:35:05.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-223" for this suite.
Sep 18 21:35:11.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:35:11.917: INFO: namespace kubectl-223 deletion completed in 6.094711132s

â€¢ [SLOW TEST:6.414 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:35:11.917: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8959
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-5qkm
STEP: Creating a pod to test atomic-volume-subpath
Sep 18 21:35:12.099: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-5qkm" in namespace "subpath-8959" to be "success or failure"
Sep 18 21:35:12.108: INFO: Pod "pod-subpath-test-projected-5qkm": Phase="Pending", Reason="", readiness=false. Elapsed: 9.032434ms
Sep 18 21:35:14.112: INFO: Pod "pod-subpath-test-projected-5qkm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012890385s
Sep 18 21:35:16.116: INFO: Pod "pod-subpath-test-projected-5qkm": Phase="Running", Reason="", readiness=true. Elapsed: 4.016538941s
Sep 18 21:35:18.121: INFO: Pod "pod-subpath-test-projected-5qkm": Phase="Running", Reason="", readiness=true. Elapsed: 6.021719487s
Sep 18 21:35:20.124: INFO: Pod "pod-subpath-test-projected-5qkm": Phase="Running", Reason="", readiness=true. Elapsed: 8.025368146s
Sep 18 21:35:22.129: INFO: Pod "pod-subpath-test-projected-5qkm": Phase="Running", Reason="", readiness=true. Elapsed: 10.029535203s
Sep 18 21:35:24.133: INFO: Pod "pod-subpath-test-projected-5qkm": Phase="Running", Reason="", readiness=true. Elapsed: 12.033692761s
Sep 18 21:35:26.136: INFO: Pod "pod-subpath-test-projected-5qkm": Phase="Running", Reason="", readiness=true. Elapsed: 14.037361726s
Sep 18 21:35:28.140: INFO: Pod "pod-subpath-test-projected-5qkm": Phase="Running", Reason="", readiness=true. Elapsed: 16.041356589s
Sep 18 21:35:30.204: INFO: Pod "pod-subpath-test-projected-5qkm": Phase="Running", Reason="", readiness=true. Elapsed: 18.105180715s
Sep 18 21:35:32.209: INFO: Pod "pod-subpath-test-projected-5qkm": Phase="Running", Reason="", readiness=true. Elapsed: 20.10946188s
Sep 18 21:35:34.213: INFO: Pod "pod-subpath-test-projected-5qkm": Phase="Running", Reason="", readiness=true. Elapsed: 22.113808147s
Sep 18 21:35:36.218: INFO: Pod "pod-subpath-test-projected-5qkm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.118674011s
STEP: Saw pod success
Sep 18 21:35:36.218: INFO: Pod "pod-subpath-test-projected-5qkm" satisfied condition "success or failure"
Sep 18 21:35:36.222: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-subpath-test-projected-5qkm container test-container-subpath-projected-5qkm: <nil>
STEP: delete the pod
Sep 18 21:35:36.314: INFO: Waiting for pod pod-subpath-test-projected-5qkm to disappear
Sep 18 21:35:36.319: INFO: Pod pod-subpath-test-projected-5qkm no longer exists
STEP: Deleting pod pod-subpath-test-projected-5qkm
Sep 18 21:35:36.319: INFO: Deleting pod "pod-subpath-test-projected-5qkm" in namespace "subpath-8959"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:35:36.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8959" for this suite.
Sep 18 21:35:42.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:35:42.435: INFO: namespace subpath-8959 deletion completed in 6.105981633s

â€¢ [SLOW TEST:30.518 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:35:42.440: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-827
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-4c553cdc-3036-43d2-8e6b-066a5edaa1fa
STEP: Creating a pod to test consume configMaps
Sep 18 21:35:42.621: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-13703274-d04a-4993-b3f6-a41dd42cd3f4" in namespace "projected-827" to be "success or failure"
Sep 18 21:35:42.634: INFO: Pod "pod-projected-configmaps-13703274-d04a-4993-b3f6-a41dd42cd3f4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.644107ms
Sep 18 21:35:44.637: INFO: Pod "pod-projected-configmaps-13703274-d04a-4993-b3f6-a41dd42cd3f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016134889s
Sep 18 21:35:46.641: INFO: Pod "pod-projected-configmaps-13703274-d04a-4993-b3f6-a41dd42cd3f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019890871s
STEP: Saw pod success
Sep 18 21:35:46.641: INFO: Pod "pod-projected-configmaps-13703274-d04a-4993-b3f6-a41dd42cd3f4" satisfied condition "success or failure"
Sep 18 21:35:46.644: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod pod-projected-configmaps-13703274-d04a-4993-b3f6-a41dd42cd3f4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 18 21:35:46.669: INFO: Waiting for pod pod-projected-configmaps-13703274-d04a-4993-b3f6-a41dd42cd3f4 to disappear
Sep 18 21:35:46.676: INFO: Pod pod-projected-configmaps-13703274-d04a-4993-b3f6-a41dd42cd3f4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:35:46.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-827" for this suite.
Sep 18 21:35:52.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:35:52.793: INFO: namespace projected-827 deletion completed in 6.111132823s

â€¢ [SLOW TEST:10.354 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:35:52.794: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7440
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Sep 18 21:35:53.013: INFO: Waiting up to 5m0s for pod "downward-api-ec3a4761-47cf-47c2-b178-974a380da170" in namespace "downward-api-7440" to be "success or failure"
Sep 18 21:35:53.090: INFO: Pod "downward-api-ec3a4761-47cf-47c2-b178-974a380da170": Phase="Pending", Reason="", readiness=false. Elapsed: 76.467239ms
Sep 18 21:35:55.095: INFO: Pod "downward-api-ec3a4761-47cf-47c2-b178-974a380da170": Phase="Pending", Reason="", readiness=false. Elapsed: 2.081442119s
Sep 18 21:35:57.098: INFO: Pod "downward-api-ec3a4761-47cf-47c2-b178-974a380da170": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.084719113s
STEP: Saw pod success
Sep 18 21:35:57.098: INFO: Pod "downward-api-ec3a4761-47cf-47c2-b178-974a380da170" satisfied condition "success or failure"
Sep 18 21:35:57.101: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod downward-api-ec3a4761-47cf-47c2-b178-974a380da170 container dapi-container: <nil>
STEP: delete the pod
Sep 18 21:35:57.122: INFO: Waiting for pod downward-api-ec3a4761-47cf-47c2-b178-974a380da170 to disappear
Sep 18 21:35:57.124: INFO: Pod downward-api-ec3a4761-47cf-47c2-b178-974a380da170 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:35:57.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7440" for this suite.
Sep 18 21:36:03.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:36:03.232: INFO: namespace downward-api-7440 deletion completed in 6.100142431s

â€¢ [SLOW TEST:10.439 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:36:03.233: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6312
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-6312
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Sep 18 21:36:03.407: INFO: Found 0 stateful pods, waiting for 3
Sep 18 21:36:13.414: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 18 21:36:13.414: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 18 21:36:13.414: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Sep 18 21:36:23.412: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 18 21:36:23.412: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 18 21:36:23.412: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Sep 18 21:36:23.442: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep 18 21:36:33.476: INFO: Updating stateful set ss2
Sep 18 21:36:33.499: INFO: Waiting for Pod statefulset-6312/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Sep 18 21:36:43.566: INFO: Found 2 stateful pods, waiting for 3
Sep 18 21:36:53.570: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 18 21:36:53.570: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 18 21:36:53.570: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep 18 21:36:53.593: INFO: Updating stateful set ss2
Sep 18 21:36:53.621: INFO: Waiting for Pod statefulset-6312/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 18 21:37:03.646: INFO: Updating stateful set ss2
Sep 18 21:37:03.656: INFO: Waiting for StatefulSet statefulset-6312/ss2 to complete update
Sep 18 21:37:03.656: INFO: Waiting for Pod statefulset-6312/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 18 21:37:13.663: INFO: Waiting for StatefulSet statefulset-6312/ss2 to complete update
Sep 18 21:37:13.663: INFO: Waiting for Pod statefulset-6312/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Sep 18 21:37:23.663: INFO: Deleting all statefulset in ns statefulset-6312
Sep 18 21:37:23.666: INFO: Scaling statefulset ss2 to 0
Sep 18 21:37:33.678: INFO: Waiting for statefulset status.replicas updated to 0
Sep 18 21:37:33.681: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:37:33.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6312" for this suite.
Sep 18 21:37:39.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:37:39.799: INFO: namespace statefulset-6312 deletion completed in 6.096170087s

â€¢ [SLOW TEST:96.566 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:37:39.800: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8691
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Sep 18 21:37:39.982: INFO: Waiting up to 5m0s for pod "downward-api-e94f85a6-b948-47c6-a834-7482b67b28d6" in namespace "downward-api-8691" to be "success or failure"
Sep 18 21:37:39.988: INFO: Pod "downward-api-e94f85a6-b948-47c6-a834-7482b67b28d6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.104455ms
Sep 18 21:37:41.992: INFO: Pod "downward-api-e94f85a6-b948-47c6-a834-7482b67b28d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010308424s
Sep 18 21:37:43.996: INFO: Pod "downward-api-e94f85a6-b948-47c6-a834-7482b67b28d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014275695s
STEP: Saw pod success
Sep 18 21:37:43.996: INFO: Pod "downward-api-e94f85a6-b948-47c6-a834-7482b67b28d6" satisfied condition "success or failure"
Sep 18 21:37:44.000: INFO: Trying to get logs from node k8s-agentpool1-40209065-vmss000002 pod downward-api-e94f85a6-b948-47c6-a834-7482b67b28d6 container dapi-container: <nil>
STEP: delete the pod
Sep 18 21:37:44.057: INFO: Waiting for pod downward-api-e94f85a6-b948-47c6-a834-7482b67b28d6 to disappear
Sep 18 21:37:44.061: INFO: Pod downward-api-e94f85a6-b948-47c6-a834-7482b67b28d6 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:37:44.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8691" for this suite.
Sep 18 21:37:50.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:37:50.181: INFO: namespace downward-api-8691 deletion completed in 6.115827266s

â€¢ [SLOW TEST:10.381 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 18 21:37:50.183: INFO: >>> kubeConfig: /tmp/kubeconfig-815280101
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3799
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Sep 18 21:37:50.344: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 18 21:37:55.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3799" for this suite.
Sep 18 21:38:23.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 18 21:38:23.123: INFO: namespace init-container-3799 deletion completed in 28.10510399s

â€¢ [SLOW TEST:32.940 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSep 18 21:38:23.123: INFO: Running AfterSuite actions on all nodes
Sep 18 21:38:23.123: INFO: Running AfterSuite actions on node 1
Sep 18 21:38:23.123: INFO: Skipping dumping logs from cluster

Ran 274 of 4897 Specs in 7450.569 seconds
SUCCESS! -- 274 Passed | 0 Failed | 0 Pending | 4623 Skipped
PASS

Ginkgo ran 1 suite in 2h4m12.339983321s
Test Suite Passed
