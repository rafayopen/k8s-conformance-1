I0211 08:31:39.665115      23 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-208978535
I0211 08:31:39.665272      23 e2e.go:92] Starting e2e run "eb2ccd27-6f2e-4217-afe1-0ed128452c55" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1581409898 - Will randomize all specs
Will run 276 of 4731 specs

Feb 11 08:31:39.689: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 08:31:39.692: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 11 08:31:39.707: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 11 08:31:39.737: INFO: 36 / 36 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 11 08:31:39.737: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Feb 11 08:31:39.737: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 11 08:31:39.744: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cloud-controller-manager' (0 seconds elapsed)
Feb 11 08:31:39.744: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'flexvolume' (0 seconds elapsed)
Feb 11 08:31:39.744: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
Feb 11 08:31:39.744: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-master' (0 seconds elapsed)
Feb 11 08:31:39.744: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-worker' (0 seconds elapsed)
Feb 11 08:31:39.744: INFO: e2e test version: v1.16.6
Feb 11 08:31:39.745: INFO: kube-apiserver version: v1.16.6-aliyun.1
Feb 11 08:31:39.745: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 08:31:39.750: INFO: Cluster IP family: ipv4
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:31:39.750: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename downward-api
Feb 11 08:31:39.786: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 11 08:31:39.794: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1c34ab96-5c25-4dbc-ae6d-1d6bf2a9c6f4" in namespace "downward-api-6860" to be "success or failure"
Feb 11 08:31:39.797: INFO: Pod "downwardapi-volume-1c34ab96-5c25-4dbc-ae6d-1d6bf2a9c6f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.923631ms
Feb 11 08:31:41.801: INFO: Pod "downwardapi-volume-1c34ab96-5c25-4dbc-ae6d-1d6bf2a9c6f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006569967s
Feb 11 08:31:43.804: INFO: Pod "downwardapi-volume-1c34ab96-5c25-4dbc-ae6d-1d6bf2a9c6f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00990188s
STEP: Saw pod success
Feb 11 08:31:43.804: INFO: Pod "downwardapi-volume-1c34ab96-5c25-4dbc-ae6d-1d6bf2a9c6f4" satisfied condition "success or failure"
Feb 11 08:31:43.807: INFO: Trying to get logs from node cn-hongkong.192.168.0.20 pod downwardapi-volume-1c34ab96-5c25-4dbc-ae6d-1d6bf2a9c6f4 container client-container: <nil>
STEP: delete the pod
Feb 11 08:31:43.831: INFO: Waiting for pod downwardapi-volume-1c34ab96-5c25-4dbc-ae6d-1d6bf2a9c6f4 to disappear
Feb 11 08:31:43.833: INFO: Pod downwardapi-volume-1c34ab96-5c25-4dbc-ae6d-1d6bf2a9c6f4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:31:43.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6860" for this suite.
Feb 11 08:31:49.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:31:49.913: INFO: namespace downward-api-6860 deletion completed in 6.077350067s

â€¢ [SLOW TEST:10.164 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:31:49.914: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4637
I0211 08:31:49.939505      23 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4637, replica count: 1
I0211 08:31:50.989899      23 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0211 08:31:51.990071      23 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 11 08:31:52.097: INFO: Created: latency-svc-npcqb
Feb 11 08:31:52.103: INFO: Got endpoints: latency-svc-npcqb [13.247491ms]
Feb 11 08:31:52.112: INFO: Created: latency-svc-zpzn2
Feb 11 08:31:52.116: INFO: Got endpoints: latency-svc-zpzn2 [12.640144ms]
Feb 11 08:31:52.120: INFO: Created: latency-svc-7cvkj
Feb 11 08:31:52.122: INFO: Got endpoints: latency-svc-7cvkj [18.066821ms]
Feb 11 08:31:52.123: INFO: Created: latency-svc-qrqm7
Feb 11 08:31:52.126: INFO: Got endpoints: latency-svc-qrqm7 [22.532896ms]
Feb 11 08:31:52.129: INFO: Created: latency-svc-gxxf6
Feb 11 08:31:52.132: INFO: Got endpoints: latency-svc-gxxf6 [27.762143ms]
Feb 11 08:31:52.133: INFO: Created: latency-svc-6cg5q
Feb 11 08:31:52.138: INFO: Got endpoints: latency-svc-6cg5q [34.021287ms]
Feb 11 08:31:52.138: INFO: Created: latency-svc-966fx
Feb 11 08:31:52.142: INFO: Created: latency-svc-xjvlt
Feb 11 08:31:52.143: INFO: Got endpoints: latency-svc-966fx [38.640085ms]
Feb 11 08:31:52.146: INFO: Got endpoints: latency-svc-xjvlt [41.507091ms]
Feb 11 08:31:52.151: INFO: Created: latency-svc-vmsbl
Feb 11 08:31:52.154: INFO: Got endpoints: latency-svc-vmsbl [49.809969ms]
Feb 11 08:31:52.156: INFO: Created: latency-svc-954dz
Feb 11 08:31:52.159: INFO: Got endpoints: latency-svc-954dz [54.346057ms]
Feb 11 08:31:52.161: INFO: Created: latency-svc-jzrgv
Feb 11 08:31:52.164: INFO: Got endpoints: latency-svc-jzrgv [59.925006ms]
Feb 11 08:31:52.166: INFO: Created: latency-svc-gmcbx
Feb 11 08:31:52.169: INFO: Created: latency-svc-8prkt
Feb 11 08:31:52.173: INFO: Got endpoints: latency-svc-8prkt [68.19241ms]
Feb 11 08:31:52.174: INFO: Got endpoints: latency-svc-gmcbx [69.743917ms]
Feb 11 08:31:52.176: INFO: Created: latency-svc-q26tq
Feb 11 08:31:52.178: INFO: Got endpoints: latency-svc-q26tq [73.485675ms]
Feb 11 08:31:52.181: INFO: Created: latency-svc-mz8xj
Feb 11 08:31:52.185: INFO: Got endpoints: latency-svc-mz8xj [80.292465ms]
Feb 11 08:31:52.186: INFO: Created: latency-svc-9g8pj
Feb 11 08:31:52.190: INFO: Got endpoints: latency-svc-9g8pj [85.562121ms]
Feb 11 08:31:52.192: INFO: Created: latency-svc-lf5k2
Feb 11 08:31:52.194: INFO: Got endpoints: latency-svc-lf5k2 [78.279756ms]
Feb 11 08:31:52.197: INFO: Created: latency-svc-mdng5
Feb 11 08:31:52.201: INFO: Created: latency-svc-djlkb
Feb 11 08:31:52.201: INFO: Got endpoints: latency-svc-mdng5 [79.43291ms]
Feb 11 08:31:52.204: INFO: Got endpoints: latency-svc-djlkb [77.795367ms]
Feb 11 08:31:52.206: INFO: Created: latency-svc-9rwcv
Feb 11 08:31:52.210: INFO: Got endpoints: latency-svc-9rwcv [77.995731ms]
Feb 11 08:31:52.212: INFO: Created: latency-svc-m9khd
Feb 11 08:31:52.215: INFO: Got endpoints: latency-svc-m9khd [77.218651ms]
Feb 11 08:31:52.215: INFO: Created: latency-svc-zfgrc
Feb 11 08:31:52.219: INFO: Got endpoints: latency-svc-zfgrc [75.840638ms]
Feb 11 08:31:52.221: INFO: Created: latency-svc-454x8
Feb 11 08:31:52.225: INFO: Got endpoints: latency-svc-454x8 [78.915141ms]
Feb 11 08:31:52.228: INFO: Created: latency-svc-9kvkt
Feb 11 08:31:52.231: INFO: Got endpoints: latency-svc-9kvkt [76.504652ms]
Feb 11 08:31:52.232: INFO: Created: latency-svc-2pxm4
Feb 11 08:31:52.235: INFO: Got endpoints: latency-svc-2pxm4 [76.688393ms]
Feb 11 08:31:52.236: INFO: Created: latency-svc-gc7r5
Feb 11 08:31:52.240: INFO: Got endpoints: latency-svc-gc7r5 [75.682786ms]
Feb 11 08:31:52.242: INFO: Created: latency-svc-2qbs5
Feb 11 08:31:52.246: INFO: Created: latency-svc-jkc2m
Feb 11 08:31:52.250: INFO: Got endpoints: latency-svc-2qbs5 [76.756193ms]
Feb 11 08:31:52.262: INFO: Got endpoints: latency-svc-jkc2m [87.932792ms]
Feb 11 08:31:52.268: INFO: Created: latency-svc-hjvb2
Feb 11 08:31:52.270: INFO: Got endpoints: latency-svc-hjvb2 [92.098651ms]
Feb 11 08:31:52.273: INFO: Created: latency-svc-786c8
Feb 11 08:31:52.276: INFO: Got endpoints: latency-svc-786c8 [91.282464ms]
Feb 11 08:31:52.277: INFO: Created: latency-svc-67rz8
Feb 11 08:31:52.280: INFO: Got endpoints: latency-svc-67rz8 [89.882988ms]
Feb 11 08:31:52.281: INFO: Created: latency-svc-x457j
Feb 11 08:31:52.285: INFO: Got endpoints: latency-svc-x457j [90.387503ms]
Feb 11 08:31:52.288: INFO: Created: latency-svc-t4m62
Feb 11 08:31:52.292: INFO: Got endpoints: latency-svc-t4m62 [90.981262ms]
Feb 11 08:31:52.293: INFO: Created: latency-svc-pgjbl
Feb 11 08:31:52.297: INFO: Created: latency-svc-w8f5v
Feb 11 08:31:52.300: INFO: Got endpoints: latency-svc-pgjbl [95.901743ms]
Feb 11 08:31:52.302: INFO: Created: latency-svc-2n8sp
Feb 11 08:31:52.305: INFO: Created: latency-svc-2snlz
Feb 11 08:31:52.309: INFO: Created: latency-svc-mg6mm
Feb 11 08:31:52.312: INFO: Created: latency-svc-zzrn8
Feb 11 08:31:52.316: INFO: Created: latency-svc-4d66j
Feb 11 08:31:52.320: INFO: Created: latency-svc-sw49n
Feb 11 08:31:52.324: INFO: Created: latency-svc-fj9n6
Feb 11 08:31:52.327: INFO: Created: latency-svc-tvqwj
Feb 11 08:31:52.333: INFO: Created: latency-svc-d574c
Feb 11 08:31:52.338: INFO: Created: latency-svc-4fj62
Feb 11 08:31:52.341: INFO: Created: latency-svc-9rlg4
Feb 11 08:31:52.345: INFO: Created: latency-svc-gxnk4
Feb 11 08:31:52.350: INFO: Created: latency-svc-bwm6j
Feb 11 08:31:52.351: INFO: Got endpoints: latency-svc-w8f5v [141.28945ms]
Feb 11 08:31:52.353: INFO: Created: latency-svc-xdfdm
Feb 11 08:31:52.358: INFO: Created: latency-svc-jq6q9
Feb 11 08:31:52.401: INFO: Got endpoints: latency-svc-2n8sp [186.202761ms]
Feb 11 08:31:52.409: INFO: Created: latency-svc-9lrfg
Feb 11 08:31:52.451: INFO: Got endpoints: latency-svc-2snlz [232.395451ms]
Feb 11 08:31:52.457: INFO: Created: latency-svc-7gkbf
Feb 11 08:31:52.502: INFO: Got endpoints: latency-svc-mg6mm [277.064255ms]
Feb 11 08:31:52.508: INFO: Created: latency-svc-xwmwm
Feb 11 08:31:52.550: INFO: Got endpoints: latency-svc-zzrn8 [319.911299ms]
Feb 11 08:31:52.558: INFO: Created: latency-svc-t952g
Feb 11 08:31:52.601: INFO: Got endpoints: latency-svc-4d66j [365.225089ms]
Feb 11 08:31:52.607: INFO: Created: latency-svc-qvmq2
Feb 11 08:31:52.651: INFO: Got endpoints: latency-svc-sw49n [410.60846ms]
Feb 11 08:31:52.657: INFO: Created: latency-svc-9vfk9
Feb 11 08:31:52.701: INFO: Got endpoints: latency-svc-fj9n6 [451.097844ms]
Feb 11 08:31:52.709: INFO: Created: latency-svc-c5cnh
Feb 11 08:31:52.752: INFO: Got endpoints: latency-svc-tvqwj [489.407187ms]
Feb 11 08:31:52.760: INFO: Created: latency-svc-482mv
Feb 11 08:31:52.801: INFO: Got endpoints: latency-svc-d574c [530.406297ms]
Feb 11 08:31:52.813: INFO: Created: latency-svc-x7c6n
Feb 11 08:31:52.858: INFO: Got endpoints: latency-svc-4fj62 [581.198361ms]
Feb 11 08:31:52.865: INFO: Created: latency-svc-xjw8r
Feb 11 08:31:52.900: INFO: Got endpoints: latency-svc-9rlg4 [620.257223ms]
Feb 11 08:31:52.906: INFO: Created: latency-svc-whz8t
Feb 11 08:31:52.952: INFO: Got endpoints: latency-svc-gxnk4 [666.903382ms]
Feb 11 08:31:52.959: INFO: Created: latency-svc-5vmlt
Feb 11 08:31:53.001: INFO: Got endpoints: latency-svc-bwm6j [708.545134ms]
Feb 11 08:31:53.010: INFO: Created: latency-svc-5xzxp
Feb 11 08:31:53.051: INFO: Got endpoints: latency-svc-xdfdm [750.494322ms]
Feb 11 08:31:53.057: INFO: Created: latency-svc-r29g7
Feb 11 08:31:53.102: INFO: Got endpoints: latency-svc-jq6q9 [750.918175ms]
Feb 11 08:31:53.110: INFO: Created: latency-svc-b2grp
Feb 11 08:31:53.152: INFO: Got endpoints: latency-svc-9lrfg [750.770658ms]
Feb 11 08:31:53.162: INFO: Created: latency-svc-bwn78
Feb 11 08:31:53.202: INFO: Got endpoints: latency-svc-7gkbf [750.601335ms]
Feb 11 08:31:53.210: INFO: Created: latency-svc-kggtt
Feb 11 08:31:53.251: INFO: Got endpoints: latency-svc-xwmwm [749.412183ms]
Feb 11 08:31:53.260: INFO: Created: latency-svc-7txrl
Feb 11 08:31:53.301: INFO: Got endpoints: latency-svc-t952g [750.372016ms]
Feb 11 08:31:53.311: INFO: Created: latency-svc-2587n
Feb 11 08:31:53.351: INFO: Got endpoints: latency-svc-qvmq2 [750.548217ms]
Feb 11 08:31:53.358: INFO: Created: latency-svc-f6gpw
Feb 11 08:31:53.405: INFO: Got endpoints: latency-svc-9vfk9 [754.793405ms]
Feb 11 08:31:53.412: INFO: Created: latency-svc-w749p
Feb 11 08:31:53.452: INFO: Got endpoints: latency-svc-c5cnh [751.061196ms]
Feb 11 08:31:53.462: INFO: Created: latency-svc-8lswz
Feb 11 08:31:53.502: INFO: Got endpoints: latency-svc-482mv [749.80836ms]
Feb 11 08:31:53.508: INFO: Created: latency-svc-r85lb
Feb 11 08:31:53.551: INFO: Got endpoints: latency-svc-x7c6n [750.035706ms]
Feb 11 08:31:53.558: INFO: Created: latency-svc-fwkq8
Feb 11 08:31:53.601: INFO: Got endpoints: latency-svc-xjw8r [743.687369ms]
Feb 11 08:31:53.611: INFO: Created: latency-svc-pcbg5
Feb 11 08:31:53.651: INFO: Got endpoints: latency-svc-whz8t [750.120768ms]
Feb 11 08:31:53.659: INFO: Created: latency-svc-wrlff
Feb 11 08:31:53.701: INFO: Got endpoints: latency-svc-5vmlt [749.124403ms]
Feb 11 08:31:53.709: INFO: Created: latency-svc-68cqq
Feb 11 08:31:53.751: INFO: Got endpoints: latency-svc-5xzxp [750.609677ms]
Feb 11 08:31:53.759: INFO: Created: latency-svc-f2h8z
Feb 11 08:31:53.802: INFO: Got endpoints: latency-svc-r29g7 [751.248483ms]
Feb 11 08:31:53.808: INFO: Created: latency-svc-5wcfs
Feb 11 08:31:53.852: INFO: Got endpoints: latency-svc-b2grp [749.634499ms]
Feb 11 08:31:53.858: INFO: Created: latency-svc-gslzs
Feb 11 08:31:53.902: INFO: Got endpoints: latency-svc-bwn78 [749.737992ms]
Feb 11 08:31:53.910: INFO: Created: latency-svc-p2t77
Feb 11 08:31:53.951: INFO: Got endpoints: latency-svc-kggtt [749.26832ms]
Feb 11 08:31:53.962: INFO: Created: latency-svc-fhll8
Feb 11 08:31:54.001: INFO: Got endpoints: latency-svc-7txrl [749.799458ms]
Feb 11 08:31:54.008: INFO: Created: latency-svc-g4bx6
Feb 11 08:31:54.051: INFO: Got endpoints: latency-svc-2587n [750.057696ms]
Feb 11 08:31:54.060: INFO: Created: latency-svc-mtzfl
Feb 11 08:31:54.101: INFO: Got endpoints: latency-svc-f6gpw [749.641112ms]
Feb 11 08:31:54.109: INFO: Created: latency-svc-8zdr4
Feb 11 08:31:54.151: INFO: Got endpoints: latency-svc-w749p [745.693678ms]
Feb 11 08:31:54.159: INFO: Created: latency-svc-8n2g9
Feb 11 08:31:54.201: INFO: Got endpoints: latency-svc-8lswz [749.58619ms]
Feb 11 08:31:54.210: INFO: Created: latency-svc-mmhcx
Feb 11 08:31:54.251: INFO: Got endpoints: latency-svc-r85lb [749.862138ms]
Feb 11 08:31:54.258: INFO: Created: latency-svc-xjbs4
Feb 11 08:31:54.302: INFO: Got endpoints: latency-svc-fwkq8 [751.094292ms]
Feb 11 08:31:54.309: INFO: Created: latency-svc-wbw8r
Feb 11 08:31:54.352: INFO: Got endpoints: latency-svc-pcbg5 [750.894561ms]
Feb 11 08:31:54.361: INFO: Created: latency-svc-4hbpx
Feb 11 08:31:54.401: INFO: Got endpoints: latency-svc-wrlff [750.619937ms]
Feb 11 08:31:54.410: INFO: Created: latency-svc-2tcj6
Feb 11 08:31:54.451: INFO: Got endpoints: latency-svc-68cqq [749.842584ms]
Feb 11 08:31:54.458: INFO: Created: latency-svc-rp5kf
Feb 11 08:31:54.501: INFO: Got endpoints: latency-svc-f2h8z [749.308672ms]
Feb 11 08:31:54.509: INFO: Created: latency-svc-l4bqr
Feb 11 08:31:54.551: INFO: Got endpoints: latency-svc-5wcfs [749.649254ms]
Feb 11 08:31:54.562: INFO: Created: latency-svc-pwnjw
Feb 11 08:31:54.601: INFO: Got endpoints: latency-svc-gslzs [749.79057ms]
Feb 11 08:31:54.610: INFO: Created: latency-svc-zjzz5
Feb 11 08:31:54.651: INFO: Got endpoints: latency-svc-p2t77 [748.594508ms]
Feb 11 08:31:54.658: INFO: Created: latency-svc-ff8xt
Feb 11 08:31:54.701: INFO: Got endpoints: latency-svc-fhll8 [750.542686ms]
Feb 11 08:31:54.708: INFO: Created: latency-svc-fpdh9
Feb 11 08:31:54.751: INFO: Got endpoints: latency-svc-g4bx6 [749.806774ms]
Feb 11 08:31:54.758: INFO: Created: latency-svc-67gvx
Feb 11 08:31:54.801: INFO: Got endpoints: latency-svc-mtzfl [750.237225ms]
Feb 11 08:31:54.809: INFO: Created: latency-svc-kll9g
Feb 11 08:31:54.851: INFO: Got endpoints: latency-svc-8zdr4 [749.888353ms]
Feb 11 08:31:54.860: INFO: Created: latency-svc-qncqp
Feb 11 08:31:54.901: INFO: Got endpoints: latency-svc-8n2g9 [749.826261ms]
Feb 11 08:31:54.909: INFO: Created: latency-svc-bw8m8
Feb 11 08:31:54.952: INFO: Got endpoints: latency-svc-mmhcx [750.62294ms]
Feb 11 08:31:54.961: INFO: Created: latency-svc-rnnm6
Feb 11 08:31:55.001: INFO: Got endpoints: latency-svc-xjbs4 [749.247037ms]
Feb 11 08:31:55.008: INFO: Created: latency-svc-6s7m4
Feb 11 08:31:55.051: INFO: Got endpoints: latency-svc-wbw8r [748.971411ms]
Feb 11 08:31:55.059: INFO: Created: latency-svc-6rwt9
Feb 11 08:31:55.101: INFO: Got endpoints: latency-svc-4hbpx [748.93493ms]
Feb 11 08:31:55.109: INFO: Created: latency-svc-5hlg4
Feb 11 08:31:55.152: INFO: Got endpoints: latency-svc-2tcj6 [750.694523ms]
Feb 11 08:31:55.158: INFO: Created: latency-svc-9sfqr
Feb 11 08:31:55.201: INFO: Got endpoints: latency-svc-rp5kf [750.389852ms]
Feb 11 08:31:55.210: INFO: Created: latency-svc-p85l6
Feb 11 08:31:55.251: INFO: Got endpoints: latency-svc-l4bqr [750.309198ms]
Feb 11 08:31:55.260: INFO: Created: latency-svc-cskgq
Feb 11 08:31:55.301: INFO: Got endpoints: latency-svc-pwnjw [749.740209ms]
Feb 11 08:31:55.309: INFO: Created: latency-svc-w8hp8
Feb 11 08:31:55.351: INFO: Got endpoints: latency-svc-zjzz5 [749.176494ms]
Feb 11 08:31:55.358: INFO: Created: latency-svc-4vbqz
Feb 11 08:31:55.402: INFO: Got endpoints: latency-svc-ff8xt [751.29706ms]
Feb 11 08:31:55.409: INFO: Created: latency-svc-fhhz7
Feb 11 08:31:55.451: INFO: Got endpoints: latency-svc-fpdh9 [749.561708ms]
Feb 11 08:31:55.458: INFO: Created: latency-svc-8mpwc
Feb 11 08:31:55.502: INFO: Got endpoints: latency-svc-67gvx [751.021877ms]
Feb 11 08:31:55.509: INFO: Created: latency-svc-tbjl5
Feb 11 08:31:55.552: INFO: Got endpoints: latency-svc-kll9g [750.777943ms]
Feb 11 08:31:55.559: INFO: Created: latency-svc-pt4w4
Feb 11 08:31:55.601: INFO: Got endpoints: latency-svc-qncqp [750.154984ms]
Feb 11 08:31:55.608: INFO: Created: latency-svc-64l5l
Feb 11 08:31:55.651: INFO: Got endpoints: latency-svc-bw8m8 [750.311766ms]
Feb 11 08:31:55.660: INFO: Created: latency-svc-f5bsc
Feb 11 08:31:55.702: INFO: Got endpoints: latency-svc-rnnm6 [749.655789ms]
Feb 11 08:31:55.709: INFO: Created: latency-svc-6996s
Feb 11 08:31:55.751: INFO: Got endpoints: latency-svc-6s7m4 [750.580158ms]
Feb 11 08:31:55.760: INFO: Created: latency-svc-wjl78
Feb 11 08:31:55.801: INFO: Got endpoints: latency-svc-6rwt9 [750.101515ms]
Feb 11 08:31:55.810: INFO: Created: latency-svc-cmqhf
Feb 11 08:31:55.851: INFO: Got endpoints: latency-svc-5hlg4 [749.56328ms]
Feb 11 08:31:55.859: INFO: Created: latency-svc-5tfw9
Feb 11 08:31:55.901: INFO: Got endpoints: latency-svc-9sfqr [748.867073ms]
Feb 11 08:31:55.907: INFO: Created: latency-svc-8vjf6
Feb 11 08:31:55.951: INFO: Got endpoints: latency-svc-p85l6 [749.671032ms]
Feb 11 08:31:55.958: INFO: Created: latency-svc-btc95
Feb 11 08:31:56.001: INFO: Got endpoints: latency-svc-cskgq [750.314275ms]
Feb 11 08:31:56.008: INFO: Created: latency-svc-5sgjf
Feb 11 08:31:56.050: INFO: Got endpoints: latency-svc-w8hp8 [749.157195ms]
Feb 11 08:31:56.058: INFO: Created: latency-svc-hx6hp
Feb 11 08:31:56.101: INFO: Got endpoints: latency-svc-4vbqz [750.096697ms]
Feb 11 08:31:56.108: INFO: Created: latency-svc-sks7d
Feb 11 08:31:56.152: INFO: Got endpoints: latency-svc-fhhz7 [749.960875ms]
Feb 11 08:31:56.160: INFO: Created: latency-svc-6v8d5
Feb 11 08:31:56.201: INFO: Got endpoints: latency-svc-8mpwc [749.778366ms]
Feb 11 08:31:56.207: INFO: Created: latency-svc-k6gzx
Feb 11 08:31:56.251: INFO: Got endpoints: latency-svc-tbjl5 [748.806433ms]
Feb 11 08:31:56.259: INFO: Created: latency-svc-975pj
Feb 11 08:31:56.301: INFO: Got endpoints: latency-svc-pt4w4 [748.765688ms]
Feb 11 08:31:56.308: INFO: Created: latency-svc-jp8m8
Feb 11 08:31:56.351: INFO: Got endpoints: latency-svc-64l5l [749.992516ms]
Feb 11 08:31:56.358: INFO: Created: latency-svc-s69lw
Feb 11 08:31:56.401: INFO: Got endpoints: latency-svc-f5bsc [749.610208ms]
Feb 11 08:31:56.409: INFO: Created: latency-svc-4cz74
Feb 11 08:31:56.453: INFO: Got endpoints: latency-svc-6996s [750.884379ms]
Feb 11 08:31:56.463: INFO: Created: latency-svc-kmw9v
Feb 11 08:31:56.501: INFO: Got endpoints: latency-svc-wjl78 [749.907415ms]
Feb 11 08:31:56.508: INFO: Created: latency-svc-zzjc2
Feb 11 08:31:56.553: INFO: Got endpoints: latency-svc-cmqhf [751.440832ms]
Feb 11 08:31:56.560: INFO: Created: latency-svc-5x57j
Feb 11 08:31:56.602: INFO: Got endpoints: latency-svc-5tfw9 [751.037147ms]
Feb 11 08:31:56.611: INFO: Created: latency-svc-mpwf6
Feb 11 08:31:56.652: INFO: Got endpoints: latency-svc-8vjf6 [750.649416ms]
Feb 11 08:31:56.658: INFO: Created: latency-svc-wnq28
Feb 11 08:31:56.701: INFO: Got endpoints: latency-svc-btc95 [750.337163ms]
Feb 11 08:31:56.710: INFO: Created: latency-svc-gnvrj
Feb 11 08:31:56.751: INFO: Got endpoints: latency-svc-5sgjf [749.586816ms]
Feb 11 08:31:56.758: INFO: Created: latency-svc-4tph4
Feb 11 08:31:56.812: INFO: Got endpoints: latency-svc-hx6hp [761.224998ms]
Feb 11 08:31:56.820: INFO: Created: latency-svc-jg2p6
Feb 11 08:31:56.852: INFO: Got endpoints: latency-svc-sks7d [750.957187ms]
Feb 11 08:31:56.858: INFO: Created: latency-svc-qbdhv
Feb 11 08:31:56.901: INFO: Got endpoints: latency-svc-6v8d5 [749.190913ms]
Feb 11 08:31:56.909: INFO: Created: latency-svc-7482w
Feb 11 08:31:56.952: INFO: Got endpoints: latency-svc-k6gzx [751.096127ms]
Feb 11 08:31:56.959: INFO: Created: latency-svc-8l8zs
Feb 11 08:31:57.001: INFO: Got endpoints: latency-svc-975pj [749.916011ms]
Feb 11 08:31:57.007: INFO: Created: latency-svc-cnmvs
Feb 11 08:31:57.051: INFO: Got endpoints: latency-svc-jp8m8 [749.883096ms]
Feb 11 08:31:57.058: INFO: Created: latency-svc-9rq5f
Feb 11 08:31:57.100: INFO: Got endpoints: latency-svc-s69lw [749.269483ms]
Feb 11 08:31:57.107: INFO: Created: latency-svc-pkkp2
Feb 11 08:31:57.151: INFO: Got endpoints: latency-svc-4cz74 [749.720338ms]
Feb 11 08:31:57.158: INFO: Created: latency-svc-nbx5b
Feb 11 08:31:57.201: INFO: Got endpoints: latency-svc-kmw9v [747.875419ms]
Feb 11 08:31:57.209: INFO: Created: latency-svc-nhw4j
Feb 11 08:31:57.251: INFO: Got endpoints: latency-svc-zzjc2 [749.587473ms]
Feb 11 08:31:57.258: INFO: Created: latency-svc-vf5rp
Feb 11 08:31:57.302: INFO: Got endpoints: latency-svc-5x57j [748.99062ms]
Feb 11 08:31:57.309: INFO: Created: latency-svc-s6tth
Feb 11 08:31:57.352: INFO: Got endpoints: latency-svc-mpwf6 [750.390472ms]
Feb 11 08:31:57.360: INFO: Created: latency-svc-snjfr
Feb 11 08:31:57.401: INFO: Got endpoints: latency-svc-wnq28 [749.28345ms]
Feb 11 08:31:57.409: INFO: Created: latency-svc-69brl
Feb 11 08:31:57.451: INFO: Got endpoints: latency-svc-gnvrj [750.101168ms]
Feb 11 08:31:57.459: INFO: Created: latency-svc-mtxx8
Feb 11 08:31:57.501: INFO: Got endpoints: latency-svc-4tph4 [749.76055ms]
Feb 11 08:31:57.509: INFO: Created: latency-svc-6pnzl
Feb 11 08:31:57.551: INFO: Got endpoints: latency-svc-jg2p6 [739.303293ms]
Feb 11 08:31:57.559: INFO: Created: latency-svc-ljnqd
Feb 11 08:31:57.602: INFO: Got endpoints: latency-svc-qbdhv [750.026811ms]
Feb 11 08:31:57.609: INFO: Created: latency-svc-bmdrx
Feb 11 08:31:57.651: INFO: Got endpoints: latency-svc-7482w [749.941602ms]
Feb 11 08:31:57.660: INFO: Created: latency-svc-5b94b
Feb 11 08:31:57.701: INFO: Got endpoints: latency-svc-8l8zs [749.361956ms]
Feb 11 08:31:57.709: INFO: Created: latency-svc-5cjqj
Feb 11 08:31:57.753: INFO: Got endpoints: latency-svc-cnmvs [752.533231ms]
Feb 11 08:31:57.761: INFO: Created: latency-svc-c4nj2
Feb 11 08:31:57.802: INFO: Got endpoints: latency-svc-9rq5f [750.919819ms]
Feb 11 08:31:57.808: INFO: Created: latency-svc-z4gzp
Feb 11 08:31:57.851: INFO: Got endpoints: latency-svc-pkkp2 [750.479842ms]
Feb 11 08:31:57.857: INFO: Created: latency-svc-vbxzv
Feb 11 08:31:57.901: INFO: Got endpoints: latency-svc-nbx5b [749.752276ms]
Feb 11 08:31:57.910: INFO: Created: latency-svc-dc72v
Feb 11 08:31:57.951: INFO: Got endpoints: latency-svc-nhw4j [750.452983ms]
Feb 11 08:31:57.969: INFO: Created: latency-svc-5s2dl
Feb 11 08:31:58.002: INFO: Got endpoints: latency-svc-vf5rp [751.207784ms]
Feb 11 08:31:58.015: INFO: Created: latency-svc-vzrz6
Feb 11 08:31:58.051: INFO: Got endpoints: latency-svc-s6tth [749.519401ms]
Feb 11 08:31:58.060: INFO: Created: latency-svc-wtmwb
Feb 11 08:31:58.102: INFO: Got endpoints: latency-svc-snjfr [749.23459ms]
Feb 11 08:31:58.108: INFO: Created: latency-svc-6frdf
Feb 11 08:31:58.153: INFO: Got endpoints: latency-svc-69brl [752.252084ms]
Feb 11 08:31:58.162: INFO: Created: latency-svc-xmcdx
Feb 11 08:31:58.203: INFO: Got endpoints: latency-svc-mtxx8 [751.294036ms]
Feb 11 08:31:58.214: INFO: Created: latency-svc-fhxvs
Feb 11 08:31:58.251: INFO: Got endpoints: latency-svc-6pnzl [750.081114ms]
Feb 11 08:31:58.258: INFO: Created: latency-svc-6qw5x
Feb 11 08:31:58.303: INFO: Got endpoints: latency-svc-ljnqd [751.458924ms]
Feb 11 08:31:58.310: INFO: Created: latency-svc-x8m7d
Feb 11 08:31:58.356: INFO: Got endpoints: latency-svc-bmdrx [754.268535ms]
Feb 11 08:31:58.368: INFO: Created: latency-svc-2g8pq
Feb 11 08:31:58.401: INFO: Got endpoints: latency-svc-5b94b [749.654027ms]
Feb 11 08:31:58.410: INFO: Created: latency-svc-2h48w
Feb 11 08:31:58.452: INFO: Got endpoints: latency-svc-5cjqj [750.316121ms]
Feb 11 08:31:58.459: INFO: Created: latency-svc-j9bks
Feb 11 08:31:58.502: INFO: Got endpoints: latency-svc-c4nj2 [748.643429ms]
Feb 11 08:31:58.510: INFO: Created: latency-svc-gjqd4
Feb 11 08:31:58.551: INFO: Got endpoints: latency-svc-z4gzp [748.846629ms]
Feb 11 08:31:58.560: INFO: Created: latency-svc-twqpn
Feb 11 08:31:58.601: INFO: Got endpoints: latency-svc-vbxzv [749.594378ms]
Feb 11 08:31:58.608: INFO: Created: latency-svc-xqlkx
Feb 11 08:31:58.652: INFO: Got endpoints: latency-svc-dc72v [751.458556ms]
Feb 11 08:31:58.661: INFO: Created: latency-svc-hsfvs
Feb 11 08:31:58.701: INFO: Got endpoints: latency-svc-5s2dl [750.260332ms]
Feb 11 08:31:58.708: INFO: Created: latency-svc-jpvvh
Feb 11 08:31:58.752: INFO: Got endpoints: latency-svc-vzrz6 [749.387734ms]
Feb 11 08:31:58.758: INFO: Created: latency-svc-86bft
Feb 11 08:31:58.802: INFO: Got endpoints: latency-svc-wtmwb [750.511777ms]
Feb 11 08:31:58.809: INFO: Created: latency-svc-sfjz8
Feb 11 08:31:58.851: INFO: Got endpoints: latency-svc-6frdf [749.418493ms]
Feb 11 08:31:58.861: INFO: Created: latency-svc-t4wgq
Feb 11 08:31:58.902: INFO: Got endpoints: latency-svc-xmcdx [748.387336ms]
Feb 11 08:31:58.908: INFO: Created: latency-svc-q5m7s
Feb 11 08:31:58.951: INFO: Got endpoints: latency-svc-fhxvs [748.682345ms]
Feb 11 08:31:58.959: INFO: Created: latency-svc-jvszd
Feb 11 08:31:59.001: INFO: Got endpoints: latency-svc-6qw5x [749.753762ms]
Feb 11 08:31:59.010: INFO: Created: latency-svc-n6jcn
Feb 11 08:31:59.051: INFO: Got endpoints: latency-svc-x8m7d [748.51905ms]
Feb 11 08:31:59.057: INFO: Created: latency-svc-nxsjq
Feb 11 08:31:59.102: INFO: Got endpoints: latency-svc-2g8pq [745.522705ms]
Feb 11 08:31:59.109: INFO: Created: latency-svc-hvzrn
Feb 11 08:31:59.152: INFO: Got endpoints: latency-svc-2h48w [750.596611ms]
Feb 11 08:31:59.161: INFO: Created: latency-svc-rbj7t
Feb 11 08:31:59.202: INFO: Got endpoints: latency-svc-j9bks [749.822693ms]
Feb 11 08:31:59.208: INFO: Created: latency-svc-htnnp
Feb 11 08:31:59.251: INFO: Got endpoints: latency-svc-gjqd4 [749.483234ms]
Feb 11 08:31:59.261: INFO: Created: latency-svc-qwhvs
Feb 11 08:31:59.301: INFO: Got endpoints: latency-svc-twqpn [750.13165ms]
Feb 11 08:31:59.309: INFO: Created: latency-svc-zsnzx
Feb 11 08:31:59.351: INFO: Got endpoints: latency-svc-xqlkx [750.37279ms]
Feb 11 08:31:59.360: INFO: Created: latency-svc-t2m2k
Feb 11 08:31:59.404: INFO: Got endpoints: latency-svc-hsfvs [751.406505ms]
Feb 11 08:31:59.416: INFO: Created: latency-svc-9cmqc
Feb 11 08:31:59.451: INFO: Got endpoints: latency-svc-jpvvh [749.437251ms]
Feb 11 08:31:59.459: INFO: Created: latency-svc-rxs4x
Feb 11 08:31:59.502: INFO: Got endpoints: latency-svc-86bft [750.214331ms]
Feb 11 08:31:59.509: INFO: Created: latency-svc-4xbvm
Feb 11 08:31:59.551: INFO: Got endpoints: latency-svc-sfjz8 [749.496198ms]
Feb 11 08:31:59.560: INFO: Created: latency-svc-cs2n7
Feb 11 08:31:59.601: INFO: Got endpoints: latency-svc-t4wgq [749.969884ms]
Feb 11 08:31:59.615: INFO: Created: latency-svc-9jczp
Feb 11 08:31:59.651: INFO: Got endpoints: latency-svc-q5m7s [749.237534ms]
Feb 11 08:31:59.657: INFO: Created: latency-svc-9gll8
Feb 11 08:31:59.702: INFO: Got endpoints: latency-svc-jvszd [750.273913ms]
Feb 11 08:31:59.709: INFO: Created: latency-svc-krc5b
Feb 11 08:31:59.751: INFO: Got endpoints: latency-svc-n6jcn [750.499271ms]
Feb 11 08:31:59.758: INFO: Created: latency-svc-89jxg
Feb 11 08:31:59.801: INFO: Got endpoints: latency-svc-nxsjq [750.185718ms]
Feb 11 08:31:59.810: INFO: Created: latency-svc-4zkzp
Feb 11 08:31:59.852: INFO: Got endpoints: latency-svc-hvzrn [750.504294ms]
Feb 11 08:31:59.863: INFO: Created: latency-svc-mbx2g
Feb 11 08:31:59.902: INFO: Got endpoints: latency-svc-rbj7t [750.029881ms]
Feb 11 08:31:59.915: INFO: Created: latency-svc-s7mbw
Feb 11 08:31:59.952: INFO: Got endpoints: latency-svc-htnnp [750.284155ms]
Feb 11 08:32:00.005: INFO: Got endpoints: latency-svc-qwhvs [753.104485ms]
Feb 11 08:32:00.052: INFO: Got endpoints: latency-svc-zsnzx [750.668416ms]
Feb 11 08:32:00.101: INFO: Got endpoints: latency-svc-t2m2k [750.053479ms]
Feb 11 08:32:00.153: INFO: Got endpoints: latency-svc-9cmqc [749.853644ms]
Feb 11 08:32:00.202: INFO: Got endpoints: latency-svc-rxs4x [750.91137ms]
Feb 11 08:32:00.253: INFO: Got endpoints: latency-svc-4xbvm [750.677093ms]
Feb 11 08:32:00.302: INFO: Got endpoints: latency-svc-cs2n7 [750.302205ms]
Feb 11 08:32:00.352: INFO: Got endpoints: latency-svc-9jczp [750.646156ms]
Feb 11 08:32:00.401: INFO: Got endpoints: latency-svc-9gll8 [750.568201ms]
Feb 11 08:32:00.454: INFO: Got endpoints: latency-svc-krc5b [752.198254ms]
Feb 11 08:32:00.502: INFO: Got endpoints: latency-svc-89jxg [750.559314ms]
Feb 11 08:32:00.552: INFO: Got endpoints: latency-svc-4zkzp [750.137231ms]
Feb 11 08:32:00.601: INFO: Got endpoints: latency-svc-mbx2g [749.270137ms]
Feb 11 08:32:00.652: INFO: Got endpoints: latency-svc-s7mbw [749.89192ms]
Feb 11 08:32:00.652: INFO: Latencies: [12.640144ms 18.066821ms 22.532896ms 27.762143ms 34.021287ms 38.640085ms 41.507091ms 49.809969ms 54.346057ms 59.925006ms 68.19241ms 69.743917ms 73.485675ms 75.682786ms 75.840638ms 76.504652ms 76.688393ms 76.756193ms 77.218651ms 77.795367ms 77.995731ms 78.279756ms 78.915141ms 79.43291ms 80.292465ms 85.562121ms 87.932792ms 89.882988ms 90.387503ms 90.981262ms 91.282464ms 92.098651ms 95.901743ms 141.28945ms 186.202761ms 232.395451ms 277.064255ms 319.911299ms 365.225089ms 410.60846ms 451.097844ms 489.407187ms 530.406297ms 581.198361ms 620.257223ms 666.903382ms 708.545134ms 739.303293ms 743.687369ms 745.522705ms 745.693678ms 747.875419ms 748.387336ms 748.51905ms 748.594508ms 748.643429ms 748.682345ms 748.765688ms 748.806433ms 748.846629ms 748.867073ms 748.93493ms 748.971411ms 748.99062ms 749.124403ms 749.157195ms 749.176494ms 749.190913ms 749.23459ms 749.237534ms 749.247037ms 749.26832ms 749.269483ms 749.270137ms 749.28345ms 749.308672ms 749.361956ms 749.387734ms 749.412183ms 749.418493ms 749.437251ms 749.483234ms 749.496198ms 749.519401ms 749.561708ms 749.56328ms 749.58619ms 749.586816ms 749.587473ms 749.594378ms 749.610208ms 749.634499ms 749.641112ms 749.649254ms 749.654027ms 749.655789ms 749.671032ms 749.720338ms 749.737992ms 749.740209ms 749.752276ms 749.753762ms 749.76055ms 749.778366ms 749.79057ms 749.799458ms 749.806774ms 749.80836ms 749.822693ms 749.826261ms 749.842584ms 749.853644ms 749.862138ms 749.883096ms 749.888353ms 749.89192ms 749.907415ms 749.916011ms 749.941602ms 749.960875ms 749.969884ms 749.992516ms 750.026811ms 750.029881ms 750.035706ms 750.053479ms 750.057696ms 750.081114ms 750.096697ms 750.101168ms 750.101515ms 750.120768ms 750.13165ms 750.137231ms 750.154984ms 750.185718ms 750.214331ms 750.237225ms 750.260332ms 750.273913ms 750.284155ms 750.302205ms 750.309198ms 750.311766ms 750.314275ms 750.316121ms 750.337163ms 750.372016ms 750.37279ms 750.389852ms 750.390472ms 750.452983ms 750.479842ms 750.494322ms 750.499271ms 750.504294ms 750.511777ms 750.542686ms 750.548217ms 750.559314ms 750.568201ms 750.580158ms 750.596611ms 750.601335ms 750.609677ms 750.619937ms 750.62294ms 750.646156ms 750.649416ms 750.668416ms 750.677093ms 750.694523ms 750.770658ms 750.777943ms 750.884379ms 750.894561ms 750.91137ms 750.918175ms 750.919819ms 750.957187ms 751.021877ms 751.037147ms 751.061196ms 751.094292ms 751.096127ms 751.207784ms 751.248483ms 751.294036ms 751.29706ms 751.406505ms 751.440832ms 751.458556ms 751.458924ms 752.198254ms 752.252084ms 752.533231ms 753.104485ms 754.268535ms 754.793405ms 761.224998ms]
Feb 11 08:32:00.652: INFO: 50 %ile: 749.752276ms
Feb 11 08:32:00.652: INFO: 90 %ile: 751.021877ms
Feb 11 08:32:00.652: INFO: 99 %ile: 754.793405ms
Feb 11 08:32:00.652: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:32:00.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4637" for this suite.
Feb 11 08:32:10.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:32:10.730: INFO: namespace svc-latency-4637 deletion completed in 10.07533921s

â€¢ [SLOW TEST:20.817 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:32:10.731: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb 11 08:32:10.990: INFO: Pod name wrapped-volume-race-50a7273d-d049-44af-8b99-ac5c4ddac087: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-50a7273d-d049-44af-8b99-ac5c4ddac087 in namespace emptydir-wrapper-3599, will wait for the garbage collector to delete the pods
Feb 11 08:32:45.120: INFO: Deleting ReplicationController wrapped-volume-race-50a7273d-d049-44af-8b99-ac5c4ddac087 took: 7.263587ms
Feb 11 08:32:45.520: INFO: Terminating ReplicationController wrapped-volume-race-50a7273d-d049-44af-8b99-ac5c4ddac087 pods took: 400.233457ms
STEP: Creating RC which spawns configmap-volume pods
Feb 11 08:33:20.635: INFO: Pod name wrapped-volume-race-d12100dc-cef6-440b-a42c-20f32fec5dd7: Found 0 pods out of 5
Feb 11 08:33:25.639: INFO: Pod name wrapped-volume-race-d12100dc-cef6-440b-a42c-20f32fec5dd7: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d12100dc-cef6-440b-a42c-20f32fec5dd7 in namespace emptydir-wrapper-3599, will wait for the garbage collector to delete the pods
Feb 11 08:33:35.714: INFO: Deleting ReplicationController wrapped-volume-race-d12100dc-cef6-440b-a42c-20f32fec5dd7 took: 6.428685ms
Feb 11 08:33:36.114: INFO: Terminating ReplicationController wrapped-volume-race-d12100dc-cef6-440b-a42c-20f32fec5dd7 pods took: 400.234591ms
STEP: Creating RC which spawns configmap-volume pods
Feb 11 08:34:19.731: INFO: Pod name wrapped-volume-race-7061aa19-a262-4b3c-99d7-6a624a207347: Found 0 pods out of 5
Feb 11 08:34:24.736: INFO: Pod name wrapped-volume-race-7061aa19-a262-4b3c-99d7-6a624a207347: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7061aa19-a262-4b3c-99d7-6a624a207347 in namespace emptydir-wrapper-3599, will wait for the garbage collector to delete the pods
Feb 11 08:34:36.812: INFO: Deleting ReplicationController wrapped-volume-race-7061aa19-a262-4b3c-99d7-6a624a207347 took: 6.417744ms
Feb 11 08:34:37.212: INFO: Terminating ReplicationController wrapped-volume-race-7061aa19-a262-4b3c-99d7-6a624a207347 pods took: 400.244764ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:35:11.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3599" for this suite.
Feb 11 08:35:17.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:35:17.527: INFO: namespace emptydir-wrapper-3599 deletion completed in 6.072138361s

â€¢ [SLOW TEST:186.796 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:35:17.527: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Feb 11 08:35:18.067: INFO: created pod pod-service-account-defaultsa
Feb 11 08:35:18.067: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 11 08:35:18.071: INFO: created pod pod-service-account-mountsa
Feb 11 08:35:18.071: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 11 08:35:18.076: INFO: created pod pod-service-account-nomountsa
Feb 11 08:35:18.076: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 11 08:35:18.082: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 11 08:35:18.082: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 11 08:35:18.086: INFO: created pod pod-service-account-mountsa-mountspec
Feb 11 08:35:18.086: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 11 08:35:18.092: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 11 08:35:18.092: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 11 08:35:18.097: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 11 08:35:18.097: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 11 08:35:18.100: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 11 08:35:18.100: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 11 08:35:18.118: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 11 08:35:18.118: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:35:18.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-585" for this suite.
Feb 11 08:35:30.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:35:30.206: INFO: namespace svcaccounts-585 deletion completed in 12.073668094s

â€¢ [SLOW TEST:12.679 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:35:30.206: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Feb 11 08:35:30.228: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:35:37.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3084" for this suite.
Feb 11 08:35:43.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:35:43.295: INFO: namespace init-container-3084 deletion completed in 6.079693255s

â€¢ [SLOW TEST:13.089 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:35:43.296: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-9f88337d-f18c-47ff-93f6-24a9f7d12f8e in namespace container-probe-4411
Feb 11 08:35:47.331: INFO: Started pod test-webserver-9f88337d-f18c-47ff-93f6-24a9f7d12f8e in namespace container-probe-4411
STEP: checking the pod's current state and verifying that restartCount is present
Feb 11 08:35:47.334: INFO: Initial restart count of pod test-webserver-9f88337d-f18c-47ff-93f6-24a9f7d12f8e is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:39:47.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4411" for this suite.
Feb 11 08:39:53.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:39:53.834: INFO: namespace container-probe-4411 deletion completed in 6.071964092s

â€¢ [SLOW TEST:250.538 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:39:53.834: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 11 08:39:53.862: INFO: Waiting up to 5m0s for pod "pod-c9758772-4aac-4ecb-bcad-0c84745c346c" in namespace "emptydir-18" to be "success or failure"
Feb 11 08:39:53.865: INFO: Pod "pod-c9758772-4aac-4ecb-bcad-0c84745c346c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.4685ms
Feb 11 08:39:55.868: INFO: Pod "pod-c9758772-4aac-4ecb-bcad-0c84745c346c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006064866s
Feb 11 08:39:57.872: INFO: Pod "pod-c9758772-4aac-4ecb-bcad-0c84745c346c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009576086s
STEP: Saw pod success
Feb 11 08:39:57.872: INFO: Pod "pod-c9758772-4aac-4ecb-bcad-0c84745c346c" satisfied condition "success or failure"
Feb 11 08:39:57.875: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-c9758772-4aac-4ecb-bcad-0c84745c346c container test-container: <nil>
STEP: delete the pod
Feb 11 08:39:57.898: INFO: Waiting for pod pod-c9758772-4aac-4ecb-bcad-0c84745c346c to disappear
Feb 11 08:39:57.901: INFO: Pod pod-c9758772-4aac-4ecb-bcad-0c84745c346c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:39:57.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-18" for this suite.
Feb 11 08:40:03.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:40:03.977: INFO: namespace emptydir-18 deletion completed in 6.073209949s

â€¢ [SLOW TEST:10.143 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:40:03.977: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 08:40:03.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 create -f - --namespace=kubectl-7753'
Feb 11 08:40:04.387: INFO: stderr: ""
Feb 11 08:40:04.387: INFO: stdout: "replicationcontroller/redis-master created\n"
Feb 11 08:40:04.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 create -f - --namespace=kubectl-7753'
Feb 11 08:40:04.589: INFO: stderr: ""
Feb 11 08:40:04.589: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 11 08:40:05.593: INFO: Selector matched 1 pods for map[app:redis]
Feb 11 08:40:05.593: INFO: Found 0 / 1
Feb 11 08:40:06.593: INFO: Selector matched 1 pods for map[app:redis]
Feb 11 08:40:06.593: INFO: Found 0 / 1
Feb 11 08:40:07.593: INFO: Selector matched 1 pods for map[app:redis]
Feb 11 08:40:07.593: INFO: Found 0 / 1
Feb 11 08:40:08.593: INFO: Selector matched 1 pods for map[app:redis]
Feb 11 08:40:08.594: INFO: Found 0 / 1
Feb 11 08:40:09.593: INFO: Selector matched 1 pods for map[app:redis]
Feb 11 08:40:09.593: INFO: Found 0 / 1
Feb 11 08:40:10.593: INFO: Selector matched 1 pods for map[app:redis]
Feb 11 08:40:10.593: INFO: Found 0 / 1
Feb 11 08:40:11.593: INFO: Selector matched 1 pods for map[app:redis]
Feb 11 08:40:11.593: INFO: Found 0 / 1
Feb 11 08:40:12.593: INFO: Selector matched 1 pods for map[app:redis]
Feb 11 08:40:12.593: INFO: Found 1 / 1
Feb 11 08:40:12.593: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 11 08:40:12.596: INFO: Selector matched 1 pods for map[app:redis]
Feb 11 08:40:12.596: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 11 08:40:12.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 describe pod redis-master-k99pf --namespace=kubectl-7753'
Feb 11 08:40:12.678: INFO: stderr: ""
Feb 11 08:40:12.678: INFO: stdout: "Name:         redis-master-k99pf\nNamespace:    kubectl-7753\nPriority:     0\nNode:         cn-hongkong.192.168.0.20/192.168.0.20\nStart Time:   Tue, 11 Feb 2020 08:40:04 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nStatus:       Running\nIP:           172.22.1.137\nIPs:\n  IP:           172.22.1.137\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://32104df610523ce0b7f3ae4c2e830ddd4435715870e7de57b8f5de5341629ca4\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 11 Feb 2020 08:40:11 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-46wxv (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-46wxv:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-46wxv\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                               Message\n  ----    ------     ----       ----                               -------\n  Normal  Scheduled  <unknown>  default-scheduler                  Successfully assigned kubectl-7753/redis-master-k99pf to cn-hongkong.192.168.0.20\n  Normal  Pulling    7s         kubelet, cn-hongkong.192.168.0.20  Pulling image \"docker.io/library/redis:5.0.5-alpine\"\n  Normal  Pulled     1s         kubelet, cn-hongkong.192.168.0.20  Successfully pulled image \"docker.io/library/redis:5.0.5-alpine\"\n  Normal  Created    1s         kubelet, cn-hongkong.192.168.0.20  Created container redis-master\n  Normal  Started    1s         kubelet, cn-hongkong.192.168.0.20  Started container redis-master\n"
Feb 11 08:40:12.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 describe rc redis-master --namespace=kubectl-7753'
Feb 11 08:40:12.771: INFO: stderr: ""
Feb 11 08:40:12.771: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-7753\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  8s    replication-controller  Created pod: redis-master-k99pf\n"
Feb 11 08:40:12.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 describe service redis-master --namespace=kubectl-7753'
Feb 11 08:40:12.869: INFO: stderr: ""
Feb 11 08:40:12.869: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-7753\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.23.10.121\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.22.1.137:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 11 08:40:12.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 describe node cn-hongkong.192.168.0.15'
Feb 11 08:40:12.972: INFO: stderr: ""
Feb 11 08:40:12.972: INFO: stdout: "Name:               cn-hongkong.192.168.0.15\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=ecs.g5.xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=cn-hongkong\n                    failure-domain.beta.kubernetes.io/zone=cn-hongkong-b\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=cn-hongkong.192.168.0.15\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        flannel.alpha.coreos.com/backend-data: null\n                    flannel.alpha.coreos.com/backend-type: \n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.0.15\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\nCreationTimestamp:  Tue, 11 Feb 2020 07:55:54 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 11 Feb 2020 08:37:22 +0000   Tue, 11 Feb 2020 08:37:22 +0000   RouteCreated                 RouteController created a route\n  MemoryPressure       False   Tue, 11 Feb 2020 08:39:38 +0000   Tue, 11 Feb 2020 07:55:53 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 11 Feb 2020 08:39:38 +0000   Tue, 11 Feb 2020 07:55:53 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 11 Feb 2020 08:39:38 +0000   Tue, 11 Feb 2020 07:55:53 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 11 Feb 2020 08:39:38 +0000   Tue, 11 Feb 2020 08:01:19 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.0.15\n  Hostname:    cn-hongkong.192.168.0.15\nCapacity:\n cpu:                4\n ephemeral-storage:  123722716Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16266408Ki\n pods:               110\nAllocatable:\n cpu:                4\n ephemeral-storage:  114022854877\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             15242408Ki\n pods:               110\nSystem Info:\n Machine ID:                 20190619144105153505835812255700\n System UUID:                2C246D1C-51AD-40ED-9DDA-22B9B7632A09\n Boot ID:                    974a6f5d-5ffd-44ac-aa97-32a4290b9bb0\n Kernel Version:             3.10.0-957.21.3.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://19.3.5\n Kubelet Version:            v1.16.6-aliyun.1\n Kube-Proxy Version:         v1.16.6-aliyun.1\nPodCIDR:                     172.22.0.0/25\nPodCIDRs:                    172.22.0.0/25\nProviderID:                  cn-hongkong.i-j6c65tsjaqwtd222gq9t\nNon-terminated Pods:         (8 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                cloud-controller-manager-4xtsr                             200m (5%)     0 (0%)      0 (0%)           0 (0%)         39m\n  kube-system                flexvolume-p2mbg                                           100m (2%)     1 (25%)     100Mi (0%)       1000Mi (6%)    39m\n  kube-system                kube-apiserver-cn-hongkong.192.168.0.15                    250m (6%)     0 (0%)      0 (0%)           0 (0%)         42m\n  kube-system                kube-controller-manager-cn-hongkong.192.168.0.15           200m (5%)     0 (0%)      0 (0%)           0 (0%)         42m\n  kube-system                kube-flannel-ds-gpxck                                      100m (2%)     100m (2%)   50Mi (0%)        50Mi (0%)      39m\n  kube-system                kube-proxy-master-cjc2q                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         39m\n  kube-system                kube-scheduler-cn-hongkong.192.168.0.15                    100m (2%)     0 (0%)      0 (0%)           0 (0%)         44m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-cf27379a033c4f18-fcxzs    0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m2s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                950m (23%)  1100m (27%)\n  memory             150Mi (1%)  1050Mi (7%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:\n  Type    Reason                   Age                From                                  Message\n  ----    ------                   ----               ----                                  -------\n  Normal  Starting                 44m                kubelet, cn-hongkong.192.168.0.15     Starting kubelet.\n  Normal  NodeHasSufficientMemory  44m (x3 over 44m)  kubelet, cn-hongkong.192.168.0.15     Node cn-hongkong.192.168.0.15 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    44m (x3 over 44m)  kubelet, cn-hongkong.192.168.0.15     Node cn-hongkong.192.168.0.15 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     44m (x3 over 44m)  kubelet, cn-hongkong.192.168.0.15     Node cn-hongkong.192.168.0.15 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  44m                kubelet, cn-hongkong.192.168.0.15     Updated Node Allocatable limit across pods\n  Normal  Starting                 44m                kubelet, cn-hongkong.192.168.0.15     Starting kubelet.\n  Normal  NodeHasSufficientMemory  44m (x8 over 44m)  kubelet, cn-hongkong.192.168.0.15     Node cn-hongkong.192.168.0.15 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    44m (x8 over 44m)  kubelet, cn-hongkong.192.168.0.15     Node cn-hongkong.192.168.0.15 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     44m (x7 over 44m)  kubelet, cn-hongkong.192.168.0.15     Node cn-hongkong.192.168.0.15 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  44m                kubelet, cn-hongkong.192.168.0.15     Updated Node Allocatable limit across pods\n  Normal  Starting                 39m                kube-proxy, cn-hongkong.192.168.0.15  Starting kube-proxy.\n  Normal  NodeReady                38m                kubelet, cn-hongkong.192.168.0.15     Node cn-hongkong.192.168.0.15 status is now: NodeReady\n"
Feb 11 08:40:12.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 describe namespace kubectl-7753'
Feb 11 08:40:13.049: INFO: stderr: ""
Feb 11 08:40:13.049: INFO: stdout: "Name:         kubectl-7753\nLabels:       e2e-framework=kubectl\n              e2e-run=eb2ccd27-6f2e-4217-afe1-0ed128452c55\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:40:13.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7753" for this suite.
Feb 11 08:40:25.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:40:25.126: INFO: namespace kubectl-7753 deletion completed in 12.073223742s

â€¢ [SLOW TEST:21.149 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:40:25.126: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-8182
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 11 08:40:25.148: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 11 08:40:47.220: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.22.2.27 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8182 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 11 08:40:47.220: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 08:40:48.354: INFO: Found all expected endpoints: [netserver-0]
Feb 11 08:40:48.361: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.22.2.137 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8182 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 11 08:40:48.361: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 08:40:49.513: INFO: Found all expected endpoints: [netserver-1]
Feb 11 08:40:49.516: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.22.1.138 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8182 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 11 08:40:49.516: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 08:40:50.674: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:40:50.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8182" for this suite.
Feb 11 08:41:02.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:41:02.750: INFO: namespace pod-network-test-8182 deletion completed in 12.072817581s

â€¢ [SLOW TEST:37.624 seconds]
[sig-network] Networking
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:41:02.750: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:41:13.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2125" for this suite.
Feb 11 08:41:19.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:41:19.882: INFO: namespace resourcequota-2125 deletion completed in 6.074919692s

â€¢ [SLOW TEST:17.132 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:41:19.882: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Feb 11 08:41:22.444: INFO: Successfully updated pod "annotationupdatebd6c40ed-ff81-4b97-88f2-f7b63daf0bb8"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:41:26.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8448" for this suite.
Feb 11 08:41:38.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:41:38.545: INFO: namespace downward-api-8448 deletion completed in 12.07316079s

â€¢ [SLOW TEST:18.663 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:41:38.545: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 11 08:41:38.573: INFO: Waiting up to 5m0s for pod "pod-33d5fde8-d462-43ef-bd60-c4e1d7ca93d1" in namespace "emptydir-1051" to be "success or failure"
Feb 11 08:41:38.575: INFO: Pod "pod-33d5fde8-d462-43ef-bd60-c4e1d7ca93d1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.942337ms
Feb 11 08:41:40.578: INFO: Pod "pod-33d5fde8-d462-43ef-bd60-c4e1d7ca93d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00537773s
STEP: Saw pod success
Feb 11 08:41:40.578: INFO: Pod "pod-33d5fde8-d462-43ef-bd60-c4e1d7ca93d1" satisfied condition "success or failure"
Feb 11 08:41:40.581: INFO: Trying to get logs from node cn-hongkong.192.168.0.19 pod pod-33d5fde8-d462-43ef-bd60-c4e1d7ca93d1 container test-container: <nil>
STEP: delete the pod
Feb 11 08:41:40.607: INFO: Waiting for pod pod-33d5fde8-d462-43ef-bd60-c4e1d7ca93d1 to disappear
Feb 11 08:41:40.609: INFO: Pod pod-33d5fde8-d462-43ef-bd60-c4e1d7ca93d1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:41:40.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1051" for this suite.
Feb 11 08:41:46.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:41:46.686: INFO: namespace emptydir-1051 deletion completed in 6.074195127s

â€¢ [SLOW TEST:8.141 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:41:46.686: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 11 08:41:46.724: INFO: Waiting up to 5m0s for pod "pod-afa45d5b-9fba-4f12-8f02-a3379e8928ad" in namespace "emptydir-4496" to be "success or failure"
Feb 11 08:41:46.726: INFO: Pod "pod-afa45d5b-9fba-4f12-8f02-a3379e8928ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1.984293ms
Feb 11 08:41:48.730: INFO: Pod "pod-afa45d5b-9fba-4f12-8f02-a3379e8928ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005391428s
STEP: Saw pod success
Feb 11 08:41:48.730: INFO: Pod "pod-afa45d5b-9fba-4f12-8f02-a3379e8928ad" satisfied condition "success or failure"
Feb 11 08:41:48.732: INFO: Trying to get logs from node cn-hongkong.192.168.0.20 pod pod-afa45d5b-9fba-4f12-8f02-a3379e8928ad container test-container: <nil>
STEP: delete the pod
Feb 11 08:41:48.747: INFO: Waiting for pod pod-afa45d5b-9fba-4f12-8f02-a3379e8928ad to disappear
Feb 11 08:41:48.749: INFO: Pod pod-afa45d5b-9fba-4f12-8f02-a3379e8928ad no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:41:48.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4496" for this suite.
Feb 11 08:41:54.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:41:54.827: INFO: namespace emptydir-4496 deletion completed in 6.075188072s

â€¢ [SLOW TEST:8.141 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:41:54.827: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Feb 11 08:41:54.850: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:42:00.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8739" for this suite.
Feb 11 08:42:28.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:42:28.199: INFO: namespace init-container-8739 deletion completed in 28.07324541s

â€¢ [SLOW TEST:33.372 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:42:28.200: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 11 08:42:28.228: INFO: Waiting up to 5m0s for pod "pod-57bf2099-d028-477b-adc0-928b0833e6c3" in namespace "emptydir-3384" to be "success or failure"
Feb 11 08:42:28.231: INFO: Pod "pod-57bf2099-d028-477b-adc0-928b0833e6c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.764472ms
Feb 11 08:42:30.234: INFO: Pod "pod-57bf2099-d028-477b-adc0-928b0833e6c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006380325s
STEP: Saw pod success
Feb 11 08:42:30.234: INFO: Pod "pod-57bf2099-d028-477b-adc0-928b0833e6c3" satisfied condition "success or failure"
Feb 11 08:42:30.237: INFO: Trying to get logs from node cn-hongkong.192.168.0.19 pod pod-57bf2099-d028-477b-adc0-928b0833e6c3 container test-container: <nil>
STEP: delete the pod
Feb 11 08:42:30.252: INFO: Waiting for pod pod-57bf2099-d028-477b-adc0-928b0833e6c3 to disappear
Feb 11 08:42:30.254: INFO: Pod pod-57bf2099-d028-477b-adc0-928b0833e6c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:42:30.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3384" for this suite.
Feb 11 08:42:36.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:42:36.329: INFO: namespace emptydir-3384 deletion completed in 6.07234715s

â€¢ [SLOW TEST:8.130 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:42:36.330: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 11 08:42:36.718: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 11 08:42:39.732: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:42:39.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9880" for this suite.
Feb 11 08:42:45.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:42:45.922: INFO: namespace webhook-9880 deletion completed in 6.069888276s
STEP: Destroying namespace "webhook-9880-markers" for this suite.
Feb 11 08:42:51.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:42:51.998: INFO: namespace webhook-9880-markers deletion completed in 6.075212788s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:15.680 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:42:52.009: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:42:58.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3603" for this suite.
Feb 11 08:43:04.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:43:04.162: INFO: namespace namespaces-3603 deletion completed in 6.070086871s
STEP: Destroying namespace "nsdeletetest-7974" for this suite.
Feb 11 08:43:04.164: INFO: Namespace nsdeletetest-7974 was already deleted
STEP: Destroying namespace "nsdeletetest-6778" for this suite.
Feb 11 08:43:10.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:43:10.234: INFO: namespace nsdeletetest-6778 deletion completed in 6.070211645s

â€¢ [SLOW TEST:18.225 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:43:10.235: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Feb 11 08:43:12.273: INFO: Pod pod-hostip-04220eb9-864c-49ec-8f7d-84005518a9f5 has hostIP: 192.168.0.20
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:43:12.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7544" for this suite.
Feb 11 08:43:40.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:43:40.350: INFO: namespace pods-7544 deletion completed in 28.073660303s

â€¢ [SLOW TEST:30.115 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:43:40.350: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: Gathering metrics
Feb 11 08:43:41.408: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:43:41.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0211 08:43:41.408082      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-320" for this suite.
Feb 11 08:43:47.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:43:47.490: INFO: namespace gc-320 deletion completed in 6.078860731s

â€¢ [SLOW TEST:7.140 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:43:47.490: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 11 08:43:47.526: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9986 /api/v1/namespaces/watch-9986/configmaps/e2e-watch-test-label-changed 6203ebd5-6493-4c69-9a65-a841366471f7 11673 0 2020-02-11 08:43:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 11 08:43:47.526: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9986 /api/v1/namespaces/watch-9986/configmaps/e2e-watch-test-label-changed 6203ebd5-6493-4c69-9a65-a841366471f7 11674 0 2020-02-11 08:43:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 11 08:43:47.526: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9986 /api/v1/namespaces/watch-9986/configmaps/e2e-watch-test-label-changed 6203ebd5-6493-4c69-9a65-a841366471f7 11675 0 2020-02-11 08:43:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 11 08:43:57.551: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9986 /api/v1/namespaces/watch-9986/configmaps/e2e-watch-test-label-changed 6203ebd5-6493-4c69-9a65-a841366471f7 11702 0 2020-02-11 08:43:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 11 08:43:57.551: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9986 /api/v1/namespaces/watch-9986/configmaps/e2e-watch-test-label-changed 6203ebd5-6493-4c69-9a65-a841366471f7 11703 0 2020-02-11 08:43:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb 11 08:43:57.552: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9986 /api/v1/namespaces/watch-9986/configmaps/e2e-watch-test-label-changed 6203ebd5-6493-4c69-9a65-a841366471f7 11704 0 2020-02-11 08:43:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:43:57.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9986" for this suite.
Feb 11 08:44:03.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:44:03.628: INFO: namespace watch-9986 deletion completed in 6.073439678s

â€¢ [SLOW TEST:16.138 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:44:03.628: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-68f2c89a-5700-4a47-93ec-c9f964cc5d3f
STEP: Creating a pod to test consume configMaps
Feb 11 08:44:03.657: INFO: Waiting up to 5m0s for pod "pod-configmaps-5ca04d51-6707-4e01-8a98-38d5f1b59e32" in namespace "configmap-7370" to be "success or failure"
Feb 11 08:44:03.660: INFO: Pod "pod-configmaps-5ca04d51-6707-4e01-8a98-38d5f1b59e32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.215252ms
Feb 11 08:44:05.663: INFO: Pod "pod-configmaps-5ca04d51-6707-4e01-8a98-38d5f1b59e32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005294258s
STEP: Saw pod success
Feb 11 08:44:05.663: INFO: Pod "pod-configmaps-5ca04d51-6707-4e01-8a98-38d5f1b59e32" satisfied condition "success or failure"
Feb 11 08:44:05.666: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-configmaps-5ca04d51-6707-4e01-8a98-38d5f1b59e32 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 11 08:44:05.687: INFO: Waiting for pod pod-configmaps-5ca04d51-6707-4e01-8a98-38d5f1b59e32 to disappear
Feb 11 08:44:05.689: INFO: Pod pod-configmaps-5ca04d51-6707-4e01-8a98-38d5f1b59e32 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:44:05.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7370" for this suite.
Feb 11 08:44:11.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:44:11.763: INFO: namespace configmap-7370 deletion completed in 6.071895422s

â€¢ [SLOW TEST:8.136 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:44:11.764: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:44:11.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5270" for this suite.
Feb 11 08:44:17.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:44:17.862: INFO: namespace services-5270 deletion completed in 6.071000156s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:6.098 seconds]
[sig-network] Services
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:44:17.862: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Feb 11 08:44:17.884: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 11 08:44:17.892: INFO: Waiting for terminating namespaces to be deleted...
Feb 11 08:44:17.894: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.18 before test
Feb 11 08:44:17.899: INFO: coredns-7bbcddf6b7-bjckz from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:17.899: INFO: 	Container coredns ready: true, restart count 0
Feb 11 08:44:17.899: INFO: flexvolume-mv7bv from kube-system started at 2020-02-11 08:01:04 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:17.899: INFO: 	Container acs-flexvolume ready: true, restart count 0
Feb 11 08:44:17.899: INFO: kube-flannel-ds-xjcpn from kube-system started at 2020-02-11 08:01:10 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:17.899: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 11 08:44:17.899: INFO: alicloud-application-controller-66cd594458-dzs7s from kube-system started at 2020-02-11 08:01:23 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:17.899: INFO: 	Container alicloud-application-controller ready: true, restart count 0
Feb 11 08:44:17.899: INFO: nginx-ingress-controller-5f645986c-gw8nx from kube-system started at 2020-02-11 08:01:27 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:17.899: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 11 08:44:17.899: INFO: sonobuoy-systemd-logs-daemon-set-cf27379a033c4f18-ts8rk from sonobuoy started at 2020-02-11 08:31:10 +0000 UTC (2 container statuses recorded)
Feb 11 08:44:17.899: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 11 08:44:17.899: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 11 08:44:17.899: INFO: kube-proxy-worker-c92sw from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:17.899: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Feb 11 08:44:17.900: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.19 before test
Feb 11 08:44:17.910: INFO: metrics-server-74d79db469-q9c6v from kube-system started at 2020-02-11 08:01:26 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:17.910: INFO: 	Container metrics-server ready: true, restart count 0
Feb 11 08:44:17.910: INFO: flexvolume-lfp7h from kube-system started at 2020-02-11 08:01:04 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:17.910: INFO: 	Container acs-flexvolume ready: true, restart count 0
Feb 11 08:44:17.910: INFO: kube-flannel-ds-l5588 from kube-system started at 2020-02-11 08:01:11 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:17.910: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 11 08:44:17.910: INFO: nginx-ingress-controller-5f645986c-lbm58 from kube-system started at 2020-02-11 08:01:27 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:17.910: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 11 08:44:17.910: INFO: sonobuoy from sonobuoy started at 2020-02-11 08:31:09 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:17.910: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 11 08:44:17.910: INFO: sonobuoy-systemd-logs-daemon-set-cf27379a033c4f18-w4gvp from sonobuoy started at 2020-02-11 08:31:10 +0000 UTC (2 container statuses recorded)
Feb 11 08:44:17.910: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 11 08:44:17.910: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 11 08:44:17.910: INFO: kube-proxy-worker-gv69f from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:17.910: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Feb 11 08:44:17.910: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.20 before test
Feb 11 08:44:17.921: INFO: kube-proxy-worker-6tcbf from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:17.921: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Feb 11 08:44:17.921: INFO: coredns-7bbcddf6b7-r9zv8 from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:17.921: INFO: 	Container coredns ready: true, restart count 0
Feb 11 08:44:17.921: INFO: flexvolume-rg5mr from kube-system started at 2020-02-11 08:01:04 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:17.921: INFO: 	Container acs-flexvolume ready: true, restart count 0
Feb 11 08:44:17.921: INFO: kube-flannel-ds-xskqs from kube-system started at 2020-02-11 08:01:10 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:17.921: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 11 08:44:17.921: INFO: sonobuoy-e2e-job-a3471aaa24714387 from sonobuoy started at 2020-02-11 08:31:10 +0000 UTC (2 container statuses recorded)
Feb 11 08:44:17.921: INFO: 	Container e2e ready: true, restart count 0
Feb 11 08:44:17.921: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 11 08:44:17.921: INFO: sonobuoy-systemd-logs-daemon-set-cf27379a033c4f18-rfthz from sonobuoy started at 2020-02-11 08:31:10 +0000 UTC (2 container statuses recorded)
Feb 11 08:44:17.921: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 11 08:44:17.921: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-acb99bd0-3721-43ca-9acc-5dfe5d3ffe4b 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-acb99bd0-3721-43ca-9acc-5dfe5d3ffe4b off the node cn-hongkong.192.168.0.20
STEP: verifying the node doesn't have the label kubernetes.io/e2e-acb99bd0-3721-43ca-9acc-5dfe5d3ffe4b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:44:25.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8996" for this suite.
Feb 11 08:44:44.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:44:44.070: INFO: namespace sched-pred-8996 deletion completed in 18.0784777s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:26.208 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:44:44.070: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-a4b2b9df-dcd1-414a-96f2-41d9b44717d4
STEP: Creating a pod to test consume configMaps
Feb 11 08:44:44.101: INFO: Waiting up to 5m0s for pod "pod-configmaps-aadbe322-8ba9-4d0e-b3de-ad48e96edd94" in namespace "configmap-5687" to be "success or failure"
Feb 11 08:44:44.103: INFO: Pod "pod-configmaps-aadbe322-8ba9-4d0e-b3de-ad48e96edd94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076785ms
Feb 11 08:44:46.106: INFO: Pod "pod-configmaps-aadbe322-8ba9-4d0e-b3de-ad48e96edd94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005803219s
STEP: Saw pod success
Feb 11 08:44:46.106: INFO: Pod "pod-configmaps-aadbe322-8ba9-4d0e-b3de-ad48e96edd94" satisfied condition "success or failure"
Feb 11 08:44:46.109: INFO: Trying to get logs from node cn-hongkong.192.168.0.20 pod pod-configmaps-aadbe322-8ba9-4d0e-b3de-ad48e96edd94 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 11 08:44:46.123: INFO: Waiting for pod pod-configmaps-aadbe322-8ba9-4d0e-b3de-ad48e96edd94 to disappear
Feb 11 08:44:46.125: INFO: Pod pod-configmaps-aadbe322-8ba9-4d0e-b3de-ad48e96edd94 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:44:46.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5687" for this suite.
Feb 11 08:44:52.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:44:52.201: INFO: namespace configmap-5687 deletion completed in 6.072401459s

â€¢ [SLOW TEST:8.131 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:44:52.201: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Feb 11 08:44:52.223: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 11 08:44:52.231: INFO: Waiting for terminating namespaces to be deleted...
Feb 11 08:44:52.233: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.18 before test
Feb 11 08:44:52.238: INFO: kube-proxy-worker-c92sw from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:52.238: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Feb 11 08:44:52.238: INFO: coredns-7bbcddf6b7-bjckz from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:52.238: INFO: 	Container coredns ready: true, restart count 0
Feb 11 08:44:52.238: INFO: flexvolume-mv7bv from kube-system started at 2020-02-11 08:01:04 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:52.238: INFO: 	Container acs-flexvolume ready: true, restart count 0
Feb 11 08:44:52.238: INFO: kube-flannel-ds-xjcpn from kube-system started at 2020-02-11 08:01:10 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:52.238: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 11 08:44:52.238: INFO: alicloud-application-controller-66cd594458-dzs7s from kube-system started at 2020-02-11 08:01:23 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:52.238: INFO: 	Container alicloud-application-controller ready: true, restart count 0
Feb 11 08:44:52.238: INFO: nginx-ingress-controller-5f645986c-gw8nx from kube-system started at 2020-02-11 08:01:27 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:52.238: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 11 08:44:52.238: INFO: sonobuoy-systemd-logs-daemon-set-cf27379a033c4f18-ts8rk from sonobuoy started at 2020-02-11 08:31:10 +0000 UTC (2 container statuses recorded)
Feb 11 08:44:52.238: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 11 08:44:52.238: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 11 08:44:52.238: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.19 before test
Feb 11 08:44:52.243: INFO: kube-proxy-worker-gv69f from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:52.243: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Feb 11 08:44:52.243: INFO: kube-flannel-ds-l5588 from kube-system started at 2020-02-11 08:01:11 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:52.243: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 11 08:44:52.243: INFO: nginx-ingress-controller-5f645986c-lbm58 from kube-system started at 2020-02-11 08:01:27 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:52.243: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 11 08:44:52.243: INFO: sonobuoy from sonobuoy started at 2020-02-11 08:31:09 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:52.243: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 11 08:44:52.243: INFO: sonobuoy-systemd-logs-daemon-set-cf27379a033c4f18-w4gvp from sonobuoy started at 2020-02-11 08:31:10 +0000 UTC (2 container statuses recorded)
Feb 11 08:44:52.243: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 11 08:44:52.243: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 11 08:44:52.243: INFO: flexvolume-lfp7h from kube-system started at 2020-02-11 08:01:04 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:52.243: INFO: 	Container acs-flexvolume ready: true, restart count 0
Feb 11 08:44:52.243: INFO: metrics-server-74d79db469-q9c6v from kube-system started at 2020-02-11 08:01:26 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:52.243: INFO: 	Container metrics-server ready: true, restart count 0
Feb 11 08:44:52.243: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.20 before test
Feb 11 08:44:52.247: INFO: kube-proxy-worker-6tcbf from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:52.247: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Feb 11 08:44:52.247: INFO: flexvolume-rg5mr from kube-system started at 2020-02-11 08:01:04 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:52.247: INFO: 	Container acs-flexvolume ready: true, restart count 0
Feb 11 08:44:52.247: INFO: coredns-7bbcddf6b7-r9zv8 from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:52.247: INFO: 	Container coredns ready: true, restart count 0
Feb 11 08:44:52.247: INFO: kube-flannel-ds-xskqs from kube-system started at 2020-02-11 08:01:10 +0000 UTC (1 container statuses recorded)
Feb 11 08:44:52.247: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 11 08:44:52.247: INFO: sonobuoy-e2e-job-a3471aaa24714387 from sonobuoy started at 2020-02-11 08:31:10 +0000 UTC (2 container statuses recorded)
Feb 11 08:44:52.247: INFO: 	Container e2e ready: true, restart count 0
Feb 11 08:44:52.247: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 11 08:44:52.247: INFO: sonobuoy-systemd-logs-daemon-set-cf27379a033c4f18-rfthz from sonobuoy started at 2020-02-11 08:31:10 +0000 UTC (2 container statuses recorded)
Feb 11 08:44:52.247: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 11 08:44:52.247: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-0bc4665e-5988-4bb5-a8ff-0a1a07f2f96a 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-0bc4665e-5988-4bb5-a8ff-0a1a07f2f96a off the node cn-hongkong.192.168.0.18
STEP: verifying the node doesn't have the label kubernetes.io/e2e-0bc4665e-5988-4bb5-a8ff-0a1a07f2f96a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:44:56.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3986" for this suite.
Feb 11 08:45:04.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:45:04.374: INFO: namespace sched-pred-3986 deletion completed in 8.071517656s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:12.173 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:45:04.374: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-0cca6357-384a-4079-b86c-f6773c5e6b60
STEP: Creating a pod to test consume secrets
Feb 11 08:45:04.403: INFO: Waiting up to 5m0s for pod "pod-secrets-fd912ced-f549-4798-84aa-930713515fc0" in namespace "secrets-8203" to be "success or failure"
Feb 11 08:45:04.405: INFO: Pod "pod-secrets-fd912ced-f549-4798-84aa-930713515fc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.817873ms
Feb 11 08:45:06.409: INFO: Pod "pod-secrets-fd912ced-f549-4798-84aa-930713515fc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005373409s
STEP: Saw pod success
Feb 11 08:45:06.409: INFO: Pod "pod-secrets-fd912ced-f549-4798-84aa-930713515fc0" satisfied condition "success or failure"
Feb 11 08:45:06.411: INFO: Trying to get logs from node cn-hongkong.192.168.0.19 pod pod-secrets-fd912ced-f549-4798-84aa-930713515fc0 container secret-volume-test: <nil>
STEP: delete the pod
Feb 11 08:45:06.426: INFO: Waiting for pod pod-secrets-fd912ced-f549-4798-84aa-930713515fc0 to disappear
Feb 11 08:45:06.428: INFO: Pod pod-secrets-fd912ced-f549-4798-84aa-930713515fc0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:45:06.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8203" for this suite.
Feb 11 08:45:12.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:45:12.509: INFO: namespace secrets-8203 deletion completed in 6.078688898s

â€¢ [SLOW TEST:8.135 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:45:12.510: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Feb 11 08:45:12.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 create -f - --namespace=kubectl-3409'
Feb 11 08:45:12.692: INFO: stderr: ""
Feb 11 08:45:12.692: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 11 08:45:13.696: INFO: Selector matched 1 pods for map[app:redis]
Feb 11 08:45:13.696: INFO: Found 0 / 1
Feb 11 08:45:14.695: INFO: Selector matched 1 pods for map[app:redis]
Feb 11 08:45:14.696: INFO: Found 0 / 1
Feb 11 08:45:15.695: INFO: Selector matched 1 pods for map[app:redis]
Feb 11 08:45:15.695: INFO: Found 0 / 1
Feb 11 08:45:16.695: INFO: Selector matched 1 pods for map[app:redis]
Feb 11 08:45:16.695: INFO: Found 0 / 1
Feb 11 08:45:17.696: INFO: Selector matched 1 pods for map[app:redis]
Feb 11 08:45:17.696: INFO: Found 0 / 1
Feb 11 08:45:18.695: INFO: Selector matched 1 pods for map[app:redis]
Feb 11 08:45:18.695: INFO: Found 0 / 1
Feb 11 08:45:19.695: INFO: Selector matched 1 pods for map[app:redis]
Feb 11 08:45:19.696: INFO: Found 0 / 1
Feb 11 08:45:20.695: INFO: Selector matched 1 pods for map[app:redis]
Feb 11 08:45:20.696: INFO: Found 1 / 1
Feb 11 08:45:20.696: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 11 08:45:20.698: INFO: Selector matched 1 pods for map[app:redis]
Feb 11 08:45:20.698: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 11 08:45:20.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 patch pod redis-master-x96c6 --namespace=kubectl-3409 -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 11 08:45:20.769: INFO: stderr: ""
Feb 11 08:45:20.769: INFO: stdout: "pod/redis-master-x96c6 patched\n"
STEP: checking annotations
Feb 11 08:45:20.771: INFO: Selector matched 1 pods for map[app:redis]
Feb 11 08:45:20.771: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:45:20.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3409" for this suite.
Feb 11 08:45:48.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:45:48.847: INFO: namespace kubectl-3409 deletion completed in 28.072176777s

â€¢ [SLOW TEST:36.337 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:45:48.847: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7952
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Feb 11 08:45:48.879: INFO: Found 0 stateful pods, waiting for 3
Feb 11 08:45:58.882: INFO: Found 1 stateful pods, waiting for 3
Feb 11 08:46:08.882: INFO: Found 2 stateful pods, waiting for 3
Feb 11 08:46:18.883: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 11 08:46:18.883: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 11 08:46:18.883: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb 11 08:46:28.882: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 11 08:46:28.882: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 11 08:46:28.882: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 11 08:46:28.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=statefulset-7952 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 11 08:46:29.096: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 11 08:46:29.096: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 11 08:46:29.096: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Feb 11 08:46:39.125: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 11 08:46:49.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=statefulset-7952 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 11 08:46:49.347: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 11 08:46:49.348: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 11 08:46:49.348: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 11 08:46:59.367: INFO: Waiting for StatefulSet statefulset-7952/ss2 to complete update
Feb 11 08:46:59.367: INFO: Waiting for Pod statefulset-7952/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 11 08:46:59.367: INFO: Waiting for Pod statefulset-7952/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 11 08:47:09.373: INFO: Waiting for StatefulSet statefulset-7952/ss2 to complete update
Feb 11 08:47:09.373: INFO: Waiting for Pod statefulset-7952/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 11 08:47:09.373: INFO: Waiting for Pod statefulset-7952/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 11 08:47:19.373: INFO: Waiting for StatefulSet statefulset-7952/ss2 to complete update
Feb 11 08:47:19.373: INFO: Waiting for Pod statefulset-7952/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 11 08:47:19.373: INFO: Waiting for Pod statefulset-7952/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 11 08:47:29.373: INFO: Waiting for StatefulSet statefulset-7952/ss2 to complete update
Feb 11 08:47:29.373: INFO: Waiting for Pod statefulset-7952/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 11 08:47:39.373: INFO: Waiting for StatefulSet statefulset-7952/ss2 to complete update
Feb 11 08:47:49.373: INFO: Waiting for StatefulSet statefulset-7952/ss2 to complete update
STEP: Rolling back to a previous revision
Feb 11 08:47:59.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=statefulset-7952 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 11 08:47:59.587: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 11 08:47:59.587: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 11 08:47:59.587: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 11 08:48:09.615: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 11 08:48:19.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=statefulset-7952 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 11 08:48:19.844: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 11 08:48:19.844: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 11 08:48:19.844: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 11 08:48:29.861: INFO: Waiting for StatefulSet statefulset-7952/ss2 to complete update
Feb 11 08:48:29.861: INFO: Waiting for Pod statefulset-7952/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 11 08:48:39.867: INFO: Waiting for StatefulSet statefulset-7952/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Feb 11 08:48:49.866: INFO: Deleting all statefulset in ns statefulset-7952
Feb 11 08:48:49.869: INFO: Scaling statefulset ss2 to 0
Feb 11 08:48:59.882: INFO: Waiting for statefulset status.replicas updated to 0
Feb 11 08:48:59.885: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:48:59.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7952" for this suite.
Feb 11 08:49:05.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:49:05.975: INFO: namespace statefulset-7952 deletion completed in 6.076230949s

â€¢ [SLOW TEST:197.128 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:49:05.976: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 08:49:05.998: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:49:14.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9547" for this suite.
Feb 11 08:50:04.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:50:04.222: INFO: namespace pods-9547 deletion completed in 50.08044059s

â€¢ [SLOW TEST:58.247 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:50:04.222: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 08:50:04.248: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 11 08:50:04.256: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 11 08:50:09.260: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 11 08:50:09.260: INFO: Creating deployment "test-rolling-update-deployment"
Feb 11 08:50:09.264: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 11 08:50:09.268: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 11 08:50:11.274: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 11 08:50:11.276: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717007809, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717007809, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717007809, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717007809, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 11 08:50:13.281: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717007809, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717007809, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717007809, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717007809, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 11 08:50:15.280: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717007809, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717007809, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717007809, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717007809, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 11 08:50:17.280: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717007809, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717007809, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717007809, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717007809, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 11 08:50:19.280: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Feb 11 08:50:19.288: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7604 /apis/apps/v1/namespaces/deployment-7604/deployments/test-rolling-update-deployment 644b96c6-a870-48d2-8df0-e1eebf069a30 13405 1 2020-02-11 08:50:09 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002271608 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-02-11 08:50:09 +0000 UTC,LastTransitionTime:2020-02-11 08:50:09 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-02-11 08:50:17 +0000 UTC,LastTransitionTime:2020-02-11 08:50:09 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 11 08:50:19.291: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-7604 /apis/apps/v1/namespaces/deployment-7604/replicasets/test-rolling-update-deployment-55d946486 ac1136fb-70c8-42d2-8582-90dc48ec77bf 13394 1 2020-02-11 08:50:09 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 644b96c6-a870-48d2-8df0-e1eebf069a30 0xc002271af0 0xc002271af1}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002271b58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 11 08:50:19.291: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 11 08:50:19.291: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7604 /apis/apps/v1/namespaces/deployment-7604/replicasets/test-rolling-update-controller 748a6be5-d3c2-4c77-8e38-74836dc99494 13403 2 2020-02-11 08:50:04 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 644b96c6-a870-48d2-8df0-e1eebf069a30 0xc002271a27 0xc002271a28}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002271a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 11 08:50:19.293: INFO: Pod "test-rolling-update-deployment-55d946486-4mwcb" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-4mwcb test-rolling-update-deployment-55d946486- deployment-7604 /api/v1/namespaces/deployment-7604/pods/test-rolling-update-deployment-55d946486-4mwcb 9aa5b19f-fe53-4d0f-8cfd-bec56ff45bd1 13393 0 2020-02-11 08:50:09 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 ac1136fb-70c8-42d2-8582-90dc48ec77bf 0xc002271fd0 0xc002271fd1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tq4ss,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tq4ss,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tq4ss,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 08:50:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 08:50:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 08:50:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 08:50:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.18,PodIP:172.22.2.36,StartTime:2020-02-11 08:50:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-11 08:50:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://a9bc2e65c671d32cbd19e941cadda489fc256ffe2aebfd07944661cfba3b94fd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.22.2.36,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:50:19.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7604" for this suite.
Feb 11 08:50:25.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:50:25.369: INFO: namespace deployment-7604 deletion completed in 6.072651002s

â€¢ [SLOW TEST:21.146 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:50:25.369: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 08:50:25.405: INFO: (0) /api/v1/nodes/cn-hongkong.192.168.0.18/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 12.480001ms)
Feb 11 08:50:25.409: INFO: (1) /api/v1/nodes/cn-hongkong.192.168.0.18/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 3.296581ms)
Feb 11 08:50:25.412: INFO: (2) /api/v1/nodes/cn-hongkong.192.168.0.18/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 3.150528ms)
Feb 11 08:50:25.414: INFO: (3) /api/v1/nodes/cn-hongkong.192.168.0.18/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.638543ms)
Feb 11 08:50:25.417: INFO: (4) /api/v1/nodes/cn-hongkong.192.168.0.18/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.865738ms)
Feb 11 08:50:25.420: INFO: (5) /api/v1/nodes/cn-hongkong.192.168.0.18/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.872836ms)
Feb 11 08:50:25.423: INFO: (6) /api/v1/nodes/cn-hongkong.192.168.0.18/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.746516ms)
Feb 11 08:50:25.426: INFO: (7) /api/v1/nodes/cn-hongkong.192.168.0.18/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 3.100461ms)
Feb 11 08:50:25.429: INFO: (8) /api/v1/nodes/cn-hongkong.192.168.0.18/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.709649ms)
Feb 11 08:50:25.431: INFO: (9) /api/v1/nodes/cn-hongkong.192.168.0.18/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.679775ms)
Feb 11 08:50:25.434: INFO: (10) /api/v1/nodes/cn-hongkong.192.168.0.18/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.81896ms)
Feb 11 08:50:25.437: INFO: (11) /api/v1/nodes/cn-hongkong.192.168.0.18/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.847635ms)
Feb 11 08:50:25.440: INFO: (12) /api/v1/nodes/cn-hongkong.192.168.0.18/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.762105ms)
Feb 11 08:50:25.446: INFO: (13) /api/v1/nodes/cn-hongkong.192.168.0.18/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 5.522956ms)
Feb 11 08:50:25.448: INFO: (14) /api/v1/nodes/cn-hongkong.192.168.0.18/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.781919ms)
Feb 11 08:50:25.451: INFO: (15) /api/v1/nodes/cn-hongkong.192.168.0.18/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.685029ms)
Feb 11 08:50:25.455: INFO: (16) /api/v1/nodes/cn-hongkong.192.168.0.18/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 3.503926ms)
Feb 11 08:50:25.458: INFO: (17) /api/v1/nodes/cn-hongkong.192.168.0.18/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.95166ms)
Feb 11 08:50:25.460: INFO: (18) /api/v1/nodes/cn-hongkong.192.168.0.18/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.700644ms)
Feb 11 08:50:25.463: INFO: (19) /api/v1/nodes/cn-hongkong.192.168.0.18/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.863135ms)
[AfterEach] version v1
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:50:25.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6969" for this suite.
Feb 11 08:50:31.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:50:31.537: INFO: namespace proxy-6969 deletion completed in 6.070764463s

â€¢ [SLOW TEST:6.168 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:50:31.537: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Feb 11 08:50:31.564: INFO: Waiting up to 5m0s for pod "downward-api-a42d0355-edd7-4370-a237-3baa8ce7a584" in namespace "downward-api-8430" to be "success or failure"
Feb 11 08:50:31.566: INFO: Pod "downward-api-a42d0355-edd7-4370-a237-3baa8ce7a584": Phase="Pending", Reason="", readiness=false. Elapsed: 1.932461ms
Feb 11 08:50:33.569: INFO: Pod "downward-api-a42d0355-edd7-4370-a237-3baa8ce7a584": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005518158s
STEP: Saw pod success
Feb 11 08:50:33.569: INFO: Pod "downward-api-a42d0355-edd7-4370-a237-3baa8ce7a584" satisfied condition "success or failure"
Feb 11 08:50:33.572: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod downward-api-a42d0355-edd7-4370-a237-3baa8ce7a584 container dapi-container: <nil>
STEP: delete the pod
Feb 11 08:50:33.591: INFO: Waiting for pod downward-api-a42d0355-edd7-4370-a237-3baa8ce7a584 to disappear
Feb 11 08:50:33.593: INFO: Pod downward-api-a42d0355-edd7-4370-a237-3baa8ce7a584 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:50:33.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8430" for this suite.
Feb 11 08:50:39.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:50:39.667: INFO: namespace downward-api-8430 deletion completed in 6.071920643s

â€¢ [SLOW TEST:8.131 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:50:39.668: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:50:41.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-231" for this suite.
Feb 11 08:51:31.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:51:31.804: INFO: namespace kubelet-test-231 deletion completed in 50.0742022s

â€¢ [SLOW TEST:52.136 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:51:31.804: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3675
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3675
STEP: creating replication controller externalsvc in namespace services-3675
I0211 08:51:31.856378      23 runners.go:184] Created replication controller with name: externalsvc, namespace: services-3675, replica count: 2
I0211 08:51:34.906752      23 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Feb 11 08:51:34.920: INFO: Creating new exec pod
Feb 11 08:51:36.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=services-3675 execpodxs7kx -- /bin/sh -x -c nslookup clusterip-service'
Feb 11 08:51:37.340: INFO: stderr: "+ nslookup clusterip-service\n"
Feb 11 08:51:37.340: INFO: stdout: "Server:\t\t172.23.0.10\nAddress:\t172.23.0.10#53\n\nclusterip-service.services-3675.svc.cluster.local\tcanonical name = externalsvc.services-3675.svc.cluster.local.\nName:\texternalsvc.services-3675.svc.cluster.local\nAddress: 172.23.2.44\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3675, will wait for the garbage collector to delete the pods
Feb 11 08:51:37.403: INFO: Deleting ReplicationController externalsvc took: 9.115746ms
Feb 11 08:51:37.803: INFO: Terminating ReplicationController externalsvc pods took: 400.187081ms
Feb 11 08:51:51.818: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:51:51.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3675" for this suite.
Feb 11 08:51:57.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:51:57.903: INFO: namespace services-3675 deletion completed in 6.071797382s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:26.100 seconds]
[sig-network] Services
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:51:57.904: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Feb 11 08:52:07.975: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0211 08:52:07.975249      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:52:07.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2121" for this suite.
Feb 11 08:52:13.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:52:14.053: INFO: namespace gc-2121 deletion completed in 6.075791084s

â€¢ [SLOW TEST:16.150 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:52:14.054: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3538
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-3538
I0211 08:52:14.094525      23 runners.go:184] Created replication controller with name: externalname-service, namespace: services-3538, replica count: 2
I0211 08:52:17.144845      23 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 11 08:52:17.144: INFO: Creating new exec pod
Feb 11 08:52:20.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=services-3538 execpods8p8q -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Feb 11 08:52:20.364: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 11 08:52:20.364: INFO: stdout: ""
Feb 11 08:52:20.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=services-3538 execpods8p8q -- /bin/sh -x -c nc -zv -t -w 2 172.23.12.228 80'
Feb 11 08:52:20.561: INFO: stderr: "+ nc -zv -t -w 2 172.23.12.228 80\nConnection to 172.23.12.228 80 port [tcp/http] succeeded!\n"
Feb 11 08:52:20.561: INFO: stdout: ""
Feb 11 08:52:20.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=services-3538 execpods8p8q -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.18 31577'
Feb 11 08:52:20.761: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.18 31577\nConnection to 192.168.0.18 31577 port [tcp/31577] succeeded!\n"
Feb 11 08:52:20.761: INFO: stdout: ""
Feb 11 08:52:20.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=services-3538 execpods8p8q -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.19 31577'
Feb 11 08:52:20.973: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.19 31577\nConnection to 192.168.0.19 31577 port [tcp/31577] succeeded!\n"
Feb 11 08:52:20.973: INFO: stdout: ""
Feb 11 08:52:20.973: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:52:20.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3538" for this suite.
Feb 11 08:52:27.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:52:27.082: INFO: namespace services-3538 deletion completed in 6.080204341s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:13.028 seconds]
[sig-network] Services
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:52:27.082: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Feb 11 08:52:29.624: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7386 pod-service-account-9b36a5ba-b69e-457a-be8c-4ed84068cb9c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Feb 11 08:52:29.827: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7386 pod-service-account-9b36a5ba-b69e-457a-be8c-4ed84068cb9c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Feb 11 08:52:30.035: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7386 pod-service-account-9b36a5ba-b69e-457a-be8c-4ed84068cb9c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:52:30.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7386" for this suite.
Feb 11 08:52:36.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:52:36.331: INFO: namespace svcaccounts-7386 deletion completed in 6.074009212s

â€¢ [SLOW TEST:9.249 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:52:36.331: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Feb 11 08:52:38.377: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-208978535 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 11 08:52:53.448: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:52:53.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6117" for this suite.
Feb 11 08:52:59.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:52:59.528: INFO: namespace pods-6117 deletion completed in 6.074659123s

â€¢ [SLOW TEST:23.197 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:52:59.529: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:53:24.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-385" for this suite.
Feb 11 08:53:30.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:53:30.793: INFO: namespace container-runtime-385 deletion completed in 6.072290893s

â€¢ [SLOW TEST:31.264 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:53:30.793: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 11 08:53:31.144: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 11 08:53:34.161: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 08:53:34.164: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:53:39.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2921" for this suite.
Feb 11 08:53:45.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:53:45.910: INFO: namespace crd-webhook-2921 deletion completed in 6.071975918s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:15.127 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:53:45.921: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Feb 11 08:53:45.943: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:54:04.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6618" for this suite.
Feb 11 08:54:10.127: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:54:10.192: INFO: namespace crd-publish-openapi-6618 deletion completed in 6.074759763s

â€¢ [SLOW TEST:24.271 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:54:10.193: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-3c8270e1-eaf5-4055-9487-3464ec6dde6e
Feb 11 08:54:10.218: INFO: Pod name my-hostname-basic-3c8270e1-eaf5-4055-9487-3464ec6dde6e: Found 0 pods out of 1
Feb 11 08:54:15.222: INFO: Pod name my-hostname-basic-3c8270e1-eaf5-4055-9487-3464ec6dde6e: Found 1 pods out of 1
Feb 11 08:54:15.222: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-3c8270e1-eaf5-4055-9487-3464ec6dde6e" are running
Feb 11 08:54:15.225: INFO: Pod "my-hostname-basic-3c8270e1-eaf5-4055-9487-3464ec6dde6e-rm7w7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-11 08:54:10 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-11 08:54:11 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-11 08:54:11 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-11 08:54:10 +0000 UTC Reason: Message:}])
Feb 11 08:54:15.225: INFO: Trying to dial the pod
Feb 11 08:54:20.234: INFO: Controller my-hostname-basic-3c8270e1-eaf5-4055-9487-3464ec6dde6e: Got expected result from replica 1 [my-hostname-basic-3c8270e1-eaf5-4055-9487-3464ec6dde6e-rm7w7]: "my-hostname-basic-3c8270e1-eaf5-4055-9487-3464ec6dde6e-rm7w7", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:54:20.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9632" for this suite.
Feb 11 08:54:26.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:54:26.308: INFO: namespace replication-controller-9632 deletion completed in 6.071345762s

â€¢ [SLOW TEST:16.115 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:54:26.308: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-d4f13896-b0af-4163-8de1-57328ce5bd6b
STEP: Creating a pod to test consume configMaps
Feb 11 08:54:26.337: INFO: Waiting up to 5m0s for pod "pod-configmaps-1115e162-8315-451a-bca3-cff319d38ff1" in namespace "configmap-261" to be "success or failure"
Feb 11 08:54:26.339: INFO: Pod "pod-configmaps-1115e162-8315-451a-bca3-cff319d38ff1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.788905ms
Feb 11 08:54:28.342: INFO: Pod "pod-configmaps-1115e162-8315-451a-bca3-cff319d38ff1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004946809s
STEP: Saw pod success
Feb 11 08:54:28.342: INFO: Pod "pod-configmaps-1115e162-8315-451a-bca3-cff319d38ff1" satisfied condition "success or failure"
Feb 11 08:54:28.344: INFO: Trying to get logs from node cn-hongkong.192.168.0.19 pod pod-configmaps-1115e162-8315-451a-bca3-cff319d38ff1 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 11 08:54:28.368: INFO: Waiting for pod pod-configmaps-1115e162-8315-451a-bca3-cff319d38ff1 to disappear
Feb 11 08:54:28.370: INFO: Pod pod-configmaps-1115e162-8315-451a-bca3-cff319d38ff1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:54:28.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-261" for this suite.
Feb 11 08:54:34.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:54:34.445: INFO: namespace configmap-261 deletion completed in 6.072203206s

â€¢ [SLOW TEST:8.138 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:54:34.446: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 11 08:54:34.486: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2017 /api/v1/namespaces/watch-2017/configmaps/e2e-watch-test-resource-version 3d23f4cd-0797-42b4-b8d4-e967a4b2afab 14830 0 2020-02-11 08:54:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 11 08:54:34.486: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2017 /api/v1/namespaces/watch-2017/configmaps/e2e-watch-test-resource-version 3d23f4cd-0797-42b4-b8d4-e967a4b2afab 14831 0 2020-02-11 08:54:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:54:34.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2017" for this suite.
Feb 11 08:54:40.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:54:40.562: INFO: namespace watch-2017 deletion completed in 6.07364541s

â€¢ [SLOW TEST:6.116 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:54:40.562: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Feb 11 08:54:40.584: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-208978535 proxy --unix-socket=/tmp/kubectl-proxy-unix801849218/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:54:40.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4634" for this suite.
Feb 11 08:54:46.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:54:46.715: INFO: namespace kubectl-4634 deletion completed in 6.073887674s

â€¢ [SLOW TEST:6.153 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:54:46.715: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-1196f0c1-97f5-4e4c-9169-8ef2250203d6
STEP: Creating a pod to test consume secrets
Feb 11 08:54:46.744: INFO: Waiting up to 5m0s for pod "pod-secrets-7b1833a7-ca5f-4580-ac31-a9e35a694f81" in namespace "secrets-8245" to be "success or failure"
Feb 11 08:54:46.747: INFO: Pod "pod-secrets-7b1833a7-ca5f-4580-ac31-a9e35a694f81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.866091ms
Feb 11 08:54:48.751: INFO: Pod "pod-secrets-7b1833a7-ca5f-4580-ac31-a9e35a694f81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006240634s
STEP: Saw pod success
Feb 11 08:54:48.751: INFO: Pod "pod-secrets-7b1833a7-ca5f-4580-ac31-a9e35a694f81" satisfied condition "success or failure"
Feb 11 08:54:48.753: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-secrets-7b1833a7-ca5f-4580-ac31-a9e35a694f81 container secret-volume-test: <nil>
STEP: delete the pod
Feb 11 08:54:48.775: INFO: Waiting for pod pod-secrets-7b1833a7-ca5f-4580-ac31-a9e35a694f81 to disappear
Feb 11 08:54:48.777: INFO: Pod pod-secrets-7b1833a7-ca5f-4580-ac31-a9e35a694f81 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:54:48.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8245" for this suite.
Feb 11 08:54:54.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:54:54.852: INFO: namespace secrets-8245 deletion completed in 6.071664929s

â€¢ [SLOW TEST:8.137 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:54:54.852: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 11 08:54:55.321: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 11 08:54:58.336: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:54:58.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2062" for this suite.
Feb 11 08:55:04.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:55:04.447: INFO: namespace webhook-2062 deletion completed in 6.081605027s
STEP: Destroying namespace "webhook-2062-markers" for this suite.
Feb 11 08:55:10.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:55:10.521: INFO: namespace webhook-2062-markers deletion completed in 6.073777974s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:15.680 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:55:10.532: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 11 08:55:10.559: INFO: Waiting up to 5m0s for pod "pod-77776913-8c1c-46d0-9836-a28442895bf4" in namespace "emptydir-4396" to be "success or failure"
Feb 11 08:55:10.561: INFO: Pod "pod-77776913-8c1c-46d0-9836-a28442895bf4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.153687ms
Feb 11 08:55:12.566: INFO: Pod "pod-77776913-8c1c-46d0-9836-a28442895bf4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006668454s
STEP: Saw pod success
Feb 11 08:55:12.566: INFO: Pod "pod-77776913-8c1c-46d0-9836-a28442895bf4" satisfied condition "success or failure"
Feb 11 08:55:12.568: INFO: Trying to get logs from node cn-hongkong.192.168.0.19 pod pod-77776913-8c1c-46d0-9836-a28442895bf4 container test-container: <nil>
STEP: delete the pod
Feb 11 08:55:12.582: INFO: Waiting for pod pod-77776913-8c1c-46d0-9836-a28442895bf4 to disappear
Feb 11 08:55:12.584: INFO: Pod pod-77776913-8c1c-46d0-9836-a28442895bf4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:55:12.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4396" for this suite.
Feb 11 08:55:18.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:55:18.659: INFO: namespace emptydir-4396 deletion completed in 6.072449985s

â€¢ [SLOW TEST:8.127 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:55:18.660: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Feb 11 08:55:18.681: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 11 08:55:18.689: INFO: Waiting for terminating namespaces to be deleted...
Feb 11 08:55:18.691: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.18 before test
Feb 11 08:55:18.696: INFO: kube-flannel-ds-xjcpn from kube-system started at 2020-02-11 08:01:10 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:18.696: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 11 08:55:18.696: INFO: alicloud-application-controller-66cd594458-dzs7s from kube-system started at 2020-02-11 08:01:23 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:18.696: INFO: 	Container alicloud-application-controller ready: true, restart count 0
Feb 11 08:55:18.696: INFO: nginx-ingress-controller-5f645986c-gw8nx from kube-system started at 2020-02-11 08:01:27 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:18.696: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 11 08:55:18.696: INFO: sonobuoy-systemd-logs-daemon-set-cf27379a033c4f18-ts8rk from sonobuoy started at 2020-02-11 08:31:10 +0000 UTC (2 container statuses recorded)
Feb 11 08:55:18.696: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 11 08:55:18.696: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 11 08:55:18.696: INFO: kube-proxy-worker-c92sw from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:18.696: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Feb 11 08:55:18.696: INFO: coredns-7bbcddf6b7-bjckz from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:18.696: INFO: 	Container coredns ready: true, restart count 0
Feb 11 08:55:18.696: INFO: flexvolume-mv7bv from kube-system started at 2020-02-11 08:01:04 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:18.696: INFO: 	Container acs-flexvolume ready: true, restart count 0
Feb 11 08:55:18.696: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.19 before test
Feb 11 08:55:18.701: INFO: kube-proxy-worker-gv69f from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:18.701: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Feb 11 08:55:18.701: INFO: kube-flannel-ds-l5588 from kube-system started at 2020-02-11 08:01:11 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:18.701: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 11 08:55:18.701: INFO: nginx-ingress-controller-5f645986c-lbm58 from kube-system started at 2020-02-11 08:01:27 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:18.701: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 11 08:55:18.701: INFO: sonobuoy from sonobuoy started at 2020-02-11 08:31:09 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:18.701: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 11 08:55:18.701: INFO: sonobuoy-systemd-logs-daemon-set-cf27379a033c4f18-w4gvp from sonobuoy started at 2020-02-11 08:31:10 +0000 UTC (2 container statuses recorded)
Feb 11 08:55:18.701: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 11 08:55:18.701: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 11 08:55:18.701: INFO: flexvolume-lfp7h from kube-system started at 2020-02-11 08:01:04 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:18.701: INFO: 	Container acs-flexvolume ready: true, restart count 0
Feb 11 08:55:18.701: INFO: metrics-server-74d79db469-q9c6v from kube-system started at 2020-02-11 08:01:26 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:18.701: INFO: 	Container metrics-server ready: true, restart count 0
Feb 11 08:55:18.701: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.20 before test
Feb 11 08:55:18.711: INFO: kube-proxy-worker-6tcbf from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:18.711: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Feb 11 08:55:18.711: INFO: flexvolume-rg5mr from kube-system started at 2020-02-11 08:01:04 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:18.711: INFO: 	Container acs-flexvolume ready: true, restart count 0
Feb 11 08:55:18.711: INFO: coredns-7bbcddf6b7-r9zv8 from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:18.711: INFO: 	Container coredns ready: true, restart count 0
Feb 11 08:55:18.711: INFO: kube-flannel-ds-xskqs from kube-system started at 2020-02-11 08:01:10 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:18.711: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 11 08:55:18.711: INFO: sonobuoy-e2e-job-a3471aaa24714387 from sonobuoy started at 2020-02-11 08:31:10 +0000 UTC (2 container statuses recorded)
Feb 11 08:55:18.711: INFO: 	Container e2e ready: true, restart count 0
Feb 11 08:55:18.711: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 11 08:55:18.711: INFO: sonobuoy-systemd-logs-daemon-set-cf27379a033c4f18-rfthz from sonobuoy started at 2020-02-11 08:31:10 +0000 UTC (2 container statuses recorded)
Feb 11 08:55:18.711: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 11 08:55:18.711: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15f24d3739322afa], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:55:19.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7150" for this suite.
Feb 11 08:55:25.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:55:25.807: INFO: namespace sched-pred-7150 deletion completed in 6.073639645s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:7.147 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:55:25.807: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-06725d62-0dbe-48c0-a7c5-3eacf1a9a639
STEP: Creating a pod to test consume configMaps
Feb 11 08:55:25.838: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c8a0ecf9-5710-4d45-b6c1-1756730de879" in namespace "projected-5350" to be "success or failure"
Feb 11 08:55:25.840: INFO: Pod "pod-projected-configmaps-c8a0ecf9-5710-4d45-b6c1-1756730de879": Phase="Pending", Reason="", readiness=false. Elapsed: 2.218329ms
Feb 11 08:55:27.843: INFO: Pod "pod-projected-configmaps-c8a0ecf9-5710-4d45-b6c1-1756730de879": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005558607s
STEP: Saw pod success
Feb 11 08:55:27.843: INFO: Pod "pod-projected-configmaps-c8a0ecf9-5710-4d45-b6c1-1756730de879" satisfied condition "success or failure"
Feb 11 08:55:27.846: INFO: Trying to get logs from node cn-hongkong.192.168.0.20 pod pod-projected-configmaps-c8a0ecf9-5710-4d45-b6c1-1756730de879 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 11 08:55:27.860: INFO: Waiting for pod pod-projected-configmaps-c8a0ecf9-5710-4d45-b6c1-1756730de879 to disappear
Feb 11 08:55:27.862: INFO: Pod pod-projected-configmaps-c8a0ecf9-5710-4d45-b6c1-1756730de879 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:55:27.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5350" for this suite.
Feb 11 08:55:33.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:55:33.934: INFO: namespace projected-5350 deletion completed in 6.068334191s

â€¢ [SLOW TEST:8.127 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:55:33.934: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-0c5c9f10-db99-485f-a3f1-2251916877e7
STEP: Creating a pod to test consume secrets
Feb 11 08:55:33.964: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4f5d117f-0c90-4df5-885e-d1fcfe8ec030" in namespace "projected-9369" to be "success or failure"
Feb 11 08:55:33.966: INFO: Pod "pod-projected-secrets-4f5d117f-0c90-4df5-885e-d1fcfe8ec030": Phase="Pending", Reason="", readiness=false. Elapsed: 2.257886ms
Feb 11 08:55:35.970: INFO: Pod "pod-projected-secrets-4f5d117f-0c90-4df5-885e-d1fcfe8ec030": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005654862s
STEP: Saw pod success
Feb 11 08:55:35.970: INFO: Pod "pod-projected-secrets-4f5d117f-0c90-4df5-885e-d1fcfe8ec030" satisfied condition "success or failure"
Feb 11 08:55:35.973: INFO: Trying to get logs from node cn-hongkong.192.168.0.20 pod pod-projected-secrets-4f5d117f-0c90-4df5-885e-d1fcfe8ec030 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 11 08:55:35.987: INFO: Waiting for pod pod-projected-secrets-4f5d117f-0c90-4df5-885e-d1fcfe8ec030 to disappear
Feb 11 08:55:35.989: INFO: Pod pod-projected-secrets-4f5d117f-0c90-4df5-885e-d1fcfe8ec030 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 08:55:35.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9369" for this suite.
Feb 11 08:55:42.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 08:55:42.065: INFO: namespace projected-9369 deletion completed in 6.073069473s

â€¢ [SLOW TEST:8.131 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 08:55:42.065: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Feb 11 08:55:42.088: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 11 08:55:42.097: INFO: Waiting for terminating namespaces to be deleted...
Feb 11 08:55:42.099: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.18 before test
Feb 11 08:55:42.106: INFO: nginx-ingress-controller-5f645986c-gw8nx from kube-system started at 2020-02-11 08:01:27 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:42.106: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 11 08:55:42.106: INFO: sonobuoy-systemd-logs-daemon-set-cf27379a033c4f18-ts8rk from sonobuoy started at 2020-02-11 08:31:10 +0000 UTC (2 container statuses recorded)
Feb 11 08:55:42.106: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 11 08:55:42.106: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 11 08:55:42.106: INFO: kube-proxy-worker-c92sw from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:42.106: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Feb 11 08:55:42.106: INFO: coredns-7bbcddf6b7-bjckz from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:42.106: INFO: 	Container coredns ready: true, restart count 0
Feb 11 08:55:42.106: INFO: flexvolume-mv7bv from kube-system started at 2020-02-11 08:01:04 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:42.106: INFO: 	Container acs-flexvolume ready: true, restart count 0
Feb 11 08:55:42.106: INFO: kube-flannel-ds-xjcpn from kube-system started at 2020-02-11 08:01:10 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:42.106: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 11 08:55:42.106: INFO: alicloud-application-controller-66cd594458-dzs7s from kube-system started at 2020-02-11 08:01:23 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:42.106: INFO: 	Container alicloud-application-controller ready: true, restart count 0
Feb 11 08:55:42.106: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.19 before test
Feb 11 08:55:42.111: INFO: kube-proxy-worker-gv69f from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:42.111: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Feb 11 08:55:42.111: INFO: kube-flannel-ds-l5588 from kube-system started at 2020-02-11 08:01:11 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:42.111: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 11 08:55:42.111: INFO: nginx-ingress-controller-5f645986c-lbm58 from kube-system started at 2020-02-11 08:01:27 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:42.111: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 11 08:55:42.111: INFO: sonobuoy from sonobuoy started at 2020-02-11 08:31:09 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:42.111: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 11 08:55:42.111: INFO: sonobuoy-systemd-logs-daemon-set-cf27379a033c4f18-w4gvp from sonobuoy started at 2020-02-11 08:31:10 +0000 UTC (2 container statuses recorded)
Feb 11 08:55:42.111: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 11 08:55:42.111: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 11 08:55:42.111: INFO: flexvolume-lfp7h from kube-system started at 2020-02-11 08:01:04 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:42.111: INFO: 	Container acs-flexvolume ready: true, restart count 0
Feb 11 08:55:42.111: INFO: metrics-server-74d79db469-q9c6v from kube-system started at 2020-02-11 08:01:26 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:42.111: INFO: 	Container metrics-server ready: true, restart count 0
Feb 11 08:55:42.111: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.20 before test
Feb 11 08:55:42.118: INFO: coredns-7bbcddf6b7-r9zv8 from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:42.118: INFO: 	Container coredns ready: true, restart count 0
Feb 11 08:55:42.118: INFO: kube-flannel-ds-xskqs from kube-system started at 2020-02-11 08:01:10 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:42.118: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 11 08:55:42.118: INFO: sonobuoy-e2e-job-a3471aaa24714387 from sonobuoy started at 2020-02-11 08:31:10 +0000 UTC (2 container statuses recorded)
Feb 11 08:55:42.118: INFO: 	Container e2e ready: true, restart count 0
Feb 11 08:55:42.118: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 11 08:55:42.118: INFO: sonobuoy-systemd-logs-daemon-set-cf27379a033c4f18-rfthz from sonobuoy started at 2020-02-11 08:31:10 +0000 UTC (2 container statuses recorded)
Feb 11 08:55:42.118: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 11 08:55:42.118: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 11 08:55:42.118: INFO: kube-proxy-worker-6tcbf from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:42.118: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Feb 11 08:55:42.118: INFO: flexvolume-rg5mr from kube-system started at 2020-02-11 08:01:04 +0000 UTC (1 container statuses recorded)
Feb 11 08:55:42.118: INFO: 	Container acs-flexvolume ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-3e66b781-6273-479c-abd8-a87812b584d3 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-3e66b781-6273-479c-abd8-a87812b584d3 off the node cn-hongkong.192.168.0.18
STEP: verifying the node doesn't have the label kubernetes.io/e2e-3e66b781-6273-479c-abd8-a87812b584d3
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:00:46.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8171" for this suite.
Feb 11 09:00:54.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:00:54.255: INFO: namespace sched-pred-8171 deletion completed in 8.072933258s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:312.190 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:00:54.255: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:00:57.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4155" for this suite.
Feb 11 09:01:09.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:01:09.381: INFO: namespace replication-controller-4155 deletion completed in 12.076837623s

â€¢ [SLOW TEST:15.126 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:01:09.382: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Feb 11 09:01:09.404: INFO: namespace kubectl-9879
Feb 11 09:01:09.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 create -f - --namespace=kubectl-9879'
Feb 11 09:01:09.618: INFO: stderr: ""
Feb 11 09:01:09.618: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 11 09:01:10.622: INFO: Selector matched 1 pods for map[app:redis]
Feb 11 09:01:10.622: INFO: Found 0 / 1
Feb 11 09:01:11.622: INFO: Selector matched 1 pods for map[app:redis]
Feb 11 09:01:11.622: INFO: Found 1 / 1
Feb 11 09:01:11.622: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 11 09:01:11.625: INFO: Selector matched 1 pods for map[app:redis]
Feb 11 09:01:11.625: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 11 09:01:11.625: INFO: wait on redis-master startup in kubectl-9879 
Feb 11 09:01:11.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 logs redis-master-7shw5 redis-master --namespace=kubectl-9879'
Feb 11 09:01:11.712: INFO: stderr: ""
Feb 11 09:01:11.712: INFO: stdout: "1:C 11 Feb 2020 09:01:10.482 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 11 Feb 2020 09:01:10.482 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 11 Feb 2020 09:01:10.482 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 11 Feb 2020 09:01:10.484 * Running mode=standalone, port=6379.\n1:M 11 Feb 2020 09:01:10.484 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 11 Feb 2020 09:01:10.484 # Server initialized\n1:M 11 Feb 2020 09:01:10.484 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 11 Feb 2020 09:01:10.484 * Ready to accept connections\n"
STEP: exposing RC
Feb 11 09:01:11.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-9879'
Feb 11 09:01:11.795: INFO: stderr: ""
Feb 11 09:01:11.796: INFO: stdout: "service/rm2 exposed\n"
Feb 11 09:01:11.799: INFO: Service rm2 in namespace kubectl-9879 found.
STEP: exposing service
Feb 11 09:01:13.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-9879'
Feb 11 09:01:13.886: INFO: stderr: ""
Feb 11 09:01:13.886: INFO: stdout: "service/rm3 exposed\n"
Feb 11 09:01:13.888: INFO: Service rm3 in namespace kubectl-9879 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:01:15.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9879" for this suite.
Feb 11 09:01:27.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:01:27.973: INFO: namespace kubectl-9879 deletion completed in 12.075611411s

â€¢ [SLOW TEST:18.592 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:01:27.974: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Feb 11 09:01:28.001: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-6025" to be "success or failure"
Feb 11 09:01:28.003: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.276101ms
Feb 11 09:01:30.008: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006626224s
STEP: Saw pod success
Feb 11 09:01:30.008: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb 11 09:01:30.010: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb 11 09:01:30.037: INFO: Waiting for pod pod-host-path-test to disappear
Feb 11 09:01:30.039: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:01:30.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-6025" for this suite.
Feb 11 09:01:36.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:01:36.117: INFO: namespace hostpath-6025 deletion completed in 6.074691009s

â€¢ [SLOW TEST:8.144 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:01:36.117: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-0e84d6a3-a579-4393-b73e-954451714a5b
STEP: Creating a pod to test consume secrets
Feb 11 09:01:36.148: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-101aad21-8324-4831-bed3-5ed8008dcb9e" in namespace "projected-8169" to be "success or failure"
Feb 11 09:01:36.150: INFO: Pod "pod-projected-secrets-101aad21-8324-4831-bed3-5ed8008dcb9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.279095ms
Feb 11 09:01:38.153: INFO: Pod "pod-projected-secrets-101aad21-8324-4831-bed3-5ed8008dcb9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005340392s
STEP: Saw pod success
Feb 11 09:01:38.153: INFO: Pod "pod-projected-secrets-101aad21-8324-4831-bed3-5ed8008dcb9e" satisfied condition "success or failure"
Feb 11 09:01:38.156: INFO: Trying to get logs from node cn-hongkong.192.168.0.19 pod pod-projected-secrets-101aad21-8324-4831-bed3-5ed8008dcb9e container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 11 09:01:38.181: INFO: Waiting for pod pod-projected-secrets-101aad21-8324-4831-bed3-5ed8008dcb9e to disappear
Feb 11 09:01:38.183: INFO: Pod pod-projected-secrets-101aad21-8324-4831-bed3-5ed8008dcb9e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:01:38.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8169" for this suite.
Feb 11 09:01:44.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:01:44.259: INFO: namespace projected-8169 deletion completed in 6.072152521s

â€¢ [SLOW TEST:8.141 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:01:44.259: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:01:46.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7599" for this suite.
Feb 11 09:02:30.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:02:30.379: INFO: namespace kubelet-test-7599 deletion completed in 44.074513031s

â€¢ [SLOW TEST:46.120 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:02:30.379: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-2deaa1f1-dc47-4edb-8a76-3979febaae32
STEP: Creating a pod to test consume secrets
Feb 11 09:02:30.409: INFO: Waiting up to 5m0s for pod "pod-secrets-dd305ccf-91b1-4451-b0f5-2af79d9297da" in namespace "secrets-7373" to be "success or failure"
Feb 11 09:02:30.412: INFO: Pod "pod-secrets-dd305ccf-91b1-4451-b0f5-2af79d9297da": Phase="Pending", Reason="", readiness=false. Elapsed: 3.271568ms
Feb 11 09:02:32.415: INFO: Pod "pod-secrets-dd305ccf-91b1-4451-b0f5-2af79d9297da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006472988s
STEP: Saw pod success
Feb 11 09:02:32.415: INFO: Pod "pod-secrets-dd305ccf-91b1-4451-b0f5-2af79d9297da" satisfied condition "success or failure"
Feb 11 09:02:32.418: INFO: Trying to get logs from node cn-hongkong.192.168.0.20 pod pod-secrets-dd305ccf-91b1-4451-b0f5-2af79d9297da container secret-volume-test: <nil>
STEP: delete the pod
Feb 11 09:02:32.438: INFO: Waiting for pod pod-secrets-dd305ccf-91b1-4451-b0f5-2af79d9297da to disappear
Feb 11 09:02:32.441: INFO: Pod pod-secrets-dd305ccf-91b1-4451-b0f5-2af79d9297da no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:02:32.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7373" for this suite.
Feb 11 09:02:38.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:02:38.521: INFO: namespace secrets-7373 deletion completed in 6.076960216s

â€¢ [SLOW TEST:8.142 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:02:38.522: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 09:02:38.543: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Feb 11 09:02:40.566: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:02:41.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-107" for this suite.
Feb 11 09:02:47.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:02:47.647: INFO: namespace replication-controller-107 deletion completed in 6.070414953s

â€¢ [SLOW TEST:9.125 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:02:47.647: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Feb 11 09:02:47.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-7501 -- logs-generator --log-lines-total 100 --run-duration 20s'
Feb 11 09:02:47.932: INFO: stderr: ""
Feb 11 09:02:47.932: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Feb 11 09:02:47.932: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Feb 11 09:02:47.932: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7501" to be "running and ready, or succeeded"
Feb 11 09:02:47.934: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.738206ms
Feb 11 09:02:49.938: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.005923774s
Feb 11 09:02:49.938: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Feb 11 09:02:49.938: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Feb 11 09:02:49.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 logs logs-generator logs-generator --namespace=kubectl-7501'
Feb 11 09:02:50.026: INFO: stderr: ""
Feb 11 09:02:50.026: INFO: stdout: "I0211 09:02:48.767550       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/zqcv 352\nI0211 09:02:48.967683       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/gj8 579\nI0211 09:02:49.167676       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/mfs5 355\nI0211 09:02:49.367698       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/q8w9 270\nI0211 09:02:49.567686       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/qj9h 349\nI0211 09:02:49.767673       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/k8m 355\nI0211 09:02:49.967909       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/jwpv 430\n"
STEP: limiting log lines
Feb 11 09:02:50.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 logs logs-generator logs-generator --namespace=kubectl-7501 --tail=1'
Feb 11 09:02:50.105: INFO: stderr: ""
Feb 11 09:02:50.105: INFO: stdout: "I0211 09:02:49.967909       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/jwpv 430\n"
STEP: limiting log bytes
Feb 11 09:02:50.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 logs logs-generator logs-generator --namespace=kubectl-7501 --limit-bytes=1'
Feb 11 09:02:50.190: INFO: stderr: ""
Feb 11 09:02:50.190: INFO: stdout: "I"
STEP: exposing timestamps
Feb 11 09:02:50.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 logs logs-generator logs-generator --namespace=kubectl-7501 --tail=1 --timestamps'
Feb 11 09:02:50.265: INFO: stderr: ""
Feb 11 09:02:50.265: INFO: stdout: "2020-02-11T09:02:50.167846481Z I0211 09:02:50.167709       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/749r 440\n"
STEP: restricting to a time range
Feb 11 09:02:52.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 logs logs-generator logs-generator --namespace=kubectl-7501 --since=1s'
Feb 11 09:02:52.863: INFO: stderr: ""
Feb 11 09:02:52.863: INFO: stdout: "I0211 09:02:51.967694       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/lpjh 382\nI0211 09:02:52.167691       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/7vz 326\nI0211 09:02:52.367687       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/bmpc 267\nI0211 09:02:52.567697       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/dhw 543\nI0211 09:02:52.767727       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/ngrn 497\n"
Feb 11 09:02:52.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 logs logs-generator logs-generator --namespace=kubectl-7501 --since=24h'
Feb 11 09:02:52.947: INFO: stderr: ""
Feb 11 09:02:52.947: INFO: stdout: "I0211 09:02:48.767550       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/zqcv 352\nI0211 09:02:48.967683       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/gj8 579\nI0211 09:02:49.167676       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/mfs5 355\nI0211 09:02:49.367698       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/q8w9 270\nI0211 09:02:49.567686       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/qj9h 349\nI0211 09:02:49.767673       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/k8m 355\nI0211 09:02:49.967909       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/jwpv 430\nI0211 09:02:50.167709       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/749r 440\nI0211 09:02:50.367672       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/tjs 299\nI0211 09:02:50.567714       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/zcqm 266\nI0211 09:02:50.767689       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/pp87 224\nI0211 09:02:50.967692       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/h27d 365\nI0211 09:02:51.167702       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/rrmc 503\nI0211 09:02:51.367690       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/qvt 228\nI0211 09:02:51.567696       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/nfp 563\nI0211 09:02:51.767696       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/vt5 474\nI0211 09:02:51.967694       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/lpjh 382\nI0211 09:02:52.167691       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/7vz 326\nI0211 09:02:52.367687       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/bmpc 267\nI0211 09:02:52.567697       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/dhw 543\nI0211 09:02:52.767727       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/ngrn 497\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Feb 11 09:02:52.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 delete pod logs-generator --namespace=kubectl-7501'
Feb 11 09:02:57.966: INFO: stderr: ""
Feb 11 09:02:57.966: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:02:57.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7501" for this suite.
Feb 11 09:03:03.979: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:03:04.043: INFO: namespace kubectl-7501 deletion completed in 6.073295517s

â€¢ [SLOW TEST:16.396 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:03:04.043: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-5f7869f1-fcd4-4c4d-85db-233e09d35ede
STEP: Creating a pod to test consume configMaps
Feb 11 09:03:04.072: INFO: Waiting up to 5m0s for pod "pod-configmaps-29fad3a5-5c54-40b8-8e02-20013ea07dee" in namespace "configmap-1495" to be "success or failure"
Feb 11 09:03:04.074: INFO: Pod "pod-configmaps-29fad3a5-5c54-40b8-8e02-20013ea07dee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179152ms
Feb 11 09:03:06.078: INFO: Pod "pod-configmaps-29fad3a5-5c54-40b8-8e02-20013ea07dee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005385762s
STEP: Saw pod success
Feb 11 09:03:06.078: INFO: Pod "pod-configmaps-29fad3a5-5c54-40b8-8e02-20013ea07dee" satisfied condition "success or failure"
Feb 11 09:03:06.080: INFO: Trying to get logs from node cn-hongkong.192.168.0.19 pod pod-configmaps-29fad3a5-5c54-40b8-8e02-20013ea07dee container configmap-volume-test: <nil>
STEP: delete the pod
Feb 11 09:03:06.095: INFO: Waiting for pod pod-configmaps-29fad3a5-5c54-40b8-8e02-20013ea07dee to disappear
Feb 11 09:03:06.097: INFO: Pod pod-configmaps-29fad3a5-5c54-40b8-8e02-20013ea07dee no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:03:06.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1495" for this suite.
Feb 11 09:03:12.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:03:12.173: INFO: namespace configmap-1495 deletion completed in 6.073302818s

â€¢ [SLOW TEST:8.130 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:03:12.174: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 11 09:03:12.200: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cee561af-8fae-4365-b8cf-6c0f368ecb5d" in namespace "downward-api-2770" to be "success or failure"
Feb 11 09:03:12.203: INFO: Pod "downwardapi-volume-cee561af-8fae-4365-b8cf-6c0f368ecb5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.253142ms
Feb 11 09:03:14.206: INFO: Pod "downwardapi-volume-cee561af-8fae-4365-b8cf-6c0f368ecb5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005297633s
STEP: Saw pod success
Feb 11 09:03:14.206: INFO: Pod "downwardapi-volume-cee561af-8fae-4365-b8cf-6c0f368ecb5d" satisfied condition "success or failure"
Feb 11 09:03:14.208: INFO: Trying to get logs from node cn-hongkong.192.168.0.20 pod downwardapi-volume-cee561af-8fae-4365-b8cf-6c0f368ecb5d container client-container: <nil>
STEP: delete the pod
Feb 11 09:03:14.222: INFO: Waiting for pod downwardapi-volume-cee561af-8fae-4365-b8cf-6c0f368ecb5d to disappear
Feb 11 09:03:14.224: INFO: Pod downwardapi-volume-cee561af-8fae-4365-b8cf-6c0f368ecb5d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:03:14.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2770" for this suite.
Feb 11 09:03:20.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:03:20.297: INFO: namespace downward-api-2770 deletion completed in 6.069573447s

â€¢ [SLOW TEST:8.123 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:03:20.297: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 09:03:20.334: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"93050af0-03d8-4713-a19b-5158360a1df9", Controller:(*bool)(0xc0044e267e), BlockOwnerDeletion:(*bool)(0xc0044e267f)}}
Feb 11 09:03:20.338: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"6af7693d-c584-4c12-9ac5-f61068af0f74", Controller:(*bool)(0xc0043d914e), BlockOwnerDeletion:(*bool)(0xc0043d914f)}}
Feb 11 09:03:20.341: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"2ab5af49-42ee-4cb7-b233-343d54ae1f96", Controller:(*bool)(0xc0044e284e), BlockOwnerDeletion:(*bool)(0xc0044e284f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:03:25.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2678" for this suite.
Feb 11 09:03:31.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:03:31.424: INFO: namespace gc-2678 deletion completed in 6.072680331s

â€¢ [SLOW TEST:11.127 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:03:31.424: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 09:03:31.451: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-6f2021db-562a-4f06-8056-c7b595243f03" in namespace "security-context-test-2150" to be "success or failure"
Feb 11 09:03:31.453: INFO: Pod "alpine-nnp-false-6f2021db-562a-4f06-8056-c7b595243f03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.213131ms
Feb 11 09:03:33.456: INFO: Pod "alpine-nnp-false-6f2021db-562a-4f06-8056-c7b595243f03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005330731s
Feb 11 09:03:35.460: INFO: Pod "alpine-nnp-false-6f2021db-562a-4f06-8056-c7b595243f03": Phase="Running", Reason="", readiness=true. Elapsed: 4.008783673s
Feb 11 09:03:37.463: INFO: Pod "alpine-nnp-false-6f2021db-562a-4f06-8056-c7b595243f03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012071133s
Feb 11 09:03:37.463: INFO: Pod "alpine-nnp-false-6f2021db-562a-4f06-8056-c7b595243f03" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:03:37.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2150" for this suite.
Feb 11 09:03:43.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:03:43.546: INFO: namespace security-context-test-2150 deletion completed in 6.072560407s

â€¢ [SLOW TEST:12.122 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:03:43.547: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Feb 11 09:03:43.569: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 11 09:03:43.577: INFO: Waiting for terminating namespaces to be deleted...
Feb 11 09:03:43.579: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.18 before test
Feb 11 09:03:43.590: INFO: sonobuoy-systemd-logs-daemon-set-cf27379a033c4f18-ts8rk from sonobuoy started at 2020-02-11 08:31:10 +0000 UTC (2 container statuses recorded)
Feb 11 09:03:43.590: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 11 09:03:43.590: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 11 09:03:43.590: INFO: kube-proxy-worker-c92sw from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 09:03:43.590: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Feb 11 09:03:43.590: INFO: coredns-7bbcddf6b7-bjckz from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 09:03:43.590: INFO: 	Container coredns ready: true, restart count 0
Feb 11 09:03:43.590: INFO: flexvolume-mv7bv from kube-system started at 2020-02-11 08:01:04 +0000 UTC (1 container statuses recorded)
Feb 11 09:03:43.590: INFO: 	Container acs-flexvolume ready: true, restart count 0
Feb 11 09:03:43.590: INFO: kube-flannel-ds-xjcpn from kube-system started at 2020-02-11 08:01:10 +0000 UTC (1 container statuses recorded)
Feb 11 09:03:43.590: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 11 09:03:43.590: INFO: alicloud-application-controller-66cd594458-dzs7s from kube-system started at 2020-02-11 08:01:23 +0000 UTC (1 container statuses recorded)
Feb 11 09:03:43.590: INFO: 	Container alicloud-application-controller ready: true, restart count 0
Feb 11 09:03:43.590: INFO: nginx-ingress-controller-5f645986c-gw8nx from kube-system started at 2020-02-11 08:01:27 +0000 UTC (1 container statuses recorded)
Feb 11 09:03:43.590: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 11 09:03:43.590: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.19 before test
Feb 11 09:03:43.595: INFO: flexvolume-lfp7h from kube-system started at 2020-02-11 08:01:04 +0000 UTC (1 container statuses recorded)
Feb 11 09:03:43.595: INFO: 	Container acs-flexvolume ready: true, restart count 0
Feb 11 09:03:43.595: INFO: metrics-server-74d79db469-q9c6v from kube-system started at 2020-02-11 08:01:26 +0000 UTC (1 container statuses recorded)
Feb 11 09:03:43.595: INFO: 	Container metrics-server ready: true, restart count 0
Feb 11 09:03:43.595: INFO: sonobuoy-systemd-logs-daemon-set-cf27379a033c4f18-w4gvp from sonobuoy started at 2020-02-11 08:31:10 +0000 UTC (2 container statuses recorded)
Feb 11 09:03:43.595: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 11 09:03:43.595: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 11 09:03:43.595: INFO: kube-proxy-worker-gv69f from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 09:03:43.595: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Feb 11 09:03:43.595: INFO: kube-flannel-ds-l5588 from kube-system started at 2020-02-11 08:01:11 +0000 UTC (1 container statuses recorded)
Feb 11 09:03:43.595: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 11 09:03:43.595: INFO: nginx-ingress-controller-5f645986c-lbm58 from kube-system started at 2020-02-11 08:01:27 +0000 UTC (1 container statuses recorded)
Feb 11 09:03:43.595: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 11 09:03:43.595: INFO: sonobuoy from sonobuoy started at 2020-02-11 08:31:09 +0000 UTC (1 container statuses recorded)
Feb 11 09:03:43.595: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 11 09:03:43.595: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.20 before test
Feb 11 09:03:43.599: INFO: kube-proxy-worker-6tcbf from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 09:03:43.599: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Feb 11 09:03:43.599: INFO: flexvolume-rg5mr from kube-system started at 2020-02-11 08:01:04 +0000 UTC (1 container statuses recorded)
Feb 11 09:03:43.599: INFO: 	Container acs-flexvolume ready: true, restart count 0
Feb 11 09:03:43.599: INFO: sonobuoy-e2e-job-a3471aaa24714387 from sonobuoy started at 2020-02-11 08:31:10 +0000 UTC (2 container statuses recorded)
Feb 11 09:03:43.599: INFO: 	Container e2e ready: true, restart count 0
Feb 11 09:03:43.599: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 11 09:03:43.599: INFO: sonobuoy-systemd-logs-daemon-set-cf27379a033c4f18-rfthz from sonobuoy started at 2020-02-11 08:31:10 +0000 UTC (2 container statuses recorded)
Feb 11 09:03:43.599: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 11 09:03:43.599: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 11 09:03:43.599: INFO: coredns-7bbcddf6b7-r9zv8 from kube-system started at 2020-02-11 08:01:03 +0000 UTC (1 container statuses recorded)
Feb 11 09:03:43.600: INFO: 	Container coredns ready: true, restart count 0
Feb 11 09:03:43.600: INFO: kube-flannel-ds-xskqs from kube-system started at 2020-02-11 08:01:10 +0000 UTC (1 container statuses recorded)
Feb 11 09:03:43.600: INFO: 	Container kube-flannel ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node cn-hongkong.192.168.0.18
STEP: verifying the node has the label node cn-hongkong.192.168.0.19
STEP: verifying the node has the label node cn-hongkong.192.168.0.20
Feb 11 09:03:43.634: INFO: Pod alicloud-application-controller-66cd594458-dzs7s requesting resource cpu=100m on Node cn-hongkong.192.168.0.18
Feb 11 09:03:43.634: INFO: Pod coredns-7bbcddf6b7-bjckz requesting resource cpu=100m on Node cn-hongkong.192.168.0.18
Feb 11 09:03:43.634: INFO: Pod coredns-7bbcddf6b7-r9zv8 requesting resource cpu=100m on Node cn-hongkong.192.168.0.20
Feb 11 09:03:43.634: INFO: Pod flexvolume-lfp7h requesting resource cpu=100m on Node cn-hongkong.192.168.0.19
Feb 11 09:03:43.634: INFO: Pod flexvolume-mv7bv requesting resource cpu=100m on Node cn-hongkong.192.168.0.18
Feb 11 09:03:43.634: INFO: Pod flexvolume-rg5mr requesting resource cpu=100m on Node cn-hongkong.192.168.0.20
Feb 11 09:03:43.634: INFO: Pod kube-flannel-ds-l5588 requesting resource cpu=100m on Node cn-hongkong.192.168.0.19
Feb 11 09:03:43.634: INFO: Pod kube-flannel-ds-xjcpn requesting resource cpu=100m on Node cn-hongkong.192.168.0.18
Feb 11 09:03:43.635: INFO: Pod kube-flannel-ds-xskqs requesting resource cpu=100m on Node cn-hongkong.192.168.0.20
Feb 11 09:03:43.635: INFO: Pod kube-proxy-worker-6tcbf requesting resource cpu=0m on Node cn-hongkong.192.168.0.20
Feb 11 09:03:43.635: INFO: Pod kube-proxy-worker-c92sw requesting resource cpu=0m on Node cn-hongkong.192.168.0.18
Feb 11 09:03:43.635: INFO: Pod kube-proxy-worker-gv69f requesting resource cpu=0m on Node cn-hongkong.192.168.0.19
Feb 11 09:03:43.635: INFO: Pod metrics-server-74d79db469-q9c6v requesting resource cpu=0m on Node cn-hongkong.192.168.0.19
Feb 11 09:03:43.635: INFO: Pod nginx-ingress-controller-5f645986c-gw8nx requesting resource cpu=100m on Node cn-hongkong.192.168.0.18
Feb 11 09:03:43.635: INFO: Pod nginx-ingress-controller-5f645986c-lbm58 requesting resource cpu=100m on Node cn-hongkong.192.168.0.19
Feb 11 09:03:43.635: INFO: Pod sonobuoy requesting resource cpu=0m on Node cn-hongkong.192.168.0.19
Feb 11 09:03:43.635: INFO: Pod sonobuoy-e2e-job-a3471aaa24714387 requesting resource cpu=0m on Node cn-hongkong.192.168.0.20
Feb 11 09:03:43.635: INFO: Pod sonobuoy-systemd-logs-daemon-set-cf27379a033c4f18-rfthz requesting resource cpu=0m on Node cn-hongkong.192.168.0.20
Feb 11 09:03:43.635: INFO: Pod sonobuoy-systemd-logs-daemon-set-cf27379a033c4f18-ts8rk requesting resource cpu=0m on Node cn-hongkong.192.168.0.18
Feb 11 09:03:43.635: INFO: Pod sonobuoy-systemd-logs-daemon-set-cf27379a033c4f18-w4gvp requesting resource cpu=0m on Node cn-hongkong.192.168.0.19
STEP: Starting Pods to consume most of the cluster CPU.
Feb 11 09:03:43.635: INFO: Creating a pod which consumes cpu=2590m on Node cn-hongkong.192.168.0.19
Feb 11 09:03:43.640: INFO: Creating a pod which consumes cpu=2590m on Node cn-hongkong.192.168.0.20
Feb 11 09:03:43.643: INFO: Creating a pod which consumes cpu=2450m on Node cn-hongkong.192.168.0.18
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0857e9dc-7ebd-4175-9606-b51ce59ed012.15f24dacc8480a6e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8594/filler-pod-0857e9dc-7ebd-4175-9606-b51ce59ed012 to cn-hongkong.192.168.0.19]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0857e9dc-7ebd-4175-9606-b51ce59ed012.15f24dacf0faf2ea], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0857e9dc-7ebd-4175-9606-b51ce59ed012.15f24dad2fe3f374], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0857e9dc-7ebd-4175-9606-b51ce59ed012.15f24dad324da98e], Reason = [Created], Message = [Created container filler-pod-0857e9dc-7ebd-4175-9606-b51ce59ed012]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0857e9dc-7ebd-4175-9606-b51ce59ed012.15f24dad39e6efe3], Reason = [Started], Message = [Started container filler-pod-0857e9dc-7ebd-4175-9606-b51ce59ed012]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d89960e-24e0-4a16-9e39-5fa062c79670.15f24dacc8a3c657], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8594/filler-pod-5d89960e-24e0-4a16-9e39-5fa062c79670 to cn-hongkong.192.168.0.18]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d89960e-24e0-4a16-9e39-5fa062c79670.15f24dacef1417ac], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d89960e-24e0-4a16-9e39-5fa062c79670.15f24dacf1d08c3b], Reason = [Created], Message = [Created container filler-pod-5d89960e-24e0-4a16-9e39-5fa062c79670]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d89960e-24e0-4a16-9e39-5fa062c79670.15f24dacf9abbff6], Reason = [Started], Message = [Started container filler-pod-5d89960e-24e0-4a16-9e39-5fa062c79670]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6c3b8044-c38a-45c0-8bd6-4c3e3b640024.15f24dacc8f031a5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8594/filler-pod-6c3b8044-c38a-45c0-8bd6-4c3e3b640024 to cn-hongkong.192.168.0.20]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6c3b8044-c38a-45c0-8bd6-4c3e3b640024.15f24dacee9dbc43], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6c3b8044-c38a-45c0-8bd6-4c3e3b640024.15f24dacf0f09504], Reason = [Created], Message = [Created container filler-pod-6c3b8044-c38a-45c0-8bd6-4c3e3b640024]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6c3b8044-c38a-45c0-8bd6-4c3e3b640024.15f24dacf8e0471c], Reason = [Started], Message = [Started container filler-pod-6c3b8044-c38a-45c0-8bd6-4c3e3b640024]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15f24dadb805cc75], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had taints that the pod didn't tolerate.]
STEP: removing the label node off the node cn-hongkong.192.168.0.18
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node cn-hongkong.192.168.0.19
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node cn-hongkong.192.168.0.20
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:03:48.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8594" for this suite.
Feb 11 09:03:54.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:03:54.772: INFO: namespace sched-pred-8594 deletion completed in 6.070695644s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:11.225 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:03:54.772: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 09:03:54.794: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 11 09:04:01.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 --namespace=crd-publish-openapi-8051 create -f -'
Feb 11 09:04:02.028: INFO: stderr: ""
Feb 11 09:04:02.028: INFO: stdout: "e2e-test-crd-publish-openapi-1223-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 11 09:04:02.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 --namespace=crd-publish-openapi-8051 delete e2e-test-crd-publish-openapi-1223-crds test-cr'
Feb 11 09:04:02.131: INFO: stderr: ""
Feb 11 09:04:02.131: INFO: stdout: "e2e-test-crd-publish-openapi-1223-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Feb 11 09:04:02.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 --namespace=crd-publish-openapi-8051 apply -f -'
Feb 11 09:04:02.367: INFO: stderr: ""
Feb 11 09:04:02.367: INFO: stdout: "e2e-test-crd-publish-openapi-1223-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 11 09:04:02.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 --namespace=crd-publish-openapi-8051 delete e2e-test-crd-publish-openapi-1223-crds test-cr'
Feb 11 09:04:02.439: INFO: stderr: ""
Feb 11 09:04:02.439: INFO: stdout: "e2e-test-crd-publish-openapi-1223-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 11 09:04:02.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 explain e2e-test-crd-publish-openapi-1223-crds'
Feb 11 09:04:02.635: INFO: stderr: ""
Feb 11 09:04:02.635: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1223-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:04:05.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8051" for this suite.
Feb 11 09:04:11.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:04:11.471: INFO: namespace crd-publish-openapi-8051 deletion completed in 6.07337996s

â€¢ [SLOW TEST:16.700 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:04:11.472: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4066.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4066.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4066.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4066.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 11 09:04:29.518: INFO: DNS probes using dns-test-16559173-0e29-4cb3-8538-9d0e409cc9c3 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4066.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4066.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4066.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4066.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 11 09:04:33.552: INFO: DNS probes using dns-test-277d47c8-dd36-4742-bbe1-0f1b4cb5dda0 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4066.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4066.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4066.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4066.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 11 09:04:37.593: INFO: DNS probes using dns-test-e6f63661-2dd8-4c14-8886-81adf50cad6b succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:04:37.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4066" for this suite.
Feb 11 09:04:43.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:04:43.689: INFO: namespace dns-4066 deletion completed in 6.072901418s

â€¢ [SLOW TEST:32.217 seconds]
[sig-network] DNS
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:04:43.689: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb 11 09:04:46.737: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:04:47.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5305" for this suite.
Feb 11 09:05:15.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:05:15.824: INFO: namespace replicaset-5305 deletion completed in 28.071531375s

â€¢ [SLOW TEST:32.135 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:05:15.824: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:05:31.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7665" for this suite.
Feb 11 09:05:37.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:05:37.992: INFO: namespace resourcequota-7665 deletion completed in 6.07331679s

â€¢ [SLOW TEST:22.169 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:05:37.993: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 11 09:05:38.020: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9d7eb7b2-6fb7-4fab-ad71-50d3dee844df" in namespace "downward-api-1873" to be "success or failure"
Feb 11 09:05:38.023: INFO: Pod "downwardapi-volume-9d7eb7b2-6fb7-4fab-ad71-50d3dee844df": Phase="Pending", Reason="", readiness=false. Elapsed: 3.139207ms
Feb 11 09:05:40.027: INFO: Pod "downwardapi-volume-9d7eb7b2-6fb7-4fab-ad71-50d3dee844df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006738893s
STEP: Saw pod success
Feb 11 09:05:40.027: INFO: Pod "downwardapi-volume-9d7eb7b2-6fb7-4fab-ad71-50d3dee844df" satisfied condition "success or failure"
Feb 11 09:05:40.029: INFO: Trying to get logs from node cn-hongkong.192.168.0.20 pod downwardapi-volume-9d7eb7b2-6fb7-4fab-ad71-50d3dee844df container client-container: <nil>
STEP: delete the pod
Feb 11 09:05:40.052: INFO: Waiting for pod downwardapi-volume-9d7eb7b2-6fb7-4fab-ad71-50d3dee844df to disappear
Feb 11 09:05:40.054: INFO: Pod downwardapi-volume-9d7eb7b2-6fb7-4fab-ad71-50d3dee844df no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:05:40.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1873" for this suite.
Feb 11 09:05:46.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:05:46.130: INFO: namespace downward-api-1873 deletion completed in 6.072709327s

â€¢ [SLOW TEST:8.137 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:05:46.130: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Feb 11 09:05:46.165: INFO: PodSpec: initContainers in spec.initContainers
Feb 11 09:06:32.643: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-9d8de850-95d0-4209-a141-0ce5d51e4734", GenerateName:"", Namespace:"init-container-4134", SelfLink:"/api/v1/namespaces/init-container-4134/pods/pod-init-9d8de850-95d0-4209-a141-0ce5d51e4734", UID:"b345d930-34b3-4556-8547-6179c849820d", ResourceVersion:"17795", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63717008746, loc:(*time.Location)(0x78896e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"165039523"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-rtf2n", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc005e241c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-rtf2n", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-rtf2n", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-rtf2n", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004cca2a8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"cn-hongkong.192.168.0.20", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0028ec060), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004cca330)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004cca350)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004cca358), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004cca35c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717008746, loc:(*time.Location)(0x78896e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717008746, loc:(*time.Location)(0x78896e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717008746, loc:(*time.Location)(0x78896e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717008746, loc:(*time.Location)(0x78896e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.0.20", PodIP:"172.22.1.177", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.22.1.177"}}, StartTime:(*v1.Time)(0xc00403c140), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0022f41c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0022f4230)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://018c059f3af2573349dcedcefa732385d67676dddec9576cd1dab802a18a2a7b", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00403c180), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00403c160), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc004cca3df)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:06:32.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4134" for this suite.
Feb 11 09:07:00.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:07:00.720: INFO: namespace init-container-4134 deletion completed in 28.072933587s

â€¢ [SLOW TEST:74.590 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:07:00.720: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-97fa95ba-fe78-48aa-9f79-7a71cb2314ef
STEP: Creating secret with name s-test-opt-upd-a7071e45-4131-41fb-94c0-0aa0f52602df
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-97fa95ba-fe78-48aa-9f79-7a71cb2314ef
STEP: Updating secret s-test-opt-upd-a7071e45-4131-41fb-94c0-0aa0f52602df
STEP: Creating secret with name s-test-opt-create-1597a86d-fd89-416d-88ce-e21761a70d15
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:08:15.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9049" for this suite.
Feb 11 09:08:27.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:08:27.189: INFO: namespace projected-9049 deletion completed in 12.075001748s

â€¢ [SLOW TEST:86.468 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:08:27.189: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 11 09:08:27.927: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 11 09:08:29.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717008907, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717008907, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717008907, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717008907, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 11 09:08:32.946: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:08:32.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3649" for this suite.
Feb 11 09:08:44.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:08:45.053: INFO: namespace webhook-3649 deletion completed in 12.071946591s
STEP: Destroying namespace "webhook-3649-markers" for this suite.
Feb 11 09:08:51.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:08:51.127: INFO: namespace webhook-3649-markers deletion completed in 6.07450622s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:23.951 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:08:51.140: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb 11 09:08:51.170: INFO: Pod name pod-release: Found 0 pods out of 1
Feb 11 09:08:56.174: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:08:57.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5188" for this suite.
Feb 11 09:09:03.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:09:03.274: INFO: namespace replication-controller-5188 deletion completed in 6.084546285s

â€¢ [SLOW TEST:12.134 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:09:03.274: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-6881/secret-test-81f13530-a76b-4675-8c91-f61b845911c4
STEP: Creating a pod to test consume secrets
Feb 11 09:09:03.310: INFO: Waiting up to 5m0s for pod "pod-configmaps-e1f38091-53e7-43d4-bcda-d0a841f25588" in namespace "secrets-6881" to be "success or failure"
Feb 11 09:09:03.314: INFO: Pod "pod-configmaps-e1f38091-53e7-43d4-bcda-d0a841f25588": Phase="Pending", Reason="", readiness=false. Elapsed: 3.063811ms
Feb 11 09:09:05.317: INFO: Pod "pod-configmaps-e1f38091-53e7-43d4-bcda-d0a841f25588": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006657671s
STEP: Saw pod success
Feb 11 09:09:05.317: INFO: Pod "pod-configmaps-e1f38091-53e7-43d4-bcda-d0a841f25588" satisfied condition "success or failure"
Feb 11 09:09:05.320: INFO: Trying to get logs from node cn-hongkong.192.168.0.19 pod pod-configmaps-e1f38091-53e7-43d4-bcda-d0a841f25588 container env-test: <nil>
STEP: delete the pod
Feb 11 09:09:05.335: INFO: Waiting for pod pod-configmaps-e1f38091-53e7-43d4-bcda-d0a841f25588 to disappear
Feb 11 09:09:05.337: INFO: Pod pod-configmaps-e1f38091-53e7-43d4-bcda-d0a841f25588 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:09:05.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6881" for this suite.
Feb 11 09:09:11.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:09:11.411: INFO: namespace secrets-6881 deletion completed in 6.071350323s

â€¢ [SLOW TEST:8.137 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:09:11.412: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8591
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-8591
I0211 09:09:11.449930      23 runners.go:184] Created replication controller with name: externalname-service, namespace: services-8591, replica count: 2
Feb 11 09:09:14.500: INFO: Creating new exec pod
I0211 09:09:14.500319      23 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 11 09:09:17.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=services-8591 execpodskfb8 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Feb 11 09:09:17.711: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 11 09:09:17.711: INFO: stdout: ""
Feb 11 09:09:17.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=services-8591 execpodskfb8 -- /bin/sh -x -c nc -zv -t -w 2 172.23.15.94 80'
Feb 11 09:09:17.910: INFO: stderr: "+ nc -zv -t -w 2 172.23.15.94 80\nConnection to 172.23.15.94 80 port [tcp/http] succeeded!\n"
Feb 11 09:09:17.910: INFO: stdout: ""
Feb 11 09:09:17.910: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:09:17.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8591" for this suite.
Feb 11 09:09:23.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:09:24.006: INFO: namespace services-8591 deletion completed in 6.074455292s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:12.595 seconds]
[sig-network] Services
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:09:24.007: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 11 09:09:24.690: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 11 09:09:27.705: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:09:27.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1232" for this suite.
Feb 11 09:09:33.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:09:33.787: INFO: namespace webhook-1232 deletion completed in 6.073178106s
STEP: Destroying namespace "webhook-1232-markers" for this suite.
Feb 11 09:09:39.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:09:39.860: INFO: namespace webhook-1232-markers deletion completed in 6.072769382s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:15.864 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:09:39.871: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 11 09:09:45.921: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5454 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 11 09:09:45.921: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 09:09:46.056: INFO: Exec stderr: ""
Feb 11 09:09:46.056: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5454 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 11 09:09:46.056: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 09:09:46.212: INFO: Exec stderr: ""
Feb 11 09:09:46.212: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5454 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 11 09:09:46.212: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 09:09:46.377: INFO: Exec stderr: ""
Feb 11 09:09:46.377: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5454 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 11 09:09:46.377: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 09:09:46.553: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 11 09:09:46.554: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5454 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 11 09:09:46.554: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 09:09:46.698: INFO: Exec stderr: ""
Feb 11 09:09:46.698: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5454 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 11 09:09:46.698: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 09:09:46.858: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 11 09:09:46.858: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5454 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 11 09:09:46.858: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 09:09:46.991: INFO: Exec stderr: ""
Feb 11 09:09:46.991: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5454 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 11 09:09:46.991: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 09:09:47.166: INFO: Exec stderr: ""
Feb 11 09:09:47.166: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5454 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 11 09:09:47.166: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 09:09:47.314: INFO: Exec stderr: ""
Feb 11 09:09:47.314: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5454 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 11 09:09:47.314: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 09:09:47.471: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:09:47.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5454" for this suite.
Feb 11 09:10:31.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:10:31.548: INFO: namespace e2e-kubelet-etc-hosts-5454 deletion completed in 44.072643672s

â€¢ [SLOW TEST:51.677 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:10:31.548: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 11 09:10:31.577: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7f0680c8-671e-4779-a452-396ea2672346" in namespace "projected-5835" to be "success or failure"
Feb 11 09:10:31.579: INFO: Pod "downwardapi-volume-7f0680c8-671e-4779-a452-396ea2672346": Phase="Pending", Reason="", readiness=false. Elapsed: 2.092779ms
Feb 11 09:10:33.582: INFO: Pod "downwardapi-volume-7f0680c8-671e-4779-a452-396ea2672346": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005415951s
STEP: Saw pod success
Feb 11 09:10:33.582: INFO: Pod "downwardapi-volume-7f0680c8-671e-4779-a452-396ea2672346" satisfied condition "success or failure"
Feb 11 09:10:33.585: INFO: Trying to get logs from node cn-hongkong.192.168.0.19 pod downwardapi-volume-7f0680c8-671e-4779-a452-396ea2672346 container client-container: <nil>
STEP: delete the pod
Feb 11 09:10:33.600: INFO: Waiting for pod downwardapi-volume-7f0680c8-671e-4779-a452-396ea2672346 to disappear
Feb 11 09:10:33.602: INFO: Pod downwardapi-volume-7f0680c8-671e-4779-a452-396ea2672346 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:10:33.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5835" for this suite.
Feb 11 09:10:39.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:10:39.680: INFO: namespace projected-5835 deletion completed in 6.075279873s

â€¢ [SLOW TEST:8.132 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:10:39.681: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:10:50.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-692" for this suite.
Feb 11 09:10:56.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:10:56.828: INFO: namespace resourcequota-692 deletion completed in 6.078622522s

â€¢ [SLOW TEST:17.148 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:10:56.829: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Feb 11 09:10:56.856: INFO: Waiting up to 5m0s for pod "var-expansion-c5ff37ef-e10d-4702-b798-94e0c05bb4f0" in namespace "var-expansion-5410" to be "success or failure"
Feb 11 09:10:56.862: INFO: Pod "var-expansion-c5ff37ef-e10d-4702-b798-94e0c05bb4f0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.821027ms
Feb 11 09:10:58.864: INFO: Pod "var-expansion-c5ff37ef-e10d-4702-b798-94e0c05bb4f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008535777s
STEP: Saw pod success
Feb 11 09:10:58.865: INFO: Pod "var-expansion-c5ff37ef-e10d-4702-b798-94e0c05bb4f0" satisfied condition "success or failure"
Feb 11 09:10:58.867: INFO: Trying to get logs from node cn-hongkong.192.168.0.20 pod var-expansion-c5ff37ef-e10d-4702-b798-94e0c05bb4f0 container dapi-container: <nil>
STEP: delete the pod
Feb 11 09:10:58.890: INFO: Waiting for pod var-expansion-c5ff37ef-e10d-4702-b798-94e0c05bb4f0 to disappear
Feb 11 09:10:58.892: INFO: Pod var-expansion-c5ff37ef-e10d-4702-b798-94e0c05bb4f0 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:10:58.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5410" for this suite.
Feb 11 09:11:04.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:11:04.969: INFO: namespace var-expansion-5410 deletion completed in 6.074135148s

â€¢ [SLOW TEST:8.141 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:11:04.969: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Feb 11 09:11:04.996: INFO: Waiting up to 5m0s for pod "downward-api-0b45d736-65d4-41ec-a88c-ce28245f8eb9" in namespace "downward-api-5331" to be "success or failure"
Feb 11 09:11:04.999: INFO: Pod "downward-api-0b45d736-65d4-41ec-a88c-ce28245f8eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.298703ms
Feb 11 09:11:07.002: INFO: Pod "downward-api-0b45d736-65d4-41ec-a88c-ce28245f8eb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005519159s
STEP: Saw pod success
Feb 11 09:11:07.002: INFO: Pod "downward-api-0b45d736-65d4-41ec-a88c-ce28245f8eb9" satisfied condition "success or failure"
Feb 11 09:11:07.005: INFO: Trying to get logs from node cn-hongkong.192.168.0.19 pod downward-api-0b45d736-65d4-41ec-a88c-ce28245f8eb9 container dapi-container: <nil>
STEP: delete the pod
Feb 11 09:11:07.019: INFO: Waiting for pod downward-api-0b45d736-65d4-41ec-a88c-ce28245f8eb9 to disappear
Feb 11 09:11:07.021: INFO: Pod downward-api-0b45d736-65d4-41ec-a88c-ce28245f8eb9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:11:07.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5331" for this suite.
Feb 11 09:11:13.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:11:13.104: INFO: namespace downward-api-5331 deletion completed in 6.079568426s

â€¢ [SLOW TEST:8.134 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:11:13.104: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 09:11:13.144: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 11 09:11:13.152: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:13.152: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:13.152: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:13.154: INFO: Number of nodes with available pods: 0
Feb 11 09:11:13.154: INFO: Node cn-hongkong.192.168.0.18 is running more than one daemon pod
Feb 11 09:11:14.158: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:14.158: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:14.158: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:14.161: INFO: Number of nodes with available pods: 0
Feb 11 09:11:14.161: INFO: Node cn-hongkong.192.168.0.18 is running more than one daemon pod
Feb 11 09:11:15.158: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:15.159: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:15.159: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:15.161: INFO: Number of nodes with available pods: 3
Feb 11 09:11:15.161: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 11 09:11:15.184: INFO: Wrong image for pod: daemon-set-8qwnr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:15.184: INFO: Wrong image for pod: daemon-set-cjqzh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:15.184: INFO: Wrong image for pod: daemon-set-vznq8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:15.187: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:15.187: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:15.187: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:16.191: INFO: Wrong image for pod: daemon-set-8qwnr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:16.191: INFO: Wrong image for pod: daemon-set-cjqzh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:16.191: INFO: Wrong image for pod: daemon-set-vznq8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:16.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:16.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:16.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:17.191: INFO: Wrong image for pod: daemon-set-8qwnr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:17.191: INFO: Wrong image for pod: daemon-set-cjqzh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:17.191: INFO: Wrong image for pod: daemon-set-vznq8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:17.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:17.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:17.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:18.190: INFO: Wrong image for pod: daemon-set-8qwnr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:18.190: INFO: Pod daemon-set-8qwnr is not available
Feb 11 09:11:18.190: INFO: Wrong image for pod: daemon-set-cjqzh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:18.190: INFO: Wrong image for pod: daemon-set-vznq8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:18.193: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:18.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:18.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:19.191: INFO: Wrong image for pod: daemon-set-8qwnr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:19.191: INFO: Pod daemon-set-8qwnr is not available
Feb 11 09:11:19.191: INFO: Wrong image for pod: daemon-set-cjqzh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:19.191: INFO: Wrong image for pod: daemon-set-vznq8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:19.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:19.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:19.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:20.191: INFO: Wrong image for pod: daemon-set-8qwnr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:20.191: INFO: Pod daemon-set-8qwnr is not available
Feb 11 09:11:20.191: INFO: Wrong image for pod: daemon-set-cjqzh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:20.191: INFO: Wrong image for pod: daemon-set-vznq8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:20.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:20.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:20.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:21.190: INFO: Wrong image for pod: daemon-set-8qwnr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:21.190: INFO: Pod daemon-set-8qwnr is not available
Feb 11 09:11:21.190: INFO: Wrong image for pod: daemon-set-cjqzh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:21.190: INFO: Wrong image for pod: daemon-set-vznq8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:21.193: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:21.193: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:21.193: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:22.190: INFO: Wrong image for pod: daemon-set-cjqzh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:22.190: INFO: Pod daemon-set-tfkvt is not available
Feb 11 09:11:22.190: INFO: Wrong image for pod: daemon-set-vznq8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:22.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:22.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:22.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:23.191: INFO: Wrong image for pod: daemon-set-cjqzh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:23.191: INFO: Wrong image for pod: daemon-set-vznq8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:23.195: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:23.195: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:23.195: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:24.191: INFO: Wrong image for pod: daemon-set-cjqzh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:24.191: INFO: Wrong image for pod: daemon-set-vznq8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:24.191: INFO: Pod daemon-set-vznq8 is not available
Feb 11 09:11:24.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:24.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:24.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:25.191: INFO: Pod daemon-set-8xwml is not available
Feb 11 09:11:25.191: INFO: Wrong image for pod: daemon-set-cjqzh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:25.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:25.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:25.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:26.190: INFO: Wrong image for pod: daemon-set-cjqzh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:26.193: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:26.193: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:26.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:27.191: INFO: Wrong image for pod: daemon-set-cjqzh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:27.191: INFO: Pod daemon-set-cjqzh is not available
Feb 11 09:11:27.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:27.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:27.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:28.190: INFO: Wrong image for pod: daemon-set-cjqzh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:28.190: INFO: Pod daemon-set-cjqzh is not available
Feb 11 09:11:28.193: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:28.193: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:28.193: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:29.190: INFO: Wrong image for pod: daemon-set-cjqzh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:29.190: INFO: Pod daemon-set-cjqzh is not available
Feb 11 09:11:29.193: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:29.193: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:29.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:30.191: INFO: Wrong image for pod: daemon-set-cjqzh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:30.191: INFO: Pod daemon-set-cjqzh is not available
Feb 11 09:11:30.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:30.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:30.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:31.190: INFO: Wrong image for pod: daemon-set-cjqzh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:31.190: INFO: Pod daemon-set-cjqzh is not available
Feb 11 09:11:31.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:31.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:31.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:32.190: INFO: Wrong image for pod: daemon-set-cjqzh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:32.190: INFO: Pod daemon-set-cjqzh is not available
Feb 11 09:11:32.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:32.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:32.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:33.191: INFO: Wrong image for pod: daemon-set-cjqzh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:33.191: INFO: Pod daemon-set-cjqzh is not available
Feb 11 09:11:33.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:33.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:33.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:34.190: INFO: Wrong image for pod: daemon-set-cjqzh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:34.190: INFO: Pod daemon-set-cjqzh is not available
Feb 11 09:11:34.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:34.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:34.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:35.191: INFO: Wrong image for pod: daemon-set-cjqzh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:35.191: INFO: Pod daemon-set-cjqzh is not available
Feb 11 09:11:35.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:35.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:35.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:36.191: INFO: Wrong image for pod: daemon-set-cjqzh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:36.191: INFO: Pod daemon-set-cjqzh is not available
Feb 11 09:11:36.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:36.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:36.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:37.190: INFO: Wrong image for pod: daemon-set-cjqzh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 11 09:11:37.190: INFO: Pod daemon-set-cjqzh is not available
Feb 11 09:11:37.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:37.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:37.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:38.190: INFO: Pod daemon-set-sc7mw is not available
Feb 11 09:11:38.193: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:38.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:38.194: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 11 09:11:38.197: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:38.197: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:38.197: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:38.200: INFO: Number of nodes with available pods: 2
Feb 11 09:11:38.200: INFO: Node cn-hongkong.192.168.0.20 is running more than one daemon pod
Feb 11 09:11:39.204: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:39.204: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:39.204: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:11:39.207: INFO: Number of nodes with available pods: 3
Feb 11 09:11:39.207: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1714, will wait for the garbage collector to delete the pods
Feb 11 09:11:39.277: INFO: Deleting DaemonSet.extensions daemon-set took: 6.033507ms
Feb 11 09:11:39.677: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.21487ms
Feb 11 09:11:51.780: INFO: Number of nodes with available pods: 0
Feb 11 09:11:51.780: INFO: Number of running nodes: 0, number of available pods: 0
Feb 11 09:11:51.783: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1714/daemonsets","resourceVersion":"19260"},"items":null}

Feb 11 09:11:51.785: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1714/pods","resourceVersion":"19260"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:11:51.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1714" for this suite.
Feb 11 09:11:57.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:11:57.871: INFO: namespace daemonsets-1714 deletion completed in 6.072844293s

â€¢ [SLOW TEST:44.767 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:11:57.871: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 11 09:11:59.909: INFO: &Pod{ObjectMeta:{send-events-55fbccd0-9ced-484e-947f-98b3f08aba6c  events-139 /api/v1/namespaces/events-139/pods/send-events-55fbccd0-9ced-484e-947f-98b3f08aba6c 6cea2581-ca92-489f-bf33-4e69cb4dd747 19335 0 2020-02-11 09:11:57 +0000 UTC <nil> <nil> map[name:foo time:894144856] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-b85kb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-b85kb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-b85kb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:11:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:11:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:11:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:11:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.18,PodIP:172.22.2.65,StartTime:2020-02-11 09:11:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-11 09:11:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:docker://c8bb42a4b4e3b2b1e429e58c6b57ef16986b2d585bf952bebe702921d876b0ce,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.22.2.65,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Feb 11 09:12:01.913: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 11 09:12:03.917: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:12:03.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-139" for this suite.
Feb 11 09:12:47.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:12:48.001: INFO: namespace events-139 deletion completed in 44.075425925s

â€¢ [SLOW TEST:50.129 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:12:48.001: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-8dxx
STEP: Creating a pod to test atomic-volume-subpath
Feb 11 09:12:48.037: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8dxx" in namespace "subpath-9452" to be "success or failure"
Feb 11 09:12:48.039: INFO: Pod "pod-subpath-test-configmap-8dxx": Phase="Pending", Reason="", readiness=false. Elapsed: 1.996366ms
Feb 11 09:12:50.043: INFO: Pod "pod-subpath-test-configmap-8dxx": Phase="Running", Reason="", readiness=true. Elapsed: 2.005545752s
Feb 11 09:12:52.046: INFO: Pod "pod-subpath-test-configmap-8dxx": Phase="Running", Reason="", readiness=true. Elapsed: 4.008728635s
Feb 11 09:12:54.049: INFO: Pod "pod-subpath-test-configmap-8dxx": Phase="Running", Reason="", readiness=true. Elapsed: 6.012380935s
Feb 11 09:12:56.053: INFO: Pod "pod-subpath-test-configmap-8dxx": Phase="Running", Reason="", readiness=true. Elapsed: 8.016118269s
Feb 11 09:12:58.057: INFO: Pod "pod-subpath-test-configmap-8dxx": Phase="Running", Reason="", readiness=true. Elapsed: 10.01962474s
Feb 11 09:13:00.060: INFO: Pod "pod-subpath-test-configmap-8dxx": Phase="Running", Reason="", readiness=true. Elapsed: 12.023357856s
Feb 11 09:13:02.064: INFO: Pod "pod-subpath-test-configmap-8dxx": Phase="Running", Reason="", readiness=true. Elapsed: 14.027021398s
Feb 11 09:13:04.067: INFO: Pod "pod-subpath-test-configmap-8dxx": Phase="Running", Reason="", readiness=true. Elapsed: 16.030169703s
Feb 11 09:13:06.071: INFO: Pod "pod-subpath-test-configmap-8dxx": Phase="Running", Reason="", readiness=true. Elapsed: 18.03382412s
Feb 11 09:13:08.075: INFO: Pod "pod-subpath-test-configmap-8dxx": Phase="Running", Reason="", readiness=true. Elapsed: 20.037631569s
Feb 11 09:13:10.078: INFO: Pod "pod-subpath-test-configmap-8dxx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.041001124s
STEP: Saw pod success
Feb 11 09:13:10.078: INFO: Pod "pod-subpath-test-configmap-8dxx" satisfied condition "success or failure"
Feb 11 09:13:10.082: INFO: Trying to get logs from node cn-hongkong.192.168.0.19 pod pod-subpath-test-configmap-8dxx container test-container-subpath-configmap-8dxx: <nil>
STEP: delete the pod
Feb 11 09:13:10.106: INFO: Waiting for pod pod-subpath-test-configmap-8dxx to disappear
Feb 11 09:13:10.108: INFO: Pod pod-subpath-test-configmap-8dxx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-8dxx
Feb 11 09:13:10.108: INFO: Deleting pod "pod-subpath-test-configmap-8dxx" in namespace "subpath-9452"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:13:10.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9452" for this suite.
Feb 11 09:13:16.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:13:16.190: INFO: namespace subpath-9452 deletion completed in 6.077532675s

â€¢ [SLOW TEST:28.189 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:13:16.191: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Feb 11 09:13:16.213: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 11 09:14:16.239: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 09:14:16.242: INFO: Starting informer...
STEP: Starting pod...
Feb 11 09:14:16.452: INFO: Pod is running on cn-hongkong.192.168.0.18. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Feb 11 09:14:16.464: INFO: Pod wasn't evicted. Proceeding
Feb 11 09:14:16.464: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Feb 11 09:15:31.477: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:15:31.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-9701" for this suite.
Feb 11 09:15:43.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:15:43.555: INFO: namespace taint-single-pod-9701 deletion completed in 12.072852046s

â€¢ [SLOW TEST:147.364 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:15:43.555: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Feb 11 09:15:43.584: INFO: Waiting up to 5m0s for pod "client-containers-2e9a88c0-eec0-4eef-9ed7-03bed3dc9c6f" in namespace "containers-3257" to be "success or failure"
Feb 11 09:15:43.586: INFO: Pod "client-containers-2e9a88c0-eec0-4eef-9ed7-03bed3dc9c6f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.981591ms
Feb 11 09:15:45.590: INFO: Pod "client-containers-2e9a88c0-eec0-4eef-9ed7-03bed3dc9c6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005637015s
STEP: Saw pod success
Feb 11 09:15:45.590: INFO: Pod "client-containers-2e9a88c0-eec0-4eef-9ed7-03bed3dc9c6f" satisfied condition "success or failure"
Feb 11 09:15:45.592: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod client-containers-2e9a88c0-eec0-4eef-9ed7-03bed3dc9c6f container test-container: <nil>
STEP: delete the pod
Feb 11 09:15:45.616: INFO: Waiting for pod client-containers-2e9a88c0-eec0-4eef-9ed7-03bed3dc9c6f to disappear
Feb 11 09:15:45.618: INFO: Pod client-containers-2e9a88c0-eec0-4eef-9ed7-03bed3dc9c6f no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:15:45.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3257" for this suite.
Feb 11 09:15:51.631: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:15:51.694: INFO: namespace containers-3257 deletion completed in 6.073387455s

â€¢ [SLOW TEST:8.139 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:15:51.694: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-8437
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8437 to expose endpoints map[]
Feb 11 09:15:51.723: INFO: Get endpoints failed (2.426597ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Feb 11 09:15:52.727: INFO: successfully validated that service endpoint-test2 in namespace services-8437 exposes endpoints map[] (1.006208454s elapsed)
STEP: Creating pod pod1 in namespace services-8437
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8437 to expose endpoints map[pod1:[80]]
Feb 11 09:15:54.749: INFO: successfully validated that service endpoint-test2 in namespace services-8437 exposes endpoints map[pod1:[80]] (2.015511938s elapsed)
STEP: Creating pod pod2 in namespace services-8437
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8437 to expose endpoints map[pod1:[80] pod2:[80]]
Feb 11 09:15:56.776: INFO: successfully validated that service endpoint-test2 in namespace services-8437 exposes endpoints map[pod1:[80] pod2:[80]] (2.023184976s elapsed)
STEP: Deleting pod pod1 in namespace services-8437
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8437 to expose endpoints map[pod2:[80]]
Feb 11 09:15:57.793: INFO: successfully validated that service endpoint-test2 in namespace services-8437 exposes endpoints map[pod2:[80]] (1.011625765s elapsed)
STEP: Deleting pod pod2 in namespace services-8437
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8437 to expose endpoints map[]
Feb 11 09:15:57.800: INFO: successfully validated that service endpoint-test2 in namespace services-8437 exposes endpoints map[] (2.844583ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:15:57.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8437" for this suite.
Feb 11 09:16:03.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:16:03.893: INFO: namespace services-8437 deletion completed in 6.073722457s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:12.199 seconds]
[sig-network] Services
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:16:03.894: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 09:16:03.924: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 11 09:16:08.927: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 11 09:16:08.927: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 11 09:16:10.931: INFO: Creating deployment "test-rollover-deployment"
Feb 11 09:16:10.938: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 11 09:16:12.944: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 11 09:16:12.949: INFO: Ensure that both replica sets have 1 created replica
Feb 11 09:16:12.954: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 11 09:16:12.960: INFO: Updating deployment test-rollover-deployment
Feb 11 09:16:12.960: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 11 09:16:14.969: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 11 09:16:14.974: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 11 09:16:14.979: INFO: all replica sets need to contain the pod-template-hash label
Feb 11 09:16:14.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009370, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009370, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009374, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009370, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 11 09:16:16.989: INFO: all replica sets need to contain the pod-template-hash label
Feb 11 09:16:16.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009370, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009370, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009374, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009370, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 11 09:16:18.986: INFO: all replica sets need to contain the pod-template-hash label
Feb 11 09:16:18.986: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009370, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009370, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009374, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009370, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 11 09:16:20.986: INFO: all replica sets need to contain the pod-template-hash label
Feb 11 09:16:20.986: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009370, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009370, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009374, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009370, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 11 09:16:22.986: INFO: all replica sets need to contain the pod-template-hash label
Feb 11 09:16:22.986: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009370, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009370, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009374, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009370, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 11 09:16:24.986: INFO: 
Feb 11 09:16:24.986: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Feb 11 09:16:24.993: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-1336 /apis/apps/v1/namespaces/deployment-1336/deployments/test-rollover-deployment 7f96ef75-1616-4710-afe0-ae591ce06259 20329 2 2020-02-11 09:16:10 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004574978 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-02-11 09:16:10 +0000 UTC,LastTransitionTime:2020-02-11 09:16:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-02-11 09:16:24 +0000 UTC,LastTransitionTime:2020-02-11 09:16:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 11 09:16:24.996: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-1336 /apis/apps/v1/namespaces/deployment-1336/replicasets/test-rollover-deployment-7d7dc6548c 5dc0f266-d6a3-4e8f-a202-ab686bffc2bf 20318 2 2020-02-11 09:16:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 7f96ef75-1616-4710-afe0-ae591ce06259 0xc004574e37 0xc004574e38}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004574e98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 11 09:16:24.996: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 11 09:16:24.996: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1336 /apis/apps/v1/namespaces/deployment-1336/replicasets/test-rollover-controller 3f1ba88c-b4b1-4b74-bac0-32dad653057c 20327 2 2020-02-11 09:16:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 7f96ef75-1616-4710-afe0-ae591ce06259 0xc004574d67 0xc004574d68}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004574dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 11 09:16:24.997: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-1336 /apis/apps/v1/namespaces/deployment-1336/replicasets/test-rollover-deployment-f6c94f66c d807e0a2-1cb1-45d1-b655-c81b2291ab67 20272 2 2020-02-11 09:16:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 7f96ef75-1616-4710-afe0-ae591ce06259 0xc004574f00 0xc004574f01}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004574f78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 11 09:16:24.999: INFO: Pod "test-rollover-deployment-7d7dc6548c-2mh4z" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-2mh4z test-rollover-deployment-7d7dc6548c- deployment-1336 /api/v1/namespaces/deployment-1336/pods/test-rollover-deployment-7d7dc6548c-2mh4z 766880fe-16f5-4c74-bf07-da39a7e26168 20284 0 2020-02-11 09:16:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c 5dc0f266-d6a3-4e8f-a202-ab686bffc2bf 0xc0045754d7 0xc0045754d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5nl2j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5nl2j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5nl2j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:16:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:16:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:16:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:16:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.18,PodIP:172.22.2.71,StartTime:2020-02-11 09:16:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-11 09:16:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://b653b0f0348b0736eca2431cf37a6efd2f449725d68093a5a5fccf7563a522a6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.22.2.71,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:16:24.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1336" for this suite.
Feb 11 09:16:31.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:16:31.087: INFO: namespace deployment-1336 deletion completed in 6.084958328s

â€¢ [SLOW TEST:27.194 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:16:31.088: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-9967
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Feb 11 09:16:31.126: INFO: Found 0 stateful pods, waiting for 3
Feb 11 09:16:41.130: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 11 09:16:41.130: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 11 09:16:41.130: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Feb 11 09:16:41.156: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 11 09:16:51.187: INFO: Updating stateful set ss2
Feb 11 09:16:51.193: INFO: Waiting for Pod statefulset-9967/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Feb 11 09:17:01.223: INFO: Found 2 stateful pods, waiting for 3
Feb 11 09:17:11.227: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 11 09:17:11.227: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 11 09:17:11.227: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 11 09:17:11.248: INFO: Updating stateful set ss2
Feb 11 09:17:11.254: INFO: Waiting for Pod statefulset-9967/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 11 09:17:21.260: INFO: Waiting for Pod statefulset-9967/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 11 09:17:31.277: INFO: Updating stateful set ss2
Feb 11 09:17:31.284: INFO: Waiting for StatefulSet statefulset-9967/ss2 to complete update
Feb 11 09:17:31.284: INFO: Waiting for Pod statefulset-9967/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Feb 11 09:17:41.291: INFO: Deleting all statefulset in ns statefulset-9967
Feb 11 09:17:41.293: INFO: Scaling statefulset ss2 to 0
Feb 11 09:18:01.306: INFO: Waiting for statefulset status.replicas updated to 0
Feb 11 09:18:01.309: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:18:01.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9967" for this suite.
Feb 11 09:18:07.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:18:07.398: INFO: namespace statefulset-9967 deletion completed in 6.074907511s

â€¢ [SLOW TEST:96.310 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:18:07.398: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 11 09:18:07.676: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Feb 11 09:18:09.684: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009487, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009487, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009487, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009487, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 11 09:18:12.695: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 09:18:12.698: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:18:18.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9642" for this suite.
Feb 11 09:18:24.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:18:24.898: INFO: namespace crd-webhook-9642 deletion completed in 6.093148211s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:17.511 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:18:24.909: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Feb 11 09:18:24.935: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:18:38.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5033" for this suite.
Feb 11 09:18:44.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:18:44.727: INFO: namespace pods-5033 deletion completed in 6.07150232s

â€¢ [SLOW TEST:19.818 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:18:44.728: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-6314/configmap-test-4055b9b2-3c14-4cf0-9a4b-4db45d6ccd8e
STEP: Creating a pod to test consume configMaps
Feb 11 09:18:44.757: INFO: Waiting up to 5m0s for pod "pod-configmaps-2f45137e-8912-4dc3-afa6-b78c87054825" in namespace "configmap-6314" to be "success or failure"
Feb 11 09:18:44.759: INFO: Pod "pod-configmaps-2f45137e-8912-4dc3-afa6-b78c87054825": Phase="Pending", Reason="", readiness=false. Elapsed: 2.093508ms
Feb 11 09:18:46.762: INFO: Pod "pod-configmaps-2f45137e-8912-4dc3-afa6-b78c87054825": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005100817s
STEP: Saw pod success
Feb 11 09:18:46.762: INFO: Pod "pod-configmaps-2f45137e-8912-4dc3-afa6-b78c87054825" satisfied condition "success or failure"
Feb 11 09:18:46.764: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-configmaps-2f45137e-8912-4dc3-afa6-b78c87054825 container env-test: <nil>
STEP: delete the pod
Feb 11 09:18:46.782: INFO: Waiting for pod pod-configmaps-2f45137e-8912-4dc3-afa6-b78c87054825 to disappear
Feb 11 09:18:46.784: INFO: Pod pod-configmaps-2f45137e-8912-4dc3-afa6-b78c87054825 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:18:46.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6314" for this suite.
Feb 11 09:18:52.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:18:52.866: INFO: namespace configmap-6314 deletion completed in 6.079329386s

â€¢ [SLOW TEST:8.139 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:18:52.867: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 11 09:18:52.896: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79496dd1-33b5-4d1f-8b5d-d4a3f5a1e90b" in namespace "downward-api-998" to be "success or failure"
Feb 11 09:18:52.898: INFO: Pod "downwardapi-volume-79496dd1-33b5-4d1f-8b5d-d4a3f5a1e90b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.267249ms
Feb 11 09:18:54.902: INFO: Pod "downwardapi-volume-79496dd1-33b5-4d1f-8b5d-d4a3f5a1e90b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005749669s
STEP: Saw pod success
Feb 11 09:18:54.902: INFO: Pod "downwardapi-volume-79496dd1-33b5-4d1f-8b5d-d4a3f5a1e90b" satisfied condition "success or failure"
Feb 11 09:18:54.905: INFO: Trying to get logs from node cn-hongkong.192.168.0.19 pod downwardapi-volume-79496dd1-33b5-4d1f-8b5d-d4a3f5a1e90b container client-container: <nil>
STEP: delete the pod
Feb 11 09:18:54.928: INFO: Waiting for pod downwardapi-volume-79496dd1-33b5-4d1f-8b5d-d4a3f5a1e90b to disappear
Feb 11 09:18:54.930: INFO: Pod downwardapi-volume-79496dd1-33b5-4d1f-8b5d-d4a3f5a1e90b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:18:54.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-998" for this suite.
Feb 11 09:19:00.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:19:01.006: INFO: namespace downward-api-998 deletion completed in 6.073055191s

â€¢ [SLOW TEST:8.139 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:19:01.006: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Feb 11 09:19:03.560: INFO: Successfully updated pod "labelsupdateb8635ed0-559a-46c1-b9bb-d0acb3993987"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:19:07.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9682" for this suite.
Feb 11 09:19:19.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:19:19.659: INFO: namespace downward-api-9682 deletion completed in 12.072767918s

â€¢ [SLOW TEST:18.652 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:19:19.659: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6119.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6119.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 11 09:19:37.717: INFO: DNS probes using dns-6119/dns-test-97080062-327a-4598-84e1-876d8e2155af succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:19:37.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6119" for this suite.
Feb 11 09:19:43.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:19:43.803: INFO: namespace dns-6119 deletion completed in 6.074545594s

â€¢ [SLOW TEST:24.144 seconds]
[sig-network] DNS
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:19:43.803: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Feb 11 09:19:43.825: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Feb 11 09:19:44.047: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Feb 11 09:19:46.085: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 11 09:19:48.089: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 11 09:19:50.089: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 11 09:19:52.089: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 11 09:19:54.089: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 11 09:19:56.088: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717009584, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 11 09:19:58.819: INFO: Waited 720.853776ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:19:59.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9585" for this suite.
Feb 11 09:20:05.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:20:05.527: INFO: namespace aggregator-9585 deletion completed in 6.165039543s

â€¢ [SLOW TEST:21.724 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:20:05.527: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-12483c93-1215-4e7a-a777-7e7360853134
STEP: Creating a pod to test consume configMaps
Feb 11 09:20:05.557: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-073790be-fa80-48e4-840e-a7e647b2cc50" in namespace "projected-2295" to be "success or failure"
Feb 11 09:20:05.560: INFO: Pod "pod-projected-configmaps-073790be-fa80-48e4-840e-a7e647b2cc50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.40575ms
Feb 11 09:20:07.564: INFO: Pod "pod-projected-configmaps-073790be-fa80-48e4-840e-a7e647b2cc50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006012033s
STEP: Saw pod success
Feb 11 09:20:07.564: INFO: Pod "pod-projected-configmaps-073790be-fa80-48e4-840e-a7e647b2cc50" satisfied condition "success or failure"
Feb 11 09:20:07.566: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-projected-configmaps-073790be-fa80-48e4-840e-a7e647b2cc50 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 11 09:20:07.581: INFO: Waiting for pod pod-projected-configmaps-073790be-fa80-48e4-840e-a7e647b2cc50 to disappear
Feb 11 09:20:07.584: INFO: Pod pod-projected-configmaps-073790be-fa80-48e4-840e-a7e647b2cc50 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:20:07.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2295" for this suite.
Feb 11 09:20:13.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:20:13.660: INFO: namespace projected-2295 deletion completed in 6.073615919s

â€¢ [SLOW TEST:8.133 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:20:13.660: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 11 09:20:13.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-5438'
Feb 11 09:20:13.961: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 11 09:20:13.961: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Feb 11 09:20:15.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 delete deployment e2e-test-httpd-deployment --namespace=kubectl-5438'
Feb 11 09:20:16.040: INFO: stderr: ""
Feb 11 09:20:16.040: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:20:16.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5438" for this suite.
Feb 11 09:20:22.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:20:22.115: INFO: namespace kubectl-5438 deletion completed in 6.071625509s

â€¢ [SLOW TEST:8.455 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:20:22.115: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 09:20:22.137: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Creating first CR 
Feb 11 09:20:27.693: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-11T09:20:27Z generation:1 name:name1 resourceVersion:21660 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:6fb48839-066d-49b2-8da8-1219b6c48158] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Feb 11 09:20:37.698: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-11T09:20:37Z generation:1 name:name2 resourceVersion:21689 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:5fb51244-8e98-42f9-9f2b-17c90624cb81] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Feb 11 09:20:47.703: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-11T09:20:27Z generation:2 name:name1 resourceVersion:21716 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:6fb48839-066d-49b2-8da8-1219b6c48158] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Feb 11 09:20:57.708: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-11T09:20:37Z generation:2 name:name2 resourceVersion:21744 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:5fb51244-8e98-42f9-9f2b-17c90624cb81] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Feb 11 09:21:07.716: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-11T09:20:27Z generation:2 name:name1 resourceVersion:21772 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:6fb48839-066d-49b2-8da8-1219b6c48158] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Feb 11 09:21:17.723: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-11T09:20:37Z generation:2 name:name2 resourceVersion:21798 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:5fb51244-8e98-42f9-9f2b-17c90624cb81] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:21:28.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6624" for this suite.
Feb 11 09:21:34.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:21:34.312: INFO: namespace crd-watch-6624 deletion completed in 6.074519636s

â€¢ [SLOW TEST:72.196 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:21:34.312: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 09:21:34.339: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-be192e6f-5a9a-46e9-9d58-e83c676724f2" in namespace "security-context-test-6872" to be "success or failure"
Feb 11 09:21:34.341: INFO: Pod "busybox-privileged-false-be192e6f-5a9a-46e9-9d58-e83c676724f2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.862692ms
Feb 11 09:21:36.345: INFO: Pod "busybox-privileged-false-be192e6f-5a9a-46e9-9d58-e83c676724f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006198759s
Feb 11 09:21:36.345: INFO: Pod "busybox-privileged-false-be192e6f-5a9a-46e9-9d58-e83c676724f2" satisfied condition "success or failure"
Feb 11 09:21:36.354: INFO: Got logs for pod "busybox-privileged-false-be192e6f-5a9a-46e9-9d58-e83c676724f2": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:21:36.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6872" for this suite.
Feb 11 09:21:42.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:21:42.432: INFO: namespace security-context-test-6872 deletion completed in 6.073806113s

â€¢ [SLOW TEST:8.120 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:21:42.432: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-cj6hs in namespace proxy-2447
I0211 09:21:42.467462      23 runners.go:184] Created replication controller with name: proxy-service-cj6hs, namespace: proxy-2447, replica count: 1
I0211 09:21:43.517800      23 runners.go:184] proxy-service-cj6hs Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0211 09:21:44.517967      23 runners.go:184] proxy-service-cj6hs Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0211 09:21:45.518135      23 runners.go:184] proxy-service-cj6hs Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0211 09:21:46.518336      23 runners.go:184] proxy-service-cj6hs Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0211 09:21:47.518509      23 runners.go:184] proxy-service-cj6hs Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0211 09:21:48.518672      23 runners.go:184] proxy-service-cj6hs Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0211 09:21:49.518848      23 runners.go:184] proxy-service-cj6hs Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0211 09:21:50.519056      23 runners.go:184] proxy-service-cj6hs Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0211 09:21:51.519244      23 runners.go:184] proxy-service-cj6hs Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 11 09:21:51.522: INFO: setup took 9.068258754s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 11 09:21:51.527: INFO: (0) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 5.367353ms)
Feb 11 09:21:51.527: INFO: (0) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 5.212957ms)
Feb 11 09:21:51.528: INFO: (0) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 5.548968ms)
Feb 11 09:21:51.528: INFO: (0) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 5.850035ms)
Feb 11 09:21:51.529: INFO: (0) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">test<... (200; 6.708989ms)
Feb 11 09:21:51.529: INFO: (0) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">... (200; 6.599228ms)
Feb 11 09:21:51.529: INFO: (0) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/rewriteme">test</a> (200; 6.684364ms)
Feb 11 09:21:51.531: INFO: (0) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname2/proxy/: bar (200; 7.990436ms)
Feb 11 09:21:51.532: INFO: (0) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname1/proxy/: foo (200; 9.696383ms)
Feb 11 09:21:51.532: INFO: (0) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname2/proxy/: bar (200; 9.897804ms)
Feb 11 09:21:51.533: INFO: (0) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname1/proxy/: tls baz (200; 10.770235ms)
Feb 11 09:21:51.533: INFO: (0) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname1/proxy/: foo (200; 10.87281ms)
Feb 11 09:21:51.533: INFO: (0) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:460/proxy/: tls baz (200; 11.350016ms)
Feb 11 09:21:51.534: INFO: (0) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:462/proxy/: tls qux (200; 12.382818ms)
Feb 11 09:21:51.534: INFO: (0) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/tlsrewritem... (200; 12.034979ms)
Feb 11 09:21:51.535: INFO: (0) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname2/proxy/: tls qux (200; 12.221812ms)
Feb 11 09:21:51.539: INFO: (1) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 4.320452ms)
Feb 11 09:21:51.539: INFO: (1) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:462/proxy/: tls qux (200; 4.502257ms)
Feb 11 09:21:51.540: INFO: (1) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 5.504749ms)
Feb 11 09:21:51.540: INFO: (1) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 5.237477ms)
Feb 11 09:21:51.541: INFO: (1) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/rewriteme">test</a> (200; 5.328665ms)
Feb 11 09:21:51.541: INFO: (1) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 6.045296ms)
Feb 11 09:21:51.541: INFO: (1) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:460/proxy/: tls baz (200; 6.229112ms)
Feb 11 09:21:51.541: INFO: (1) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname1/proxy/: foo (200; 6.454916ms)
Feb 11 09:21:51.542: INFO: (1) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">... (200; 6.29782ms)
Feb 11 09:21:51.542: INFO: (1) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/tlsrewritem... (200; 6.648418ms)
Feb 11 09:21:51.542: INFO: (1) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname2/proxy/: bar (200; 7.487114ms)
Feb 11 09:21:51.542: INFO: (1) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname1/proxy/: foo (200; 7.038421ms)
Feb 11 09:21:51.542: INFO: (1) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname2/proxy/: bar (200; 7.257276ms)
Feb 11 09:21:51.542: INFO: (1) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname1/proxy/: tls baz (200; 7.614373ms)
Feb 11 09:21:51.543: INFO: (1) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname2/proxy/: tls qux (200; 7.733562ms)
Feb 11 09:21:51.543: INFO: (1) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">test<... (200; 7.959126ms)
Feb 11 09:21:51.548: INFO: (2) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/rewriteme">test</a> (200; 4.569521ms)
Feb 11 09:21:51.549: INFO: (2) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/tlsrewritem... (200; 4.883567ms)
Feb 11 09:21:51.549: INFO: (2) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">test<... (200; 5.283995ms)
Feb 11 09:21:51.549: INFO: (2) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:460/proxy/: tls baz (200; 5.21342ms)
Feb 11 09:21:51.549: INFO: (2) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:462/proxy/: tls qux (200; 5.550369ms)
Feb 11 09:21:51.549: INFO: (2) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 5.579132ms)
Feb 11 09:21:51.549: INFO: (2) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 5.912231ms)
Feb 11 09:21:51.549: INFO: (2) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 5.639116ms)
Feb 11 09:21:51.549: INFO: (2) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 5.758194ms)
Feb 11 09:21:51.549: INFO: (2) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">... (200; 6.005211ms)
Feb 11 09:21:51.551: INFO: (2) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname2/proxy/: bar (200; 7.25017ms)
Feb 11 09:21:51.551: INFO: (2) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname2/proxy/: tls qux (200; 7.887047ms)
Feb 11 09:21:51.551: INFO: (2) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname2/proxy/: bar (200; 7.778948ms)
Feb 11 09:21:51.552: INFO: (2) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname1/proxy/: foo (200; 8.009715ms)
Feb 11 09:21:51.552: INFO: (2) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname1/proxy/: tls baz (200; 8.097644ms)
Feb 11 09:21:51.552: INFO: (2) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname1/proxy/: foo (200; 8.274787ms)
Feb 11 09:21:51.557: INFO: (3) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:462/proxy/: tls qux (200; 4.53032ms)
Feb 11 09:21:51.557: INFO: (3) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 4.479886ms)
Feb 11 09:21:51.558: INFO: (3) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">... (200; 5.26425ms)
Feb 11 09:21:51.558: INFO: (3) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/rewriteme">test</a> (200; 5.920503ms)
Feb 11 09:21:51.558: INFO: (3) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:460/proxy/: tls baz (200; 5.814197ms)
Feb 11 09:21:51.558: INFO: (3) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">test<... (200; 5.562162ms)
Feb 11 09:21:51.559: INFO: (3) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 6.201593ms)
Feb 11 09:21:51.559: INFO: (3) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/tlsrewritem... (200; 6.177828ms)
Feb 11 09:21:51.559: INFO: (3) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 6.517191ms)
Feb 11 09:21:51.559: INFO: (3) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 6.97461ms)
Feb 11 09:21:51.559: INFO: (3) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname2/proxy/: bar (200; 7.456571ms)
Feb 11 09:21:51.560: INFO: (3) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname2/proxy/: tls qux (200; 7.618161ms)
Feb 11 09:21:51.560: INFO: (3) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname1/proxy/: foo (200; 7.878906ms)
Feb 11 09:21:51.561: INFO: (3) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname1/proxy/: tls baz (200; 8.976582ms)
Feb 11 09:21:51.561: INFO: (3) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname1/proxy/: foo (200; 9.168304ms)
Feb 11 09:21:51.563: INFO: (3) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname2/proxy/: bar (200; 10.918532ms)
Feb 11 09:21:51.567: INFO: (4) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/tlsrewritem... (200; 4.055394ms)
Feb 11 09:21:51.568: INFO: (4) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 4.161251ms)
Feb 11 09:21:51.568: INFO: (4) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/rewriteme">test</a> (200; 4.844139ms)
Feb 11 09:21:51.569: INFO: (4) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:462/proxy/: tls qux (200; 5.011887ms)
Feb 11 09:21:51.569: INFO: (4) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">... (200; 5.759237ms)
Feb 11 09:21:51.569: INFO: (4) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 5.72456ms)
Feb 11 09:21:51.569: INFO: (4) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 5.688905ms)
Feb 11 09:21:51.570: INFO: (4) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname1/proxy/: foo (200; 6.161483ms)
Feb 11 09:21:51.570: INFO: (4) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname2/proxy/: bar (200; 6.859923ms)
Feb 11 09:21:51.570: INFO: (4) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname1/proxy/: tls baz (200; 6.762062ms)
Feb 11 09:21:51.570: INFO: (4) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:460/proxy/: tls baz (200; 6.966815ms)
Feb 11 09:21:51.571: INFO: (4) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname1/proxy/: foo (200; 6.765544ms)
Feb 11 09:21:51.571: INFO: (4) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">test<... (200; 7.094098ms)
Feb 11 09:21:51.571: INFO: (4) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 6.92577ms)
Feb 11 09:21:51.571: INFO: (4) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname2/proxy/: tls qux (200; 7.266729ms)
Feb 11 09:21:51.571: INFO: (4) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname2/proxy/: bar (200; 7.768737ms)
Feb 11 09:21:51.575: INFO: (5) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:462/proxy/: tls qux (200; 3.875075ms)
Feb 11 09:21:51.575: INFO: (5) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:460/proxy/: tls baz (200; 3.930274ms)
Feb 11 09:21:51.576: INFO: (5) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/tlsrewritem... (200; 4.320426ms)
Feb 11 09:21:51.576: INFO: (5) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 4.412901ms)
Feb 11 09:21:51.576: INFO: (5) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 4.307473ms)
Feb 11 09:21:51.577: INFO: (5) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">test<... (200; 4.582508ms)
Feb 11 09:21:51.577: INFO: (5) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 5.123055ms)
Feb 11 09:21:51.577: INFO: (5) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 5.380625ms)
Feb 11 09:21:51.577: INFO: (5) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/rewriteme">test</a> (200; 5.181544ms)
Feb 11 09:21:51.577: INFO: (5) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname1/proxy/: tls baz (200; 5.127355ms)
Feb 11 09:21:51.577: INFO: (5) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">... (200; 5.372858ms)
Feb 11 09:21:51.577: INFO: (5) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname1/proxy/: foo (200; 5.66377ms)
Feb 11 09:21:51.578: INFO: (5) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname1/proxy/: foo (200; 5.782681ms)
Feb 11 09:21:51.578: INFO: (5) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname2/proxy/: tls qux (200; 6.624636ms)
Feb 11 09:21:51.578: INFO: (5) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname2/proxy/: bar (200; 6.436631ms)
Feb 11 09:21:51.579: INFO: (5) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname2/proxy/: bar (200; 6.802226ms)
Feb 11 09:21:51.582: INFO: (6) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 3.681256ms)
Feb 11 09:21:51.583: INFO: (6) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/rewriteme">test</a> (200; 3.977486ms)
Feb 11 09:21:51.583: INFO: (6) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 3.794203ms)
Feb 11 09:21:51.584: INFO: (6) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:460/proxy/: tls baz (200; 4.423527ms)
Feb 11 09:21:51.584: INFO: (6) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:462/proxy/: tls qux (200; 4.836818ms)
Feb 11 09:21:51.584: INFO: (6) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 5.209423ms)
Feb 11 09:21:51.585: INFO: (6) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">test<... (200; 6.101297ms)
Feb 11 09:21:51.585: INFO: (6) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">... (200; 6.068332ms)
Feb 11 09:21:51.585: INFO: (6) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname2/proxy/: bar (200; 6.198102ms)
Feb 11 09:21:51.585: INFO: (6) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/tlsrewritem... (200; 6.167338ms)
Feb 11 09:21:51.585: INFO: (6) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 6.402871ms)
Feb 11 09:21:51.585: INFO: (6) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname2/proxy/: bar (200; 6.503938ms)
Feb 11 09:21:51.586: INFO: (6) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname1/proxy/: foo (200; 6.87945ms)
Feb 11 09:21:51.586: INFO: (6) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname2/proxy/: tls qux (200; 7.037331ms)
Feb 11 09:21:51.586: INFO: (6) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname1/proxy/: foo (200; 7.199641ms)
Feb 11 09:21:51.587: INFO: (6) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname1/proxy/: tls baz (200; 7.555279ms)
Feb 11 09:21:51.590: INFO: (7) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 3.594578ms)
Feb 11 09:21:51.590: INFO: (7) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 3.887789ms)
Feb 11 09:21:51.591: INFO: (7) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 4.661892ms)
Feb 11 09:21:51.592: INFO: (7) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">test<... (200; 5.363566ms)
Feb 11 09:21:51.593: INFO: (7) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/rewriteme">test</a> (200; 5.676141ms)
Feb 11 09:21:51.593: INFO: (7) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 6.209417ms)
Feb 11 09:21:51.593: INFO: (7) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">... (200; 6.185383ms)
Feb 11 09:21:51.594: INFO: (7) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:462/proxy/: tls qux (200; 6.849173ms)
Feb 11 09:21:51.594: INFO: (7) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/tlsrewritem... (200; 6.751862ms)
Feb 11 09:21:51.594: INFO: (7) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname2/proxy/: tls qux (200; 6.647318ms)
Feb 11 09:21:51.594: INFO: (7) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:460/proxy/: tls baz (200; 6.833913ms)
Feb 11 09:21:51.594: INFO: (7) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname1/proxy/: foo (200; 6.752383ms)
Feb 11 09:21:51.594: INFO: (7) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname2/proxy/: bar (200; 7.331322ms)
Feb 11 09:21:51.594: INFO: (7) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname1/proxy/: foo (200; 7.468093ms)
Feb 11 09:21:51.594: INFO: (7) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname2/proxy/: bar (200; 7.569188ms)
Feb 11 09:21:51.594: INFO: (7) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname1/proxy/: tls baz (200; 7.571118ms)
Feb 11 09:21:51.599: INFO: (8) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 4.597825ms)
Feb 11 09:21:51.599: INFO: (8) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:460/proxy/: tls baz (200; 4.535905ms)
Feb 11 09:21:51.599: INFO: (8) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/rewriteme">test</a> (200; 4.501493ms)
Feb 11 09:21:51.600: INFO: (8) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 5.227293ms)
Feb 11 09:21:51.600: INFO: (8) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:462/proxy/: tls qux (200; 5.535307ms)
Feb 11 09:21:51.600: INFO: (8) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">... (200; 5.340464ms)
Feb 11 09:21:51.601: INFO: (8) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/tlsrewritem... (200; 5.61296ms)
Feb 11 09:21:51.601: INFO: (8) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 5.708304ms)
Feb 11 09:21:51.601: INFO: (8) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname1/proxy/: foo (200; 5.909361ms)
Feb 11 09:21:51.601: INFO: (8) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname2/proxy/: bar (200; 5.809704ms)
Feb 11 09:21:51.601: INFO: (8) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">test<... (200; 6.110093ms)
Feb 11 09:21:51.601: INFO: (8) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 5.926459ms)
Feb 11 09:21:51.602: INFO: (8) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname1/proxy/: foo (200; 7.26175ms)
Feb 11 09:21:51.602: INFO: (8) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname1/proxy/: tls baz (200; 7.403502ms)
Feb 11 09:21:51.602: INFO: (8) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname2/proxy/: bar (200; 7.124738ms)
Feb 11 09:21:51.603: INFO: (8) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname2/proxy/: tls qux (200; 8.028475ms)
Feb 11 09:21:51.606: INFO: (9) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:462/proxy/: tls qux (200; 3.001708ms)
Feb 11 09:21:51.607: INFO: (9) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 4.518992ms)
Feb 11 09:21:51.608: INFO: (9) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/rewriteme">test</a> (200; 4.890771ms)
Feb 11 09:21:51.608: INFO: (9) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 5.065427ms)
Feb 11 09:21:51.608: INFO: (9) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 4.955847ms)
Feb 11 09:21:51.608: INFO: (9) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname1/proxy/: foo (200; 5.261811ms)
Feb 11 09:21:51.608: INFO: (9) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">test<... (200; 5.036399ms)
Feb 11 09:21:51.608: INFO: (9) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:460/proxy/: tls baz (200; 5.339617ms)
Feb 11 09:21:51.609: INFO: (9) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/tlsrewritem... (200; 5.654265ms)
Feb 11 09:21:51.610: INFO: (9) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 6.548017ms)
Feb 11 09:21:51.610: INFO: (9) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">... (200; 6.585179ms)
Feb 11 09:21:51.610: INFO: (9) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname1/proxy/: tls baz (200; 7.06681ms)
Feb 11 09:21:51.610: INFO: (9) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname2/proxy/: tls qux (200; 7.256297ms)
Feb 11 09:21:51.611: INFO: (9) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname2/proxy/: bar (200; 7.582432ms)
Feb 11 09:21:51.611: INFO: (9) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname2/proxy/: bar (200; 7.649947ms)
Feb 11 09:21:51.611: INFO: (9) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname1/proxy/: foo (200; 7.833301ms)
Feb 11 09:21:51.615: INFO: (10) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 4.102658ms)
Feb 11 09:21:51.615: INFO: (10) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/rewriteme">test</a> (200; 4.003524ms)
Feb 11 09:21:51.616: INFO: (10) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:462/proxy/: tls qux (200; 5.36023ms)
Feb 11 09:21:51.616: INFO: (10) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:460/proxy/: tls baz (200; 5.229944ms)
Feb 11 09:21:51.617: INFO: (10) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 5.611668ms)
Feb 11 09:21:51.617: INFO: (10) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/tlsrewritem... (200; 5.788317ms)
Feb 11 09:21:51.617: INFO: (10) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">... (200; 5.887993ms)
Feb 11 09:21:51.618: INFO: (10) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 6.494601ms)
Feb 11 09:21:51.618: INFO: (10) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 6.384663ms)
Feb 11 09:21:51.618: INFO: (10) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">test<... (200; 6.674022ms)
Feb 11 09:21:51.618: INFO: (10) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname1/proxy/: foo (200; 7.014478ms)
Feb 11 09:21:51.618: INFO: (10) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname1/proxy/: tls baz (200; 7.035366ms)
Feb 11 09:21:51.618: INFO: (10) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname2/proxy/: tls qux (200; 7.306987ms)
Feb 11 09:21:51.619: INFO: (10) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname2/proxy/: bar (200; 7.514905ms)
Feb 11 09:21:51.619: INFO: (10) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname2/proxy/: bar (200; 7.438245ms)
Feb 11 09:21:51.619: INFO: (10) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname1/proxy/: foo (200; 8.023443ms)
Feb 11 09:21:51.622: INFO: (11) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/tlsrewritem... (200; 3.054067ms)
Feb 11 09:21:51.623: INFO: (11) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname2/proxy/: bar (200; 4.084707ms)
Feb 11 09:21:51.623: INFO: (11) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 4.117177ms)
Feb 11 09:21:51.623: INFO: (11) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 3.917604ms)
Feb 11 09:21:51.624: INFO: (11) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 4.742887ms)
Feb 11 09:21:51.624: INFO: (11) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:462/proxy/: tls qux (200; 5.161522ms)
Feb 11 09:21:51.625: INFO: (11) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname2/proxy/: tls qux (200; 5.887361ms)
Feb 11 09:21:51.625: INFO: (11) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:460/proxy/: tls baz (200; 6.145847ms)
Feb 11 09:21:51.625: INFO: (11) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">... (200; 5.706803ms)
Feb 11 09:21:51.626: INFO: (11) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/rewriteme">test</a> (200; 5.949167ms)
Feb 11 09:21:51.626: INFO: (11) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">test<... (200; 6.064823ms)
Feb 11 09:21:51.626: INFO: (11) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 5.871297ms)
Feb 11 09:21:51.627: INFO: (11) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname2/proxy/: bar (200; 7.37046ms)
Feb 11 09:21:51.627: INFO: (11) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname1/proxy/: foo (200; 7.86779ms)
Feb 11 09:21:51.627: INFO: (11) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname1/proxy/: tls baz (200; 7.886082ms)
Feb 11 09:21:51.628: INFO: (11) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname1/proxy/: foo (200; 8.09296ms)
Feb 11 09:21:51.630: INFO: (12) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 2.789709ms)
Feb 11 09:21:51.631: INFO: (12) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:462/proxy/: tls qux (200; 3.757177ms)
Feb 11 09:21:51.632: INFO: (12) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/rewriteme">test</a> (200; 3.921204ms)
Feb 11 09:21:51.632: INFO: (12) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 3.85865ms)
Feb 11 09:21:51.632: INFO: (12) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:460/proxy/: tls baz (200; 4.018089ms)
Feb 11 09:21:51.632: INFO: (12) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">... (200; 4.550973ms)
Feb 11 09:21:51.633: INFO: (12) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname1/proxy/: foo (200; 5.096903ms)
Feb 11 09:21:51.633: INFO: (12) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/tlsrewritem... (200; 4.779834ms)
Feb 11 09:21:51.633: INFO: (12) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 4.857651ms)
Feb 11 09:21:51.633: INFO: (12) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">test<... (200; 5.20128ms)
Feb 11 09:21:51.633: INFO: (12) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 5.174029ms)
Feb 11 09:21:51.634: INFO: (12) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname2/proxy/: bar (200; 6.322145ms)
Feb 11 09:21:51.635: INFO: (12) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname2/proxy/: bar (200; 6.67113ms)
Feb 11 09:21:51.635: INFO: (12) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname1/proxy/: tls baz (200; 7.218446ms)
Feb 11 09:21:51.635: INFO: (12) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname1/proxy/: foo (200; 7.664562ms)
Feb 11 09:21:51.635: INFO: (12) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname2/proxy/: tls qux (200; 7.625713ms)
Feb 11 09:21:51.639: INFO: (13) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:462/proxy/: tls qux (200; 3.490084ms)
Feb 11 09:21:51.640: INFO: (13) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 4.284955ms)
Feb 11 09:21:51.640: INFO: (13) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:460/proxy/: tls baz (200; 4.539952ms)
Feb 11 09:21:51.640: INFO: (13) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 4.766284ms)
Feb 11 09:21:51.641: INFO: (13) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/tlsrewritem... (200; 4.503506ms)
Feb 11 09:21:51.641: INFO: (13) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">... (200; 4.910688ms)
Feb 11 09:21:51.641: INFO: (13) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">test<... (200; 4.831576ms)
Feb 11 09:21:51.641: INFO: (13) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/rewriteme">test</a> (200; 5.137936ms)
Feb 11 09:21:51.641: INFO: (13) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 5.17949ms)
Feb 11 09:21:51.641: INFO: (13) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 5.575063ms)
Feb 11 09:21:51.643: INFO: (13) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname2/proxy/: bar (200; 7.379348ms)
Feb 11 09:21:51.643: INFO: (13) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname2/proxy/: bar (200; 7.860825ms)
Feb 11 09:21:51.643: INFO: (13) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname1/proxy/: foo (200; 7.928959ms)
Feb 11 09:21:51.643: INFO: (13) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname2/proxy/: tls qux (200; 7.787579ms)
Feb 11 09:21:51.643: INFO: (13) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname1/proxy/: tls baz (200; 7.830991ms)
Feb 11 09:21:51.644: INFO: (13) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname1/proxy/: foo (200; 8.024294ms)
Feb 11 09:21:51.647: INFO: (14) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:460/proxy/: tls baz (200; 3.901286ms)
Feb 11 09:21:51.648: INFO: (14) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">test<... (200; 4.168134ms)
Feb 11 09:21:51.648: INFO: (14) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 4.08365ms)
Feb 11 09:21:51.648: INFO: (14) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 4.527786ms)
Feb 11 09:21:51.649: INFO: (14) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">... (200; 5.187477ms)
Feb 11 09:21:51.649: INFO: (14) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/tlsrewritem... (200; 5.720643ms)
Feb 11 09:21:51.650: INFO: (14) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/rewriteme">test</a> (200; 6.003524ms)
Feb 11 09:21:51.650: INFO: (14) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:462/proxy/: tls qux (200; 6.506347ms)
Feb 11 09:21:51.650: INFO: (14) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 6.468658ms)
Feb 11 09:21:51.650: INFO: (14) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname1/proxy/: foo (200; 6.810211ms)
Feb 11 09:21:51.651: INFO: (14) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 6.785679ms)
Feb 11 09:21:51.651: INFO: (14) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname2/proxy/: bar (200; 7.112186ms)
Feb 11 09:21:51.651: INFO: (14) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname1/proxy/: tls baz (200; 7.157898ms)
Feb 11 09:21:51.651: INFO: (14) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname2/proxy/: bar (200; 7.551157ms)
Feb 11 09:21:51.651: INFO: (14) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname2/proxy/: tls qux (200; 7.755039ms)
Feb 11 09:21:51.652: INFO: (14) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname1/proxy/: foo (200; 7.974487ms)
Feb 11 09:21:51.656: INFO: (15) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:462/proxy/: tls qux (200; 4.635404ms)
Feb 11 09:21:51.656: INFO: (15) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 4.701212ms)
Feb 11 09:21:51.658: INFO: (15) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/tlsrewritem... (200; 5.776084ms)
Feb 11 09:21:51.658: INFO: (15) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/rewriteme">test</a> (200; 5.795622ms)
Feb 11 09:21:51.658: INFO: (15) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 6.121825ms)
Feb 11 09:21:51.658: INFO: (15) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">... (200; 5.941761ms)
Feb 11 09:21:51.658: INFO: (15) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 6.384947ms)
Feb 11 09:21:51.658: INFO: (15) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">test<... (200; 6.201565ms)
Feb 11 09:21:51.658: INFO: (15) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 6.140246ms)
Feb 11 09:21:51.658: INFO: (15) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:460/proxy/: tls baz (200; 6.347268ms)
Feb 11 09:21:51.659: INFO: (15) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname2/proxy/: bar (200; 6.603677ms)
Feb 11 09:21:51.659: INFO: (15) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname1/proxy/: foo (200; 7.440498ms)
Feb 11 09:21:51.660: INFO: (15) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname2/proxy/: bar (200; 7.519936ms)
Feb 11 09:21:51.660: INFO: (15) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname2/proxy/: tls qux (200; 7.877598ms)
Feb 11 09:21:51.660: INFO: (15) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname1/proxy/: foo (200; 7.963185ms)
Feb 11 09:21:51.660: INFO: (15) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname1/proxy/: tls baz (200; 8.061988ms)
Feb 11 09:21:51.664: INFO: (16) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:462/proxy/: tls qux (200; 3.509695ms)
Feb 11 09:21:51.664: INFO: (16) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 3.432438ms)
Feb 11 09:21:51.664: INFO: (16) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/rewriteme">test</a> (200; 3.864549ms)
Feb 11 09:21:51.666: INFO: (16) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/tlsrewritem... (200; 5.87316ms)
Feb 11 09:21:51.666: INFO: (16) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">... (200; 6.053502ms)
Feb 11 09:21:51.666: INFO: (16) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 5.814106ms)
Feb 11 09:21:51.666: INFO: (16) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname2/proxy/: tls qux (200; 6.461559ms)
Feb 11 09:21:51.667: INFO: (16) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 6.667771ms)
Feb 11 09:21:51.667: INFO: (16) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:460/proxy/: tls baz (200; 6.471357ms)
Feb 11 09:21:51.667: INFO: (16) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname2/proxy/: bar (200; 6.871571ms)
Feb 11 09:21:51.667: INFO: (16) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 6.842382ms)
Feb 11 09:21:51.667: INFO: (16) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname1/proxy/: foo (200; 6.82848ms)
Feb 11 09:21:51.667: INFO: (16) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname1/proxy/: tls baz (200; 6.778089ms)
Feb 11 09:21:51.667: INFO: (16) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname2/proxy/: bar (200; 7.169689ms)
Feb 11 09:21:51.667: INFO: (16) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname1/proxy/: foo (200; 7.281144ms)
Feb 11 09:21:51.667: INFO: (16) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">test<... (200; 7.506954ms)
Feb 11 09:21:51.672: INFO: (17) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">... (200; 4.328091ms)
Feb 11 09:21:51.673: INFO: (17) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:460/proxy/: tls baz (200; 5.261729ms)
Feb 11 09:21:51.673: INFO: (17) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/rewriteme">test</a> (200; 5.471762ms)
Feb 11 09:21:51.673: INFO: (17) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:462/proxy/: tls qux (200; 5.343303ms)
Feb 11 09:21:51.673: INFO: (17) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 5.701164ms)
Feb 11 09:21:51.674: INFO: (17) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname2/proxy/: tls qux (200; 6.105428ms)
Feb 11 09:21:51.674: INFO: (17) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname2/proxy/: bar (200; 6.314538ms)
Feb 11 09:21:51.674: INFO: (17) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/tlsrewritem... (200; 6.166218ms)
Feb 11 09:21:51.674: INFO: (17) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">test<... (200; 6.67458ms)
Feb 11 09:21:51.675: INFO: (17) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 6.933868ms)
Feb 11 09:21:51.675: INFO: (17) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 7.549971ms)
Feb 11 09:21:51.675: INFO: (17) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname1/proxy/: tls baz (200; 7.529562ms)
Feb 11 09:21:51.675: INFO: (17) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 7.600718ms)
Feb 11 09:21:51.675: INFO: (17) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname2/proxy/: bar (200; 7.523856ms)
Feb 11 09:21:51.676: INFO: (17) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname1/proxy/: foo (200; 7.750734ms)
Feb 11 09:21:51.676: INFO: (17) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname1/proxy/: foo (200; 8.080266ms)
Feb 11 09:21:51.680: INFO: (18) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 3.978471ms)
Feb 11 09:21:51.680: INFO: (18) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 4.368663ms)
Feb 11 09:21:51.681: INFO: (18) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:460/proxy/: tls baz (200; 5.03329ms)
Feb 11 09:21:51.681: INFO: (18) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/tlsrewritem... (200; 4.721082ms)
Feb 11 09:21:51.682: INFO: (18) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">test<... (200; 5.485964ms)
Feb 11 09:21:51.682: INFO: (18) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname2/proxy/: bar (200; 6.208699ms)
Feb 11 09:21:51.682: INFO: (18) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 6.261603ms)
Feb 11 09:21:51.682: INFO: (18) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">... (200; 6.02528ms)
Feb 11 09:21:51.683: INFO: (18) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 6.76929ms)
Feb 11 09:21:51.683: INFO: (18) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:462/proxy/: tls qux (200; 6.933429ms)
Feb 11 09:21:51.683: INFO: (18) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname2/proxy/: bar (200; 7.000564ms)
Feb 11 09:21:51.683: INFO: (18) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/rewriteme">test</a> (200; 6.836729ms)
Feb 11 09:21:51.683: INFO: (18) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname1/proxy/: foo (200; 7.421822ms)
Feb 11 09:21:51.684: INFO: (18) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname2/proxy/: tls qux (200; 7.652983ms)
Feb 11 09:21:51.684: INFO: (18) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname1/proxy/: foo (200; 8.247522ms)
Feb 11 09:21:51.684: INFO: (18) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname1/proxy/: tls baz (200; 8.444244ms)
Feb 11 09:21:51.689: INFO: (19) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 4.156232ms)
Feb 11 09:21:51.689: INFO: (19) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:162/proxy/: bar (200; 4.013481ms)
Feb 11 09:21:51.689: INFO: (19) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 4.377268ms)
Feb 11 09:21:51.689: INFO: (19) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:462/proxy/: tls qux (200; 4.026148ms)
Feb 11 09:21:51.689: INFO: (19) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">... (200; 4.342821ms)
Feb 11 09:21:51.689: INFO: (19) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:443/proxy/tlsrewritem... (200; 4.490785ms)
Feb 11 09:21:51.689: INFO: (19) /api/v1/namespaces/proxy-2447/pods/https:proxy-service-cj6hs-glbdt:460/proxy/: tls baz (200; 4.285068ms)
Feb 11 09:21:51.691: INFO: (19) /api/v1/namespaces/proxy-2447/pods/http:proxy-service-cj6hs-glbdt:160/proxy/: foo (200; 5.725376ms)
Feb 11 09:21:51.691: INFO: (19) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname2/proxy/: bar (200; 5.954581ms)
Feb 11 09:21:51.691: INFO: (19) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt/proxy/rewriteme">test</a> (200; 6.051301ms)
Feb 11 09:21:51.691: INFO: (19) /api/v1/namespaces/proxy-2447/services/http:proxy-service-cj6hs:portname1/proxy/: foo (200; 6.049243ms)
Feb 11 09:21:51.691: INFO: (19) /api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2447/pods/proxy-service-cj6hs-glbdt:1080/proxy/rewriteme">test<... (200; 6.830094ms)
Feb 11 09:21:51.692: INFO: (19) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname2/proxy/: tls qux (200; 6.796417ms)
Feb 11 09:21:51.692: INFO: (19) /api/v1/namespaces/proxy-2447/services/https:proxy-service-cj6hs:tlsportname1/proxy/: tls baz (200; 6.655229ms)
Feb 11 09:21:51.692: INFO: (19) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname2/proxy/: bar (200; 7.168463ms)
Feb 11 09:21:51.692: INFO: (19) /api/v1/namespaces/proxy-2447/services/proxy-service-cj6hs:portname1/proxy/: foo (200; 7.383167ms)
STEP: deleting ReplicationController proxy-service-cj6hs in namespace proxy-2447, will wait for the garbage collector to delete the pods
Feb 11 09:21:51.750: INFO: Deleting ReplicationController proxy-service-cj6hs took: 5.890781ms
Feb 11 09:21:52.150: INFO: Terminating ReplicationController proxy-service-cj6hs pods took: 400.233449ms
[AfterEach] version v1
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:21:58.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2447" for this suite.
Feb 11 09:22:04.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:22:04.724: INFO: namespace proxy-2447 deletion completed in 6.070144159s

â€¢ [SLOW TEST:22.293 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:22:04.725: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 11 09:22:07.271: INFO: Successfully updated pod "pod-update-activedeadlineseconds-2ffc3fc2-5ac4-4301-8f80-d862b864050d"
Feb 11 09:22:07.271: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2ffc3fc2-5ac4-4301-8f80-d862b864050d" in namespace "pods-1490" to be "terminated due to deadline exceeded"
Feb 11 09:22:07.273: INFO: Pod "pod-update-activedeadlineseconds-2ffc3fc2-5ac4-4301-8f80-d862b864050d": Phase="Running", Reason="", readiness=true. Elapsed: 2.435739ms
Feb 11 09:22:09.277: INFO: Pod "pod-update-activedeadlineseconds-2ffc3fc2-5ac4-4301-8f80-d862b864050d": Phase="Running", Reason="", readiness=true. Elapsed: 2.005804996s
Feb 11 09:22:11.280: INFO: Pod "pod-update-activedeadlineseconds-2ffc3fc2-5ac4-4301-8f80-d862b864050d": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.009178047s
Feb 11 09:22:11.280: INFO: Pod "pod-update-activedeadlineseconds-2ffc3fc2-5ac4-4301-8f80-d862b864050d" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:22:11.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1490" for this suite.
Feb 11 09:22:17.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:22:17.358: INFO: namespace pods-1490 deletion completed in 6.074674387s

â€¢ [SLOW TEST:12.633 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:22:17.358: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Feb 11 09:22:19.908: INFO: Successfully updated pod "annotationupdatef342b11a-d70c-4c1c-8253-44fedcb1f2e5"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:22:23.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6416" for this suite.
Feb 11 09:22:35.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:22:36.010: INFO: namespace projected-6416 deletion completed in 12.075411563s

â€¢ [SLOW TEST:18.652 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:22:36.010: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 11 09:22:36.040: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7509 /api/v1/namespaces/watch-7509/configmaps/e2e-watch-test-watch-closed 2b21e301-2269-4898-9922-528085ad89f0 22153 0 2020-02-11 09:22:36 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 11 09:22:36.041: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7509 /api/v1/namespaces/watch-7509/configmaps/e2e-watch-test-watch-closed 2b21e301-2269-4898-9922-528085ad89f0 22154 0 2020-02-11 09:22:36 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 11 09:22:36.051: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7509 /api/v1/namespaces/watch-7509/configmaps/e2e-watch-test-watch-closed 2b21e301-2269-4898-9922-528085ad89f0 22155 0 2020-02-11 09:22:36 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 11 09:22:36.051: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7509 /api/v1/namespaces/watch-7509/configmaps/e2e-watch-test-watch-closed 2b21e301-2269-4898-9922-528085ad89f0 22156 0 2020-02-11 09:22:36 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:22:36.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7509" for this suite.
Feb 11 09:22:42.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:22:42.126: INFO: namespace watch-7509 deletion completed in 6.071379051s

â€¢ [SLOW TEST:6.115 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:22:42.126: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:22:44.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1721" for this suite.
Feb 11 09:22:52.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:22:52.240: INFO: namespace containers-1721 deletion completed in 8.069877923s

â€¢ [SLOW TEST:10.114 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:22:52.240: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 11 09:22:52.268: INFO: Waiting up to 5m0s for pod "pod-1e63683b-f4c9-4579-ab08-7be70e78027a" in namespace "emptydir-1612" to be "success or failure"
Feb 11 09:22:52.270: INFO: Pod "pod-1e63683b-f4c9-4579-ab08-7be70e78027a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.908006ms
Feb 11 09:22:54.274: INFO: Pod "pod-1e63683b-f4c9-4579-ab08-7be70e78027a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005834197s
STEP: Saw pod success
Feb 11 09:22:54.274: INFO: Pod "pod-1e63683b-f4c9-4579-ab08-7be70e78027a" satisfied condition "success or failure"
Feb 11 09:22:54.277: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-1e63683b-f4c9-4579-ab08-7be70e78027a container test-container: <nil>
STEP: delete the pod
Feb 11 09:22:54.291: INFO: Waiting for pod pod-1e63683b-f4c9-4579-ab08-7be70e78027a to disappear
Feb 11 09:22:54.293: INFO: Pod pod-1e63683b-f4c9-4579-ab08-7be70e78027a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:22:54.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1612" for this suite.
Feb 11 09:23:00.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:23:00.370: INFO: namespace emptydir-1612 deletion completed in 6.074171853s

â€¢ [SLOW TEST:8.130 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:23:00.371: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 11 09:23:00.398: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b76fab45-044f-4fd8-a70e-979c74135f15" in namespace "downward-api-8687" to be "success or failure"
Feb 11 09:23:00.400: INFO: Pod "downwardapi-volume-b76fab45-044f-4fd8-a70e-979c74135f15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.102116ms
Feb 11 09:23:02.404: INFO: Pod "downwardapi-volume-b76fab45-044f-4fd8-a70e-979c74135f15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005803715s
STEP: Saw pod success
Feb 11 09:23:02.404: INFO: Pod "downwardapi-volume-b76fab45-044f-4fd8-a70e-979c74135f15" satisfied condition "success or failure"
Feb 11 09:23:02.406: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod downwardapi-volume-b76fab45-044f-4fd8-a70e-979c74135f15 container client-container: <nil>
STEP: delete the pod
Feb 11 09:23:02.421: INFO: Waiting for pod downwardapi-volume-b76fab45-044f-4fd8-a70e-979c74135f15 to disappear
Feb 11 09:23:02.423: INFO: Pod downwardapi-volume-b76fab45-044f-4fd8-a70e-979c74135f15 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:23:02.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8687" for this suite.
Feb 11 09:23:08.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:23:08.500: INFO: namespace downward-api-8687 deletion completed in 6.074001224s

â€¢ [SLOW TEST:8.130 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:23:08.501: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-abf018fe-f196-4551-895d-5c12dba10c00
STEP: Creating a pod to test consume configMaps
Feb 11 09:23:08.531: INFO: Waiting up to 5m0s for pod "pod-configmaps-67ae05a1-0402-4d60-9d55-bb2126f15924" in namespace "configmap-7431" to be "success or failure"
Feb 11 09:23:08.533: INFO: Pod "pod-configmaps-67ae05a1-0402-4d60-9d55-bb2126f15924": Phase="Pending", Reason="", readiness=false. Elapsed: 1.820062ms
Feb 11 09:23:10.536: INFO: Pod "pod-configmaps-67ae05a1-0402-4d60-9d55-bb2126f15924": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005176135s
STEP: Saw pod success
Feb 11 09:23:10.536: INFO: Pod "pod-configmaps-67ae05a1-0402-4d60-9d55-bb2126f15924" satisfied condition "success or failure"
Feb 11 09:23:10.538: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-configmaps-67ae05a1-0402-4d60-9d55-bb2126f15924 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 11 09:23:10.552: INFO: Waiting for pod pod-configmaps-67ae05a1-0402-4d60-9d55-bb2126f15924 to disappear
Feb 11 09:23:10.554: INFO: Pod pod-configmaps-67ae05a1-0402-4d60-9d55-bb2126f15924 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:23:10.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7431" for this suite.
Feb 11 09:23:16.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:23:16.633: INFO: namespace configmap-7431 deletion completed in 6.076071801s

â€¢ [SLOW TEST:8.133 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:23:16.633: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 11 09:23:19.178: INFO: Successfully updated pod "pod-update-7b67f4bc-17c7-4085-9d1e-6e8890c2661b"
STEP: verifying the updated pod is in kubernetes
Feb 11 09:23:19.184: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:23:19.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3771" for this suite.
Feb 11 09:23:31.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:23:31.259: INFO: namespace pods-3771 deletion completed in 12.072811867s

â€¢ [SLOW TEST:14.626 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:23:31.260: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 11 09:23:31.505: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 11 09:23:34.520: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:23:34.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6241" for this suite.
Feb 11 09:23:40.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:23:40.649: INFO: namespace webhook-6241 deletion completed in 6.076015599s
STEP: Destroying namespace "webhook-6241-markers" for this suite.
Feb 11 09:23:46.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:23:46.725: INFO: namespace webhook-6241-markers deletion completed in 6.076857786s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:15.477 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:23:46.736: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Feb 11 09:23:46.758: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 09:23:53.561: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:24:07.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-119" for this suite.
Feb 11 09:24:13.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:24:13.590: INFO: namespace crd-publish-openapi-119 deletion completed in 6.073360517s

â€¢ [SLOW TEST:26.853 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:24:13.590: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 09:24:13.628: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 11 09:24:18.632: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 11 09:24:18.632: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Feb 11 09:24:18.646: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5888 /apis/apps/v1/namespaces/deployment-5888/deployments/test-cleanup-deployment ccb4fa14-5dc7-4f85-9611-af58f52aa9a9 22665 1 2020-02-11 09:24:18 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004b04e18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Feb 11 09:24:18.649: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-5888 /apis/apps/v1/namespaces/deployment-5888/replicasets/test-cleanup-deployment-65db99849b 74681132-b5a3-4cef-b695-d0aa23d4dfed 22667 1 2020-02-11 09:24:18 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment ccb4fa14-5dc7-4f85-9611-af58f52aa9a9 0xc004b05347 0xc004b05348}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004b053b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 11 09:24:18.649: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Feb 11 09:24:18.649: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-5888 /apis/apps/v1/namespaces/deployment-5888/replicasets/test-cleanup-controller f2003598-02af-4627-b9f5-ad9404e38796 22666 1 2020-02-11 09:24:13 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment ccb4fa14-5dc7-4f85-9611-af58f52aa9a9 0xc004b05247 0xc004b05248}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004b052c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 11 09:24:18.654: INFO: Pod "test-cleanup-controller-8p8s9" is available:
&Pod{ObjectMeta:{test-cleanup-controller-8p8s9 test-cleanup-controller- deployment-5888 /api/v1/namespaces/deployment-5888/pods/test-cleanup-controller-8p8s9 8203c652-27f3-453b-a10d-954a5f111dc6 22654 0 2020-02-11 09:24:13 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller f2003598-02af-4627-b9f5-ad9404e38796 0xc004b059d7 0xc004b059d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5rjv9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5rjv9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5rjv9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:24:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:24:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:24:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:24:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.18,PodIP:172.22.2.93,StartTime:2020-02-11 09:24:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-11 09:24:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://97eebe7ac27bc8a82cfe8ab97d03530704fef3ceadb1710b89cf5d0ba8a8aae3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.22.2.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 11 09:24:18.654: INFO: Pod "test-cleanup-deployment-65db99849b-z5gqb" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-65db99849b-z5gqb test-cleanup-deployment-65db99849b- deployment-5888 /api/v1/namespaces/deployment-5888/pods/test-cleanup-deployment-65db99849b-z5gqb b3afe472-f355-4ae3-bc5f-4cc63ad7b3f0 22668 0 2020-02-11 09:24:18 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-65db99849b 74681132-b5a3-4cef-b695-d0aa23d4dfed 0xc004b05b67 0xc004b05b68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5rjv9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5rjv9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5rjv9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:24:18.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5888" for this suite.
Feb 11 09:24:24.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:24:24.733: INFO: namespace deployment-5888 deletion completed in 6.074321862s

â€¢ [SLOW TEST:11.144 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:24:24.734: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Feb 11 09:24:24.761: INFO: Waiting up to 5m0s for pod "downward-api-02483c94-0e0d-442b-95a9-748b3a2433f6" in namespace "downward-api-9204" to be "success or failure"
Feb 11 09:24:24.763: INFO: Pod "downward-api-02483c94-0e0d-442b-95a9-748b3a2433f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.391682ms
Feb 11 09:24:26.767: INFO: Pod "downward-api-02483c94-0e0d-442b-95a9-748b3a2433f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005665808s
STEP: Saw pod success
Feb 11 09:24:26.767: INFO: Pod "downward-api-02483c94-0e0d-442b-95a9-748b3a2433f6" satisfied condition "success or failure"
Feb 11 09:24:26.769: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod downward-api-02483c94-0e0d-442b-95a9-748b3a2433f6 container dapi-container: <nil>
STEP: delete the pod
Feb 11 09:24:26.784: INFO: Waiting for pod downward-api-02483c94-0e0d-442b-95a9-748b3a2433f6 to disappear
Feb 11 09:24:26.785: INFO: Pod downward-api-02483c94-0e0d-442b-95a9-748b3a2433f6 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:24:26.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9204" for this suite.
Feb 11 09:24:32.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:24:32.870: INFO: namespace downward-api-9204 deletion completed in 6.08171035s

â€¢ [SLOW TEST:8.136 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:24:32.870: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-567
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 11 09:24:32.893: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 11 09:24:52.955: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.22.2.97:8080/dial?request=hostName&protocol=udp&host=172.22.2.96&port=8081&tries=1'] Namespace:pod-network-test-567 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 11 09:24:52.955: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 09:24:53.085: INFO: Waiting for endpoints: map[]
Feb 11 09:24:53.088: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.22.2.97:8080/dial?request=hostName&protocol=udp&host=172.22.2.176&port=8081&tries=1'] Namespace:pod-network-test-567 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 11 09:24:53.088: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 09:24:53.238: INFO: Waiting for endpoints: map[]
Feb 11 09:24:53.241: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.22.2.97:8080/dial?request=hostName&protocol=udp&host=172.22.1.187&port=8081&tries=1'] Namespace:pod-network-test-567 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 11 09:24:53.241: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 09:24:53.401: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:24:53.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-567" for this suite.
Feb 11 09:25:05.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:25:05.479: INFO: namespace pod-network-test-567 deletion completed in 12.073733562s

â€¢ [SLOW TEST:32.608 seconds]
[sig-network] Networking
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:25:05.479: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 09:25:05.502: INFO: Creating deployment "test-recreate-deployment"
Feb 11 09:25:05.505: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 11 09:25:05.512: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb 11 09:25:07.519: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 11 09:25:07.521: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 11 09:25:07.527: INFO: Updating deployment test-recreate-deployment
Feb 11 09:25:07.527: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Feb 11 09:25:07.564: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-417 /apis/apps/v1/namespaces/deployment-417/deployments/test-recreate-deployment 5b091a5d-fd53-43aa-8d80-32a4bb01cd0a 22984 2 2020-02-11 09:25:05 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001732858 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-02-11 09:25:07 +0000 UTC,LastTransitionTime:2020-02-11 09:25:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-02-11 09:25:07 +0000 UTC,LastTransitionTime:2020-02-11 09:25:05 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Feb 11 09:25:07.574: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-417 /apis/apps/v1/namespaces/deployment-417/replicasets/test-recreate-deployment-5f94c574ff b7032036-d8b8-47a3-b13c-642f79621694 22981 1 2020-02-11 09:25:07 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 5b091a5d-fd53-43aa-8d80-32a4bb01cd0a 0xc001732c67 0xc001732c68}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001732cc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 11 09:25:07.574: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 11 09:25:07.574: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-417 /apis/apps/v1/namespaces/deployment-417/replicasets/test-recreate-deployment-68fc85c7bb 3e061538-525b-41a2-a374-d7b4aabb6271 22972 2 2020-02-11 09:25:05 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 5b091a5d-fd53-43aa-8d80-32a4bb01cd0a 0xc001732d37 0xc001732d38}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001732d98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 11 09:25:07.576: INFO: Pod "test-recreate-deployment-5f94c574ff-nwdhb" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-nwdhb test-recreate-deployment-5f94c574ff- deployment-417 /api/v1/namespaces/deployment-417/pods/test-recreate-deployment-5f94c574ff-nwdhb 492724ed-286c-4241-a6a4-87aba165e7d5 22985 0 2020-02-11 09:25:07 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff b7032036-d8b8-47a3-b13c-642f79621694 0xc001733227 0xc001733228}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rkqv4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rkqv4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rkqv4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:25:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:25:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:25:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:25:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.18,PodIP:,StartTime:2020-02-11 09:25:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:25:07.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-417" for this suite.
Feb 11 09:25:13.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:25:13.652: INFO: namespace deployment-417 deletion completed in 6.072718756s

â€¢ [SLOW TEST:8.173 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:25:13.653: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:25:24.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2525" for this suite.
Feb 11 09:25:30.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:25:30.792: INFO: namespace resourcequota-2525 deletion completed in 6.08202859s

â€¢ [SLOW TEST:17.140 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:25:30.792: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4200
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4200
STEP: Creating statefulset with conflicting port in namespace statefulset-4200
STEP: Waiting until pod test-pod will start running in namespace statefulset-4200
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4200
Feb 11 09:25:34.843: INFO: Observed stateful pod in namespace: statefulset-4200, name: ss-0, uid: e1fdda4a-91d2-413c-adb3-c066dc967ebc, status phase: Pending. Waiting for statefulset controller to delete.
Feb 11 09:25:35.037: INFO: Observed stateful pod in namespace: statefulset-4200, name: ss-0, uid: e1fdda4a-91d2-413c-adb3-c066dc967ebc, status phase: Failed. Waiting for statefulset controller to delete.
Feb 11 09:25:35.042: INFO: Observed stateful pod in namespace: statefulset-4200, name: ss-0, uid: e1fdda4a-91d2-413c-adb3-c066dc967ebc, status phase: Failed. Waiting for statefulset controller to delete.
Feb 11 09:25:35.045: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4200
STEP: Removing pod with conflicting port in namespace statefulset-4200
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4200 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Feb 11 09:25:39.064: INFO: Deleting all statefulset in ns statefulset-4200
Feb 11 09:25:39.067: INFO: Scaling statefulset ss to 0
Feb 11 09:25:49.079: INFO: Waiting for statefulset status.replicas updated to 0
Feb 11 09:25:49.082: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:25:49.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4200" for this suite.
Feb 11 09:25:55.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:25:55.167: INFO: namespace statefulset-4200 deletion completed in 6.071699136s

â€¢ [SLOW TEST:24.375 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:25:55.168: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:25:57.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8981" for this suite.
Feb 11 09:26:41.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:26:41.292: INFO: namespace kubelet-test-8981 deletion completed in 44.072813151s

â€¢ [SLOW TEST:46.124 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:26:41.292: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 11 09:26:41.585: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 11 09:26:44.600: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Feb 11 09:26:46.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 attach --namespace=webhook-3771 to-be-attached-pod -i -c=container1'
Feb 11 09:26:46.708: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:26:46.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3771" for this suite.
Feb 11 09:26:58.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:26:58.790: INFO: namespace webhook-3771 deletion completed in 12.071669712s
STEP: Destroying namespace "webhook-3771-markers" for this suite.
Feb 11 09:27:04.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:27:04.862: INFO: namespace webhook-3771-markers deletion completed in 6.071776065s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:23.583 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:27:04.876: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-f514d017-ad30-43b1-aeb0-297b24c94b93
STEP: Creating a pod to test consume secrets
Feb 11 09:27:04.904: INFO: Waiting up to 5m0s for pod "pod-secrets-ccd87dbc-c5b8-4ec3-8cf0-c42d6e6487ef" in namespace "secrets-9218" to be "success or failure"
Feb 11 09:27:04.906: INFO: Pod "pod-secrets-ccd87dbc-c5b8-4ec3-8cf0-c42d6e6487ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052835ms
Feb 11 09:27:06.909: INFO: Pod "pod-secrets-ccd87dbc-c5b8-4ec3-8cf0-c42d6e6487ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005304868s
STEP: Saw pod success
Feb 11 09:27:06.909: INFO: Pod "pod-secrets-ccd87dbc-c5b8-4ec3-8cf0-c42d6e6487ef" satisfied condition "success or failure"
Feb 11 09:27:06.912: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-secrets-ccd87dbc-c5b8-4ec3-8cf0-c42d6e6487ef container secret-volume-test: <nil>
STEP: delete the pod
Feb 11 09:27:06.929: INFO: Waiting for pod pod-secrets-ccd87dbc-c5b8-4ec3-8cf0-c42d6e6487ef to disappear
Feb 11 09:27:06.932: INFO: Pod pod-secrets-ccd87dbc-c5b8-4ec3-8cf0-c42d6e6487ef no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:27:06.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9218" for this suite.
Feb 11 09:27:12.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:27:13.009: INFO: namespace secrets-9218 deletion completed in 6.074589888s

â€¢ [SLOW TEST:8.133 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:27:13.010: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Feb 11 09:27:15.561: INFO: Successfully updated pod "labelsupdate41a2648c-baf7-4707-883c-53d79b7bd469"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:27:17.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-896" for this suite.
Feb 11 09:27:29.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:27:29.661: INFO: namespace projected-896 deletion completed in 12.076378301s

â€¢ [SLOW TEST:16.651 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:27:29.661: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-50654c62-d212-4f4e-b8a0-f09622bdde0f
STEP: Creating a pod to test consume secrets
Feb 11 09:27:29.690: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6433f2f4-8be3-4b6d-af97-62e92e42aa5b" in namespace "projected-4091" to be "success or failure"
Feb 11 09:27:29.692: INFO: Pod "pod-projected-secrets-6433f2f4-8be3-4b6d-af97-62e92e42aa5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.248933ms
Feb 11 09:27:31.696: INFO: Pod "pod-projected-secrets-6433f2f4-8be3-4b6d-af97-62e92e42aa5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005875026s
STEP: Saw pod success
Feb 11 09:27:31.696: INFO: Pod "pod-projected-secrets-6433f2f4-8be3-4b6d-af97-62e92e42aa5b" satisfied condition "success or failure"
Feb 11 09:27:31.698: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-projected-secrets-6433f2f4-8be3-4b6d-af97-62e92e42aa5b container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 11 09:27:31.713: INFO: Waiting for pod pod-projected-secrets-6433f2f4-8be3-4b6d-af97-62e92e42aa5b to disappear
Feb 11 09:27:31.716: INFO: Pod pod-projected-secrets-6433f2f4-8be3-4b6d-af97-62e92e42aa5b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:27:31.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4091" for this suite.
Feb 11 09:27:37.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:27:37.791: INFO: namespace projected-4091 deletion completed in 6.071624398s

â€¢ [SLOW TEST:8.130 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:27:37.791: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 11 09:27:37.818: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0affef54-cc7d-44e3-a9f9-cb850ca1e97c" in namespace "projected-6603" to be "success or failure"
Feb 11 09:27:37.820: INFO: Pod "downwardapi-volume-0affef54-cc7d-44e3-a9f9-cb850ca1e97c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008682ms
Feb 11 09:27:39.824: INFO: Pod "downwardapi-volume-0affef54-cc7d-44e3-a9f9-cb850ca1e97c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005594666s
STEP: Saw pod success
Feb 11 09:27:39.824: INFO: Pod "downwardapi-volume-0affef54-cc7d-44e3-a9f9-cb850ca1e97c" satisfied condition "success or failure"
Feb 11 09:27:39.826: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod downwardapi-volume-0affef54-cc7d-44e3-a9f9-cb850ca1e97c container client-container: <nil>
STEP: delete the pod
Feb 11 09:27:39.840: INFO: Waiting for pod downwardapi-volume-0affef54-cc7d-44e3-a9f9-cb850ca1e97c to disappear
Feb 11 09:27:39.842: INFO: Pod downwardapi-volume-0affef54-cc7d-44e3-a9f9-cb850ca1e97c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:27:39.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6603" for this suite.
Feb 11 09:27:45.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:27:45.917: INFO: namespace projected-6603 deletion completed in 6.071647796s

â€¢ [SLOW TEST:8.126 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:27:45.917: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 11 09:27:46.722: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 11 09:27:49.736: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 09:27:49.739: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9863-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:27:55.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8824" for this suite.
Feb 11 09:28:01.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:28:01.912: INFO: namespace webhook-8824 deletion completed in 6.073288074s
STEP: Destroying namespace "webhook-8824-markers" for this suite.
Feb 11 09:28:07.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:28:07.983: INFO: namespace webhook-8824-markers deletion completed in 6.071348842s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:22.077 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:28:07.995: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 11 09:28:12.054: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 11 09:28:12.057: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 11 09:28:14.057: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 11 09:28:14.060: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 11 09:28:16.057: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 11 09:28:16.060: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:28:16.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5990" for this suite.
Feb 11 09:28:28.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:28:28.142: INFO: namespace container-lifecycle-hook-5990 deletion completed in 12.07830628s

â€¢ [SLOW TEST:20.148 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:28:28.142: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-8edfe9bc-2050-439d-9b7a-91e952f6cdcb
STEP: Creating secret with name secret-projected-all-test-volume-cffca06c-1ddb-4e9f-8fdd-01e92a98bfa3
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 11 09:28:28.176: INFO: Waiting up to 5m0s for pod "projected-volume-fae9d358-6c52-4389-bfd4-e564ef77669e" in namespace "projected-3224" to be "success or failure"
Feb 11 09:28:28.180: INFO: Pod "projected-volume-fae9d358-6c52-4389-bfd4-e564ef77669e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.391564ms
Feb 11 09:28:30.183: INFO: Pod "projected-volume-fae9d358-6c52-4389-bfd4-e564ef77669e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007149904s
STEP: Saw pod success
Feb 11 09:28:30.183: INFO: Pod "projected-volume-fae9d358-6c52-4389-bfd4-e564ef77669e" satisfied condition "success or failure"
Feb 11 09:28:30.186: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod projected-volume-fae9d358-6c52-4389-bfd4-e564ef77669e container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 11 09:28:30.202: INFO: Waiting for pod projected-volume-fae9d358-6c52-4389-bfd4-e564ef77669e to disappear
Feb 11 09:28:30.204: INFO: Pod projected-volume-fae9d358-6c52-4389-bfd4-e564ef77669e no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:28:30.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3224" for this suite.
Feb 11 09:28:36.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:28:36.279: INFO: namespace projected-3224 deletion completed in 6.072820543s

â€¢ [SLOW TEST:8.137 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:28:36.280: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 11 09:28:36.307: INFO: Waiting up to 5m0s for pod "downwardapi-volume-331a9cc2-4a92-49bf-99a5-2abe931586b6" in namespace "projected-1161" to be "success or failure"
Feb 11 09:28:36.309: INFO: Pod "downwardapi-volume-331a9cc2-4a92-49bf-99a5-2abe931586b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.883579ms
Feb 11 09:28:38.313: INFO: Pod "downwardapi-volume-331a9cc2-4a92-49bf-99a5-2abe931586b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005917906s
STEP: Saw pod success
Feb 11 09:28:38.313: INFO: Pod "downwardapi-volume-331a9cc2-4a92-49bf-99a5-2abe931586b6" satisfied condition "success or failure"
Feb 11 09:28:38.315: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod downwardapi-volume-331a9cc2-4a92-49bf-99a5-2abe931586b6 container client-container: <nil>
STEP: delete the pod
Feb 11 09:28:38.329: INFO: Waiting for pod downwardapi-volume-331a9cc2-4a92-49bf-99a5-2abe931586b6 to disappear
Feb 11 09:28:38.331: INFO: Pod downwardapi-volume-331a9cc2-4a92-49bf-99a5-2abe931586b6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:28:38.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1161" for this suite.
Feb 11 09:28:44.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:28:44.407: INFO: namespace projected-1161 deletion completed in 6.072957358s

â€¢ [SLOW TEST:8.128 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:28:44.407: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6479.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6479.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6479.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6479.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 11 09:28:46.450: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local from pod dns-6479/dns-test-9189dcf7-08f4-4539-a4c0-b493dc9c7ec2: the server could not find the requested resource (get pods dns-test-9189dcf7-08f4-4539-a4c0-b493dc9c7ec2)
Feb 11 09:28:46.453: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local from pod dns-6479/dns-test-9189dcf7-08f4-4539-a4c0-b493dc9c7ec2: the server could not find the requested resource (get pods dns-test-9189dcf7-08f4-4539-a4c0-b493dc9c7ec2)
Feb 11 09:28:46.455: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6479.svc.cluster.local from pod dns-6479/dns-test-9189dcf7-08f4-4539-a4c0-b493dc9c7ec2: the server could not find the requested resource (get pods dns-test-9189dcf7-08f4-4539-a4c0-b493dc9c7ec2)
Feb 11 09:28:46.460: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6479.svc.cluster.local from pod dns-6479/dns-test-9189dcf7-08f4-4539-a4c0-b493dc9c7ec2: the server could not find the requested resource (get pods dns-test-9189dcf7-08f4-4539-a4c0-b493dc9c7ec2)
Feb 11 09:28:46.469: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local from pod dns-6479/dns-test-9189dcf7-08f4-4539-a4c0-b493dc9c7ec2: the server could not find the requested resource (get pods dns-test-9189dcf7-08f4-4539-a4c0-b493dc9c7ec2)
Feb 11 09:28:46.472: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local from pod dns-6479/dns-test-9189dcf7-08f4-4539-a4c0-b493dc9c7ec2: the server could not find the requested resource (get pods dns-test-9189dcf7-08f4-4539-a4c0-b493dc9c7ec2)
Feb 11 09:28:46.475: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6479.svc.cluster.local from pod dns-6479/dns-test-9189dcf7-08f4-4539-a4c0-b493dc9c7ec2: the server could not find the requested resource (get pods dns-test-9189dcf7-08f4-4539-a4c0-b493dc9c7ec2)
Feb 11 09:28:46.478: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6479.svc.cluster.local from pod dns-6479/dns-test-9189dcf7-08f4-4539-a4c0-b493dc9c7ec2: the server could not find the requested resource (get pods dns-test-9189dcf7-08f4-4539-a4c0-b493dc9c7ec2)
Feb 11 09:28:46.483: INFO: Lookups using dns-6479/dns-test-9189dcf7-08f4-4539-a4c0-b493dc9c7ec2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6479.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6479.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local jessie_udp@dns-test-service-2.dns-6479.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6479.svc.cluster.local]

Feb 11 09:28:51.518: INFO: DNS probes using dns-6479/dns-test-9189dcf7-08f4-4539-a4c0-b493dc9c7ec2 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:28:51.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6479" for this suite.
Feb 11 09:28:57.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:28:57.614: INFO: namespace dns-6479 deletion completed in 6.073111283s

â€¢ [SLOW TEST:13.207 seconds]
[sig-network] DNS
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:28:57.614: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Feb 11 09:28:57.640: INFO: Waiting up to 5m0s for pod "client-containers-2f1ed951-957b-46d9-ad7c-64ec52c0ac96" in namespace "containers-4604" to be "success or failure"
Feb 11 09:28:57.643: INFO: Pod "client-containers-2f1ed951-957b-46d9-ad7c-64ec52c0ac96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.276647ms
Feb 11 09:28:59.645: INFO: Pod "client-containers-2f1ed951-957b-46d9-ad7c-64ec52c0ac96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005118632s
STEP: Saw pod success
Feb 11 09:28:59.645: INFO: Pod "client-containers-2f1ed951-957b-46d9-ad7c-64ec52c0ac96" satisfied condition "success or failure"
Feb 11 09:28:59.648: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod client-containers-2f1ed951-957b-46d9-ad7c-64ec52c0ac96 container test-container: <nil>
STEP: delete the pod
Feb 11 09:28:59.662: INFO: Waiting for pod client-containers-2f1ed951-957b-46d9-ad7c-64ec52c0ac96 to disappear
Feb 11 09:28:59.664: INFO: Pod client-containers-2f1ed951-957b-46d9-ad7c-64ec52c0ac96 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:28:59.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4604" for this suite.
Feb 11 09:29:05.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:29:05.739: INFO: namespace containers-4604 deletion completed in 6.072630651s

â€¢ [SLOW TEST:8.125 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:29:05.740: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 09:29:05.764: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Feb 11 09:29:13.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 --namespace=crd-publish-openapi-7243 create -f -'
Feb 11 09:29:13.978: INFO: stderr: ""
Feb 11 09:29:13.978: INFO: stdout: "e2e-test-crd-publish-openapi-8930-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 11 09:29:13.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 --namespace=crd-publish-openapi-7243 delete e2e-test-crd-publish-openapi-8930-crds test-foo'
Feb 11 09:29:14.080: INFO: stderr: ""
Feb 11 09:29:14.080: INFO: stdout: "e2e-test-crd-publish-openapi-8930-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Feb 11 09:29:14.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 --namespace=crd-publish-openapi-7243 apply -f -'
Feb 11 09:29:14.314: INFO: stderr: ""
Feb 11 09:29:14.314: INFO: stdout: "e2e-test-crd-publish-openapi-8930-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 11 09:29:14.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 --namespace=crd-publish-openapi-7243 delete e2e-test-crd-publish-openapi-8930-crds test-foo'
Feb 11 09:29:14.385: INFO: stderr: ""
Feb 11 09:29:14.385: INFO: stdout: "e2e-test-crd-publish-openapi-8930-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Feb 11 09:29:14.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 --namespace=crd-publish-openapi-7243 create -f -'
Feb 11 09:29:14.572: INFO: rc: 1
Feb 11 09:29:14.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 --namespace=crd-publish-openapi-7243 apply -f -'
Feb 11 09:29:14.767: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Feb 11 09:29:14.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 --namespace=crd-publish-openapi-7243 create -f -'
Feb 11 09:29:14.955: INFO: rc: 1
Feb 11 09:29:14.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 --namespace=crd-publish-openapi-7243 apply -f -'
Feb 11 09:29:15.145: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Feb 11 09:29:15.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 explain e2e-test-crd-publish-openapi-8930-crds'
Feb 11 09:29:15.346: INFO: stderr: ""
Feb 11 09:29:15.346: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8930-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Feb 11 09:29:15.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 explain e2e-test-crd-publish-openapi-8930-crds.metadata'
Feb 11 09:29:15.540: INFO: stderr: ""
Feb 11 09:29:15.540: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8930-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Feb 11 09:29:15.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 explain e2e-test-crd-publish-openapi-8930-crds.spec'
Feb 11 09:29:15.733: INFO: stderr: ""
Feb 11 09:29:15.733: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8930-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Feb 11 09:29:15.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 explain e2e-test-crd-publish-openapi-8930-crds.spec.bars'
Feb 11 09:29:15.927: INFO: stderr: ""
Feb 11 09:29:15.927: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8930-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Feb 11 09:29:15.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 explain e2e-test-crd-publish-openapi-8930-crds.spec.bars2'
Feb 11 09:29:16.120: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:29:18.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7243" for this suite.
Feb 11 09:29:24.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:29:24.965: INFO: namespace crd-publish-openapi-7243 deletion completed in 6.080584311s

â€¢ [SLOW TEST:19.226 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:29:24.966: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Feb 11 09:29:24.994: INFO: Waiting up to 5m0s for pod "downward-api-1da68298-9419-4e0d-922e-3e28910bbcdd" in namespace "downward-api-4285" to be "success or failure"
Feb 11 09:29:24.997: INFO: Pod "downward-api-1da68298-9419-4e0d-922e-3e28910bbcdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.622385ms
Feb 11 09:29:27.000: INFO: Pod "downward-api-1da68298-9419-4e0d-922e-3e28910bbcdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006122385s
STEP: Saw pod success
Feb 11 09:29:27.000: INFO: Pod "downward-api-1da68298-9419-4e0d-922e-3e28910bbcdd" satisfied condition "success or failure"
Feb 11 09:29:27.002: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod downward-api-1da68298-9419-4e0d-922e-3e28910bbcdd container dapi-container: <nil>
STEP: delete the pod
Feb 11 09:29:27.017: INFO: Waiting for pod downward-api-1da68298-9419-4e0d-922e-3e28910bbcdd to disappear
Feb 11 09:29:27.019: INFO: Pod downward-api-1da68298-9419-4e0d-922e-3e28910bbcdd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:29:27.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4285" for this suite.
Feb 11 09:29:33.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:29:33.102: INFO: namespace downward-api-4285 deletion completed in 6.080584348s

â€¢ [SLOW TEST:8.137 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:29:33.103: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 09:29:35.166: INFO: Waiting up to 5m0s for pod "client-envvars-13e68887-9160-4b15-8aee-9252dcd124e5" in namespace "pods-2746" to be "success or failure"
Feb 11 09:29:35.168: INFO: Pod "client-envvars-13e68887-9160-4b15-8aee-9252dcd124e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075921ms
Feb 11 09:29:37.172: INFO: Pod "client-envvars-13e68887-9160-4b15-8aee-9252dcd124e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005670361s
STEP: Saw pod success
Feb 11 09:29:37.172: INFO: Pod "client-envvars-13e68887-9160-4b15-8aee-9252dcd124e5" satisfied condition "success or failure"
Feb 11 09:29:37.174: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod client-envvars-13e68887-9160-4b15-8aee-9252dcd124e5 container env3cont: <nil>
STEP: delete the pod
Feb 11 09:29:37.189: INFO: Waiting for pod client-envvars-13e68887-9160-4b15-8aee-9252dcd124e5 to disappear
Feb 11 09:29:37.192: INFO: Pod client-envvars-13e68887-9160-4b15-8aee-9252dcd124e5 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:29:37.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2746" for this suite.
Feb 11 09:30:05.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:30:05.265: INFO: namespace pods-2746 deletion completed in 28.070641141s

â€¢ [SLOW TEST:32.163 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:30:05.266: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Feb 11 09:30:05.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 create -f - --namespace=kubectl-5214'
Feb 11 09:30:05.493: INFO: stderr: ""
Feb 11 09:30:05.493: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 11 09:30:05.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5214'
Feb 11 09:30:05.565: INFO: stderr: ""
Feb 11 09:30:05.565: INFO: stdout: "update-demo-nautilus-5thlm update-demo-nautilus-tc2zg "
Feb 11 09:30:05.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-nautilus-5thlm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5214'
Feb 11 09:30:05.631: INFO: stderr: ""
Feb 11 09:30:05.631: INFO: stdout: ""
Feb 11 09:30:05.631: INFO: update-demo-nautilus-5thlm is created but not running
Feb 11 09:30:10.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5214'
Feb 11 09:30:10.701: INFO: stderr: ""
Feb 11 09:30:10.701: INFO: stdout: "update-demo-nautilus-5thlm update-demo-nautilus-tc2zg "
Feb 11 09:30:10.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-nautilus-5thlm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5214'
Feb 11 09:30:10.769: INFO: stderr: ""
Feb 11 09:30:10.769: INFO: stdout: "true"
Feb 11 09:30:10.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-nautilus-5thlm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5214'
Feb 11 09:30:10.836: INFO: stderr: ""
Feb 11 09:30:10.836: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 11 09:30:10.836: INFO: validating pod update-demo-nautilus-5thlm
Feb 11 09:30:10.840: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 11 09:30:10.840: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 11 09:30:10.840: INFO: update-demo-nautilus-5thlm is verified up and running
Feb 11 09:30:10.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-nautilus-tc2zg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5214'
Feb 11 09:30:10.907: INFO: stderr: ""
Feb 11 09:30:10.907: INFO: stdout: "true"
Feb 11 09:30:10.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-nautilus-tc2zg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5214'
Feb 11 09:30:10.972: INFO: stderr: ""
Feb 11 09:30:10.972: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 11 09:30:10.972: INFO: validating pod update-demo-nautilus-tc2zg
Feb 11 09:30:10.977: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 11 09:30:10.977: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 11 09:30:10.977: INFO: update-demo-nautilus-tc2zg is verified up and running
STEP: using delete to clean up resources
Feb 11 09:30:10.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 delete --grace-period=0 --force -f - --namespace=kubectl-5214'
Feb 11 09:30:11.049: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 11 09:30:11.049: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 11 09:30:11.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5214'
Feb 11 09:30:11.121: INFO: stderr: "No resources found in kubectl-5214 namespace.\n"
Feb 11 09:30:11.121: INFO: stdout: ""
Feb 11 09:30:11.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods -l name=update-demo --namespace=kubectl-5214 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 11 09:30:11.189: INFO: stderr: ""
Feb 11 09:30:11.189: INFO: stdout: "update-demo-nautilus-5thlm\nupdate-demo-nautilus-tc2zg\n"
Feb 11 09:30:11.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5214'
Feb 11 09:30:11.763: INFO: stderr: "No resources found in kubectl-5214 namespace.\n"
Feb 11 09:30:11.763: INFO: stdout: ""
Feb 11 09:30:11.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods -l name=update-demo --namespace=kubectl-5214 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 11 09:30:11.832: INFO: stderr: ""
Feb 11 09:30:11.832: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:30:11.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5214" for this suite.
Feb 11 09:30:23.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:30:23.911: INFO: namespace kubectl-5214 deletion completed in 12.075488829s

â€¢ [SLOW TEST:18.646 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:30:23.912: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:30:23.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8719" for this suite.
Feb 11 09:30:29.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:30:30.014: INFO: namespace custom-resource-definition-8719 deletion completed in 6.074955137s

â€¢ [SLOW TEST:6.102 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:30:30.014: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Feb 11 09:30:32.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec pod-sharedvolume-93c04889-775d-4f04-8eaf-e8f94fefacdf -c busybox-main-container --namespace=emptydir-5018 -- cat /usr/share/volumeshare/shareddata.txt'
Feb 11 09:30:32.254: INFO: stderr: ""
Feb 11 09:30:32.254: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:30:32.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5018" for this suite.
Feb 11 09:30:38.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:30:38.331: INFO: namespace emptydir-5018 deletion completed in 6.073226673s

â€¢ [SLOW TEST:8.318 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:30:38.332: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8272.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8272.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8272.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8272.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8272.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8272.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8272.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8272.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8272.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8272.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8272.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8272.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8272.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 231.6.23.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.23.6.231_udp@PTR;check="$$(dig +tcp +noall +answer +search 231.6.23.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.23.6.231_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8272.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8272.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8272.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8272.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8272.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8272.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8272.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8272.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8272.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8272.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8272.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8272.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8272.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 231.6.23.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.23.6.231_udp@PTR;check="$$(dig +tcp +noall +answer +search 231.6.23.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.23.6.231_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 11 09:30:42.385: INFO: Unable to read wheezy_udp@dns-test-service.dns-8272.svc.cluster.local from pod dns-8272/dns-test-6a329725-ffb5-4969-bca9-18b422ba88ba: the server could not find the requested resource (get pods dns-test-6a329725-ffb5-4969-bca9-18b422ba88ba)
Feb 11 09:30:42.388: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8272.svc.cluster.local from pod dns-8272/dns-test-6a329725-ffb5-4969-bca9-18b422ba88ba: the server could not find the requested resource (get pods dns-test-6a329725-ffb5-4969-bca9-18b422ba88ba)
Feb 11 09:30:42.390: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8272.svc.cluster.local from pod dns-8272/dns-test-6a329725-ffb5-4969-bca9-18b422ba88ba: the server could not find the requested resource (get pods dns-test-6a329725-ffb5-4969-bca9-18b422ba88ba)
Feb 11 09:30:42.393: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8272.svc.cluster.local from pod dns-8272/dns-test-6a329725-ffb5-4969-bca9-18b422ba88ba: the server could not find the requested resource (get pods dns-test-6a329725-ffb5-4969-bca9-18b422ba88ba)
Feb 11 09:30:42.411: INFO: Unable to read jessie_udp@dns-test-service.dns-8272.svc.cluster.local from pod dns-8272/dns-test-6a329725-ffb5-4969-bca9-18b422ba88ba: the server could not find the requested resource (get pods dns-test-6a329725-ffb5-4969-bca9-18b422ba88ba)
Feb 11 09:30:42.414: INFO: Unable to read jessie_tcp@dns-test-service.dns-8272.svc.cluster.local from pod dns-8272/dns-test-6a329725-ffb5-4969-bca9-18b422ba88ba: the server could not find the requested resource (get pods dns-test-6a329725-ffb5-4969-bca9-18b422ba88ba)
Feb 11 09:30:42.416: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8272.svc.cluster.local from pod dns-8272/dns-test-6a329725-ffb5-4969-bca9-18b422ba88ba: the server could not find the requested resource (get pods dns-test-6a329725-ffb5-4969-bca9-18b422ba88ba)
Feb 11 09:30:42.419: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8272.svc.cluster.local from pod dns-8272/dns-test-6a329725-ffb5-4969-bca9-18b422ba88ba: the server could not find the requested resource (get pods dns-test-6a329725-ffb5-4969-bca9-18b422ba88ba)
Feb 11 09:30:42.433: INFO: Lookups using dns-8272/dns-test-6a329725-ffb5-4969-bca9-18b422ba88ba failed for: [wheezy_udp@dns-test-service.dns-8272.svc.cluster.local wheezy_tcp@dns-test-service.dns-8272.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8272.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8272.svc.cluster.local jessie_udp@dns-test-service.dns-8272.svc.cluster.local jessie_tcp@dns-test-service.dns-8272.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8272.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8272.svc.cluster.local]

Feb 11 09:30:47.492: INFO: DNS probes using dns-8272/dns-test-6a329725-ffb5-4969-bca9-18b422ba88ba succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:30:47.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8272" for this suite.
Feb 11 09:30:53.543: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:30:53.605: INFO: namespace dns-8272 deletion completed in 6.071744293s

â€¢ [SLOW TEST:15.274 seconds]
[sig-network] DNS
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:30:53.606: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-9xzf
STEP: Creating a pod to test atomic-volume-subpath
Feb 11 09:30:53.638: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-9xzf" in namespace "subpath-7085" to be "success or failure"
Feb 11 09:30:53.640: INFO: Pod "pod-subpath-test-projected-9xzf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.92631ms
Feb 11 09:30:55.643: INFO: Pod "pod-subpath-test-projected-9xzf": Phase="Running", Reason="", readiness=true. Elapsed: 2.005489823s
Feb 11 09:30:57.647: INFO: Pod "pod-subpath-test-projected-9xzf": Phase="Running", Reason="", readiness=true. Elapsed: 4.008906696s
Feb 11 09:30:59.651: INFO: Pod "pod-subpath-test-projected-9xzf": Phase="Running", Reason="", readiness=true. Elapsed: 6.012851479s
Feb 11 09:31:01.654: INFO: Pod "pod-subpath-test-projected-9xzf": Phase="Running", Reason="", readiness=true. Elapsed: 8.01664362s
Feb 11 09:31:03.657: INFO: Pod "pod-subpath-test-projected-9xzf": Phase="Running", Reason="", readiness=true. Elapsed: 10.019789182s
Feb 11 09:31:05.661: INFO: Pod "pod-subpath-test-projected-9xzf": Phase="Running", Reason="", readiness=true. Elapsed: 12.022850135s
Feb 11 09:31:07.664: INFO: Pod "pod-subpath-test-projected-9xzf": Phase="Running", Reason="", readiness=true. Elapsed: 14.026775649s
Feb 11 09:31:09.668: INFO: Pod "pod-subpath-test-projected-9xzf": Phase="Running", Reason="", readiness=true. Elapsed: 16.029887947s
Feb 11 09:31:11.671: INFO: Pod "pod-subpath-test-projected-9xzf": Phase="Running", Reason="", readiness=true. Elapsed: 18.033255627s
Feb 11 09:31:13.675: INFO: Pod "pod-subpath-test-projected-9xzf": Phase="Running", Reason="", readiness=true. Elapsed: 20.037099463s
Feb 11 09:31:15.678: INFO: Pod "pod-subpath-test-projected-9xzf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.040631639s
STEP: Saw pod success
Feb 11 09:31:15.678: INFO: Pod "pod-subpath-test-projected-9xzf" satisfied condition "success or failure"
Feb 11 09:31:15.680: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-subpath-test-projected-9xzf container test-container-subpath-projected-9xzf: <nil>
STEP: delete the pod
Feb 11 09:31:15.705: INFO: Waiting for pod pod-subpath-test-projected-9xzf to disappear
Feb 11 09:31:15.707: INFO: Pod pod-subpath-test-projected-9xzf no longer exists
STEP: Deleting pod pod-subpath-test-projected-9xzf
Feb 11 09:31:15.707: INFO: Deleting pod "pod-subpath-test-projected-9xzf" in namespace "subpath-7085"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:31:15.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7085" for this suite.
Feb 11 09:31:21.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:31:21.784: INFO: namespace subpath-7085 deletion completed in 6.071907209s

â€¢ [SLOW TEST:28.179 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:31:21.785: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:31:21.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-2563" for this suite.
Feb 11 09:31:27.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:31:27.884: INFO: namespace tables-2563 deletion completed in 6.072749929s

â€¢ [SLOW TEST:6.099 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:31:27.884: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 11 09:31:27.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-6283'
Feb 11 09:31:27.999: INFO: stderr: ""
Feb 11 09:31:27.999: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Feb 11 09:31:28.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 delete pods e2e-test-httpd-pod --namespace=kubectl-6283'
Feb 11 09:31:30.873: INFO: stderr: ""
Feb 11 09:31:30.873: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:31:30.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6283" for this suite.
Feb 11 09:31:36.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:31:36.951: INFO: namespace kubectl-6283 deletion completed in 6.073814259s

â€¢ [SLOW TEST:9.066 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:31:36.951: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-4274
STEP: creating replication controller nodeport-test in namespace services-4274
I0211 09:31:36.986057      23 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-4274, replica count: 2
Feb 11 09:31:40.036: INFO: Creating new exec pod
I0211 09:31:40.036379      23 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 11 09:31:43.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=services-4274 execpoddb22d -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Feb 11 09:31:43.265: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 11 09:31:43.265: INFO: stdout: ""
Feb 11 09:31:43.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=services-4274 execpoddb22d -- /bin/sh -x -c nc -zv -t -w 2 172.23.2.193 80'
Feb 11 09:31:43.466: INFO: stderr: "+ nc -zv -t -w 2 172.23.2.193 80\nConnection to 172.23.2.193 80 port [tcp/http] succeeded!\n"
Feb 11 09:31:43.466: INFO: stdout: ""
Feb 11 09:31:43.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=services-4274 execpoddb22d -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.18 31940'
Feb 11 09:31:43.662: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.18 31940\nConnection to 192.168.0.18 31940 port [tcp/31940] succeeded!\n"
Feb 11 09:31:43.662: INFO: stdout: ""
Feb 11 09:31:43.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=services-4274 execpoddb22d -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.19 31940'
Feb 11 09:31:43.857: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.19 31940\nConnection to 192.168.0.19 31940 port [tcp/31940] succeeded!\n"
Feb 11 09:31:43.857: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:31:43.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4274" for this suite.
Feb 11 09:31:49.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:31:49.934: INFO: namespace services-4274 deletion completed in 6.072981489s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:12.984 seconds]
[sig-network] Services
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:31:49.934: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 11 09:31:50.573: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 11 09:31:52.582: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717010310, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717010310, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717010310, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717010310, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 11 09:31:55.593: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:32:07.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4055" for this suite.
Feb 11 09:32:13.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:32:13.769: INFO: namespace webhook-4055 deletion completed in 6.074631145s
STEP: Destroying namespace "webhook-4055-markers" for this suite.
Feb 11 09:32:19.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:32:19.842: INFO: namespace webhook-4055-markers deletion completed in 6.07334687s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:29.918 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:32:19.853: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:32:19.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-521" for this suite.
Feb 11 09:32:25.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:32:25.964: INFO: namespace kubelet-test-521 deletion completed in 6.072729447s

â€¢ [SLOW TEST:6.111 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:32:25.964: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 09:32:25.987: INFO: Creating ReplicaSet my-hostname-basic-69a5a6af-56c8-407e-b506-6f4b0d013d8f
Feb 11 09:32:25.992: INFO: Pod name my-hostname-basic-69a5a6af-56c8-407e-b506-6f4b0d013d8f: Found 0 pods out of 1
Feb 11 09:32:30.996: INFO: Pod name my-hostname-basic-69a5a6af-56c8-407e-b506-6f4b0d013d8f: Found 1 pods out of 1
Feb 11 09:32:30.996: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-69a5a6af-56c8-407e-b506-6f4b0d013d8f" is running
Feb 11 09:32:31.000: INFO: Pod "my-hostname-basic-69a5a6af-56c8-407e-b506-6f4b0d013d8f-pcpxl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-11 09:32:25 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-11 09:32:27 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-11 09:32:27 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-11 09:32:25 +0000 UTC Reason: Message:}])
Feb 11 09:32:31.000: INFO: Trying to dial the pod
Feb 11 09:32:36.010: INFO: Controller my-hostname-basic-69a5a6af-56c8-407e-b506-6f4b0d013d8f: Got expected result from replica 1 [my-hostname-basic-69a5a6af-56c8-407e-b506-6f4b0d013d8f-pcpxl]: "my-hostname-basic-69a5a6af-56c8-407e-b506-6f4b0d013d8f-pcpxl", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:32:36.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6598" for this suite.
Feb 11 09:32:42.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:32:42.089: INFO: namespace replicaset-6598 deletion completed in 6.076480091s

â€¢ [SLOW TEST:16.125 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:32:42.089: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 09:32:42.112: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 11 09:32:49.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 --namespace=crd-publish-openapi-6446 create -f -'
Feb 11 09:32:50.339: INFO: stderr: ""
Feb 11 09:32:50.339: INFO: stdout: "e2e-test-crd-publish-openapi-5858-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 11 09:32:50.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 --namespace=crd-publish-openapi-6446 delete e2e-test-crd-publish-openapi-5858-crds test-cr'
Feb 11 09:32:50.411: INFO: stderr: ""
Feb 11 09:32:50.411: INFO: stdout: "e2e-test-crd-publish-openapi-5858-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Feb 11 09:32:50.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 --namespace=crd-publish-openapi-6446 apply -f -'
Feb 11 09:32:50.686: INFO: stderr: ""
Feb 11 09:32:50.686: INFO: stdout: "e2e-test-crd-publish-openapi-5858-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 11 09:32:50.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 --namespace=crd-publish-openapi-6446 delete e2e-test-crd-publish-openapi-5858-crds test-cr'
Feb 11 09:32:50.759: INFO: stderr: ""
Feb 11 09:32:50.759: INFO: stdout: "e2e-test-crd-publish-openapi-5858-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 11 09:32:50.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 explain e2e-test-crd-publish-openapi-5858-crds'
Feb 11 09:32:50.955: INFO: stderr: ""
Feb 11 09:32:50.955: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5858-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:32:53.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6446" for this suite.
Feb 11 09:32:59.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:32:59.790: INFO: namespace crd-publish-openapi-6446 deletion completed in 6.071535655s

â€¢ [SLOW TEST:17.700 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:32:59.790: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-fqsk
STEP: Creating a pod to test atomic-volume-subpath
Feb 11 09:32:59.822: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-fqsk" in namespace "subpath-8146" to be "success or failure"
Feb 11 09:32:59.825: INFO: Pod "pod-subpath-test-downwardapi-fqsk": Phase="Pending", Reason="", readiness=false. Elapsed: 3.029771ms
Feb 11 09:33:01.829: INFO: Pod "pod-subpath-test-downwardapi-fqsk": Phase="Running", Reason="", readiness=true. Elapsed: 2.006507003s
Feb 11 09:33:03.832: INFO: Pod "pod-subpath-test-downwardapi-fqsk": Phase="Running", Reason="", readiness=true. Elapsed: 4.009942514s
Feb 11 09:33:05.835: INFO: Pod "pod-subpath-test-downwardapi-fqsk": Phase="Running", Reason="", readiness=true. Elapsed: 6.013234253s
Feb 11 09:33:07.839: INFO: Pod "pod-subpath-test-downwardapi-fqsk": Phase="Running", Reason="", readiness=true. Elapsed: 8.016635539s
Feb 11 09:33:09.842: INFO: Pod "pod-subpath-test-downwardapi-fqsk": Phase="Running", Reason="", readiness=true. Elapsed: 10.020121454s
Feb 11 09:33:11.846: INFO: Pod "pod-subpath-test-downwardapi-fqsk": Phase="Running", Reason="", readiness=true. Elapsed: 12.023598622s
Feb 11 09:33:13.849: INFO: Pod "pod-subpath-test-downwardapi-fqsk": Phase="Running", Reason="", readiness=true. Elapsed: 14.027120024s
Feb 11 09:33:15.853: INFO: Pod "pod-subpath-test-downwardapi-fqsk": Phase="Running", Reason="", readiness=true. Elapsed: 16.030370641s
Feb 11 09:33:17.856: INFO: Pod "pod-subpath-test-downwardapi-fqsk": Phase="Running", Reason="", readiness=true. Elapsed: 18.033975353s
Feb 11 09:33:19.860: INFO: Pod "pod-subpath-test-downwardapi-fqsk": Phase="Running", Reason="", readiness=true. Elapsed: 20.037430616s
Feb 11 09:33:21.863: INFO: Pod "pod-subpath-test-downwardapi-fqsk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.040289411s
STEP: Saw pod success
Feb 11 09:33:21.863: INFO: Pod "pod-subpath-test-downwardapi-fqsk" satisfied condition "success or failure"
Feb 11 09:33:21.865: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-subpath-test-downwardapi-fqsk container test-container-subpath-downwardapi-fqsk: <nil>
STEP: delete the pod
Feb 11 09:33:21.889: INFO: Waiting for pod pod-subpath-test-downwardapi-fqsk to disappear
Feb 11 09:33:21.891: INFO: Pod pod-subpath-test-downwardapi-fqsk no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-fqsk
Feb 11 09:33:21.891: INFO: Deleting pod "pod-subpath-test-downwardapi-fqsk" in namespace "subpath-8146"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:33:21.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8146" for this suite.
Feb 11 09:33:27.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:33:27.971: INFO: namespace subpath-8146 deletion completed in 6.074288682s

â€¢ [SLOW TEST:28.181 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:33:27.971: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 11 09:33:27.998: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-40 /api/v1/namespaces/watch-40/configmaps/e2e-watch-test-configmap-a 253b584b-0ea4-49f1-a65c-ee0e89adb946 25583 0 2020-02-11 09:33:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 11 09:33:27.998: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-40 /api/v1/namespaces/watch-40/configmaps/e2e-watch-test-configmap-a 253b584b-0ea4-49f1-a65c-ee0e89adb946 25583 0 2020-02-11 09:33:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 11 09:33:38.005: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-40 /api/v1/namespaces/watch-40/configmaps/e2e-watch-test-configmap-a 253b584b-0ea4-49f1-a65c-ee0e89adb946 25614 0 2020-02-11 09:33:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 11 09:33:38.005: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-40 /api/v1/namespaces/watch-40/configmaps/e2e-watch-test-configmap-a 253b584b-0ea4-49f1-a65c-ee0e89adb946 25614 0 2020-02-11 09:33:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 11 09:33:48.012: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-40 /api/v1/namespaces/watch-40/configmaps/e2e-watch-test-configmap-a 253b584b-0ea4-49f1-a65c-ee0e89adb946 25640 0 2020-02-11 09:33:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 11 09:33:48.012: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-40 /api/v1/namespaces/watch-40/configmaps/e2e-watch-test-configmap-a 253b584b-0ea4-49f1-a65c-ee0e89adb946 25640 0 2020-02-11 09:33:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 11 09:33:58.019: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-40 /api/v1/namespaces/watch-40/configmaps/e2e-watch-test-configmap-a 253b584b-0ea4-49f1-a65c-ee0e89adb946 25668 0 2020-02-11 09:33:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 11 09:33:58.019: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-40 /api/v1/namespaces/watch-40/configmaps/e2e-watch-test-configmap-a 253b584b-0ea4-49f1-a65c-ee0e89adb946 25668 0 2020-02-11 09:33:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 11 09:34:08.026: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-40 /api/v1/namespaces/watch-40/configmaps/e2e-watch-test-configmap-b cdcf8624-c74f-47e5-8efb-faa9394099eb 25695 0 2020-02-11 09:34:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 11 09:34:08.026: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-40 /api/v1/namespaces/watch-40/configmaps/e2e-watch-test-configmap-b cdcf8624-c74f-47e5-8efb-faa9394099eb 25695 0 2020-02-11 09:34:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 11 09:34:18.033: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-40 /api/v1/namespaces/watch-40/configmaps/e2e-watch-test-configmap-b cdcf8624-c74f-47e5-8efb-faa9394099eb 25721 0 2020-02-11 09:34:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 11 09:34:18.033: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-40 /api/v1/namespaces/watch-40/configmaps/e2e-watch-test-configmap-b cdcf8624-c74f-47e5-8efb-faa9394099eb 25721 0 2020-02-11 09:34:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:34:28.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-40" for this suite.
Feb 11 09:34:34.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:34:34.157: INFO: namespace watch-40 deletion completed in 6.112193329s

â€¢ [SLOW TEST:66.186 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:34:34.157: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 09:34:34.184: INFO: Waiting up to 5m0s for pod "busybox-user-65534-8cd37b2f-4d71-4d50-8100-af19b59cc6e6" in namespace "security-context-test-5118" to be "success or failure"
Feb 11 09:34:34.187: INFO: Pod "busybox-user-65534-8cd37b2f-4d71-4d50-8100-af19b59cc6e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.971708ms
Feb 11 09:34:36.191: INFO: Pod "busybox-user-65534-8cd37b2f-4d71-4d50-8100-af19b59cc6e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00694031s
Feb 11 09:34:36.191: INFO: Pod "busybox-user-65534-8cd37b2f-4d71-4d50-8100-af19b59cc6e6" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:34:36.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5118" for this suite.
Feb 11 09:34:42.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:34:42.269: INFO: namespace security-context-test-5118 deletion completed in 6.074577483s

â€¢ [SLOW TEST:8.112 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:34:42.269: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 11 09:34:42.344: INFO: Waiting up to 5m0s for pod "pod-83776d95-7b95-4388-a81f-0a23afcc6bd8" in namespace "emptydir-905" to be "success or failure"
Feb 11 09:34:42.347: INFO: Pod "pod-83776d95-7b95-4388-a81f-0a23afcc6bd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.980711ms
Feb 11 09:34:44.350: INFO: Pod "pod-83776d95-7b95-4388-a81f-0a23afcc6bd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006576781s
STEP: Saw pod success
Feb 11 09:34:44.350: INFO: Pod "pod-83776d95-7b95-4388-a81f-0a23afcc6bd8" satisfied condition "success or failure"
Feb 11 09:34:44.353: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-83776d95-7b95-4388-a81f-0a23afcc6bd8 container test-container: <nil>
STEP: delete the pod
Feb 11 09:34:44.367: INFO: Waiting for pod pod-83776d95-7b95-4388-a81f-0a23afcc6bd8 to disappear
Feb 11 09:34:44.369: INFO: Pod pod-83776d95-7b95-4388-a81f-0a23afcc6bd8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:34:44.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-905" for this suite.
Feb 11 09:34:50.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:34:50.449: INFO: namespace emptydir-905 deletion completed in 6.076892882s

â€¢ [SLOW TEST:8.180 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:34:50.449: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 11 09:34:50.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-4637'
Feb 11 09:34:50.548: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 11 09:34:50.548: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Feb 11 09:34:50.553: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Feb 11 09:34:50.555: INFO: scanned /root for discovery docs: <nil>
Feb 11 09:34:50.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-4637'
Feb 11 09:35:06.301: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 11 09:35:06.301: INFO: stdout: "Created e2e-test-httpd-rc-1045def233a13ad576017a2cefddac4d\nScaling up e2e-test-httpd-rc-1045def233a13ad576017a2cefddac4d from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-1045def233a13ad576017a2cefddac4d up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-1045def233a13ad576017a2cefddac4d to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Feb 11 09:35:06.301: INFO: stdout: "Created e2e-test-httpd-rc-1045def233a13ad576017a2cefddac4d\nScaling up e2e-test-httpd-rc-1045def233a13ad576017a2cefddac4d from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-1045def233a13ad576017a2cefddac4d up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-1045def233a13ad576017a2cefddac4d to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Feb 11 09:35:06.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-4637'
Feb 11 09:35:06.370: INFO: stderr: ""
Feb 11 09:35:06.370: INFO: stdout: "e2e-test-httpd-rc-1045def233a13ad576017a2cefddac4d-h5g2p e2e-test-httpd-rc-rxzcq "
STEP: Replicas for run=e2e-test-httpd-rc: expected=1 actual=2
Feb 11 09:35:11.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-4637'
Feb 11 09:35:11.442: INFO: stderr: ""
Feb 11 09:35:11.442: INFO: stdout: "e2e-test-httpd-rc-1045def233a13ad576017a2cefddac4d-h5g2p "
Feb 11 09:35:11.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods e2e-test-httpd-rc-1045def233a13ad576017a2cefddac4d-h5g2p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4637'
Feb 11 09:35:11.510: INFO: stderr: ""
Feb 11 09:35:11.510: INFO: stdout: "true"
Feb 11 09:35:11.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods e2e-test-httpd-rc-1045def233a13ad576017a2cefddac4d-h5g2p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4637'
Feb 11 09:35:11.578: INFO: stderr: ""
Feb 11 09:35:11.578: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Feb 11 09:35:11.578: INFO: e2e-test-httpd-rc-1045def233a13ad576017a2cefddac4d-h5g2p is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Feb 11 09:35:11.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 delete rc e2e-test-httpd-rc --namespace=kubectl-4637'
Feb 11 09:35:11.655: INFO: stderr: ""
Feb 11 09:35:11.655: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:35:11.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4637" for this suite.
Feb 11 09:35:23.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:35:23.733: INFO: namespace kubectl-4637 deletion completed in 12.07427673s

â€¢ [SLOW TEST:33.284 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:35:23.733: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Feb 11 09:35:23.755: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 09:35:30.541: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:35:44.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4467" for this suite.
Feb 11 09:35:50.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:35:50.511: INFO: namespace crd-publish-openapi-4467 deletion completed in 6.073736183s

â€¢ [SLOW TEST:26.779 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:35:50.512: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 11 09:35:52.550: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:35:52.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5486" for this suite.
Feb 11 09:35:58.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:35:58.635: INFO: namespace container-runtime-5486 deletion completed in 6.071470195s

â€¢ [SLOW TEST:8.123 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:35:58.635: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 11 09:36:00.674: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:36:00.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1325" for this suite.
Feb 11 09:36:06.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:36:06.762: INFO: namespace container-runtime-1325 deletion completed in 6.074615135s

â€¢ [SLOW TEST:8.127 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:36:06.762: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-28da066c-19d0-4550-9180-f44bf13f02fb
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-28da066c-19d0-4550-9180-f44bf13f02fb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:36:10.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5776" for this suite.
Feb 11 09:36:32.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:36:32.931: INFO: namespace configmap-5776 deletion completed in 22.072792575s

â€¢ [SLOW TEST:26.169 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:36:32.932: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 11 09:36:32.959: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf902c24-710d-4d8f-8d8a-b32c1d398cd3" in namespace "downward-api-3860" to be "success or failure"
Feb 11 09:36:32.961: INFO: Pod "downwardapi-volume-bf902c24-710d-4d8f-8d8a-b32c1d398cd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.331372ms
Feb 11 09:36:34.965: INFO: Pod "downwardapi-volume-bf902c24-710d-4d8f-8d8a-b32c1d398cd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005897815s
STEP: Saw pod success
Feb 11 09:36:34.965: INFO: Pod "downwardapi-volume-bf902c24-710d-4d8f-8d8a-b32c1d398cd3" satisfied condition "success or failure"
Feb 11 09:36:34.968: INFO: Trying to get logs from node cn-hongkong.192.168.0.20 pod downwardapi-volume-bf902c24-710d-4d8f-8d8a-b32c1d398cd3 container client-container: <nil>
STEP: delete the pod
Feb 11 09:36:34.990: INFO: Waiting for pod downwardapi-volume-bf902c24-710d-4d8f-8d8a-b32c1d398cd3 to disappear
Feb 11 09:36:34.993: INFO: Pod downwardapi-volume-bf902c24-710d-4d8f-8d8a-b32c1d398cd3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:36:34.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3860" for this suite.
Feb 11 09:36:41.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:36:41.077: INFO: namespace downward-api-3860 deletion completed in 6.081376222s

â€¢ [SLOW TEST:8.145 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:36:41.077: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 11 09:36:45.144: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 11 09:36:45.146: INFO: Pod pod-with-poststart-http-hook still exists
Feb 11 09:36:47.146: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 11 09:36:47.149: INFO: Pod pod-with-poststart-http-hook still exists
Feb 11 09:36:49.146: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 11 09:36:49.150: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:36:49.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2359" for this suite.
Feb 11 09:37:01.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:37:01.234: INFO: namespace container-lifecycle-hook-2359 deletion completed in 12.080127271s

â€¢ [SLOW TEST:20.157 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:37:01.234: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Feb 11 09:37:01.258: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-208978535 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:37:01.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8246" for this suite.
Feb 11 09:37:07.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:37:07.398: INFO: namespace kubectl-8246 deletion completed in 6.075967226s

â€¢ [SLOW TEST:6.163 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:37:07.398: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:37:11.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7834" for this suite.
Feb 11 09:37:17.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:37:17.510: INFO: namespace kubelet-test-7834 deletion completed in 6.074476196s

â€¢ [SLOW TEST:10.112 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:37:17.510: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 11 09:37:17.871: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 11 09:37:20.888: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:37:20.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8660" for this suite.
Feb 11 09:37:26.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:37:27.015: INFO: namespace webhook-8660 deletion completed in 6.071187229s
STEP: Destroying namespace "webhook-8660-markers" for this suite.
Feb 11 09:37:33.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:37:33.090: INFO: namespace webhook-8660-markers deletion completed in 6.074718498s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:15.592 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:37:33.102: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Feb 11 09:38:13.154: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0211 09:38:13.154148      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:38:13.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4920" for this suite.
Feb 11 09:38:19.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:38:19.231: INFO: namespace gc-4920 deletion completed in 6.074046131s

â€¢ [SLOW TEST:46.129 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:38:19.231: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-770b8976-f682-4218-9a77-318f878968fe
STEP: Creating a pod to test consume configMaps
Feb 11 09:38:19.259: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b955fc39-0e63-41ad-aa35-ff82228bc9aa" in namespace "projected-1563" to be "success or failure"
Feb 11 09:38:19.262: INFO: Pod "pod-projected-configmaps-b955fc39-0e63-41ad-aa35-ff82228bc9aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.288281ms
Feb 11 09:38:21.265: INFO: Pod "pod-projected-configmaps-b955fc39-0e63-41ad-aa35-ff82228bc9aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005522378s
STEP: Saw pod success
Feb 11 09:38:21.265: INFO: Pod "pod-projected-configmaps-b955fc39-0e63-41ad-aa35-ff82228bc9aa" satisfied condition "success or failure"
Feb 11 09:38:21.268: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-projected-configmaps-b955fc39-0e63-41ad-aa35-ff82228bc9aa container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 11 09:38:21.289: INFO: Waiting for pod pod-projected-configmaps-b955fc39-0e63-41ad-aa35-ff82228bc9aa to disappear
Feb 11 09:38:21.293: INFO: Pod pod-projected-configmaps-b955fc39-0e63-41ad-aa35-ff82228bc9aa no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:38:21.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1563" for this suite.
Feb 11 09:38:27.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:38:27.374: INFO: namespace projected-1563 deletion completed in 6.078007059s

â€¢ [SLOW TEST:8.143 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:38:27.374: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 11 09:38:29.411: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:38:29.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-337" for this suite.
Feb 11 09:38:35.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:38:35.498: INFO: namespace container-runtime-337 deletion completed in 6.073345852s

â€¢ [SLOW TEST:8.123 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:38:35.498: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-8993e477-56ae-426b-82c8-ca297937c48b
STEP: Creating a pod to test consume secrets
Feb 11 09:38:35.527: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9ab55448-cda6-4e16-a875-bb3c3b407fe4" in namespace "projected-5158" to be "success or failure"
Feb 11 09:38:35.529: INFO: Pod "pod-projected-secrets-9ab55448-cda6-4e16-a875-bb3c3b407fe4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.344204ms
Feb 11 09:38:37.532: INFO: Pod "pod-projected-secrets-9ab55448-cda6-4e16-a875-bb3c3b407fe4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005345973s
STEP: Saw pod success
Feb 11 09:38:37.532: INFO: Pod "pod-projected-secrets-9ab55448-cda6-4e16-a875-bb3c3b407fe4" satisfied condition "success or failure"
Feb 11 09:38:37.535: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-projected-secrets-9ab55448-cda6-4e16-a875-bb3c3b407fe4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 11 09:38:37.552: INFO: Waiting for pod pod-projected-secrets-9ab55448-cda6-4e16-a875-bb3c3b407fe4 to disappear
Feb 11 09:38:37.554: INFO: Pod pod-projected-secrets-9ab55448-cda6-4e16-a875-bb3c3b407fe4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:38:37.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5158" for this suite.
Feb 11 09:38:43.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:38:43.629: INFO: namespace projected-5158 deletion completed in 6.071865562s

â€¢ [SLOW TEST:8.131 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:38:43.629: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 11 09:38:44.172: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 11 09:38:47.186: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:38:57.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4271" for this suite.
Feb 11 09:39:03.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:39:03.348: INFO: namespace webhook-4271 deletion completed in 6.081122166s
STEP: Destroying namespace "webhook-4271-markers" for this suite.
Feb 11 09:39:09.357: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:39:09.421: INFO: namespace webhook-4271-markers deletion completed in 6.072784567s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:25.802 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:39:09.431: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:39:38.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8681" for this suite.
Feb 11 09:39:44.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:39:44.610: INFO: namespace namespaces-8681 deletion completed in 6.070514132s
STEP: Destroying namespace "nsdeletetest-8996" for this suite.
Feb 11 09:39:44.612: INFO: Namespace nsdeletetest-8996 was already deleted
STEP: Destroying namespace "nsdeletetest-214" for this suite.
Feb 11 09:39:50.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:39:50.683: INFO: namespace nsdeletetest-214 deletion completed in 6.070796565s

â€¢ [SLOW TEST:41.252 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:39:50.683: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 11 09:39:50.719: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83b71892-1b88-4886-a68c-ed4de21b95bd" in namespace "downward-api-8622" to be "success or failure"
Feb 11 09:39:50.722: INFO: Pod "downwardapi-volume-83b71892-1b88-4886-a68c-ed4de21b95bd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.296493ms
Feb 11 09:39:52.726: INFO: Pod "downwardapi-volume-83b71892-1b88-4886-a68c-ed4de21b95bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006953941s
STEP: Saw pod success
Feb 11 09:39:52.726: INFO: Pod "downwardapi-volume-83b71892-1b88-4886-a68c-ed4de21b95bd" satisfied condition "success or failure"
Feb 11 09:39:52.728: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod downwardapi-volume-83b71892-1b88-4886-a68c-ed4de21b95bd container client-container: <nil>
STEP: delete the pod
Feb 11 09:39:52.743: INFO: Waiting for pod downwardapi-volume-83b71892-1b88-4886-a68c-ed4de21b95bd to disappear
Feb 11 09:39:52.745: INFO: Pod downwardapi-volume-83b71892-1b88-4886-a68c-ed4de21b95bd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:39:52.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8622" for this suite.
Feb 11 09:39:58.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:39:58.821: INFO: namespace downward-api-8622 deletion completed in 6.072416049s

â€¢ [SLOW TEST:8.138 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:39:58.821: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-750
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-750
STEP: Deleting pre-stop pod
Feb 11 09:40:07.877: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:40:07.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-750" for this suite.
Feb 11 09:40:51.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:40:51.959: INFO: namespace prestop-750 deletion completed in 44.073100565s

â€¢ [SLOW TEST:53.138 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:40:51.959: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 11 09:40:51.987: INFO: Waiting up to 5m0s for pod "downwardapi-volume-453f3354-0d89-4d36-9cf7-5a04c25cda49" in namespace "downward-api-7952" to be "success or failure"
Feb 11 09:40:51.989: INFO: Pod "downwardapi-volume-453f3354-0d89-4d36-9cf7-5a04c25cda49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.407496ms
Feb 11 09:40:53.993: INFO: Pod "downwardapi-volume-453f3354-0d89-4d36-9cf7-5a04c25cda49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005825248s
STEP: Saw pod success
Feb 11 09:40:53.993: INFO: Pod "downwardapi-volume-453f3354-0d89-4d36-9cf7-5a04c25cda49" satisfied condition "success or failure"
Feb 11 09:40:53.995: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod downwardapi-volume-453f3354-0d89-4d36-9cf7-5a04c25cda49 container client-container: <nil>
STEP: delete the pod
Feb 11 09:40:54.011: INFO: Waiting for pod downwardapi-volume-453f3354-0d89-4d36-9cf7-5a04c25cda49 to disappear
Feb 11 09:40:54.013: INFO: Pod downwardapi-volume-453f3354-0d89-4d36-9cf7-5a04c25cda49 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:40:54.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7952" for this suite.
Feb 11 09:41:00.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:41:00.092: INFO: namespace downward-api-7952 deletion completed in 6.075266493s

â€¢ [SLOW TEST:8.133 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:41:00.092: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 11 09:41:00.120: INFO: Waiting up to 5m0s for pod "downwardapi-volume-95a77e2c-a4fd-4715-9ae4-3d4f3bf13670" in namespace "downward-api-9434" to be "success or failure"
Feb 11 09:41:00.123: INFO: Pod "downwardapi-volume-95a77e2c-a4fd-4715-9ae4-3d4f3bf13670": Phase="Pending", Reason="", readiness=false. Elapsed: 3.674326ms
Feb 11 09:41:02.127: INFO: Pod "downwardapi-volume-95a77e2c-a4fd-4715-9ae4-3d4f3bf13670": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006881202s
STEP: Saw pod success
Feb 11 09:41:02.127: INFO: Pod "downwardapi-volume-95a77e2c-a4fd-4715-9ae4-3d4f3bf13670" satisfied condition "success or failure"
Feb 11 09:41:02.129: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod downwardapi-volume-95a77e2c-a4fd-4715-9ae4-3d4f3bf13670 container client-container: <nil>
STEP: delete the pod
Feb 11 09:41:02.143: INFO: Waiting for pod downwardapi-volume-95a77e2c-a4fd-4715-9ae4-3d4f3bf13670 to disappear
Feb 11 09:41:02.145: INFO: Pod downwardapi-volume-95a77e2c-a4fd-4715-9ae4-3d4f3bf13670 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:41:02.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9434" for this suite.
Feb 11 09:41:08.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:41:08.225: INFO: namespace downward-api-9434 deletion completed in 6.076230313s

â€¢ [SLOW TEST:8.133 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:41:08.225: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-400a666d-16f7-4314-8eb7-faf80d62c410 in namespace container-probe-1883
Feb 11 09:41:10.258: INFO: Started pod liveness-400a666d-16f7-4314-8eb7-faf80d62c410 in namespace container-probe-1883
STEP: checking the pod's current state and verifying that restartCount is present
Feb 11 09:41:10.261: INFO: Initial restart count of pod liveness-400a666d-16f7-4314-8eb7-faf80d62c410 is 0
Feb 11 09:41:30.298: INFO: Restart count of pod container-probe-1883/liveness-400a666d-16f7-4314-8eb7-faf80d62c410 is now 1 (20.036899018s elapsed)
Feb 11 09:41:50.341: INFO: Restart count of pod container-probe-1883/liveness-400a666d-16f7-4314-8eb7-faf80d62c410 is now 2 (40.080139563s elapsed)
Feb 11 09:42:10.375: INFO: Restart count of pod container-probe-1883/liveness-400a666d-16f7-4314-8eb7-faf80d62c410 is now 3 (1m0.114514001s elapsed)
Feb 11 09:42:30.410: INFO: Restart count of pod container-probe-1883/liveness-400a666d-16f7-4314-8eb7-faf80d62c410 is now 4 (1m20.149354843s elapsed)
Feb 11 09:43:36.528: INFO: Restart count of pod container-probe-1883/liveness-400a666d-16f7-4314-8eb7-faf80d62c410 is now 5 (2m26.26744933s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:43:36.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1883" for this suite.
Feb 11 09:43:42.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:43:42.615: INFO: namespace container-probe-1883 deletion completed in 6.07503598s

â€¢ [SLOW TEST:154.390 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:43:42.615: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 11 09:43:42.642: INFO: Waiting up to 5m0s for pod "pod-fda2704c-e7b9-42e1-8014-e2ddc1231ce1" in namespace "emptydir-6553" to be "success or failure"
Feb 11 09:43:42.644: INFO: Pod "pod-fda2704c-e7b9-42e1-8014-e2ddc1231ce1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.370526ms
Feb 11 09:43:44.647: INFO: Pod "pod-fda2704c-e7b9-42e1-8014-e2ddc1231ce1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005486407s
STEP: Saw pod success
Feb 11 09:43:44.647: INFO: Pod "pod-fda2704c-e7b9-42e1-8014-e2ddc1231ce1" satisfied condition "success or failure"
Feb 11 09:43:44.650: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-fda2704c-e7b9-42e1-8014-e2ddc1231ce1 container test-container: <nil>
STEP: delete the pod
Feb 11 09:43:44.672: INFO: Waiting for pod pod-fda2704c-e7b9-42e1-8014-e2ddc1231ce1 to disappear
Feb 11 09:43:44.674: INFO: Pod pod-fda2704c-e7b9-42e1-8014-e2ddc1231ce1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:43:44.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6553" for this suite.
Feb 11 09:43:50.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:43:50.752: INFO: namespace emptydir-6553 deletion completed in 6.074501972s

â€¢ [SLOW TEST:8.136 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:43:50.752: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-a2b1c7c5-ebe5-4ce7-994c-3691887f4555
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:43:50.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7760" for this suite.
Feb 11 09:43:56.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:43:56.849: INFO: namespace configmap-7760 deletion completed in 6.071658188s

â€¢ [SLOW TEST:6.097 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:43:56.849: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-ac4feea4-9c27-4d04-93d0-fe3405bc4171
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:43:56.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6750" for this suite.
Feb 11 09:44:02.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:44:02.958: INFO: namespace secrets-6750 deletion completed in 6.079287184s

â€¢ [SLOW TEST:6.108 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:44:02.958: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Feb 11 09:44:09.005: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0211 09:44:09.005484      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:44:09.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4359" for this suite.
Feb 11 09:44:15.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:44:15.080: INFO: namespace gc-4359 deletion completed in 6.071980528s

â€¢ [SLOW TEST:12.123 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:44:15.081: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 11 09:44:15.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-5917'
Feb 11 09:44:15.362: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 11 09:44:15.362: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Feb 11 09:44:15.369: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-dtzn4]
Feb 11 09:44:15.369: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-dtzn4" in namespace "kubectl-5917" to be "running and ready"
Feb 11 09:44:15.372: INFO: Pod "e2e-test-httpd-rc-dtzn4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.796913ms
Feb 11 09:44:17.375: INFO: Pod "e2e-test-httpd-rc-dtzn4": Phase="Running", Reason="", readiness=true. Elapsed: 2.006150457s
Feb 11 09:44:17.375: INFO: Pod "e2e-test-httpd-rc-dtzn4" satisfied condition "running and ready"
Feb 11 09:44:17.375: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-dtzn4]
Feb 11 09:44:17.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 logs rc/e2e-test-httpd-rc --namespace=kubectl-5917'
Feb 11 09:44:17.470: INFO: stderr: ""
Feb 11 09:44:17.470: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.22.2.35. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.22.2.35. Set the 'ServerName' directive globally to suppress this message\n[Tue Feb 11 09:44:16.203564 2020] [mpm_event:notice] [pid 1:tid 140603076934504] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Tue Feb 11 09:44:16.203614 2020] [core:notice] [pid 1:tid 140603076934504] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Feb 11 09:44:17.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 delete rc e2e-test-httpd-rc --namespace=kubectl-5917'
Feb 11 09:44:17.540: INFO: stderr: ""
Feb 11 09:44:17.540: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:44:17.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5917" for this suite.
Feb 11 09:44:23.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:44:23.616: INFO: namespace kubectl-5917 deletion completed in 6.0715091s

â€¢ [SLOW TEST:8.535 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:44:23.616: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-287eee57-2d86-4ab5-b0a4-df8e4c05ac1b in namespace container-probe-7849
Feb 11 09:44:25.649: INFO: Started pod liveness-287eee57-2d86-4ab5-b0a4-df8e4c05ac1b in namespace container-probe-7849
STEP: checking the pod's current state and verifying that restartCount is present
Feb 11 09:44:25.651: INFO: Initial restart count of pod liveness-287eee57-2d86-4ab5-b0a4-df8e4c05ac1b is 0
Feb 11 09:44:43.685: INFO: Restart count of pod container-probe-7849/liveness-287eee57-2d86-4ab5-b0a4-df8e4c05ac1b is now 1 (18.033591886s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:44:43.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7849" for this suite.
Feb 11 09:44:49.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:44:49.770: INFO: namespace container-probe-7849 deletion completed in 6.072832807s

â€¢ [SLOW TEST:26.154 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:44:49.770: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 11 09:44:49.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-6008'
Feb 11 09:44:49.865: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 11 09:44:49.865: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Feb 11 09:44:51.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 delete deployment e2e-test-httpd-deployment --namespace=kubectl-6008'
Feb 11 09:44:51.943: INFO: stderr: ""
Feb 11 09:44:51.943: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:44:51.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6008" for this suite.
Feb 11 09:45:03.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:45:04.022: INFO: namespace kubectl-6008 deletion completed in 12.075039907s

â€¢ [SLOW TEST:14.252 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:45:04.022: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5897.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5897.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5897.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5897.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5897.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5897.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 11 09:45:06.089: INFO: DNS probes using dns-5897/dns-test-36ee19e4-69c3-4dc5-ab37-997ff7d05778 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:45:06.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5897" for this suite.
Feb 11 09:45:12.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:45:12.184: INFO: namespace dns-5897 deletion completed in 6.071358305s

â€¢ [SLOW TEST:8.162 seconds]
[sig-network] DNS
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:45:12.184: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 09:45:12.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 version'
Feb 11 09:45:12.273: INFO: stderr: ""
Feb 11 09:45:12.273: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.6\", GitCommit:\"72c30166b2105cd7d3350f2c28a219e6abcd79eb\", GitTreeState:\"clean\", BuildDate:\"2020-01-18T23:31:31Z\", GoVersion:\"go1.13.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16+\", GitVersion:\"v1.16.6-aliyun.1\", GitCommit:\"eace237\", GitTreeState:\"\", BuildDate:\"2020-02-04T08:03:14Z\", GoVersion:\"go1.13.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:45:12.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-588" for this suite.
Feb 11 09:45:18.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:45:18.357: INFO: namespace kubectl-588 deletion completed in 6.079482146s

â€¢ [SLOW TEST:6.172 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:45:18.357: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Feb 11 09:45:18.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 create -f - --namespace=kubectl-8243'
Feb 11 09:45:18.600: INFO: stderr: ""
Feb 11 09:45:18.600: INFO: stdout: "pod/pause created\n"
Feb 11 09:45:18.600: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 11 09:45:18.600: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8243" to be "running and ready"
Feb 11 09:45:18.603: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.759114ms
Feb 11 09:45:20.607: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006743195s
Feb 11 09:45:20.607: INFO: Pod "pause" satisfied condition "running and ready"
Feb 11 09:45:20.607: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 11 09:45:20.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 label pods pause testing-label=testing-label-value --namespace=kubectl-8243'
Feb 11 09:45:20.678: INFO: stderr: ""
Feb 11 09:45:20.678: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 11 09:45:20.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pod pause -L testing-label --namespace=kubectl-8243'
Feb 11 09:45:20.745: INFO: stderr: ""
Feb 11 09:45:20.745: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 11 09:45:20.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 label pods pause testing-label- --namespace=kubectl-8243'
Feb 11 09:45:20.814: INFO: stderr: ""
Feb 11 09:45:20.814: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 11 09:45:20.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pod pause -L testing-label --namespace=kubectl-8243'
Feb 11 09:45:20.879: INFO: stderr: ""
Feb 11 09:45:20.879: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Feb 11 09:45:20.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 delete --grace-period=0 --force -f - --namespace=kubectl-8243'
Feb 11 09:45:20.950: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 11 09:45:20.950: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 11 09:45:20.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get rc,svc -l name=pause --no-headers --namespace=kubectl-8243'
Feb 11 09:45:21.023: INFO: stderr: "No resources found in kubectl-8243 namespace.\n"
Feb 11 09:45:21.023: INFO: stdout: ""
Feb 11 09:45:21.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods -l name=pause --namespace=kubectl-8243 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 11 09:45:21.092: INFO: stderr: ""
Feb 11 09:45:21.092: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:45:21.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8243" for this suite.
Feb 11 09:45:27.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:45:27.179: INFO: namespace kubectl-8243 deletion completed in 6.082194656s

â€¢ [SLOW TEST:8.822 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:45:27.179: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:45:34.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-925" for this suite.
Feb 11 09:45:40.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:45:40.284: INFO: namespace resourcequota-925 deletion completed in 6.070946726s

â€¢ [SLOW TEST:13.105 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:45:40.284: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-7159/configmap-test-349cdaf5-1d0b-4606-a93a-9c718c09873c
STEP: Creating a pod to test consume configMaps
Feb 11 09:45:40.313: INFO: Waiting up to 5m0s for pod "pod-configmaps-898080d5-2ab9-44db-9df3-7d5008af1111" in namespace "configmap-7159" to be "success or failure"
Feb 11 09:45:40.315: INFO: Pod "pod-configmaps-898080d5-2ab9-44db-9df3-7d5008af1111": Phase="Pending", Reason="", readiness=false. Elapsed: 2.565766ms
Feb 11 09:45:42.319: INFO: Pod "pod-configmaps-898080d5-2ab9-44db-9df3-7d5008af1111": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006108964s
STEP: Saw pod success
Feb 11 09:45:42.319: INFO: Pod "pod-configmaps-898080d5-2ab9-44db-9df3-7d5008af1111" satisfied condition "success or failure"
Feb 11 09:45:42.322: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-configmaps-898080d5-2ab9-44db-9df3-7d5008af1111 container env-test: <nil>
STEP: delete the pod
Feb 11 09:45:42.342: INFO: Waiting for pod pod-configmaps-898080d5-2ab9-44db-9df3-7d5008af1111 to disappear
Feb 11 09:45:42.344: INFO: Pod pod-configmaps-898080d5-2ab9-44db-9df3-7d5008af1111 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:45:42.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7159" for this suite.
Feb 11 09:45:48.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:45:48.428: INFO: namespace configmap-7159 deletion completed in 6.080342785s

â€¢ [SLOW TEST:8.144 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:45:48.428: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:46:48.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3290" for this suite.
Feb 11 09:47:00.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:47:00.548: INFO: namespace container-probe-3290 deletion completed in 12.076740845s

â€¢ [SLOW TEST:72.120 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:47:00.548: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 11 09:47:01.095: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 11 09:47:03.107: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717011221, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717011221, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717011221, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717011221, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 11 09:47:06.119: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:47:06.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9199" for this suite.
Feb 11 09:47:12.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:47:12.322: INFO: namespace webhook-9199 deletion completed in 6.072274918s
STEP: Destroying namespace "webhook-9199-markers" for this suite.
Feb 11 09:47:18.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:47:18.406: INFO: namespace webhook-9199-markers deletion completed in 6.083869525s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:17.870 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:47:18.419: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0211 09:47:28.472008      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 11 09:47:28.472: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:47:28.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5811" for this suite.
Feb 11 09:47:34.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:47:34.548: INFO: namespace gc-5811 deletion completed in 6.073341296s

â€¢ [SLOW TEST:16.129 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:47:34.548: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:47:34.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1161" for this suite.
Feb 11 09:47:40.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:47:40.663: INFO: namespace resourcequota-1161 deletion completed in 6.072402125s

â€¢ [SLOW TEST:6.115 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:47:40.664: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:47:46.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4349" for this suite.
Feb 11 09:47:52.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:47:52.767: INFO: namespace job-4349 deletion completed in 6.073234397s

â€¢ [SLOW TEST:12.104 seconds]
[sig-apps] Job
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:47:52.768: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 11 09:47:53.803: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:47:53.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6546" for this suite.
Feb 11 09:47:59.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:47:59.890: INFO: namespace container-runtime-6546 deletion completed in 6.072483472s

â€¢ [SLOW TEST:7.123 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:47:59.890: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 09:47:59.913: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:48:04.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6044" for this suite.
Feb 11 09:48:10.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:48:11.036: INFO: namespace custom-resource-definition-6044 deletion completed in 6.076841093s

â€¢ [SLOW TEST:11.145 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:48:11.036: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 11 09:48:15.090: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 11 09:48:15.092: INFO: Pod pod-with-prestop-http-hook still exists
Feb 11 09:48:17.092: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 11 09:48:17.096: INFO: Pod pod-with-prestop-http-hook still exists
Feb 11 09:48:19.092: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 11 09:48:19.096: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:48:19.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6910" for this suite.
Feb 11 09:48:31.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:48:31.192: INFO: namespace container-lifecycle-hook-6910 deletion completed in 12.07849635s

â€¢ [SLOW TEST:20.156 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:48:31.192: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Feb 11 09:48:31.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 api-versions'
Feb 11 09:48:31.285: INFO: stderr: ""
Feb 11 09:48:31.285: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\nalicloud.com/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:48:31.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5712" for this suite.
Feb 11 09:48:37.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:48:37.363: INFO: namespace kubectl-5712 deletion completed in 6.074505939s

â€¢ [SLOW TEST:6.171 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:48:37.363: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:48:54.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9956" for this suite.
Feb 11 09:49:00.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:49:00.499: INFO: namespace resourcequota-9956 deletion completed in 6.07589854s

â€¢ [SLOW TEST:23.136 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:49:00.500: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:49:04.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-98" for this suite.
Feb 11 09:49:10.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:49:10.976: INFO: namespace watch-98 deletion completed in 6.171605252s

â€¢ [SLOW TEST:10.477 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:49:10.976: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 11 09:49:11.004: INFO: Waiting up to 5m0s for pod "pod-ec385615-d9ee-47a4-a074-4700717a24b8" in namespace "emptydir-260" to be "success or failure"
Feb 11 09:49:11.006: INFO: Pod "pod-ec385615-d9ee-47a4-a074-4700717a24b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.280832ms
Feb 11 09:49:13.010: INFO: Pod "pod-ec385615-d9ee-47a4-a074-4700717a24b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005375545s
STEP: Saw pod success
Feb 11 09:49:13.010: INFO: Pod "pod-ec385615-d9ee-47a4-a074-4700717a24b8" satisfied condition "success or failure"
Feb 11 09:49:13.012: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-ec385615-d9ee-47a4-a074-4700717a24b8 container test-container: <nil>
STEP: delete the pod
Feb 11 09:49:13.029: INFO: Waiting for pod pod-ec385615-d9ee-47a4-a074-4700717a24b8 to disappear
Feb 11 09:49:13.031: INFO: Pod pod-ec385615-d9ee-47a4-a074-4700717a24b8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:49:13.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-260" for this suite.
Feb 11 09:49:19.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:49:19.110: INFO: namespace emptydir-260 deletion completed in 6.075610905s

â€¢ [SLOW TEST:8.133 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:49:19.110: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-7b25cb7a-fdef-483c-8f30-59740065a36a
STEP: Creating configMap with name cm-test-opt-upd-6d0c41e8-e0f4-4a18-b866-ea00a6ccf39e
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-7b25cb7a-fdef-483c-8f30-59740065a36a
STEP: Updating configmap cm-test-opt-upd-6d0c41e8-e0f4-4a18-b866-ea00a6ccf39e
STEP: Creating configMap with name cm-test-opt-create-0b732045-ed46-486d-8437-4b69561e2c8e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:49:23.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9873" for this suite.
Feb 11 09:49:41.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:49:41.300: INFO: namespace projected-9873 deletion completed in 18.072737861s

â€¢ [SLOW TEST:22.190 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:49:41.300: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 11 09:49:41.328: INFO: Waiting up to 5m0s for pod "pod-857063d8-1286-4a6e-8b13-ce17fc9823b3" in namespace "emptydir-1785" to be "success or failure"
Feb 11 09:49:41.331: INFO: Pod "pod-857063d8-1286-4a6e-8b13-ce17fc9823b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.911124ms
Feb 11 09:49:43.335: INFO: Pod "pod-857063d8-1286-4a6e-8b13-ce17fc9823b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006930639s
STEP: Saw pod success
Feb 11 09:49:43.335: INFO: Pod "pod-857063d8-1286-4a6e-8b13-ce17fc9823b3" satisfied condition "success or failure"
Feb 11 09:49:43.337: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-857063d8-1286-4a6e-8b13-ce17fc9823b3 container test-container: <nil>
STEP: delete the pod
Feb 11 09:49:43.360: INFO: Waiting for pod pod-857063d8-1286-4a6e-8b13-ce17fc9823b3 to disappear
Feb 11 09:49:43.362: INFO: Pod pod-857063d8-1286-4a6e-8b13-ce17fc9823b3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:49:43.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1785" for this suite.
Feb 11 09:49:49.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:49:49.439: INFO: namespace emptydir-1785 deletion completed in 6.074103614s

â€¢ [SLOW TEST:8.139 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:49:49.440: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 11 09:49:49.467: INFO: Waiting up to 5m0s for pod "pod-e6060a27-63b1-4aa2-9d11-219912197a08" in namespace "emptydir-2326" to be "success or failure"
Feb 11 09:49:49.469: INFO: Pod "pod-e6060a27-63b1-4aa2-9d11-219912197a08": Phase="Pending", Reason="", readiness=false. Elapsed: 1.993101ms
Feb 11 09:49:51.473: INFO: Pod "pod-e6060a27-63b1-4aa2-9d11-219912197a08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005488388s
STEP: Saw pod success
Feb 11 09:49:51.473: INFO: Pod "pod-e6060a27-63b1-4aa2-9d11-219912197a08" satisfied condition "success or failure"
Feb 11 09:49:51.475: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-e6060a27-63b1-4aa2-9d11-219912197a08 container test-container: <nil>
STEP: delete the pod
Feb 11 09:49:51.491: INFO: Waiting for pod pod-e6060a27-63b1-4aa2-9d11-219912197a08 to disappear
Feb 11 09:49:51.493: INFO: Pod pod-e6060a27-63b1-4aa2-9d11-219912197a08 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:49:51.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2326" for this suite.
Feb 11 09:49:57.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:49:57.574: INFO: namespace emptydir-2326 deletion completed in 6.078391219s

â€¢ [SLOW TEST:8.135 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:49:57.575: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 09:49:57.615: INFO: Create a RollingUpdate DaemonSet
Feb 11 09:49:57.618: INFO: Check that daemon pods launch on every node of the cluster
Feb 11 09:49:57.621: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:49:57.621: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:49:57.621: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:49:57.624: INFO: Number of nodes with available pods: 0
Feb 11 09:49:57.624: INFO: Node cn-hongkong.192.168.0.18 is running more than one daemon pod
Feb 11 09:49:58.628: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:49:58.628: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:49:58.628: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:49:58.631: INFO: Number of nodes with available pods: 0
Feb 11 09:49:58.631: INFO: Node cn-hongkong.192.168.0.18 is running more than one daemon pod
Feb 11 09:49:59.628: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:49:59.628: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:49:59.628: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:49:59.631: INFO: Number of nodes with available pods: 3
Feb 11 09:49:59.631: INFO: Number of running nodes: 3, number of available pods: 3
Feb 11 09:49:59.631: INFO: Update the DaemonSet to trigger a rollout
Feb 11 09:49:59.637: INFO: Updating DaemonSet daemon-set
Feb 11 09:50:03.650: INFO: Roll back the DaemonSet before rollout is complete
Feb 11 09:50:03.656: INFO: Updating DaemonSet daemon-set
Feb 11 09:50:03.656: INFO: Make sure DaemonSet rollback is complete
Feb 11 09:50:03.658: INFO: Wrong image for pod: daemon-set-p8ptz. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 11 09:50:03.658: INFO: Pod daemon-set-p8ptz is not available
Feb 11 09:50:03.662: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:50:03.662: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:50:03.662: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:50:04.666: INFO: Wrong image for pod: daemon-set-p8ptz. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 11 09:50:04.666: INFO: Pod daemon-set-p8ptz is not available
Feb 11 09:50:04.669: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:50:04.669: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:50:04.669: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:50:05.665: INFO: Wrong image for pod: daemon-set-p8ptz. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 11 09:50:05.665: INFO: Pod daemon-set-p8ptz is not available
Feb 11 09:50:05.669: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:50:05.669: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:50:05.669: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:50:06.666: INFO: Pod daemon-set-jmb8t is not available
Feb 11 09:50:06.669: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:50:06.669: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 09:50:06.669: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6868, will wait for the garbage collector to delete the pods
Feb 11 09:50:06.734: INFO: Deleting DaemonSet.extensions daemon-set took: 7.040873ms
Feb 11 09:50:07.134: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.222693ms
Feb 11 09:50:18.037: INFO: Number of nodes with available pods: 0
Feb 11 09:50:18.037: INFO: Number of running nodes: 0, number of available pods: 0
Feb 11 09:50:18.040: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6868/daemonsets","resourceVersion":"30479"},"items":null}

Feb 11 09:50:18.042: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6868/pods","resourceVersion":"30479"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:50:18.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6868" for this suite.
Feb 11 09:50:24.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:50:24.130: INFO: namespace daemonsets-6868 deletion completed in 6.074095071s

â€¢ [SLOW TEST:26.556 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:50:24.131: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 11 09:50:24.953: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 11 09:50:27.969: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 09:50:27.972: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2279-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:50:34.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8709" for this suite.
Feb 11 09:50:40.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:50:40.222: INFO: namespace webhook-8709 deletion completed in 6.073943929s
STEP: Destroying namespace "webhook-8709-markers" for this suite.
Feb 11 09:50:46.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:50:46.294: INFO: namespace webhook-8709-markers deletion completed in 6.071729169s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:22.174 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:50:46.305: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Feb 11 09:50:46.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 create -f - --namespace=kubectl-8432'
Feb 11 09:50:46.481: INFO: stderr: ""
Feb 11 09:50:46.481: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 11 09:50:46.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8432'
Feb 11 09:50:46.549: INFO: stderr: ""
Feb 11 09:50:46.549: INFO: stdout: "update-demo-nautilus-6smbn update-demo-nautilus-k6jn7 "
Feb 11 09:50:46.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-nautilus-6smbn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8432'
Feb 11 09:50:46.615: INFO: stderr: ""
Feb 11 09:50:46.615: INFO: stdout: ""
Feb 11 09:50:46.615: INFO: update-demo-nautilus-6smbn is created but not running
Feb 11 09:50:51.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8432'
Feb 11 09:50:51.684: INFO: stderr: ""
Feb 11 09:50:51.684: INFO: stdout: "update-demo-nautilus-6smbn update-demo-nautilus-k6jn7 "
Feb 11 09:50:51.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-nautilus-6smbn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8432'
Feb 11 09:50:51.747: INFO: stderr: ""
Feb 11 09:50:51.747: INFO: stdout: "true"
Feb 11 09:50:51.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-nautilus-6smbn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8432'
Feb 11 09:50:51.812: INFO: stderr: ""
Feb 11 09:50:51.812: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 11 09:50:51.812: INFO: validating pod update-demo-nautilus-6smbn
Feb 11 09:50:51.816: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 11 09:50:51.816: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 11 09:50:51.816: INFO: update-demo-nautilus-6smbn is verified up and running
Feb 11 09:50:51.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-nautilus-k6jn7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8432'
Feb 11 09:50:51.881: INFO: stderr: ""
Feb 11 09:50:51.881: INFO: stdout: "true"
Feb 11 09:50:51.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-nautilus-k6jn7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8432'
Feb 11 09:50:51.947: INFO: stderr: ""
Feb 11 09:50:51.948: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 11 09:50:51.948: INFO: validating pod update-demo-nautilus-k6jn7
Feb 11 09:50:51.952: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 11 09:50:51.952: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 11 09:50:51.952: INFO: update-demo-nautilus-k6jn7 is verified up and running
STEP: rolling-update to new replication controller
Feb 11 09:50:51.954: INFO: scanned /root for discovery docs: <nil>
Feb 11 09:50:51.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-8432'
Feb 11 09:51:14.322: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 11 09:51:14.322: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 11 09:51:14.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8432'
Feb 11 09:51:14.392: INFO: stderr: ""
Feb 11 09:51:14.392: INFO: stdout: "update-demo-kitten-5jp76 update-demo-kitten-grxgw "
Feb 11 09:51:14.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-kitten-5jp76 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8432'
Feb 11 09:51:14.457: INFO: stderr: ""
Feb 11 09:51:14.457: INFO: stdout: "true"
Feb 11 09:51:14.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-kitten-5jp76 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8432'
Feb 11 09:51:14.522: INFO: stderr: ""
Feb 11 09:51:14.522: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 11 09:51:14.522: INFO: validating pod update-demo-kitten-5jp76
Feb 11 09:51:14.526: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 11 09:51:14.526: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 11 09:51:14.527: INFO: update-demo-kitten-5jp76 is verified up and running
Feb 11 09:51:14.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-kitten-grxgw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8432'
Feb 11 09:51:14.596: INFO: stderr: ""
Feb 11 09:51:14.596: INFO: stdout: "true"
Feb 11 09:51:14.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-kitten-grxgw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8432'
Feb 11 09:51:14.662: INFO: stderr: ""
Feb 11 09:51:14.662: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 11 09:51:14.662: INFO: validating pod update-demo-kitten-grxgw
Feb 11 09:51:14.666: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 11 09:51:14.667: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 11 09:51:14.667: INFO: update-demo-kitten-grxgw is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:51:14.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8432" for this suite.
Feb 11 09:51:42.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:51:42.745: INFO: namespace kubectl-8432 deletion completed in 28.075241684s

â€¢ [SLOW TEST:56.441 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:51:42.746: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 11 09:51:42.773: INFO: Waiting up to 5m0s for pod "downwardapi-volume-28d1e4a2-aae6-4e6d-897a-53b035104b8b" in namespace "projected-4430" to be "success or failure"
Feb 11 09:51:42.775: INFO: Pod "downwardapi-volume-28d1e4a2-aae6-4e6d-897a-53b035104b8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.701802ms
Feb 11 09:51:44.779: INFO: Pod "downwardapi-volume-28d1e4a2-aae6-4e6d-897a-53b035104b8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006236556s
STEP: Saw pod success
Feb 11 09:51:44.779: INFO: Pod "downwardapi-volume-28d1e4a2-aae6-4e6d-897a-53b035104b8b" satisfied condition "success or failure"
Feb 11 09:51:44.782: INFO: Trying to get logs from node cn-hongkong.192.168.0.19 pod downwardapi-volume-28d1e4a2-aae6-4e6d-897a-53b035104b8b container client-container: <nil>
STEP: delete the pod
Feb 11 09:51:44.805: INFO: Waiting for pod downwardapi-volume-28d1e4a2-aae6-4e6d-897a-53b035104b8b to disappear
Feb 11 09:51:44.807: INFO: Pod downwardapi-volume-28d1e4a2-aae6-4e6d-897a-53b035104b8b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:51:44.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4430" for this suite.
Feb 11 09:51:50.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:51:50.883: INFO: namespace projected-4430 deletion completed in 6.072870399s

â€¢ [SLOW TEST:8.138 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:51:50.883: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Feb 11 09:51:50.905: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:51:54.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3766" for this suite.
Feb 11 09:52:00.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:52:00.684: INFO: namespace init-container-3766 deletion completed in 6.072996828s

â€¢ [SLOW TEST:9.801 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:52:00.684: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Feb 11 09:52:00.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 --namespace=kubectl-9583 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb 11 09:52:01.831: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb 11 09:52:01.831: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:52:03.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9583" for this suite.
Feb 11 09:52:09.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:52:09.914: INFO: namespace kubectl-9583 deletion completed in 6.074242198s

â€¢ [SLOW TEST:9.229 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:52:09.914: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-bed9da67-b04c-4be4-9543-6da1ffd13347 in namespace container-probe-7789
Feb 11 09:52:11.946: INFO: Started pod busybox-bed9da67-b04c-4be4-9543-6da1ffd13347 in namespace container-probe-7789
STEP: checking the pod's current state and verifying that restartCount is present
Feb 11 09:52:11.949: INFO: Initial restart count of pod busybox-bed9da67-b04c-4be4-9543-6da1ffd13347 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:56:12.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7789" for this suite.
Feb 11 09:56:18.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:56:18.452: INFO: namespace container-probe-7789 deletion completed in 6.080633342s

â€¢ [SLOW TEST:248.538 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:56:18.452: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Feb 11 09:56:18.485: INFO: Waiting up to 5m0s for pod "var-expansion-1be895eb-a78f-4bdd-a012-db2cf4dfe517" in namespace "var-expansion-5719" to be "success or failure"
Feb 11 09:56:18.487: INFO: Pod "var-expansion-1be895eb-a78f-4bdd-a012-db2cf4dfe517": Phase="Pending", Reason="", readiness=false. Elapsed: 2.19014ms
Feb 11 09:56:20.491: INFO: Pod "var-expansion-1be895eb-a78f-4bdd-a012-db2cf4dfe517": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005425435s
STEP: Saw pod success
Feb 11 09:56:20.491: INFO: Pod "var-expansion-1be895eb-a78f-4bdd-a012-db2cf4dfe517" satisfied condition "success or failure"
Feb 11 09:56:20.493: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod var-expansion-1be895eb-a78f-4bdd-a012-db2cf4dfe517 container dapi-container: <nil>
STEP: delete the pod
Feb 11 09:56:20.519: INFO: Waiting for pod var-expansion-1be895eb-a78f-4bdd-a012-db2cf4dfe517 to disappear
Feb 11 09:56:20.521: INFO: Pod var-expansion-1be895eb-a78f-4bdd-a012-db2cf4dfe517 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:56:20.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5719" for this suite.
Feb 11 09:56:26.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:56:26.604: INFO: namespace var-expansion-5719 deletion completed in 6.079828535s

â€¢ [SLOW TEST:8.152 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:56:26.604: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-p7h7
STEP: Creating a pod to test atomic-volume-subpath
Feb 11 09:56:26.636: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-p7h7" in namespace "subpath-6297" to be "success or failure"
Feb 11 09:56:26.639: INFO: Pod "pod-subpath-test-configmap-p7h7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.172925ms
Feb 11 09:56:28.642: INFO: Pod "pod-subpath-test-configmap-p7h7": Phase="Running", Reason="", readiness=true. Elapsed: 2.005019824s
Feb 11 09:56:30.645: INFO: Pod "pod-subpath-test-configmap-p7h7": Phase="Running", Reason="", readiness=true. Elapsed: 4.008548809s
Feb 11 09:56:32.649: INFO: Pod "pod-subpath-test-configmap-p7h7": Phase="Running", Reason="", readiness=true. Elapsed: 6.012168696s
Feb 11 09:56:34.652: INFO: Pod "pod-subpath-test-configmap-p7h7": Phase="Running", Reason="", readiness=true. Elapsed: 8.015571443s
Feb 11 09:56:36.655: INFO: Pod "pod-subpath-test-configmap-p7h7": Phase="Running", Reason="", readiness=true. Elapsed: 10.018734775s
Feb 11 09:56:38.659: INFO: Pod "pod-subpath-test-configmap-p7h7": Phase="Running", Reason="", readiness=true. Elapsed: 12.021994951s
Feb 11 09:56:40.662: INFO: Pod "pod-subpath-test-configmap-p7h7": Phase="Running", Reason="", readiness=true. Elapsed: 14.025288693s
Feb 11 09:56:42.665: INFO: Pod "pod-subpath-test-configmap-p7h7": Phase="Running", Reason="", readiness=true. Elapsed: 16.028602234s
Feb 11 09:56:44.669: INFO: Pod "pod-subpath-test-configmap-p7h7": Phase="Running", Reason="", readiness=true. Elapsed: 18.032080684s
Feb 11 09:56:46.672: INFO: Pod "pod-subpath-test-configmap-p7h7": Phase="Running", Reason="", readiness=true. Elapsed: 20.035207677s
Feb 11 09:56:48.675: INFO: Pod "pod-subpath-test-configmap-p7h7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.038529002s
STEP: Saw pod success
Feb 11 09:56:48.675: INFO: Pod "pod-subpath-test-configmap-p7h7" satisfied condition "success or failure"
Feb 11 09:56:48.678: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-subpath-test-configmap-p7h7 container test-container-subpath-configmap-p7h7: <nil>
STEP: delete the pod
Feb 11 09:56:48.693: INFO: Waiting for pod pod-subpath-test-configmap-p7h7 to disappear
Feb 11 09:56:48.695: INFO: Pod pod-subpath-test-configmap-p7h7 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-p7h7
Feb 11 09:56:48.695: INFO: Deleting pod "pod-subpath-test-configmap-p7h7" in namespace "subpath-6297"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:56:48.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6297" for this suite.
Feb 11 09:56:54.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:56:54.771: INFO: namespace subpath-6297 deletion completed in 6.07074474s

â€¢ [SLOW TEST:28.167 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:56:54.771: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-502d3089-f050-4538-b1a1-6fd78df7a873
STEP: Creating configMap with name cm-test-opt-upd-73e03e7d-b8ba-44d0-83af-cbbb7168af36
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-502d3089-f050-4538-b1a1-6fd78df7a873
STEP: Updating configmap cm-test-opt-upd-73e03e7d-b8ba-44d0-83af-cbbb7168af36
STEP: Creating configMap with name cm-test-opt-create-f4d2d3ee-482e-4c2d-bc95-58b222fd794d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:57:00.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4090" for this suite.
Feb 11 09:57:22.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:57:22.951: INFO: namespace configmap-4090 deletion completed in 22.072476659s

â€¢ [SLOW TEST:28.180 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:57:22.951: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-bf12c235-adf4-4a62-a0a0-2a9037cacdb8
STEP: Creating a pod to test consume configMaps
Feb 11 09:57:22.980: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f270e5d3-767a-47ff-bddd-b89dc1213364" in namespace "projected-959" to be "success or failure"
Feb 11 09:57:22.982: INFO: Pod "pod-projected-configmaps-f270e5d3-767a-47ff-bddd-b89dc1213364": Phase="Pending", Reason="", readiness=false. Elapsed: 2.378051ms
Feb 11 09:57:24.986: INFO: Pod "pod-projected-configmaps-f270e5d3-767a-47ff-bddd-b89dc1213364": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005515564s
STEP: Saw pod success
Feb 11 09:57:24.986: INFO: Pod "pod-projected-configmaps-f270e5d3-767a-47ff-bddd-b89dc1213364" satisfied condition "success or failure"
Feb 11 09:57:24.988: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-projected-configmaps-f270e5d3-767a-47ff-bddd-b89dc1213364 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 11 09:57:25.002: INFO: Waiting for pod pod-projected-configmaps-f270e5d3-767a-47ff-bddd-b89dc1213364 to disappear
Feb 11 09:57:25.004: INFO: Pod pod-projected-configmaps-f270e5d3-767a-47ff-bddd-b89dc1213364 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:57:25.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-959" for this suite.
Feb 11 09:57:31.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:57:31.089: INFO: namespace projected-959 deletion completed in 6.081569579s

â€¢ [SLOW TEST:8.138 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:57:31.089: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 09:57:31.117: INFO: Creating deployment "webserver-deployment"
Feb 11 09:57:31.121: INFO: Waiting for observed generation 1
Feb 11 09:57:33.126: INFO: Waiting for all required pods to come up
Feb 11 09:57:33.130: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 11 09:57:35.137: INFO: Waiting for deployment "webserver-deployment" to complete
Feb 11 09:57:35.142: INFO: Updating deployment "webserver-deployment" with a non-existent image
Feb 11 09:57:35.148: INFO: Updating deployment webserver-deployment
Feb 11 09:57:35.148: INFO: Waiting for observed generation 2
Feb 11 09:57:37.154: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 11 09:57:37.157: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 11 09:57:37.159: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 11 09:57:37.166: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 11 09:57:37.166: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 11 09:57:37.168: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 11 09:57:37.172: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Feb 11 09:57:37.172: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Feb 11 09:57:37.178: INFO: Updating deployment webserver-deployment
Feb 11 09:57:37.178: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Feb 11 09:57:37.185: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 11 09:57:37.190: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Feb 11 09:57:37.207: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8949 /apis/apps/v1/namespaces/deployment-8949/deployments/webserver-deployment a3c89c01-c1ed-427e-a92a-e6d57c926765 32335 3 2020-02-11 09:57:31 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005f80908 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-02-11 09:57:35 +0000 UTC,LastTransitionTime:2020-02-11 09:57:31 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-02-11 09:57:37 +0000 UTC,LastTransitionTime:2020-02-11 09:57:37 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Feb 11 09:57:37.216: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-8949 /apis/apps/v1/namespaces/deployment-8949/replicasets/webserver-deployment-c7997dcc8 8ccc99ef-5216-4026-8608-cbdd97a8386e 32331 3 2020-02-11 09:57:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment a3c89c01-c1ed-427e-a92a-e6d57c926765 0xc005f80e37 0xc005f80e38}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005f80ea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 11 09:57:37.216: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Feb 11 09:57:37.216: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-8949 /apis/apps/v1/namespaces/deployment-8949/replicasets/webserver-deployment-595b5b9587 0b2ee672-17b1-4003-a288-1f8ecd07241f 32328 3 2020-02-11 09:57:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment a3c89c01-c1ed-427e-a92a-e6d57c926765 0xc005f80d77 0xc005f80d78}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005f80dd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Feb 11 09:57:37.234: INFO: Pod "webserver-deployment-595b5b9587-2c2pc" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2c2pc webserver-deployment-595b5b9587- deployment-8949 /api/v1/namespaces/deployment-8949/pods/webserver-deployment-595b5b9587-2c2pc cb0fb464-044f-4373-9e02-5ff406113835 32356 0 2020-02-11 09:57:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 0b2ee672-17b1-4003-a288-1f8ecd07241f 0xc005f81387 0xc005f81388}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzwhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzwhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzwhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.19,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 11 09:57:37.235: INFO: Pod "webserver-deployment-595b5b9587-49bh7" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-49bh7 webserver-deployment-595b5b9587- deployment-8949 /api/v1/namespaces/deployment-8949/pods/webserver-deployment-595b5b9587-49bh7 d9400d87-d1fc-4c19-a3cb-31be43bd45dd 32225 0 2020-02-11 09:57:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 0b2ee672-17b1-4003-a288-1f8ecd07241f 0xc005f81490 0xc005f81491}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzwhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzwhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzwhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.20,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.20,PodIP:172.22.1.203,StartTime:2020-02-11 09:57:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-11 09:57:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a3f7388f4243cba4737f05c8b9caa0a8ebb0ae81d2bcd261eeb05c1489ede0a3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.22.1.203,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 11 09:57:37.235: INFO: Pod "webserver-deployment-595b5b9587-4ftc8" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4ftc8 webserver-deployment-595b5b9587- deployment-8949 /api/v1/namespaces/deployment-8949/pods/webserver-deployment-595b5b9587-4ftc8 2d1e703d-49d5-4899-ad23-d3db758f726f 32359 0 2020-02-11 09:57:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 0b2ee672-17b1-4003-a288-1f8ecd07241f 0xc005f815f7 0xc005f815f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzwhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzwhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzwhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.18,PodIP:,StartTime:2020-02-11 09:57:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 11 09:57:37.235: INFO: Pod "webserver-deployment-595b5b9587-57d9c" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-57d9c webserver-deployment-595b5b9587- deployment-8949 /api/v1/namespaces/deployment-8949/pods/webserver-deployment-595b5b9587-57d9c 71ffd0a9-2e8a-4cdd-8c10-cfb78cf1f16d 32340 0 2020-02-11 09:57:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 0b2ee672-17b1-4003-a288-1f8ecd07241f 0xc005f81747 0xc005f81748}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzwhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzwhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzwhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 11 09:57:37.235: INFO: Pod "webserver-deployment-595b5b9587-brt6c" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-brt6c webserver-deployment-595b5b9587- deployment-8949 /api/v1/namespaces/deployment-8949/pods/webserver-deployment-595b5b9587-brt6c 5ec01244-244f-4fc4-8f19-d1504171abee 32257 0 2020-02-11 09:57:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 0b2ee672-17b1-4003-a288-1f8ecd07241f 0xc005f81850 0xc005f81851}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzwhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzwhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzwhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.19,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.19,PodIP:172.22.2.188,StartTime:2020-02-11 09:57:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-11 09:57:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://17ecf8508daf913e81981bc1f274f9feacaa84aabf5de4d31697b1281fffa182,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.22.2.188,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 11 09:57:37.235: INFO: Pod "webserver-deployment-595b5b9587-dpd6m" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-dpd6m webserver-deployment-595b5b9587- deployment-8949 /api/v1/namespaces/deployment-8949/pods/webserver-deployment-595b5b9587-dpd6m 0ebb6bb5-01de-47e9-b786-6c8a16ebe554 32222 0 2020-02-11 09:57:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 0b2ee672-17b1-4003-a288-1f8ecd07241f 0xc005f819b7 0xc005f819b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzwhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzwhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzwhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.20,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.20,PodIP:172.22.1.202,StartTime:2020-02-11 09:57:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-11 09:57:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://bd030a9982d2c36770272ddf9466f27b8bd9b9972e7b6f8f9088ba0f261c21a8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.22.1.202,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 11 09:57:37.236: INFO: Pod "webserver-deployment-595b5b9587-ffhsk" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-ffhsk webserver-deployment-595b5b9587- deployment-8949 /api/v1/namespaces/deployment-8949/pods/webserver-deployment-595b5b9587-ffhsk 0b724885-7b98-4b09-aaf3-2560940ee10c 32228 0 2020-02-11 09:57:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 0b2ee672-17b1-4003-a288-1f8ecd07241f 0xc005f81b27 0xc005f81b28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzwhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzwhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzwhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.20,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.20,PodIP:172.22.1.204,StartTime:2020-02-11 09:57:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-11 09:57:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://5a8f3e73cadaacf97e585174867230cf65a215ebcf8c020ea2b34fd377f2dcf7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.22.1.204,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 11 09:57:37.236: INFO: Pod "webserver-deployment-595b5b9587-fj54p" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fj54p webserver-deployment-595b5b9587- deployment-8949 /api/v1/namespaces/deployment-8949/pods/webserver-deployment-595b5b9587-fj54p e3bab8d4-9245-437f-86d9-6bd790663dd4 32234 0 2020-02-11 09:57:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 0b2ee672-17b1-4003-a288-1f8ecd07241f 0xc005f81c97 0xc005f81c98}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzwhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzwhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzwhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.19,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.19,PodIP:172.22.2.186,StartTime:2020-02-11 09:57:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-11 09:57:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://6483dd502aa88a8581c70cff5ee72c118991aef691a57a7e837d7f729fef7e23,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.22.2.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 11 09:57:37.236: INFO: Pod "webserver-deployment-595b5b9587-h8tl8" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-h8tl8 webserver-deployment-595b5b9587- deployment-8949 /api/v1/namespaces/deployment-8949/pods/webserver-deployment-595b5b9587-h8tl8 0233a078-616e-4695-8edf-f306db6ff827 32251 0 2020-02-11 09:57:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 0b2ee672-17b1-4003-a288-1f8ecd07241f 0xc005f81e07 0xc005f81e08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzwhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzwhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzwhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.18,PodIP:172.22.2.73,StartTime:2020-02-11 09:57:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-11 09:57:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://9dbcde9ea8261d39ea808b37b5b2243a9ec8c372524c6c3fd0f846e37da6c9f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.22.2.73,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 11 09:57:37.236: INFO: Pod "webserver-deployment-595b5b9587-lp7xs" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lp7xs webserver-deployment-595b5b9587- deployment-8949 /api/v1/namespaces/deployment-8949/pods/webserver-deployment-595b5b9587-lp7xs baf89c9b-18d0-4e1f-b920-b422e882ac61 32352 0 2020-02-11 09:57:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 0b2ee672-17b1-4003-a288-1f8ecd07241f 0xc005f81f70 0xc005f81f71}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzwhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzwhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzwhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 11 09:57:37.236: INFO: Pod "webserver-deployment-595b5b9587-lpvss" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lpvss webserver-deployment-595b5b9587- deployment-8949 /api/v1/namespaces/deployment-8949/pods/webserver-deployment-595b5b9587-lpvss 849d0ac2-86d0-43f8-88d3-aef6bad12186 32237 0 2020-02-11 09:57:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 0b2ee672-17b1-4003-a288-1f8ecd07241f 0xc001d28070 0xc001d28071}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzwhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzwhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzwhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.19,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.19,PodIP:172.22.2.189,StartTime:2020-02-11 09:57:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-11 09:57:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ad71443884d4a7811ff0a9a3029830f358cadc2e062249403192cc74cffa165c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.22.2.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 11 09:57:37.237: INFO: Pod "webserver-deployment-595b5b9587-qsh78" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qsh78 webserver-deployment-595b5b9587- deployment-8949 /api/v1/namespaces/deployment-8949/pods/webserver-deployment-595b5b9587-qsh78 b664359f-592d-477a-907c-711ba1cf845b 32360 0 2020-02-11 09:57:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 0b2ee672-17b1-4003-a288-1f8ecd07241f 0xc001d281d7 0xc001d281d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzwhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzwhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzwhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.20,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 11 09:57:37.237: INFO: Pod "webserver-deployment-595b5b9587-qxnkn" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qxnkn webserver-deployment-595b5b9587- deployment-8949 /api/v1/namespaces/deployment-8949/pods/webserver-deployment-595b5b9587-qxnkn 3041184d-fc0f-4446-bf52-aff61e4449f0 32351 0 2020-02-11 09:57:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 0b2ee672-17b1-4003-a288-1f8ecd07241f 0xc001d282e0 0xc001d282e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzwhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzwhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzwhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 11 09:57:37.237: INFO: Pod "webserver-deployment-595b5b9587-r54lz" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-r54lz webserver-deployment-595b5b9587- deployment-8949 /api/v1/namespaces/deployment-8949/pods/webserver-deployment-595b5b9587-r54lz b4c52b7e-4eae-4ccd-96c6-f3f1aa07bc56 32342 0 2020-02-11 09:57:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 0b2ee672-17b1-4003-a288-1f8ecd07241f 0xc001d284f0 0xc001d284f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzwhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzwhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzwhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 11 09:57:37.237: INFO: Pod "webserver-deployment-595b5b9587-rr52d" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rr52d webserver-deployment-595b5b9587- deployment-8949 /api/v1/namespaces/deployment-8949/pods/webserver-deployment-595b5b9587-rr52d c0a753d8-9569-4ad5-a6fe-90c6a2b60bf5 32240 0 2020-02-11 09:57:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 0b2ee672-17b1-4003-a288-1f8ecd07241f 0xc001d28690 0xc001d28691}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzwhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzwhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzwhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.19,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.19,PodIP:172.22.2.187,StartTime:2020-02-11 09:57:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-11 09:57:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://77b99418d4a26503f14e6ff850ed23aa02cd5d8493492fc0fba037da251a4a30,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.22.2.187,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 11 09:57:37.237: INFO: Pod "webserver-deployment-c7997dcc8-6dp8n" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-6dp8n webserver-deployment-c7997dcc8- deployment-8949 /api/v1/namespaces/deployment-8949/pods/webserver-deployment-c7997dcc8-6dp8n a06ea771-1e97-4a84-8bb9-756cad992487 32357 0 2020-02-11 09:57:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 8ccc99ef-5216-4026-8608-cbdd97a8386e 0xc001d28857 0xc001d28858}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzwhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzwhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzwhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.19,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 11 09:57:37.238: INFO: Pod "webserver-deployment-c7997dcc8-6l7mn" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-6l7mn webserver-deployment-c7997dcc8- deployment-8949 /api/v1/namespaces/deployment-8949/pods/webserver-deployment-c7997dcc8-6l7mn 91d871c4-fa34-4849-99a1-028343c675f5 32348 0 2020-02-11 09:57:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 8ccc99ef-5216-4026-8608-cbdd97a8386e 0xc001d28a80 0xc001d28a81}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzwhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzwhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzwhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 11 09:57:37.238: INFO: Pod "webserver-deployment-c7997dcc8-7ghcl" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-7ghcl webserver-deployment-c7997dcc8- deployment-8949 /api/v1/namespaces/deployment-8949/pods/webserver-deployment-c7997dcc8-7ghcl da418565-7079-4748-abcf-6ec98abbc008 32289 0 2020-02-11 09:57:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 8ccc99ef-5216-4026-8608-cbdd97a8386e 0xc001d28c00 0xc001d28c01}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzwhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzwhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzwhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.19,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.19,PodIP:,StartTime:2020-02-11 09:57:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 11 09:57:37.238: INFO: Pod "webserver-deployment-c7997dcc8-cnbtg" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-cnbtg webserver-deployment-c7997dcc8- deployment-8949 /api/v1/namespaces/deployment-8949/pods/webserver-deployment-c7997dcc8-cnbtg 1e2ee9b5-0444-4c30-9981-587e3a81a801 32316 0 2020-02-11 09:57:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 8ccc99ef-5216-4026-8608-cbdd97a8386e 0xc001d28e97 0xc001d28e98}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzwhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzwhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzwhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.20,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.20,PodIP:,StartTime:2020-02-11 09:57:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 11 09:57:37.238: INFO: Pod "webserver-deployment-c7997dcc8-lxpfc" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-lxpfc webserver-deployment-c7997dcc8- deployment-8949 /api/v1/namespaces/deployment-8949/pods/webserver-deployment-c7997dcc8-lxpfc c2c248e4-56b8-4110-98bd-1dadc64c9913 32291 0 2020-02-11 09:57:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 8ccc99ef-5216-4026-8608-cbdd97a8386e 0xc001d290d7 0xc001d290d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzwhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzwhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzwhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.18,PodIP:,StartTime:2020-02-11 09:57:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 11 09:57:37.238: INFO: Pod "webserver-deployment-c7997dcc8-p44p7" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-p44p7 webserver-deployment-c7997dcc8- deployment-8949 /api/v1/namespaces/deployment-8949/pods/webserver-deployment-c7997dcc8-p44p7 7c59c9cd-cf72-4f3e-ad38-438ed0cae81d 32296 0 2020-02-11 09:57:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 8ccc99ef-5216-4026-8608-cbdd97a8386e 0xc001d293c7 0xc001d293c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzwhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzwhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzwhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.20,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.20,PodIP:,StartTime:2020-02-11 09:57:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 11 09:57:37.238: INFO: Pod "webserver-deployment-c7997dcc8-rlmqf" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-rlmqf webserver-deployment-c7997dcc8- deployment-8949 /api/v1/namespaces/deployment-8949/pods/webserver-deployment-c7997dcc8-rlmqf 2b873686-0724-4987-89b1-adeda49d9af1 32315 0 2020-02-11 09:57:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 8ccc99ef-5216-4026-8608-cbdd97a8386e 0xc001d29537 0xc001d29538}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzwhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzwhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzwhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.18,PodIP:,StartTime:2020-02-11 09:57:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 11 09:57:37.239: INFO: Pod "webserver-deployment-c7997dcc8-sxjpt" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-sxjpt webserver-deployment-c7997dcc8- deployment-8949 /api/v1/namespaces/deployment-8949/pods/webserver-deployment-c7997dcc8-sxjpt 5e4cf641-94b2-453c-8f0b-2122b35efbc5 32358 0 2020-02-11 09:57:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 8ccc99ef-5216-4026-8608-cbdd97a8386e 0xc001d296b7 0xc001d296b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzwhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzwhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzwhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.20,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-11 09:57:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:57:37.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8949" for this suite.
Feb 11 09:57:43.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:57:43.350: INFO: namespace deployment-8949 deletion completed in 6.105301298s

â€¢ [SLOW TEST:12.261 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:57:43.350: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 11 09:57:43.459: INFO: Waiting up to 5m0s for pod "downwardapi-volume-085e69aa-dc7e-4d52-90c4-84f996b53373" in namespace "projected-9195" to be "success or failure"
Feb 11 09:57:43.463: INFO: Pod "downwardapi-volume-085e69aa-dc7e-4d52-90c4-84f996b53373": Phase="Pending", Reason="", readiness=false. Elapsed: 3.523998ms
Feb 11 09:57:45.466: INFO: Pod "downwardapi-volume-085e69aa-dc7e-4d52-90c4-84f996b53373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006914258s
Feb 11 09:57:47.470: INFO: Pod "downwardapi-volume-085e69aa-dc7e-4d52-90c4-84f996b53373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010471458s
STEP: Saw pod success
Feb 11 09:57:47.470: INFO: Pod "downwardapi-volume-085e69aa-dc7e-4d52-90c4-84f996b53373" satisfied condition "success or failure"
Feb 11 09:57:47.472: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod downwardapi-volume-085e69aa-dc7e-4d52-90c4-84f996b53373 container client-container: <nil>
STEP: delete the pod
Feb 11 09:57:47.488: INFO: Waiting for pod downwardapi-volume-085e69aa-dc7e-4d52-90c4-84f996b53373 to disappear
Feb 11 09:57:47.490: INFO: Pod downwardapi-volume-085e69aa-dc7e-4d52-90c4-84f996b53373 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:57:47.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9195" for this suite.
Feb 11 09:57:53.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:57:53.567: INFO: namespace projected-9195 deletion completed in 6.074267653s

â€¢ [SLOW TEST:10.216 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:57:53.567: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-edd29c24-102d-444c-a3df-5136e6597757
STEP: Creating a pod to test consume secrets
Feb 11 09:57:53.598: INFO: Waiting up to 5m0s for pod "pod-secrets-54e411f7-ca69-4e84-8647-f4a3e2777af6" in namespace "secrets-8208" to be "success or failure"
Feb 11 09:57:53.600: INFO: Pod "pod-secrets-54e411f7-ca69-4e84-8647-f4a3e2777af6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.160776ms
Feb 11 09:57:55.604: INFO: Pod "pod-secrets-54e411f7-ca69-4e84-8647-f4a3e2777af6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006001902s
STEP: Saw pod success
Feb 11 09:57:55.604: INFO: Pod "pod-secrets-54e411f7-ca69-4e84-8647-f4a3e2777af6" satisfied condition "success or failure"
Feb 11 09:57:55.606: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-secrets-54e411f7-ca69-4e84-8647-f4a3e2777af6 container secret-volume-test: <nil>
STEP: delete the pod
Feb 11 09:57:55.621: INFO: Waiting for pod pod-secrets-54e411f7-ca69-4e84-8647-f4a3e2777af6 to disappear
Feb 11 09:57:55.622: INFO: Pod pod-secrets-54e411f7-ca69-4e84-8647-f4a3e2777af6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:57:55.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8208" for this suite.
Feb 11 09:58:01.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:58:01.699: INFO: namespace secrets-8208 deletion completed in 6.073281484s

â€¢ [SLOW TEST:8.132 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:58:01.699: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-6180
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-6180
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6180
Feb 11 09:58:01.729: INFO: Found 0 stateful pods, waiting for 1
Feb 11 09:58:11.735: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 11 09:58:11.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=statefulset-6180 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 11 09:58:12.122: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 11 09:58:12.122: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 11 09:58:12.122: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 11 09:58:12.126: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 11 09:58:22.130: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 11 09:58:22.130: INFO: Waiting for statefulset status.replicas updated to 0
Feb 11 09:58:22.142: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Feb 11 09:58:22.143: INFO: ss-0  cn-hongkong.192.168.0.18  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:01 +0000 UTC  }]
Feb 11 09:58:22.143: INFO: 
Feb 11 09:58:22.143: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 11 09:58:23.146: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997241432s
Feb 11 09:58:24.150: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993456581s
Feb 11 09:58:25.154: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989616249s
Feb 11 09:58:26.157: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.98577271s
Feb 11 09:58:27.161: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982462718s
Feb 11 09:58:28.165: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.97854219s
Feb 11 09:58:29.169: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.974519422s
Feb 11 09:58:30.173: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.970921996s
Feb 11 09:58:31.177: INFO: Verifying statefulset ss doesn't scale past 3 for another 967.06839ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6180
Feb 11 09:58:32.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=statefulset-6180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 11 09:58:32.389: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 11 09:58:32.389: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 11 09:58:32.389: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 11 09:58:32.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=statefulset-6180 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 11 09:58:32.594: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 11 09:58:32.594: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 11 09:58:32.594: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 11 09:58:32.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=statefulset-6180 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 11 09:58:32.819: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 11 09:58:32.819: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 11 09:58:32.819: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 11 09:58:32.822: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Feb 11 09:58:42.826: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 11 09:58:42.826: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 11 09:58:42.826: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 11 09:58:42.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=statefulset-6180 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 11 09:58:43.033: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 11 09:58:43.033: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 11 09:58:43.033: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 11 09:58:43.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=statefulset-6180 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 11 09:58:43.237: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 11 09:58:43.237: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 11 09:58:43.237: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 11 09:58:43.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=statefulset-6180 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 11 09:58:43.436: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 11 09:58:43.436: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 11 09:58:43.436: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 11 09:58:43.436: INFO: Waiting for statefulset status.replicas updated to 0
Feb 11 09:58:43.439: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 11 09:58:53.445: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 11 09:58:53.445: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 11 09:58:53.445: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 11 09:58:53.455: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Feb 11 09:58:53.455: INFO: ss-0  cn-hongkong.192.168.0.18  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:01 +0000 UTC  }]
Feb 11 09:58:53.455: INFO: ss-1  cn-hongkong.192.168.0.19  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  }]
Feb 11 09:58:53.455: INFO: ss-2  cn-hongkong.192.168.0.20  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  }]
Feb 11 09:58:53.455: INFO: 
Feb 11 09:58:53.455: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 11 09:58:54.459: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Feb 11 09:58:54.459: INFO: ss-0  cn-hongkong.192.168.0.18  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:01 +0000 UTC  }]
Feb 11 09:58:54.459: INFO: ss-1  cn-hongkong.192.168.0.19  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  }]
Feb 11 09:58:54.459: INFO: ss-2  cn-hongkong.192.168.0.20  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  }]
Feb 11 09:58:54.459: INFO: 
Feb 11 09:58:54.459: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 11 09:58:55.463: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Feb 11 09:58:55.463: INFO: ss-0  cn-hongkong.192.168.0.18  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:01 +0000 UTC  }]
Feb 11 09:58:55.463: INFO: ss-1  cn-hongkong.192.168.0.19  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  }]
Feb 11 09:58:55.463: INFO: ss-2  cn-hongkong.192.168.0.20  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  }]
Feb 11 09:58:55.463: INFO: 
Feb 11 09:58:55.463: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 11 09:58:56.467: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Feb 11 09:58:56.468: INFO: ss-0  cn-hongkong.192.168.0.18  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:01 +0000 UTC  }]
Feb 11 09:58:56.468: INFO: ss-1  cn-hongkong.192.168.0.19  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  }]
Feb 11 09:58:56.468: INFO: ss-2  cn-hongkong.192.168.0.20  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  }]
Feb 11 09:58:56.468: INFO: 
Feb 11 09:58:56.468: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 11 09:58:57.472: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Feb 11 09:58:57.472: INFO: ss-0  cn-hongkong.192.168.0.18  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:01 +0000 UTC  }]
Feb 11 09:58:57.472: INFO: ss-1  cn-hongkong.192.168.0.19  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  }]
Feb 11 09:58:57.472: INFO: ss-2  cn-hongkong.192.168.0.20  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  }]
Feb 11 09:58:57.472: INFO: 
Feb 11 09:58:57.472: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 11 09:58:58.476: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Feb 11 09:58:58.476: INFO: ss-0  cn-hongkong.192.168.0.18  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:01 +0000 UTC  }]
Feb 11 09:58:58.476: INFO: ss-1  cn-hongkong.192.168.0.19  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  }]
Feb 11 09:58:58.476: INFO: 
Feb 11 09:58:58.476: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 11 09:58:59.480: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Feb 11 09:58:59.480: INFO: ss-1  cn-hongkong.192.168.0.19  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  }]
Feb 11 09:58:59.480: INFO: 
Feb 11 09:58:59.480: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 11 09:59:00.484: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Feb 11 09:59:00.484: INFO: ss-1  cn-hongkong.192.168.0.19  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  }]
Feb 11 09:59:00.484: INFO: 
Feb 11 09:59:00.484: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 11 09:59:01.487: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Feb 11 09:59:01.487: INFO: ss-1  cn-hongkong.192.168.0.19  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-11 09:58:22 +0000 UTC  }]
Feb 11 09:59:01.487: INFO: 
Feb 11 09:59:01.487: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 11 09:59:02.491: INFO: Verifying statefulset ss doesn't scale past 0 for another 964.988113ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6180
Feb 11 09:59:03.495: INFO: Scaling statefulset ss to 0
Feb 11 09:59:03.503: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Feb 11 09:59:03.506: INFO: Deleting all statefulset in ns statefulset-6180
Feb 11 09:59:03.508: INFO: Scaling statefulset ss to 0
Feb 11 09:59:03.515: INFO: Waiting for statefulset status.replicas updated to 0
Feb 11 09:59:03.517: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 09:59:03.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6180" for this suite.
Feb 11 09:59:09.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 09:59:09.604: INFO: namespace statefulset-6180 deletion completed in 6.07451019s

â€¢ [SLOW TEST:67.905 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 09:59:09.604: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-6689
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6689
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6689
Feb 11 09:59:09.636: INFO: Found 0 stateful pods, waiting for 1
Feb 11 09:59:19.639: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 11 09:59:19.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=statefulset-6689 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 11 09:59:19.846: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 11 09:59:19.846: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 11 09:59:19.846: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 11 09:59:19.849: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 11 09:59:29.853: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 11 09:59:29.853: INFO: Waiting for statefulset status.replicas updated to 0
Feb 11 09:59:29.864: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999719s
Feb 11 09:59:30.868: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997241272s
Feb 11 09:59:31.872: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993978623s
Feb 11 09:59:32.878: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.99012573s
Feb 11 09:59:33.881: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.983919583s
Feb 11 09:59:34.885: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.980446109s
Feb 11 09:59:35.889: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.976705862s
Feb 11 09:59:36.893: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.972644332s
Feb 11 09:59:37.897: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.969073982s
Feb 11 09:59:38.901: INFO: Verifying statefulset ss doesn't scale past 1 for another 964.956136ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6689
Feb 11 09:59:39.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=statefulset-6689 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 11 09:59:40.101: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 11 09:59:40.101: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 11 09:59:40.101: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 11 09:59:40.105: INFO: Found 1 stateful pods, waiting for 3
Feb 11 09:59:50.109: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 11 09:59:50.109: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 11 09:59:50.109: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 11 09:59:50.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=statefulset-6689 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 11 09:59:50.310: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 11 09:59:50.310: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 11 09:59:50.310: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 11 09:59:50.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=statefulset-6689 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 11 09:59:50.512: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 11 09:59:50.512: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 11 09:59:50.512: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 11 09:59:50.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=statefulset-6689 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 11 09:59:50.726: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 11 09:59:50.726: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 11 09:59:50.726: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 11 09:59:50.726: INFO: Waiting for statefulset status.replicas updated to 0
Feb 11 09:59:50.728: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 11 10:00:00.734: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 11 10:00:00.734: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 11 10:00:00.734: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 11 10:00:00.744: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999731s
Feb 11 10:00:01.748: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996622094s
Feb 11 10:00:02.751: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992469859s
Feb 11 10:00:03.755: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988748822s
Feb 11 10:00:04.758: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985502542s
Feb 11 10:00:05.762: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981706799s
Feb 11 10:00:06.766: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.978003513s
Feb 11 10:00:07.770: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.974399092s
Feb 11 10:00:08.773: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.970652003s
Feb 11 10:00:09.778: INFO: Verifying statefulset ss doesn't scale past 3 for another 966.861565ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6689
Feb 11 10:00:10.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=statefulset-6689 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 11 10:00:10.980: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 11 10:00:10.980: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 11 10:00:10.980: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 11 10:00:10.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=statefulset-6689 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 11 10:00:11.186: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 11 10:00:11.186: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 11 10:00:11.186: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 11 10:00:11.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=statefulset-6689 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 11 10:00:11.395: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 11 10:00:11.395: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 11 10:00:11.395: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 11 10:00:11.395: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Feb 11 10:00:31.409: INFO: Deleting all statefulset in ns statefulset-6689
Feb 11 10:00:31.412: INFO: Scaling statefulset ss to 0
Feb 11 10:00:31.420: INFO: Waiting for statefulset status.replicas updated to 0
Feb 11 10:00:31.422: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:00:31.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6689" for this suite.
Feb 11 10:00:37.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:00:37.511: INFO: namespace statefulset-6689 deletion completed in 6.07605366s

â€¢ [SLOW TEST:87.906 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:00:37.511: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-482635dc-456b-4c03-a032-08ae4190b3d2
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-482635dc-456b-4c03-a032-08ae4190b3d2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:00:43.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4697" for this suite.
Feb 11 10:01:01.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:01:01.673: INFO: namespace projected-4697 deletion completed in 18.08023665s

â€¢ [SLOW TEST:24.162 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:01:01.673: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 10:01:23.708: INFO: Container started at 2020-02-11 10:01:02 +0000 UTC, pod became ready at 2020-02-11 10:01:21 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:01:23.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8847" for this suite.
Feb 11 10:01:35.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:01:35.783: INFO: namespace container-probe-8847 deletion completed in 12.072154054s

â€¢ [SLOW TEST:34.110 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:01:35.783: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 11 10:01:36.169: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 11 10:01:38.181: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717012096, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717012096, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717012096, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717012096, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 11 10:01:41.191: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:01:41.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8949" for this suite.
Feb 11 10:01:47.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:01:47.322: INFO: namespace webhook-8949 deletion completed in 6.075525185s
STEP: Destroying namespace "webhook-8949-markers" for this suite.
Feb 11 10:01:53.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:01:53.397: INFO: namespace webhook-8949-markers deletion completed in 6.074633287s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:17.625 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:01:53.408: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-83e45b69-19c7-4c7c-ab24-7a7bb6f6015f
STEP: Creating secret with name s-test-opt-upd-af3b51ef-5df6-4a12-bba5-0b9a3851f004
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-83e45b69-19c7-4c7c-ab24-7a7bb6f6015f
STEP: Updating secret s-test-opt-upd-af3b51ef-5df6-4a12-bba5-0b9a3851f004
STEP: Creating secret with name s-test-opt-create-f1c9b9bb-15c7-429d-a4f8-7ec22f3567cf
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:01:57.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-235" for this suite.
Feb 11 10:02:09.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:02:09.586: INFO: namespace secrets-235 deletion completed in 12.0686038s

â€¢ [SLOW TEST:16.177 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:02:09.586: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 11 10:02:09.613: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72479f40-0ff2-4a50-800d-32eca6eb256a" in namespace "projected-8651" to be "success or failure"
Feb 11 10:02:09.615: INFO: Pod "downwardapi-volume-72479f40-0ff2-4a50-800d-32eca6eb256a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.232816ms
Feb 11 10:02:11.618: INFO: Pod "downwardapi-volume-72479f40-0ff2-4a50-800d-32eca6eb256a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005174131s
STEP: Saw pod success
Feb 11 10:02:11.618: INFO: Pod "downwardapi-volume-72479f40-0ff2-4a50-800d-32eca6eb256a" satisfied condition "success or failure"
Feb 11 10:02:11.621: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod downwardapi-volume-72479f40-0ff2-4a50-800d-32eca6eb256a container client-container: <nil>
STEP: delete the pod
Feb 11 10:02:11.635: INFO: Waiting for pod downwardapi-volume-72479f40-0ff2-4a50-800d-32eca6eb256a to disappear
Feb 11 10:02:11.637: INFO: Pod downwardapi-volume-72479f40-0ff2-4a50-800d-32eca6eb256a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:02:11.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8651" for this suite.
Feb 11 10:02:17.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:02:17.712: INFO: namespace projected-8651 deletion completed in 6.071988138s

â€¢ [SLOW TEST:8.126 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:02:17.712: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3815
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3815
STEP: creating replication controller externalsvc in namespace services-3815
I0211 10:02:17.755351      23 runners.go:184] Created replication controller with name: externalsvc, namespace: services-3815, replica count: 2
I0211 10:02:20.805731      23 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Feb 11 10:02:20.823: INFO: Creating new exec pod
Feb 11 10:02:22.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 exec --namespace=services-3815 execpodtvxsp -- /bin/sh -x -c nslookup nodeport-service'
Feb 11 10:02:23.041: INFO: stderr: "+ nslookup nodeport-service\n"
Feb 11 10:02:23.041: INFO: stdout: "Server:\t\t172.23.0.10\nAddress:\t172.23.0.10#53\n\nnodeport-service.services-3815.svc.cluster.local\tcanonical name = externalsvc.services-3815.svc.cluster.local.\nName:\texternalsvc.services-3815.svc.cluster.local\nAddress: 172.23.10.206\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3815, will wait for the garbage collector to delete the pods
Feb 11 10:02:23.101: INFO: Deleting ReplicationController externalsvc took: 6.614198ms
Feb 11 10:02:23.501: INFO: Terminating ReplicationController externalsvc pods took: 400.217585ms
Feb 11 10:02:38.719: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:02:38.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3815" for this suite.
Feb 11 10:02:44.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:02:44.806: INFO: namespace services-3815 deletion completed in 6.073659745s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:27.095 seconds]
[sig-network] Services
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:02:44.807: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Feb 11 10:02:44.834: INFO: Waiting up to 5m0s for pod "client-containers-8d55e0b1-92a2-4de2-bc39-a3d07091b17c" in namespace "containers-7206" to be "success or failure"
Feb 11 10:02:44.836: INFO: Pod "client-containers-8d55e0b1-92a2-4de2-bc39-a3d07091b17c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.394788ms
Feb 11 10:02:46.840: INFO: Pod "client-containers-8d55e0b1-92a2-4de2-bc39-a3d07091b17c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006011223s
STEP: Saw pod success
Feb 11 10:02:46.840: INFO: Pod "client-containers-8d55e0b1-92a2-4de2-bc39-a3d07091b17c" satisfied condition "success or failure"
Feb 11 10:02:46.843: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod client-containers-8d55e0b1-92a2-4de2-bc39-a3d07091b17c container test-container: <nil>
STEP: delete the pod
Feb 11 10:02:46.857: INFO: Waiting for pod client-containers-8d55e0b1-92a2-4de2-bc39-a3d07091b17c to disappear
Feb 11 10:02:46.859: INFO: Pod client-containers-8d55e0b1-92a2-4de2-bc39-a3d07091b17c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:02:46.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7206" for this suite.
Feb 11 10:02:52.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:02:52.938: INFO: namespace containers-7206 deletion completed in 6.07607887s

â€¢ [SLOW TEST:8.132 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:02:52.939: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Feb 11 10:02:55.479: INFO: Successfully updated pod "adopt-release-59t8r"
STEP: Checking that the Job readopts the Pod
Feb 11 10:02:55.479: INFO: Waiting up to 15m0s for pod "adopt-release-59t8r" in namespace "job-241" to be "adopted"
Feb 11 10:02:55.482: INFO: Pod "adopt-release-59t8r": Phase="Running", Reason="", readiness=true. Elapsed: 2.097254ms
Feb 11 10:02:57.485: INFO: Pod "adopt-release-59t8r": Phase="Running", Reason="", readiness=true. Elapsed: 2.005497848s
Feb 11 10:02:57.485: INFO: Pod "adopt-release-59t8r" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Feb 11 10:02:57.992: INFO: Successfully updated pod "adopt-release-59t8r"
STEP: Checking that the Job releases the Pod
Feb 11 10:02:57.992: INFO: Waiting up to 15m0s for pod "adopt-release-59t8r" in namespace "job-241" to be "released"
Feb 11 10:02:57.995: INFO: Pod "adopt-release-59t8r": Phase="Running", Reason="", readiness=true. Elapsed: 2.474437ms
Feb 11 10:02:59.998: INFO: Pod "adopt-release-59t8r": Phase="Running", Reason="", readiness=true. Elapsed: 2.00598551s
Feb 11 10:02:59.998: INFO: Pod "adopt-release-59t8r" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:02:59.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-241" for this suite.
Feb 11 10:03:44.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:03:44.074: INFO: namespace job-241 deletion completed in 44.072411527s

â€¢ [SLOW TEST:51.136 seconds]
[sig-apps] Job
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:03:44.074: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 11 10:03:44.431: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 11 10:03:47.450: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:03:47.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2985" for this suite.
Feb 11 10:03:53.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:03:53.573: INFO: namespace webhook-2985 deletion completed in 6.074044183s
STEP: Destroying namespace "webhook-2985-markers" for this suite.
Feb 11 10:03:59.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:03:59.644: INFO: namespace webhook-2985-markers deletion completed in 6.071101896s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:15.581 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:03:59.655: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 11 10:03:59.682: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b2d37a54-626c-4fda-a232-f3c65fb9058c" in namespace "projected-7413" to be "success or failure"
Feb 11 10:03:59.684: INFO: Pod "downwardapi-volume-b2d37a54-626c-4fda-a232-f3c65fb9058c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.255793ms
Feb 11 10:04:01.687: INFO: Pod "downwardapi-volume-b2d37a54-626c-4fda-a232-f3c65fb9058c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005574455s
STEP: Saw pod success
Feb 11 10:04:01.687: INFO: Pod "downwardapi-volume-b2d37a54-626c-4fda-a232-f3c65fb9058c" satisfied condition "success or failure"
Feb 11 10:04:01.690: INFO: Trying to get logs from node cn-hongkong.192.168.0.19 pod downwardapi-volume-b2d37a54-626c-4fda-a232-f3c65fb9058c container client-container: <nil>
STEP: delete the pod
Feb 11 10:04:01.712: INFO: Waiting for pod downwardapi-volume-b2d37a54-626c-4fda-a232-f3c65fb9058c to disappear
Feb 11 10:04:01.714: INFO: Pod downwardapi-volume-b2d37a54-626c-4fda-a232-f3c65fb9058c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:04:01.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7413" for this suite.
Feb 11 10:04:07.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:04:07.789: INFO: namespace projected-7413 deletion completed in 6.071939904s

â€¢ [SLOW TEST:8.134 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:04:07.790: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-5776
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-5776
Feb 11 10:04:07.821: INFO: Found 0 stateful pods, waiting for 1
Feb 11 10:04:17.825: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Feb 11 10:04:17.855: INFO: Deleting all statefulset in ns statefulset-5776
Feb 11 10:04:17.859: INFO: Scaling statefulset ss to 0
Feb 11 10:04:27.872: INFO: Waiting for statefulset status.replicas updated to 0
Feb 11 10:04:27.874: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:04:27.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5776" for this suite.
Feb 11 10:04:33.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:04:33.968: INFO: namespace statefulset-5776 deletion completed in 6.07965646s

â€¢ [SLOW TEST:26.178 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:04:33.968: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 10:04:33.990: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 11 10:04:41.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 --namespace=crd-publish-openapi-3033 create -f -'
Feb 11 10:04:42.190: INFO: stderr: ""
Feb 11 10:04:42.190: INFO: stdout: "e2e-test-crd-publish-openapi-601-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 11 10:04:42.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 --namespace=crd-publish-openapi-3033 delete e2e-test-crd-publish-openapi-601-crds test-cr'
Feb 11 10:04:42.295: INFO: stderr: ""
Feb 11 10:04:42.295: INFO: stdout: "e2e-test-crd-publish-openapi-601-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Feb 11 10:04:42.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 --namespace=crd-publish-openapi-3033 apply -f -'
Feb 11 10:04:42.540: INFO: stderr: ""
Feb 11 10:04:42.540: INFO: stdout: "e2e-test-crd-publish-openapi-601-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 11 10:04:42.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 --namespace=crd-publish-openapi-3033 delete e2e-test-crd-publish-openapi-601-crds test-cr'
Feb 11 10:04:42.613: INFO: stderr: ""
Feb 11 10:04:42.613: INFO: stdout: "e2e-test-crd-publish-openapi-601-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Feb 11 10:04:42.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 explain e2e-test-crd-publish-openapi-601-crds'
Feb 11 10:04:42.827: INFO: stderr: ""
Feb 11 10:04:42.827: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-601-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:04:45.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3033" for this suite.
Feb 11 10:04:51.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:04:51.680: INFO: namespace crd-publish-openapi-3033 deletion completed in 6.072214416s

â€¢ [SLOW TEST:17.712 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:04:51.680: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 11 10:04:51.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-2207'
Feb 11 10:04:51.790: INFO: stderr: ""
Feb 11 10:04:51.790: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Feb 11 10:04:56.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pod e2e-test-httpd-pod --namespace=kubectl-2207 -o json'
Feb 11 10:04:56.906: INFO: stderr: ""
Feb 11 10:04:56.906: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-02-11T10:04:51Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2207\",\n        \"resourceVersion\": \"34827\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-2207/pods/e2e-test-httpd-pod\",\n        \"uid\": \"c8d7677b-d4e1-4b75-84b7-9b65f886c802\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-lp7xd\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"cn-hongkong.192.168.0.18\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-lp7xd\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-lp7xd\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-11T10:04:51Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-11T10:04:52Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-11T10:04:52Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-11T10:04:51Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://923f4bab31550675dc29c3643c51bf5c294815d5ef2b1994df068150d8ae645d\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-02-11T10:04:52Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.0.18\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.22.2.105\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.22.2.105\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-02-11T10:04:51Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 11 10:04:56.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 replace -f - --namespace=kubectl-2207'
Feb 11 10:04:57.119: INFO: stderr: ""
Feb 11 10:04:57.119: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Feb 11 10:04:57.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 delete pods e2e-test-httpd-pod --namespace=kubectl-2207'
Feb 11 10:04:58.802: INFO: stderr: ""
Feb 11 10:04:58.802: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:04:58.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2207" for this suite.
Feb 11 10:05:04.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:05:04.878: INFO: namespace kubectl-2207 deletion completed in 6.072567798s

â€¢ [SLOW TEST:13.198 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:05:04.879: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Feb 11 10:05:04.907: INFO: Waiting up to 5m0s for pod "var-expansion-6f8d73b2-3708-4585-b12d-8297cd430059" in namespace "var-expansion-2191" to be "success or failure"
Feb 11 10:05:04.910: INFO: Pod "var-expansion-6f8d73b2-3708-4585-b12d-8297cd430059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.282535ms
Feb 11 10:05:06.913: INFO: Pod "var-expansion-6f8d73b2-3708-4585-b12d-8297cd430059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005491649s
STEP: Saw pod success
Feb 11 10:05:06.913: INFO: Pod "var-expansion-6f8d73b2-3708-4585-b12d-8297cd430059" satisfied condition "success or failure"
Feb 11 10:05:06.916: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod var-expansion-6f8d73b2-3708-4585-b12d-8297cd430059 container dapi-container: <nil>
STEP: delete the pod
Feb 11 10:05:06.937: INFO: Waiting for pod var-expansion-6f8d73b2-3708-4585-b12d-8297cd430059 to disappear
Feb 11 10:05:06.939: INFO: Pod var-expansion-6f8d73b2-3708-4585-b12d-8297cd430059 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:05:06.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2191" for this suite.
Feb 11 10:05:12.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:05:13.015: INFO: namespace var-expansion-2191 deletion completed in 6.072768088s

â€¢ [SLOW TEST:8.136 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:05:13.015: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-eaa48737-62d2-4b6f-8e53-0bb5a83234f8
STEP: Creating a pod to test consume configMaps
Feb 11 10:05:13.047: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0d4d7e74-59ea-4e65-be0e-42b3a65a1d72" in namespace "projected-5060" to be "success or failure"
Feb 11 10:05:13.048: INFO: Pod "pod-projected-configmaps-0d4d7e74-59ea-4e65-be0e-42b3a65a1d72": Phase="Pending", Reason="", readiness=false. Elapsed: 1.872428ms
Feb 11 10:05:15.052: INFO: Pod "pod-projected-configmaps-0d4d7e74-59ea-4e65-be0e-42b3a65a1d72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005356217s
STEP: Saw pod success
Feb 11 10:05:15.052: INFO: Pod "pod-projected-configmaps-0d4d7e74-59ea-4e65-be0e-42b3a65a1d72" satisfied condition "success or failure"
Feb 11 10:05:15.054: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-projected-configmaps-0d4d7e74-59ea-4e65-be0e-42b3a65a1d72 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 11 10:05:15.069: INFO: Waiting for pod pod-projected-configmaps-0d4d7e74-59ea-4e65-be0e-42b3a65a1d72 to disappear
Feb 11 10:05:15.071: INFO: Pod pod-projected-configmaps-0d4d7e74-59ea-4e65-be0e-42b3a65a1d72 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:05:15.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5060" for this suite.
Feb 11 10:05:21.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:05:21.170: INFO: namespace projected-5060 deletion completed in 6.095243022s

â€¢ [SLOW TEST:8.155 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:05:21.170: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:05:34.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8786" for this suite.
Feb 11 10:05:40.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:05:40.322: INFO: namespace resourcequota-8786 deletion completed in 6.071371809s

â€¢ [SLOW TEST:19.153 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:05:40.323: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-ecd2aad4-4fec-4b59-9363-77403ca1856a
STEP: Creating a pod to test consume configMaps
Feb 11 10:05:40.351: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3cbeb0a9-0299-47b7-9d1c-c9bc29ec19db" in namespace "projected-9759" to be "success or failure"
Feb 11 10:05:40.353: INFO: Pod "pod-projected-configmaps-3cbeb0a9-0299-47b7-9d1c-c9bc29ec19db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.119791ms
Feb 11 10:05:42.357: INFO: Pod "pod-projected-configmaps-3cbeb0a9-0299-47b7-9d1c-c9bc29ec19db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005859906s
STEP: Saw pod success
Feb 11 10:05:42.357: INFO: Pod "pod-projected-configmaps-3cbeb0a9-0299-47b7-9d1c-c9bc29ec19db" satisfied condition "success or failure"
Feb 11 10:05:42.359: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-projected-configmaps-3cbeb0a9-0299-47b7-9d1c-c9bc29ec19db container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 11 10:05:42.374: INFO: Waiting for pod pod-projected-configmaps-3cbeb0a9-0299-47b7-9d1c-c9bc29ec19db to disappear
Feb 11 10:05:42.376: INFO: Pod pod-projected-configmaps-3cbeb0a9-0299-47b7-9d1c-c9bc29ec19db no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:05:42.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9759" for this suite.
Feb 11 10:05:48.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:05:48.455: INFO: namespace projected-9759 deletion completed in 6.075612004s

â€¢ [SLOW TEST:8.132 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:05:48.455: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-a0f3d754-7208-42a5-b441-91ba94f9e921
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:05:50.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4445" for this suite.
Feb 11 10:06:02.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:06:02.592: INFO: namespace configmap-4445 deletion completed in 12.074924786s

â€¢ [SLOW TEST:14.137 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:06:02.592: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 11 10:06:02.619: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a10777ec-a9f7-4f38-9d4c-68ead7ebd712" in namespace "projected-5035" to be "success or failure"
Feb 11 10:06:02.622: INFO: Pod "downwardapi-volume-a10777ec-a9f7-4f38-9d4c-68ead7ebd712": Phase="Pending", Reason="", readiness=false. Elapsed: 2.5036ms
Feb 11 10:06:04.625: INFO: Pod "downwardapi-volume-a10777ec-a9f7-4f38-9d4c-68ead7ebd712": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005623817s
STEP: Saw pod success
Feb 11 10:06:04.625: INFO: Pod "downwardapi-volume-a10777ec-a9f7-4f38-9d4c-68ead7ebd712" satisfied condition "success or failure"
Feb 11 10:06:04.627: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod downwardapi-volume-a10777ec-a9f7-4f38-9d4c-68ead7ebd712 container client-container: <nil>
STEP: delete the pod
Feb 11 10:06:04.642: INFO: Waiting for pod downwardapi-volume-a10777ec-a9f7-4f38-9d4c-68ead7ebd712 to disappear
Feb 11 10:06:04.644: INFO: Pod downwardapi-volume-a10777ec-a9f7-4f38-9d4c-68ead7ebd712 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:06:04.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5035" for this suite.
Feb 11 10:06:10.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:06:10.724: INFO: namespace projected-5035 deletion completed in 6.076768625s

â€¢ [SLOW TEST:8.132 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:06:10.724: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 11 10:06:10.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-1326'
Feb 11 10:06:10.823: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 11 10:06:10.823: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Feb 11 10:06:10.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 delete jobs e2e-test-httpd-job --namespace=kubectl-1326'
Feb 11 10:06:10.896: INFO: stderr: ""
Feb 11 10:06:10.896: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:06:10.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1326" for this suite.
Feb 11 10:06:16.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:06:16.973: INFO: namespace kubectl-1326 deletion completed in 6.073387993s

â€¢ [SLOW TEST:6.249 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:06:16.973: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-k26r
STEP: Creating a pod to test atomic-volume-subpath
Feb 11 10:06:17.008: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-k26r" in namespace "subpath-4440" to be "success or failure"
Feb 11 10:06:17.010: INFO: Pod "pod-subpath-test-secret-k26r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.333239ms
Feb 11 10:06:19.013: INFO: Pod "pod-subpath-test-secret-k26r": Phase="Running", Reason="", readiness=true. Elapsed: 2.005082657s
Feb 11 10:06:21.016: INFO: Pod "pod-subpath-test-secret-k26r": Phase="Running", Reason="", readiness=true. Elapsed: 4.008448658s
Feb 11 10:06:23.019: INFO: Pod "pod-subpath-test-secret-k26r": Phase="Running", Reason="", readiness=true. Elapsed: 6.011653413s
Feb 11 10:06:25.022: INFO: Pod "pod-subpath-test-secret-k26r": Phase="Running", Reason="", readiness=true. Elapsed: 8.014314024s
Feb 11 10:06:27.025: INFO: Pod "pod-subpath-test-secret-k26r": Phase="Running", Reason="", readiness=true. Elapsed: 10.017442049s
Feb 11 10:06:29.028: INFO: Pod "pod-subpath-test-secret-k26r": Phase="Running", Reason="", readiness=true. Elapsed: 12.020359484s
Feb 11 10:06:31.031: INFO: Pod "pod-subpath-test-secret-k26r": Phase="Running", Reason="", readiness=true. Elapsed: 14.023526849s
Feb 11 10:06:33.034: INFO: Pod "pod-subpath-test-secret-k26r": Phase="Running", Reason="", readiness=true. Elapsed: 16.02677505s
Feb 11 10:06:35.038: INFO: Pod "pod-subpath-test-secret-k26r": Phase="Running", Reason="", readiness=true. Elapsed: 18.029872261s
Feb 11 10:06:37.040: INFO: Pod "pod-subpath-test-secret-k26r": Phase="Running", Reason="", readiness=true. Elapsed: 20.032739768s
Feb 11 10:06:39.043: INFO: Pod "pod-subpath-test-secret-k26r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.03585487s
STEP: Saw pod success
Feb 11 10:06:39.044: INFO: Pod "pod-subpath-test-secret-k26r" satisfied condition "success or failure"
Feb 11 10:06:39.046: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-subpath-test-secret-k26r container test-container-subpath-secret-k26r: <nil>
STEP: delete the pod
Feb 11 10:06:39.060: INFO: Waiting for pod pod-subpath-test-secret-k26r to disappear
Feb 11 10:06:39.063: INFO: Pod pod-subpath-test-secret-k26r no longer exists
STEP: Deleting pod pod-subpath-test-secret-k26r
Feb 11 10:06:39.063: INFO: Deleting pod "pod-subpath-test-secret-k26r" in namespace "subpath-4440"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:06:39.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4440" for this suite.
Feb 11 10:06:45.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:06:45.138: INFO: namespace subpath-4440 deletion completed in 6.070060899s

â€¢ [SLOW TEST:28.164 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:06:45.138: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Feb 11 10:06:45.166: INFO: Waiting up to 5m0s for pod "downward-api-a010cc17-aa77-442f-b9bd-276ac57c88e7" in namespace "downward-api-4195" to be "success or failure"
Feb 11 10:06:45.169: INFO: Pod "downward-api-a010cc17-aa77-442f-b9bd-276ac57c88e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.447657ms
Feb 11 10:06:47.173: INFO: Pod "downward-api-a010cc17-aa77-442f-b9bd-276ac57c88e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006194182s
STEP: Saw pod success
Feb 11 10:06:47.173: INFO: Pod "downward-api-a010cc17-aa77-442f-b9bd-276ac57c88e7" satisfied condition "success or failure"
Feb 11 10:06:47.175: INFO: Trying to get logs from node cn-hongkong.192.168.0.20 pod downward-api-a010cc17-aa77-442f-b9bd-276ac57c88e7 container dapi-container: <nil>
STEP: delete the pod
Feb 11 10:06:47.202: INFO: Waiting for pod downward-api-a010cc17-aa77-442f-b9bd-276ac57c88e7 to disappear
Feb 11 10:06:47.204: INFO: Pod downward-api-a010cc17-aa77-442f-b9bd-276ac57c88e7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:06:47.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4195" for this suite.
Feb 11 10:06:53.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:06:53.286: INFO: namespace downward-api-4195 deletion completed in 6.079185484s

â€¢ [SLOW TEST:8.149 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:06:53.286: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-e84a437d-0b2f-441d-bc57-55238d5b1d79
STEP: Creating a pod to test consume secrets
Feb 11 10:06:53.322: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-330150f8-7eaa-401b-a437-0cc120982b86" in namespace "projected-7306" to be "success or failure"
Feb 11 10:06:53.327: INFO: Pod "pod-projected-secrets-330150f8-7eaa-401b-a437-0cc120982b86": Phase="Pending", Reason="", readiness=false. Elapsed: 4.833497ms
Feb 11 10:06:55.330: INFO: Pod "pod-projected-secrets-330150f8-7eaa-401b-a437-0cc120982b86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007981475s
STEP: Saw pod success
Feb 11 10:06:55.330: INFO: Pod "pod-projected-secrets-330150f8-7eaa-401b-a437-0cc120982b86" satisfied condition "success or failure"
Feb 11 10:06:55.332: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-projected-secrets-330150f8-7eaa-401b-a437-0cc120982b86 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 11 10:06:55.347: INFO: Waiting for pod pod-projected-secrets-330150f8-7eaa-401b-a437-0cc120982b86 to disappear
Feb 11 10:06:55.349: INFO: Pod pod-projected-secrets-330150f8-7eaa-401b-a437-0cc120982b86 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:06:55.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7306" for this suite.
Feb 11 10:07:01.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:07:01.427: INFO: namespace projected-7306 deletion completed in 6.07460058s

â€¢ [SLOW TEST:8.140 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:07:01.427: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-57a75bdf-5b42-4200-9ec2-d7e77ddc60f4
STEP: Creating a pod to test consume configMaps
Feb 11 10:07:01.458: INFO: Waiting up to 5m0s for pod "pod-configmaps-b447e77a-15c9-4c26-b3d4-42313ca45b99" in namespace "configmap-4897" to be "success or failure"
Feb 11 10:07:01.460: INFO: Pod "pod-configmaps-b447e77a-15c9-4c26-b3d4-42313ca45b99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.716211ms
Feb 11 10:07:03.464: INFO: Pod "pod-configmaps-b447e77a-15c9-4c26-b3d4-42313ca45b99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006340274s
STEP: Saw pod success
Feb 11 10:07:03.464: INFO: Pod "pod-configmaps-b447e77a-15c9-4c26-b3d4-42313ca45b99" satisfied condition "success or failure"
Feb 11 10:07:03.467: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-configmaps-b447e77a-15c9-4c26-b3d4-42313ca45b99 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 11 10:07:03.485: INFO: Waiting for pod pod-configmaps-b447e77a-15c9-4c26-b3d4-42313ca45b99 to disappear
Feb 11 10:07:03.487: INFO: Pod pod-configmaps-b447e77a-15c9-4c26-b3d4-42313ca45b99 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:07:03.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4897" for this suite.
Feb 11 10:07:09.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:07:09.561: INFO: namespace configmap-4897 deletion completed in 6.070752769s

â€¢ [SLOW TEST:8.134 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:07:09.561: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 10:07:09.584: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:08:10.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-356" for this suite.
Feb 11 10:08:16.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:08:16.996: INFO: namespace custom-resource-definition-356 deletion completed in 6.074036456s

â€¢ [SLOW TEST:67.435 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:08:16.996: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-b93d0f76-3ff5-42d8-9c56-6d4087509def
STEP: Creating a pod to test consume secrets
Feb 11 10:08:17.027: INFO: Waiting up to 5m0s for pod "pod-secrets-99c772aa-d19a-41ad-9437-a2e0df8df9ec" in namespace "secrets-2228" to be "success or failure"
Feb 11 10:08:17.029: INFO: Pod "pod-secrets-99c772aa-d19a-41ad-9437-a2e0df8df9ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.234425ms
Feb 11 10:08:19.033: INFO: Pod "pod-secrets-99c772aa-d19a-41ad-9437-a2e0df8df9ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005446097s
STEP: Saw pod success
Feb 11 10:08:19.033: INFO: Pod "pod-secrets-99c772aa-d19a-41ad-9437-a2e0df8df9ec" satisfied condition "success or failure"
Feb 11 10:08:19.035: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-secrets-99c772aa-d19a-41ad-9437-a2e0df8df9ec container secret-volume-test: <nil>
STEP: delete the pod
Feb 11 10:08:19.050: INFO: Waiting for pod pod-secrets-99c772aa-d19a-41ad-9437-a2e0df8df9ec to disappear
Feb 11 10:08:19.052: INFO: Pod pod-secrets-99c772aa-d19a-41ad-9437-a2e0df8df9ec no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:08:19.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2228" for this suite.
Feb 11 10:08:25.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:08:25.130: INFO: namespace secrets-2228 deletion completed in 6.075620407s

â€¢ [SLOW TEST:8.134 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:08:25.131: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-5343
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 11 10:08:25.152: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 11 10:08:47.220: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.22.2.116:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5343 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 11 10:08:47.220: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 10:08:47.352: INFO: Found all expected endpoints: [netserver-0]
Feb 11 10:08:47.356: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.22.1.215:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5343 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 11 10:08:47.356: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 10:08:47.515: INFO: Found all expected endpoints: [netserver-1]
Feb 11 10:08:47.518: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.22.2.200:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5343 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 11 10:08:47.518: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 10:08:47.682: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:08:47.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5343" for this suite.
Feb 11 10:08:59.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:08:59.760: INFO: namespace pod-network-test-5343 deletion completed in 12.074050757s

â€¢ [SLOW TEST:34.629 seconds]
[sig-network] Networking
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:08:59.760: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-1e1a4a7a-077f-4f9a-b10e-1230c6dda95e
STEP: Creating a pod to test consume configMaps
Feb 11 10:08:59.792: INFO: Waiting up to 5m0s for pod "pod-configmaps-3dc3e582-5a93-4fe5-a93f-bd36d9b8c488" in namespace "configmap-6192" to be "success or failure"
Feb 11 10:08:59.794: INFO: Pod "pod-configmaps-3dc3e582-5a93-4fe5-a93f-bd36d9b8c488": Phase="Pending", Reason="", readiness=false. Elapsed: 2.610816ms
Feb 11 10:09:01.798: INFO: Pod "pod-configmaps-3dc3e582-5a93-4fe5-a93f-bd36d9b8c488": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005911466s
STEP: Saw pod success
Feb 11 10:09:01.798: INFO: Pod "pod-configmaps-3dc3e582-5a93-4fe5-a93f-bd36d9b8c488" satisfied condition "success or failure"
Feb 11 10:09:01.800: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-configmaps-3dc3e582-5a93-4fe5-a93f-bd36d9b8c488 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 11 10:09:01.815: INFO: Waiting for pod pod-configmaps-3dc3e582-5a93-4fe5-a93f-bd36d9b8c488 to disappear
Feb 11 10:09:01.818: INFO: Pod pod-configmaps-3dc3e582-5a93-4fe5-a93f-bd36d9b8c488 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:09:01.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6192" for this suite.
Feb 11 10:09:07.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:09:07.896: INFO: namespace configmap-6192 deletion completed in 6.07502409s

â€¢ [SLOW TEST:8.135 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:09:07.896: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:09:07.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3327" for this suite.
Feb 11 10:09:35.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:09:36.001: INFO: namespace pods-3327 deletion completed in 28.072653332s

â€¢ [SLOW TEST:28.105 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:09:36.001: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Feb 11 10:09:36.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 create -f - --namespace=kubectl-1718'
Feb 11 10:09:36.232: INFO: stderr: ""
Feb 11 10:09:36.232: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 11 10:09:36.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1718'
Feb 11 10:09:36.304: INFO: stderr: ""
Feb 11 10:09:36.304: INFO: stdout: "update-demo-nautilus-blqbd update-demo-nautilus-d697v "
Feb 11 10:09:36.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-nautilus-blqbd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1718'
Feb 11 10:09:36.372: INFO: stderr: ""
Feb 11 10:09:36.372: INFO: stdout: ""
Feb 11 10:09:36.372: INFO: update-demo-nautilus-blqbd is created but not running
Feb 11 10:09:41.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1718'
Feb 11 10:09:41.444: INFO: stderr: ""
Feb 11 10:09:41.444: INFO: stdout: "update-demo-nautilus-blqbd update-demo-nautilus-d697v "
Feb 11 10:09:41.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-nautilus-blqbd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1718'
Feb 11 10:09:41.509: INFO: stderr: ""
Feb 11 10:09:41.509: INFO: stdout: "true"
Feb 11 10:09:41.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-nautilus-blqbd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1718'
Feb 11 10:09:41.574: INFO: stderr: ""
Feb 11 10:09:41.574: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 11 10:09:41.574: INFO: validating pod update-demo-nautilus-blqbd
Feb 11 10:09:41.579: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 11 10:09:41.579: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 11 10:09:41.579: INFO: update-demo-nautilus-blqbd is verified up and running
Feb 11 10:09:41.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-nautilus-d697v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1718'
Feb 11 10:09:41.643: INFO: stderr: ""
Feb 11 10:09:41.643: INFO: stdout: "true"
Feb 11 10:09:41.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-nautilus-d697v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1718'
Feb 11 10:09:41.706: INFO: stderr: ""
Feb 11 10:09:41.706: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 11 10:09:41.706: INFO: validating pod update-demo-nautilus-d697v
Feb 11 10:09:41.711: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 11 10:09:41.711: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 11 10:09:41.711: INFO: update-demo-nautilus-d697v is verified up and running
STEP: scaling down the replication controller
Feb 11 10:09:41.712: INFO: scanned /root for discovery docs: <nil>
Feb 11 10:09:41.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-1718'
Feb 11 10:09:42.805: INFO: stderr: ""
Feb 11 10:09:42.805: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 11 10:09:42.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1718'
Feb 11 10:09:42.884: INFO: stderr: ""
Feb 11 10:09:42.884: INFO: stdout: "update-demo-nautilus-blqbd update-demo-nautilus-d697v "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 11 10:09:47.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1718'
Feb 11 10:09:47.955: INFO: stderr: ""
Feb 11 10:09:47.955: INFO: stdout: "update-demo-nautilus-blqbd update-demo-nautilus-d697v "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 11 10:09:52.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1718'
Feb 11 10:09:53.022: INFO: stderr: ""
Feb 11 10:09:53.022: INFO: stdout: "update-demo-nautilus-d697v "
Feb 11 10:09:53.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-nautilus-d697v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1718'
Feb 11 10:09:53.088: INFO: stderr: ""
Feb 11 10:09:53.088: INFO: stdout: "true"
Feb 11 10:09:53.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-nautilus-d697v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1718'
Feb 11 10:09:53.155: INFO: stderr: ""
Feb 11 10:09:53.155: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 11 10:09:53.155: INFO: validating pod update-demo-nautilus-d697v
Feb 11 10:09:53.159: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 11 10:09:53.159: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 11 10:09:53.159: INFO: update-demo-nautilus-d697v is verified up and running
STEP: scaling up the replication controller
Feb 11 10:09:53.161: INFO: scanned /root for discovery docs: <nil>
Feb 11 10:09:53.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-1718'
Feb 11 10:09:54.247: INFO: stderr: ""
Feb 11 10:09:54.247: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 11 10:09:54.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1718'
Feb 11 10:09:54.318: INFO: stderr: ""
Feb 11 10:09:54.318: INFO: stdout: "update-demo-nautilus-d697v update-demo-nautilus-s8xqm "
Feb 11 10:09:54.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-nautilus-d697v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1718'
Feb 11 10:09:54.383: INFO: stderr: ""
Feb 11 10:09:54.383: INFO: stdout: "true"
Feb 11 10:09:54.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-nautilus-d697v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1718'
Feb 11 10:09:54.448: INFO: stderr: ""
Feb 11 10:09:54.448: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 11 10:09:54.448: INFO: validating pod update-demo-nautilus-d697v
Feb 11 10:09:54.452: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 11 10:09:54.452: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 11 10:09:54.452: INFO: update-demo-nautilus-d697v is verified up and running
Feb 11 10:09:54.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-nautilus-s8xqm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1718'
Feb 11 10:09:54.522: INFO: stderr: ""
Feb 11 10:09:54.522: INFO: stdout: "true"
Feb 11 10:09:54.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods update-demo-nautilus-s8xqm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1718'
Feb 11 10:09:54.588: INFO: stderr: ""
Feb 11 10:09:54.588: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 11 10:09:54.588: INFO: validating pod update-demo-nautilus-s8xqm
Feb 11 10:09:54.592: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 11 10:09:54.592: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 11 10:09:54.592: INFO: update-demo-nautilus-s8xqm is verified up and running
STEP: using delete to clean up resources
Feb 11 10:09:54.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 delete --grace-period=0 --force -f - --namespace=kubectl-1718'
Feb 11 10:09:54.663: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 11 10:09:54.663: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 11 10:09:54.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1718'
Feb 11 10:09:54.733: INFO: stderr: "No resources found in kubectl-1718 namespace.\n"
Feb 11 10:09:54.733: INFO: stdout: ""
Feb 11 10:09:54.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods -l name=update-demo --namespace=kubectl-1718 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 11 10:09:54.801: INFO: stderr: ""
Feb 11 10:09:54.801: INFO: stdout: "update-demo-nautilus-d697v\nupdate-demo-nautilus-s8xqm\n"
Feb 11 10:09:55.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1718'
Feb 11 10:09:55.372: INFO: stderr: "No resources found in kubectl-1718 namespace.\n"
Feb 11 10:09:55.372: INFO: stdout: ""
Feb 11 10:09:55.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 get pods -l name=update-demo --namespace=kubectl-1718 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 11 10:09:55.439: INFO: stderr: ""
Feb 11 10:09:55.439: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:09:55.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1718" for this suite.
Feb 11 10:10:01.453: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:10:01.518: INFO: namespace kubectl-1718 deletion completed in 6.074796077s

â€¢ [SLOW TEST:25.516 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:10:01.518: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-703358a9-ed6f-4aba-8d8c-8c5b52933c09 in namespace container-probe-6825
Feb 11 10:10:03.563: INFO: Started pod busybox-703358a9-ed6f-4aba-8d8c-8c5b52933c09 in namespace container-probe-6825
STEP: checking the pod's current state and verifying that restartCount is present
Feb 11 10:10:03.565: INFO: Initial restart count of pod busybox-703358a9-ed6f-4aba-8d8c-8c5b52933c09 is 0
Feb 11 10:10:49.651: INFO: Restart count of pod container-probe-6825/busybox-703358a9-ed6f-4aba-8d8c-8c5b52933c09 is now 1 (46.086237736s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:10:49.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6825" for this suite.
Feb 11 10:10:55.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:10:55.736: INFO: namespace container-probe-6825 deletion completed in 6.072264151s

â€¢ [SLOW TEST:54.218 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:10:55.736: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 10:10:55.773: INFO: (0) /api/v1/nodes/cn-hongkong.192.168.0.18:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 12.122556ms)
Feb 11 10:10:55.776: INFO: (1) /api/v1/nodes/cn-hongkong.192.168.0.18:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 3.026903ms)
Feb 11 10:10:55.780: INFO: (2) /api/v1/nodes/cn-hongkong.192.168.0.18:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 3.050137ms)
Feb 11 10:10:55.783: INFO: (3) /api/v1/nodes/cn-hongkong.192.168.0.18:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.974619ms)
Feb 11 10:10:55.785: INFO: (4) /api/v1/nodes/cn-hongkong.192.168.0.18:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.786815ms)
Feb 11 10:10:55.788: INFO: (5) /api/v1/nodes/cn-hongkong.192.168.0.18:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.819807ms)
Feb 11 10:10:55.791: INFO: (6) /api/v1/nodes/cn-hongkong.192.168.0.18:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.904753ms)
Feb 11 10:10:55.794: INFO: (7) /api/v1/nodes/cn-hongkong.192.168.0.18:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.793104ms)
Feb 11 10:10:55.797: INFO: (8) /api/v1/nodes/cn-hongkong.192.168.0.18:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.86701ms)
Feb 11 10:10:55.800: INFO: (9) /api/v1/nodes/cn-hongkong.192.168.0.18:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 3.015838ms)
Feb 11 10:10:55.803: INFO: (10) /api/v1/nodes/cn-hongkong.192.168.0.18:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.69839ms)
Feb 11 10:10:55.806: INFO: (11) /api/v1/nodes/cn-hongkong.192.168.0.18:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.936504ms)
Feb 11 10:10:55.809: INFO: (12) /api/v1/nodes/cn-hongkong.192.168.0.18:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 3.411865ms)
Feb 11 10:10:55.812: INFO: (13) /api/v1/nodes/cn-hongkong.192.168.0.18:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.637358ms)
Feb 11 10:10:55.814: INFO: (14) /api/v1/nodes/cn-hongkong.192.168.0.18:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.819295ms)
Feb 11 10:10:55.817: INFO: (15) /api/v1/nodes/cn-hongkong.192.168.0.18:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.901536ms)
Feb 11 10:10:55.820: INFO: (16) /api/v1/nodes/cn-hongkong.192.168.0.18:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.645384ms)
Feb 11 10:10:55.823: INFO: (17) /api/v1/nodes/cn-hongkong.192.168.0.18:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.944898ms)
Feb 11 10:10:55.829: INFO: (18) /api/v1/nodes/cn-hongkong.192.168.0.18:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 5.899499ms)
Feb 11 10:10:55.832: INFO: (19) /api/v1/nodes/cn-hongkong.192.168.0.18:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="at.log">at.log... (200; 2.677656ms)
[AfterEach] version v1
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:10:55.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4211" for this suite.
Feb 11 10:11:01.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:11:01.908: INFO: namespace proxy-4211 deletion completed in 6.073523829s

â€¢ [SLOW TEST:6.172 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:11:01.908: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2023.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2023.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2023.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2023.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2023.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2023.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 11 10:11:05.967: INFO: DNS probes using dns-2023/dns-test-7e4aaedc-fb0b-48be-bbff-11a594a56636 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:11:05.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2023" for this suite.
Feb 11 10:11:11.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:11:12.050: INFO: namespace dns-2023 deletion completed in 6.071367493s

â€¢ [SLOW TEST:10.141 seconds]
[sig-network] DNS
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:11:12.050: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 11 10:11:12.093: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:11:12.093: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:11:12.093: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:11:12.095: INFO: Number of nodes with available pods: 0
Feb 11 10:11:12.095: INFO: Node cn-hongkong.192.168.0.18 is running more than one daemon pod
Feb 11 10:11:13.099: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:11:13.099: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:11:13.099: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:11:13.103: INFO: Number of nodes with available pods: 2
Feb 11 10:11:13.103: INFO: Node cn-hongkong.192.168.0.19 is running more than one daemon pod
Feb 11 10:11:14.099: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:11:14.099: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:11:14.099: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:11:14.102: INFO: Number of nodes with available pods: 3
Feb 11 10:11:14.102: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 11 10:11:14.115: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:11:14.115: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:11:14.115: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:11:14.117: INFO: Number of nodes with available pods: 2
Feb 11 10:11:14.117: INFO: Node cn-hongkong.192.168.0.19 is running more than one daemon pod
Feb 11 10:11:15.122: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:11:15.122: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:11:15.122: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:11:15.125: INFO: Number of nodes with available pods: 2
Feb 11 10:11:15.125: INFO: Node cn-hongkong.192.168.0.19 is running more than one daemon pod
Feb 11 10:11:16.122: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:11:16.122: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:11:16.122: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:11:16.124: INFO: Number of nodes with available pods: 3
Feb 11 10:11:16.124: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2633, will wait for the garbage collector to delete the pods
Feb 11 10:11:16.188: INFO: Deleting DaemonSet.extensions daemon-set took: 6.756075ms
Feb 11 10:11:16.588: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.240528ms
Feb 11 10:11:28.691: INFO: Number of nodes with available pods: 0
Feb 11 10:11:28.691: INFO: Number of running nodes: 0, number of available pods: 0
Feb 11 10:11:28.693: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2633/daemonsets","resourceVersion":"36634"},"items":null}

Feb 11 10:11:28.695: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2633/pods","resourceVersion":"36634"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:11:28.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2633" for this suite.
Feb 11 10:11:34.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:11:34.781: INFO: namespace daemonsets-2633 deletion completed in 6.073690902s

â€¢ [SLOW TEST:22.731 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:11:34.782: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 11 10:11:34.814: INFO: Waiting up to 5m0s for pod "pod-e906727f-3437-4782-98ab-6d72bead2cd7" in namespace "emptydir-1605" to be "success or failure"
Feb 11 10:11:34.816: INFO: Pod "pod-e906727f-3437-4782-98ab-6d72bead2cd7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.998113ms
Feb 11 10:11:36.819: INFO: Pod "pod-e906727f-3437-4782-98ab-6d72bead2cd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00511373s
STEP: Saw pod success
Feb 11 10:11:36.819: INFO: Pod "pod-e906727f-3437-4782-98ab-6d72bead2cd7" satisfied condition "success or failure"
Feb 11 10:11:36.821: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-e906727f-3437-4782-98ab-6d72bead2cd7 container test-container: <nil>
STEP: delete the pod
Feb 11 10:11:36.836: INFO: Waiting for pod pod-e906727f-3437-4782-98ab-6d72bead2cd7 to disappear
Feb 11 10:11:36.838: INFO: Pod pod-e906727f-3437-4782-98ab-6d72bead2cd7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:11:36.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1605" for this suite.
Feb 11 10:11:42.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:11:42.915: INFO: namespace emptydir-1605 deletion completed in 6.073997208s

â€¢ [SLOW TEST:8.133 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:11:42.915: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Feb 11 10:12:13.466: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0211 10:12:13.466459      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:12:13.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3696" for this suite.
Feb 11 10:12:19.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:12:19.550: INFO: namespace gc-3696 deletion completed in 6.081095632s

â€¢ [SLOW TEST:36.635 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:12:19.550: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 11 10:12:19.599: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:19.599: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:19.599: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:19.602: INFO: Number of nodes with available pods: 0
Feb 11 10:12:19.602: INFO: Node cn-hongkong.192.168.0.18 is running more than one daemon pod
Feb 11 10:12:20.606: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:20.606: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:20.606: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:20.609: INFO: Number of nodes with available pods: 1
Feb 11 10:12:20.609: INFO: Node cn-hongkong.192.168.0.18 is running more than one daemon pod
Feb 11 10:12:21.606: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:21.606: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:21.606: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:21.609: INFO: Number of nodes with available pods: 3
Feb 11 10:12:21.609: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 11 10:12:21.622: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:21.623: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:21.623: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:21.625: INFO: Number of nodes with available pods: 2
Feb 11 10:12:21.625: INFO: Node cn-hongkong.192.168.0.20 is running more than one daemon pod
Feb 11 10:12:22.629: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:22.629: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:22.629: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:22.632: INFO: Number of nodes with available pods: 2
Feb 11 10:12:22.632: INFO: Node cn-hongkong.192.168.0.20 is running more than one daemon pod
Feb 11 10:12:23.629: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:23.629: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:23.629: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:23.632: INFO: Number of nodes with available pods: 2
Feb 11 10:12:23.632: INFO: Node cn-hongkong.192.168.0.20 is running more than one daemon pod
Feb 11 10:12:24.630: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:24.630: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:24.630: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:24.633: INFO: Number of nodes with available pods: 2
Feb 11 10:12:24.633: INFO: Node cn-hongkong.192.168.0.20 is running more than one daemon pod
Feb 11 10:12:25.629: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.15 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:25.629: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.16 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:25.629: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.17 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 11 10:12:25.632: INFO: Number of nodes with available pods: 3
Feb 11 10:12:25.632: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9123, will wait for the garbage collector to delete the pods
Feb 11 10:12:25.693: INFO: Deleting DaemonSet.extensions daemon-set took: 5.580596ms
Feb 11 10:12:26.093: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.198402ms
Feb 11 10:12:38.696: INFO: Number of nodes with available pods: 0
Feb 11 10:12:38.696: INFO: Number of running nodes: 0, number of available pods: 0
Feb 11 10:12:38.699: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9123/daemonsets","resourceVersion":"37005"},"items":null}

Feb 11 10:12:38.701: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9123/pods","resourceVersion":"37005"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:12:38.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9123" for this suite.
Feb 11 10:12:44.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:12:44.788: INFO: namespace daemonsets-9123 deletion completed in 6.074264528s

â€¢ [SLOW TEST:25.238 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:12:44.788: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:13:00.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7368" for this suite.
Feb 11 10:13:06.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:13:06.959: INFO: namespace resourcequota-7368 deletion completed in 6.073343304s

â€¢ [SLOW TEST:22.171 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:13:06.959: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 11 10:13:06.986: INFO: Waiting up to 5m0s for pod "pod-73b1f872-ca86-4d6b-8f10-8f480b71b5b4" in namespace "emptydir-196" to be "success or failure"
Feb 11 10:13:06.989: INFO: Pod "pod-73b1f872-ca86-4d6b-8f10-8f480b71b5b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.403027ms
Feb 11 10:13:08.992: INFO: Pod "pod-73b1f872-ca86-4d6b-8f10-8f480b71b5b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006081953s
STEP: Saw pod success
Feb 11 10:13:08.992: INFO: Pod "pod-73b1f872-ca86-4d6b-8f10-8f480b71b5b4" satisfied condition "success or failure"
Feb 11 10:13:08.995: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-73b1f872-ca86-4d6b-8f10-8f480b71b5b4 container test-container: <nil>
STEP: delete the pod
Feb 11 10:13:09.019: INFO: Waiting for pod pod-73b1f872-ca86-4d6b-8f10-8f480b71b5b4 to disappear
Feb 11 10:13:09.022: INFO: Pod pod-73b1f872-ca86-4d6b-8f10-8f480b71b5b4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:13:09.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-196" for this suite.
Feb 11 10:13:15.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:13:15.099: INFO: namespace emptydir-196 deletion completed in 6.072055248s

â€¢ [SLOW TEST:8.140 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:13:15.100: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:13:31.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8929" for this suite.
Feb 11 10:13:37.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:13:37.238: INFO: namespace resourcequota-8929 deletion completed in 6.075304396s

â€¢ [SLOW TEST:22.138 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:13:37.238: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-9756
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 11 10:13:37.260: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 11 10:13:57.325: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.22.2.6:8080/dial?request=hostName&protocol=http&host=172.22.2.5&port=8080&tries=1'] Namespace:pod-network-test-9756 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 11 10:13:57.325: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 10:13:57.456: INFO: Waiting for endpoints: map[]
Feb 11 10:13:57.459: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.22.2.6:8080/dial?request=hostName&protocol=http&host=172.22.1.221&port=8080&tries=1'] Namespace:pod-network-test-9756 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 11 10:13:57.459: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 10:13:57.616: INFO: Waiting for endpoints: map[]
Feb 11 10:13:57.619: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.22.2.6:8080/dial?request=hostName&protocol=http&host=172.22.2.206&port=8080&tries=1'] Namespace:pod-network-test-9756 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 11 10:13:57.619: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 10:13:57.782: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:13:57.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9756" for this suite.
Feb 11 10:14:09.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:14:09.868: INFO: namespace pod-network-test-9756 deletion completed in 12.081327803s

â€¢ [SLOW TEST:32.630 seconds]
[sig-network] Networking
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:14:09.868: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 11 10:14:09.898: INFO: Waiting up to 5m0s for pod "pod-dc705fbf-5ffd-4902-b213-84b352416e6f" in namespace "emptydir-2897" to be "success or failure"
Feb 11 10:14:09.901: INFO: Pod "pod-dc705fbf-5ffd-4902-b213-84b352416e6f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.850614ms
Feb 11 10:14:11.906: INFO: Pod "pod-dc705fbf-5ffd-4902-b213-84b352416e6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007911063s
STEP: Saw pod success
Feb 11 10:14:11.906: INFO: Pod "pod-dc705fbf-5ffd-4902-b213-84b352416e6f" satisfied condition "success or failure"
Feb 11 10:14:11.908: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-dc705fbf-5ffd-4902-b213-84b352416e6f container test-container: <nil>
STEP: delete the pod
Feb 11 10:14:11.922: INFO: Waiting for pod pod-dc705fbf-5ffd-4902-b213-84b352416e6f to disappear
Feb 11 10:14:11.925: INFO: Pod pod-dc705fbf-5ffd-4902-b213-84b352416e6f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:14:11.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2897" for this suite.
Feb 11 10:14:17.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:14:17.999: INFO: namespace emptydir-2897 deletion completed in 6.071490957s

â€¢ [SLOW TEST:8.131 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:14:17.999: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 10:14:18.021: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:14:24.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4436" for this suite.
Feb 11 10:14:30.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:14:30.117: INFO: namespace custom-resource-definition-4436 deletion completed in 6.074573431s

â€¢ [SLOW TEST:12.118 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:14:30.117: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-10ebbe61-3af7-4fae-b384-c619e3993b03
STEP: Creating a pod to test consume configMaps
Feb 11 10:14:30.147: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-69c8cb33-43b6-401c-a239-b4167dfebaac" in namespace "projected-891" to be "success or failure"
Feb 11 10:14:30.149: INFO: Pod "pod-projected-configmaps-69c8cb33-43b6-401c-a239-b4167dfebaac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.33803ms
Feb 11 10:14:32.153: INFO: Pod "pod-projected-configmaps-69c8cb33-43b6-401c-a239-b4167dfebaac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005869082s
STEP: Saw pod success
Feb 11 10:14:32.153: INFO: Pod "pod-projected-configmaps-69c8cb33-43b6-401c-a239-b4167dfebaac" satisfied condition "success or failure"
Feb 11 10:14:32.156: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-projected-configmaps-69c8cb33-43b6-401c-a239-b4167dfebaac container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 11 10:14:32.170: INFO: Waiting for pod pod-projected-configmaps-69c8cb33-43b6-401c-a239-b4167dfebaac to disappear
Feb 11 10:14:32.172: INFO: Pod pod-projected-configmaps-69c8cb33-43b6-401c-a239-b4167dfebaac no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:14:32.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-891" for this suite.
Feb 11 10:14:38.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:14:38.250: INFO: namespace projected-891 deletion completed in 6.074672948s

â€¢ [SLOW TEST:8.134 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:14:38.251: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 10:14:38.271: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:14:40.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2319" for this suite.
Feb 11 10:15:24.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:15:24.373: INFO: namespace pods-2319 deletion completed in 44.072100172s

â€¢ [SLOW TEST:46.122 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:15:24.373: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Feb 11 10:15:24.397: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:15:43.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3707" for this suite.
Feb 11 10:15:49.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:15:49.892: INFO: namespace crd-publish-openapi-3707 deletion completed in 6.07393975s

â€¢ [SLOW TEST:25.519 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:15:49.892: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Feb 11 10:15:49.914: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 11 10:16:49.930: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 10:16:49.933: INFO: Starting informer...
STEP: Starting pods...
Feb 11 10:16:50.147: INFO: Pod1 is running on cn-hongkong.192.168.0.18. Tainting Node
Feb 11 10:16:52.362: INFO: Pod2 is running on cn-hongkong.192.168.0.18. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Feb 11 10:16:59.117: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Feb 11 10:17:19.249: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:17:19.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-2934" for this suite.
Feb 11 10:17:25.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:17:25.335: INFO: namespace taint-multiple-pods-2934 deletion completed in 6.07395454s

â€¢ [SLOW TEST:95.443 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:17:25.336: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 10:17:25.365: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-ac26d429-1a1a-4321-b784-1c14d4092f1d" in namespace "security-context-test-3640" to be "success or failure"
Feb 11 10:17:25.370: INFO: Pod "busybox-readonly-false-ac26d429-1a1a-4321-b784-1c14d4092f1d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.441919ms
Feb 11 10:17:27.373: INFO: Pod "busybox-readonly-false-ac26d429-1a1a-4321-b784-1c14d4092f1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008608283s
Feb 11 10:17:27.373: INFO: Pod "busybox-readonly-false-ac26d429-1a1a-4321-b784-1c14d4092f1d" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:17:27.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3640" for this suite.
Feb 11 10:17:33.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:17:33.451: INFO: namespace security-context-test-3640 deletion completed in 6.07416969s

â€¢ [SLOW TEST:8.116 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:17:33.451: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 10:17:33.496: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 11 10:17:33.501: INFO: Number of nodes with available pods: 0
Feb 11 10:17:33.501: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 11 10:17:33.513: INFO: Number of nodes with available pods: 0
Feb 11 10:17:33.513: INFO: Node cn-hongkong.192.168.0.18 is running more than one daemon pod
Feb 11 10:17:34.516: INFO: Number of nodes with available pods: 1
Feb 11 10:17:34.516: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 11 10:17:34.529: INFO: Number of nodes with available pods: 1
Feb 11 10:17:34.529: INFO: Number of running nodes: 0, number of available pods: 1
Feb 11 10:17:35.532: INFO: Number of nodes with available pods: 0
Feb 11 10:17:35.532: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 11 10:17:35.539: INFO: Number of nodes with available pods: 0
Feb 11 10:17:35.539: INFO: Node cn-hongkong.192.168.0.18 is running more than one daemon pod
Feb 11 10:17:36.542: INFO: Number of nodes with available pods: 0
Feb 11 10:17:36.542: INFO: Node cn-hongkong.192.168.0.18 is running more than one daemon pod
Feb 11 10:17:37.543: INFO: Number of nodes with available pods: 0
Feb 11 10:17:37.543: INFO: Node cn-hongkong.192.168.0.18 is running more than one daemon pod
Feb 11 10:17:38.543: INFO: Number of nodes with available pods: 0
Feb 11 10:17:38.543: INFO: Node cn-hongkong.192.168.0.18 is running more than one daemon pod
Feb 11 10:17:39.543: INFO: Number of nodes with available pods: 0
Feb 11 10:17:39.543: INFO: Node cn-hongkong.192.168.0.18 is running more than one daemon pod
Feb 11 10:17:40.543: INFO: Number of nodes with available pods: 0
Feb 11 10:17:40.543: INFO: Node cn-hongkong.192.168.0.18 is running more than one daemon pod
Feb 11 10:17:41.543: INFO: Number of nodes with available pods: 0
Feb 11 10:17:41.543: INFO: Node cn-hongkong.192.168.0.18 is running more than one daemon pod
Feb 11 10:17:42.547: INFO: Number of nodes with available pods: 0
Feb 11 10:17:42.547: INFO: Node cn-hongkong.192.168.0.18 is running more than one daemon pod
Feb 11 10:17:43.542: INFO: Number of nodes with available pods: 0
Feb 11 10:17:43.542: INFO: Node cn-hongkong.192.168.0.18 is running more than one daemon pod
Feb 11 10:17:44.543: INFO: Number of nodes with available pods: 0
Feb 11 10:17:44.543: INFO: Node cn-hongkong.192.168.0.18 is running more than one daemon pod
Feb 11 10:17:45.543: INFO: Number of nodes with available pods: 0
Feb 11 10:17:45.543: INFO: Node cn-hongkong.192.168.0.18 is running more than one daemon pod
Feb 11 10:17:46.542: INFO: Number of nodes with available pods: 0
Feb 11 10:17:46.542: INFO: Node cn-hongkong.192.168.0.18 is running more than one daemon pod
Feb 11 10:17:47.543: INFO: Number of nodes with available pods: 0
Feb 11 10:17:47.543: INFO: Node cn-hongkong.192.168.0.18 is running more than one daemon pod
Feb 11 10:17:48.542: INFO: Number of nodes with available pods: 0
Feb 11 10:17:48.542: INFO: Node cn-hongkong.192.168.0.18 is running more than one daemon pod
Feb 11 10:17:49.543: INFO: Number of nodes with available pods: 0
Feb 11 10:17:49.543: INFO: Node cn-hongkong.192.168.0.18 is running more than one daemon pod
Feb 11 10:17:50.542: INFO: Number of nodes with available pods: 1
Feb 11 10:17:50.542: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5976, will wait for the garbage collector to delete the pods
Feb 11 10:17:50.604: INFO: Deleting DaemonSet.extensions daemon-set took: 5.307502ms
Feb 11 10:17:51.004: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.174656ms
Feb 11 10:17:58.707: INFO: Number of nodes with available pods: 0
Feb 11 10:17:58.707: INFO: Number of running nodes: 0, number of available pods: 0
Feb 11 10:17:58.709: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5976/daemonsets","resourceVersion":"38312"},"items":null}

Feb 11 10:17:58.712: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5976/pods","resourceVersion":"38312"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:17:58.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5976" for this suite.
Feb 11 10:18:04.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:18:04.802: INFO: namespace daemonsets-5976 deletion completed in 6.072883958s

â€¢ [SLOW TEST:31.351 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:18:04.802: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-f7d1b9f7-c844-4e2b-acd8-4864f3cd6df5
STEP: Creating a pod to test consume secrets
Feb 11 10:18:04.832: INFO: Waiting up to 5m0s for pod "pod-secrets-692185f7-2505-4887-b1b3-d7ef116c05b7" in namespace "secrets-4404" to be "success or failure"
Feb 11 10:18:04.834: INFO: Pod "pod-secrets-692185f7-2505-4887-b1b3-d7ef116c05b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.315637ms
Feb 11 10:18:06.838: INFO: Pod "pod-secrets-692185f7-2505-4887-b1b3-d7ef116c05b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00586513s
STEP: Saw pod success
Feb 11 10:18:06.838: INFO: Pod "pod-secrets-692185f7-2505-4887-b1b3-d7ef116c05b7" satisfied condition "success or failure"
Feb 11 10:18:06.841: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-secrets-692185f7-2505-4887-b1b3-d7ef116c05b7 container secret-env-test: <nil>
STEP: delete the pod
Feb 11 10:18:06.862: INFO: Waiting for pod pod-secrets-692185f7-2505-4887-b1b3-d7ef116c05b7 to disappear
Feb 11 10:18:06.865: INFO: Pod pod-secrets-692185f7-2505-4887-b1b3-d7ef116c05b7 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:18:06.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4404" for this suite.
Feb 11 10:18:12.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:18:12.939: INFO: namespace secrets-4404 deletion completed in 6.071911683s

â€¢ [SLOW TEST:8.137 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:18:12.940: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 11 10:18:12.967: INFO: Waiting up to 5m0s for pod "downwardapi-volume-610ccdf8-f19f-4098-88dc-6822f701b2b3" in namespace "projected-8590" to be "success or failure"
Feb 11 10:18:12.969: INFO: Pod "downwardapi-volume-610ccdf8-f19f-4098-88dc-6822f701b2b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.438009ms
Feb 11 10:18:14.973: INFO: Pod "downwardapi-volume-610ccdf8-f19f-4098-88dc-6822f701b2b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005769117s
STEP: Saw pod success
Feb 11 10:18:14.973: INFO: Pod "downwardapi-volume-610ccdf8-f19f-4098-88dc-6822f701b2b3" satisfied condition "success or failure"
Feb 11 10:18:14.975: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod downwardapi-volume-610ccdf8-f19f-4098-88dc-6822f701b2b3 container client-container: <nil>
STEP: delete the pod
Feb 11 10:18:14.993: INFO: Waiting for pod downwardapi-volume-610ccdf8-f19f-4098-88dc-6822f701b2b3 to disappear
Feb 11 10:18:14.995: INFO: Pod downwardapi-volume-610ccdf8-f19f-4098-88dc-6822f701b2b3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:18:14.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8590" for this suite.
Feb 11 10:18:21.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:18:21.078: INFO: namespace projected-8590 deletion completed in 6.07968057s

â€¢ [SLOW TEST:8.138 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:18:21.078: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-ce171633-cf5c-4f4d-9d72-a8ad24591588
STEP: Creating a pod to test consume secrets
Feb 11 10:18:21.139: INFO: Waiting up to 5m0s for pod "pod-secrets-af7b40ad-7bbe-4f4f-99b3-fd351f733721" in namespace "secrets-4007" to be "success or failure"
Feb 11 10:18:21.143: INFO: Pod "pod-secrets-af7b40ad-7bbe-4f4f-99b3-fd351f733721": Phase="Pending", Reason="", readiness=false. Elapsed: 3.442934ms
Feb 11 10:18:23.146: INFO: Pod "pod-secrets-af7b40ad-7bbe-4f4f-99b3-fd351f733721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00683288s
STEP: Saw pod success
Feb 11 10:18:23.146: INFO: Pod "pod-secrets-af7b40ad-7bbe-4f4f-99b3-fd351f733721" satisfied condition "success or failure"
Feb 11 10:18:23.150: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-secrets-af7b40ad-7bbe-4f4f-99b3-fd351f733721 container secret-volume-test: <nil>
STEP: delete the pod
Feb 11 10:18:23.169: INFO: Waiting for pod pod-secrets-af7b40ad-7bbe-4f4f-99b3-fd351f733721 to disappear
Feb 11 10:18:23.173: INFO: Pod pod-secrets-af7b40ad-7bbe-4f4f-99b3-fd351f733721 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:18:23.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4007" for this suite.
Feb 11 10:18:29.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:18:29.255: INFO: namespace secrets-4007 deletion completed in 6.076509545s
STEP: Destroying namespace "secret-namespace-5998" for this suite.
Feb 11 10:18:35.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:18:35.328: INFO: namespace secret-namespace-5998 deletion completed in 6.073088242s

â€¢ [SLOW TEST:14.250 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:18:35.328: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 11 10:18:36.072: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 11 10:18:39.088: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Feb 11 10:18:39.104: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:18:39.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4995" for this suite.
Feb 11 10:18:45.127: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:18:45.191: INFO: namespace webhook-4995 deletion completed in 6.073269799s
STEP: Destroying namespace "webhook-4995-markers" for this suite.
Feb 11 10:18:51.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:18:51.262: INFO: namespace webhook-4995-markers deletion completed in 6.071844357s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:15.945 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:18:51.274: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 11 10:18:52.133: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 11 10:18:55.148: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 10:18:55.151: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:19:01.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2477" for this suite.
Feb 11 10:19:07.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:19:07.345: INFO: namespace webhook-2477 deletion completed in 6.074518005s
STEP: Destroying namespace "webhook-2477-markers" for this suite.
Feb 11 10:19:13.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:19:13.419: INFO: namespace webhook-2477-markers deletion completed in 6.074468503s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:22.157 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:19:13.431: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Feb 11 10:19:13.453: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Feb 11 10:19:13.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 create -f - --namespace=kubectl-1814'
Feb 11 10:19:13.841: INFO: stderr: ""
Feb 11 10:19:13.841: INFO: stdout: "service/redis-slave created\n"
Feb 11 10:19:13.842: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Feb 11 10:19:13.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 create -f - --namespace=kubectl-1814'
Feb 11 10:19:14.064: INFO: stderr: ""
Feb 11 10:19:14.064: INFO: stdout: "service/redis-master created\n"
Feb 11 10:19:14.064: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 11 10:19:14.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 create -f - --namespace=kubectl-1814'
Feb 11 10:19:14.286: INFO: stderr: ""
Feb 11 10:19:14.286: INFO: stdout: "service/frontend created\n"
Feb 11 10:19:14.287: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Feb 11 10:19:14.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 create -f - --namespace=kubectl-1814'
Feb 11 10:19:14.495: INFO: stderr: ""
Feb 11 10:19:14.495: INFO: stdout: "deployment.apps/frontend created\n"
Feb 11 10:19:14.495: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 11 10:19:14.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 create -f - --namespace=kubectl-1814'
Feb 11 10:19:14.701: INFO: stderr: ""
Feb 11 10:19:14.701: INFO: stdout: "deployment.apps/redis-master created\n"
Feb 11 10:19:14.701: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Feb 11 10:19:14.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 create -f - --namespace=kubectl-1814'
Feb 11 10:19:14.901: INFO: stderr: ""
Feb 11 10:19:14.901: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Feb 11 10:19:14.901: INFO: Waiting for all frontend pods to be Running.
Feb 11 10:19:34.952: INFO: Waiting for frontend to serve content.
Feb 11 10:19:34.965: INFO: Trying to add a new entry to the guestbook.
Feb 11 10:19:34.978: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb 11 10:19:34.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 delete --grace-period=0 --force -f - --namespace=kubectl-1814'
Feb 11 10:19:35.065: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 11 10:19:35.065: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb 11 10:19:35.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 delete --grace-period=0 --force -f - --namespace=kubectl-1814'
Feb 11 10:19:35.154: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 11 10:19:35.154: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 11 10:19:35.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 delete --grace-period=0 --force -f - --namespace=kubectl-1814'
Feb 11 10:19:35.241: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 11 10:19:35.241: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 11 10:19:35.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 delete --grace-period=0 --force -f - --namespace=kubectl-1814'
Feb 11 10:19:35.325: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 11 10:19:35.325: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 11 10:19:35.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 delete --grace-period=0 --force -f - --namespace=kubectl-1814'
Feb 11 10:19:35.393: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 11 10:19:35.393: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 11 10:19:35.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 delete --grace-period=0 --force -f - --namespace=kubectl-1814'
Feb 11 10:19:35.462: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 11 10:19:35.462: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:19:35.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1814" for this suite.
Feb 11 10:20:03.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:20:03.543: INFO: namespace kubectl-1814 deletion completed in 28.077662067s

â€¢ [SLOW TEST:50.112 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:20:03.543: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Feb 11 10:20:03.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-208978535 cluster-info'
Feb 11 10:20:03.639: INFO: stderr: ""
Feb 11 10:20:03.639: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.23.0.1:443\x1b[0m\n\x1b[0;32mmetrics-server\x1b[0m is running at \x1b[0;33mhttps://172.23.0.1:443/api/v1/namespaces/kube-system/services/heapster/proxy\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://172.23.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:20:03.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-492" for this suite.
Feb 11 10:20:09.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:20:09.716: INFO: namespace kubectl-492 deletion completed in 6.073445545s

â€¢ [SLOW TEST:6.172 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:20:09.716: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-5327, will wait for the garbage collector to delete the pods
Feb 11 10:20:11.806: INFO: Deleting Job.batch foo took: 6.581215ms
Feb 11 10:20:12.206: INFO: Terminating Job.batch foo pods took: 400.210612ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:20:45.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5327" for this suite.
Feb 11 10:20:51.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:20:51.489: INFO: namespace job-5327 deletion completed in 6.0747685s

â€¢ [SLOW TEST:41.773 seconds]
[sig-apps] Job
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:20:51.489: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 11 10:20:51.857: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 11 10:20:53.866: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717013251, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717013251, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717013251, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717013251, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 11 10:20:56.876: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 11 10:20:56.879: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2410-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:21:02.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6380" for this suite.
Feb 11 10:21:08.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:21:09.034: INFO: namespace webhook-6380 deletion completed in 6.075952384s
STEP: Destroying namespace "webhook-6380-markers" for this suite.
Feb 11 10:21:15.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:21:15.106: INFO: namespace webhook-6380-markers deletion completed in 6.072304874s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:23.628 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:21:15.118: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Feb 11 10:21:15.140: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Feb 11 10:21:30.800: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
Feb 11 10:21:37.606: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:21:51.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4244" for this suite.
Feb 11 10:21:57.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:21:57.581: INFO: namespace crd-publish-openapi-4244 deletion completed in 6.071107101s

â€¢ [SLOW TEST:42.463 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:21:57.581: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:21:59.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9592" for this suite.
Feb 11 10:22:05.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:22:05.713: INFO: namespace emptydir-wrapper-9592 deletion completed in 6.071016173s

â€¢ [SLOW TEST:8.132 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:22:05.713: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-3620c0cb-9805-4b18-9ec6-5ee9b2cf05ff
STEP: Creating a pod to test consume secrets
Feb 11 10:22:05.742: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3a1c5a7f-b7fd-4ee0-935a-63420c72a7d6" in namespace "projected-554" to be "success or failure"
Feb 11 10:22:05.744: INFO: Pod "pod-projected-secrets-3a1c5a7f-b7fd-4ee0-935a-63420c72a7d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.126673ms
Feb 11 10:22:07.748: INFO: Pod "pod-projected-secrets-3a1c5a7f-b7fd-4ee0-935a-63420c72a7d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005752261s
STEP: Saw pod success
Feb 11 10:22:07.748: INFO: Pod "pod-projected-secrets-3a1c5a7f-b7fd-4ee0-935a-63420c72a7d6" satisfied condition "success or failure"
Feb 11 10:22:07.750: INFO: Trying to get logs from node cn-hongkong.192.168.0.18 pod pod-projected-secrets-3a1c5a7f-b7fd-4ee0-935a-63420c72a7d6 container secret-volume-test: <nil>
STEP: delete the pod
Feb 11 10:22:07.771: INFO: Waiting for pod pod-projected-secrets-3a1c5a7f-b7fd-4ee0-935a-63420c72a7d6 to disappear
Feb 11 10:22:07.773: INFO: Pod pod-projected-secrets-3a1c5a7f-b7fd-4ee0-935a-63420c72a7d6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:22:07.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-554" for this suite.
Feb 11 10:22:13.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:22:13.849: INFO: namespace projected-554 deletion completed in 6.072323761s

â€¢ [SLOW TEST:8.136 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:22:13.849: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-4139
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4139 to expose endpoints map[]
Feb 11 10:22:13.880: INFO: Get endpoints failed (2.724283ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Feb 11 10:22:14.883: INFO: successfully validated that service multi-endpoint-test in namespace services-4139 exposes endpoints map[] (1.005893972s elapsed)
STEP: Creating pod pod1 in namespace services-4139
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4139 to expose endpoints map[pod1:[100]]
Feb 11 10:22:16.905: INFO: successfully validated that service multi-endpoint-test in namespace services-4139 exposes endpoints map[pod1:[100]] (2.015761553s elapsed)
STEP: Creating pod pod2 in namespace services-4139
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4139 to expose endpoints map[pod1:[100] pod2:[101]]
Feb 11 10:22:18.932: INFO: successfully validated that service multi-endpoint-test in namespace services-4139 exposes endpoints map[pod1:[100] pod2:[101]] (2.023424497s elapsed)
STEP: Deleting pod pod1 in namespace services-4139
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4139 to expose endpoints map[pod2:[101]]
Feb 11 10:22:19.948: INFO: successfully validated that service multi-endpoint-test in namespace services-4139 exposes endpoints map[pod2:[101]] (1.011290311s elapsed)
STEP: Deleting pod pod2 in namespace services-4139
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4139 to expose endpoints map[]
Feb 11 10:22:20.958: INFO: successfully validated that service multi-endpoint-test in namespace services-4139 exposes endpoints map[] (1.005749522s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:22:20.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4139" for this suite.
Feb 11 10:22:48.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:22:49.052: INFO: namespace services-4139 deletion completed in 28.074804759s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:35.202 seconds]
[sig-network] Services
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 11 10:22:49.052: INFO: >>> kubeConfig: /tmp/kubeconfig-208978535
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 11 10:22:53.109: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 11 10:22:53.112: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 11 10:22:55.112: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 11 10:22:55.115: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 11 10:22:57.112: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 11 10:22:57.115: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 11 10:22:59.112: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 11 10:22:59.115: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 11 10:22:59.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6496" for this suite.
Feb 11 10:23:11.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 11 10:23:11.211: INFO: namespace container-lifecycle-hook-6496 deletion completed in 12.085048672s

â€¢ [SLOW TEST:22.159 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSFeb 11 10:23:11.211: INFO: Running AfterSuite actions on all nodes
Feb 11 10:23:11.211: INFO: Running AfterSuite actions on node 1
Feb 11 10:23:11.211: INFO: Skipping dumping logs from cluster

Ran 276 of 4731 Specs in 6691.525 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4455 Skipped
PASS

Ginkgo ran 1 suite in 1h51m32.849029466s
Test Suite Passed
