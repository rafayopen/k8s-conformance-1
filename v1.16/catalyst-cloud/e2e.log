I0514 01:44:06.811902      20 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-817715921
I0514 01:44:06.813304      20 e2e.go:92] Starting e2e run "5c036a25-d9b5-46f9-8351-3b0a5a4e4a43" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1589420643 - Will randomize all specs
Will run 274 of 4732 specs

May 14 01:44:06.998: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
May 14 01:44:07.008: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 14 01:44:07.092: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 14 01:44:07.216: INFO: 25 / 25 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 14 01:44:07.217: INFO: expected 11 pod replicas in namespace 'kube-system', 11 are Running and Ready.
May 14 01:44:07.217: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 14 01:44:07.248: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
May 14 01:44:07.248: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'k8s-keystone-auth' (0 seconds elapsed)
May 14 01:44:07.248: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'magnum-auto-healer' (0 seconds elapsed)
May 14 01:44:07.248: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'npd' (0 seconds elapsed)
May 14 01:44:07.248: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
May 14 01:44:07.248: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'prometheus-operator-prometheus-node-exporter' (0 seconds elapsed)
May 14 01:44:07.248: INFO: e2e test version: v1.16.9
May 14 01:44:07.256: INFO: kube-apiserver version: v1.16.9
May 14 01:44:07.256: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
May 14 01:44:07.269: INFO: Cluster IP family: ipv4
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 01:44:07.273: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
May 14 01:44:07.338: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
May 14 01:44:07.367: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5655
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-63747886-4efe-4a90-ad51-5fc1d6024b3c
STEP: Creating a pod to test consume configMaps
May 14 01:44:07.536: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d3dccb89-7c67-4545-8fd8-d7c6569dad0f" in namespace "projected-5655" to be "success or failure"
May 14 01:44:07.543: INFO: Pod "pod-projected-configmaps-d3dccb89-7c67-4545-8fd8-d7c6569dad0f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.789331ms
May 14 01:44:09.555: INFO: Pod "pod-projected-configmaps-d3dccb89-7c67-4545-8fd8-d7c6569dad0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018271034s
May 14 01:44:12.145: INFO: Pod "pod-projected-configmaps-d3dccb89-7c67-4545-8fd8-d7c6569dad0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.608148193s
May 14 01:44:14.461: INFO: Pod "pod-projected-configmaps-d3dccb89-7c67-4545-8fd8-d7c6569dad0f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.9242082s
May 14 01:44:16.466: INFO: Pod "pod-projected-configmaps-d3dccb89-7c67-4545-8fd8-d7c6569dad0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.929532702s
STEP: Saw pod success
May 14 01:44:16.481: INFO: Pod "pod-projected-configmaps-d3dccb89-7c67-4545-8fd8-d7c6569dad0f" satisfied condition "success or failure"
May 14 01:44:16.486: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-projected-configmaps-d3dccb89-7c67-4545-8fd8-d7c6569dad0f container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 14 01:44:17.298: INFO: Waiting for pod pod-projected-configmaps-d3dccb89-7c67-4545-8fd8-d7c6569dad0f to disappear
May 14 01:44:17.665: INFO: Pod pod-projected-configmaps-d3dccb89-7c67-4545-8fd8-d7c6569dad0f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 01:44:17.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5655" for this suite.
May 14 01:44:23.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:44:24.140: INFO: namespace projected-5655 deletion completed in 6.457311447s

â€¢ [SLOW TEST:16.868 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 01:44:24.143: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3090
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 14 01:44:26.013: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017465, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017465, loc:(*time.Location)(0x78a2900)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-86d95b659d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017465, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017465, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
May 14 01:44:29.878: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017465, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017465, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017466, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017465, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 01:44:30.023: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017465, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017465, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017466, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017465, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 01:44:32.021: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017465, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017465, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017466, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017465, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 14 01:44:35.056: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 01:44:47.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3090" for this suite.
May 14 01:44:54.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:44:54.646: INFO: namespace webhook-3090 deletion completed in 6.665041662s
STEP: Destroying namespace "webhook-3090-markers" for this suite.
May 14 01:45:02.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:45:03.079: INFO: namespace webhook-3090-markers deletion completed in 8.432433519s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:38.973 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 01:45:03.117: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8555
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-8555
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
May 14 01:45:03.937: INFO: Found 0 stateful pods, waiting for 3
May 14 01:45:13.963: INFO: Found 2 stateful pods, waiting for 3
May 14 01:45:23.961: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 14 01:45:23.962: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 14 01:45:23.962: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May 14 01:45:24.006: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 14 01:45:34.088: INFO: Updating stateful set ss2
May 14 01:45:34.113: INFO: Waiting for Pod statefulset-8555/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
May 14 01:45:44.272: INFO: Found 2 stateful pods, waiting for 3
May 14 01:45:54.290: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 14 01:45:54.290: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 14 01:45:54.290: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 14 01:45:54.329: INFO: Updating stateful set ss2
May 14 01:45:54.400: INFO: Waiting for Pod statefulset-8555/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 14 01:46:04.450: INFO: Updating stateful set ss2
May 14 01:46:05.495: INFO: Waiting for StatefulSet statefulset-8555/ss2 to complete update
May 14 01:46:05.495: INFO: Waiting for Pod statefulset-8555/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 14 01:46:15.512: INFO: Waiting for StatefulSet statefulset-8555/ss2 to complete update
May 14 01:46:15.513: INFO: Waiting for Pod statefulset-8555/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 14 01:46:26.174: INFO: Waiting for StatefulSet statefulset-8555/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May 14 01:46:35.508: INFO: Deleting all statefulset in ns statefulset-8555
May 14 01:46:35.525: INFO: Scaling statefulset ss2 to 0
May 14 01:46:55.554: INFO: Waiting for statefulset status.replicas updated to 0
May 14 01:46:55.562: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 01:46:55.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8555" for this suite.
May 14 01:47:01.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:47:02.169: INFO: namespace statefulset-8555 deletion completed in 6.568031686s

â€¢ [SLOW TEST:119.053 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 01:47:02.175: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1067
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-1067/secret-test-0d2cebd7-1687-4968-8eee-fb6292dcc5d7
STEP: Creating a pod to test consume secrets
May 14 01:47:02.397: INFO: Waiting up to 5m0s for pod "pod-configmaps-060d09f4-9b53-4bd0-a241-06d436ad9fef" in namespace "secrets-1067" to be "success or failure"
May 14 01:47:02.408: INFO: Pod "pod-configmaps-060d09f4-9b53-4bd0-a241-06d436ad9fef": Phase="Pending", Reason="", readiness=false. Elapsed: 10.721068ms
May 14 01:47:07.068: INFO: Pod "pod-configmaps-060d09f4-9b53-4bd0-a241-06d436ad9fef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.670761988s
May 14 01:47:10.923: INFO: Pod "pod-configmaps-060d09f4-9b53-4bd0-a241-06d436ad9fef": Phase="Pending", Reason="", readiness=false. Elapsed: 8.525650486s
May 14 01:47:12.932: INFO: Pod "pod-configmaps-060d09f4-9b53-4bd0-a241-06d436ad9fef": Phase="Pending", Reason="", readiness=false. Elapsed: 10.534948415s
May 14 01:47:14.949: INFO: Pod "pod-configmaps-060d09f4-9b53-4bd0-a241-06d436ad9fef": Phase="Pending", Reason="", readiness=false. Elapsed: 12.551770518s
May 14 01:47:16.973: INFO: Pod "pod-configmaps-060d09f4-9b53-4bd0-a241-06d436ad9fef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.575479751s
STEP: Saw pod success
May 14 01:47:16.973: INFO: Pod "pod-configmaps-060d09f4-9b53-4bd0-a241-06d436ad9fef" satisfied condition "success or failure"
May 14 01:47:17.587: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-configmaps-060d09f4-9b53-4bd0-a241-06d436ad9fef container env-test: <nil>
STEP: delete the pod
May 14 01:47:17.840: INFO: Waiting for pod pod-configmaps-060d09f4-9b53-4bd0-a241-06d436ad9fef to disappear
May 14 01:47:17.850: INFO: Pod pod-configmaps-060d09f4-9b53-4bd0-a241-06d436ad9fef no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 01:47:17.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1067" for this suite.
May 14 01:47:23.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:47:24.205: INFO: namespace secrets-1067 deletion completed in 6.348073875s

â€¢ [SLOW TEST:22.031 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 01:47:24.212: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-473
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-473/configmap-test-2b56bfd4-1be0-451c-9c79-fd06a23a0b3d
STEP: Creating a pod to test consume configMaps
May 14 01:47:24.412: INFO: Waiting up to 5m0s for pod "pod-configmaps-654d79d0-9879-48a0-b24c-118e0ad87572" in namespace "configmap-473" to be "success or failure"
May 14 01:47:24.430: INFO: Pod "pod-configmaps-654d79d0-9879-48a0-b24c-118e0ad87572": Phase="Pending", Reason="", readiness=false. Elapsed: 17.575924ms
May 14 01:47:26.438: INFO: Pod "pod-configmaps-654d79d0-9879-48a0-b24c-118e0ad87572": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025419405s
May 14 01:47:28.481: INFO: Pod "pod-configmaps-654d79d0-9879-48a0-b24c-118e0ad87572": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068119103s
STEP: Saw pod success
May 14 01:47:28.481: INFO: Pod "pod-configmaps-654d79d0-9879-48a0-b24c-118e0ad87572" satisfied condition "success or failure"
May 14 01:47:28.485: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-configmaps-654d79d0-9879-48a0-b24c-118e0ad87572 container env-test: <nil>
STEP: delete the pod
May 14 01:47:28.540: INFO: Waiting for pod pod-configmaps-654d79d0-9879-48a0-b24c-118e0ad87572 to disappear
May 14 01:47:28.547: INFO: Pod pod-configmaps-654d79d0-9879-48a0-b24c-118e0ad87572 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 01:47:28.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-473" for this suite.
May 14 01:47:36.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:47:36.976: INFO: namespace configmap-473 deletion completed in 8.417094432s

â€¢ [SLOW TEST:12.765 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 01:47:36.982: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5235
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-206
STEP: Creating secret with name secret-test-46299d3b-d9e9-418c-bd27-a53a5f6d71a3
STEP: Creating a pod to test consume secrets
May 14 01:47:38.147: INFO: Waiting up to 5m0s for pod "pod-secrets-a5fd438f-18b0-4af1-afd0-aa75577883e1" in namespace "secrets-5235" to be "success or failure"
May 14 01:47:38.168: INFO: Pod "pod-secrets-a5fd438f-18b0-4af1-afd0-aa75577883e1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.830447ms
May 14 01:47:40.188: INFO: Pod "pod-secrets-a5fd438f-18b0-4af1-afd0-aa75577883e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040168716s
May 14 01:47:42.196: INFO: Pod "pod-secrets-a5fd438f-18b0-4af1-afd0-aa75577883e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049041022s
May 14 01:47:44.208: INFO: Pod "pod-secrets-a5fd438f-18b0-4af1-afd0-aa75577883e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.060639273s
STEP: Saw pod success
May 14 01:47:44.208: INFO: Pod "pod-secrets-a5fd438f-18b0-4af1-afd0-aa75577883e1" satisfied condition "success or failure"
May 14 01:47:44.215: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-secrets-a5fd438f-18b0-4af1-afd0-aa75577883e1 container secret-volume-test: <nil>
STEP: delete the pod
May 14 01:47:44.257: INFO: Waiting for pod pod-secrets-a5fd438f-18b0-4af1-afd0-aa75577883e1 to disappear
May 14 01:47:44.271: INFO: Pod pod-secrets-a5fd438f-18b0-4af1-afd0-aa75577883e1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 01:47:44.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5235" for this suite.
May 14 01:47:52.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:47:52.695: INFO: namespace secrets-5235 deletion completed in 8.409379334s
STEP: Destroying namespace "secret-namespace-206" for this suite.
May 14 01:47:58.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:47:59.064: INFO: namespace secret-namespace-206 deletion completed in 6.368885694s

â€¢ [SLOW TEST:22.082 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 01:47:59.067: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-255
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 14 01:48:00.861: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 14 01:48:02.875: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017680, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017680, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017680, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017680, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 01:48:04.884: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017680, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017680, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017680, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725017680, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 14 01:48:07.932: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 01:48:09.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-255" for this suite.
May 14 01:48:15.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:48:16.165: INFO: namespace webhook-255 deletion completed in 6.426624097s
STEP: Destroying namespace "webhook-255-markers" for this suite.
May 14 01:48:24.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:48:24.617: INFO: namespace webhook-255-markers deletion completed in 8.45226425s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:25.598 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 01:48:24.665: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1890
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 01:48:31.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1890" for this suite.
May 14 01:48:39.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:48:39.849: INFO: namespace resourcequota-1890 deletion completed in 7.843583487s

â€¢ [SLOW TEST:15.184 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 01:48:39.853: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8843
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-8843
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8843
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8843
May 14 01:48:40.332: INFO: Found 0 stateful pods, waiting for 1
May 14 01:48:50.339: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 14 01:48:50.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 14 01:48:54.014: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 14 01:48:54.015: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 14 01:48:54.015: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 14 01:48:54.043: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 14 01:49:04.055: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 14 01:49:04.055: INFO: Waiting for statefulset status.replicas updated to 0
May 14 01:49:04.093: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999283s
May 14 01:49:05.098: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993359302s
May 14 01:49:06.109: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987619693s
May 14 01:49:07.897: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.965753408s
May 14 01:49:09.240: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.188673995s
May 14 01:49:10.245: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.845791802s
May 14 01:49:11.261: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.841055686s
May 14 01:49:12.267: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.825379469s
May 14 01:49:13.294: INFO: Verifying statefulset ss doesn't scale past 1 for another 819.454603ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8843
May 14 01:49:14.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:49:14.951: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 14 01:49:14.951: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 14 01:49:14.951: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 14 01:49:14.959: INFO: Found 1 stateful pods, waiting for 3
May 14 01:49:29.943: INFO: Found 2 stateful pods, waiting for 3
May 14 01:49:34.967: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 14 01:49:34.967: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 14 01:49:34.967: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
May 14 01:49:44.977: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 14 01:49:44.977: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 14 01:49:44.978: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
May 14 01:49:54.965: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 14 01:49:54.966: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 14 01:49:54.966: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
May 14 01:50:04.969: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 14 01:50:04.970: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 14 01:50:04.970: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
May 14 01:50:14.967: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 14 01:50:14.967: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 14 01:50:14.967: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=false
May 14 01:50:24.969: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 14 01:50:24.969: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 14 01:50:24.969: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 14 01:50:24.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 14 01:50:25.503: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 14 01:50:25.503: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 14 01:50:25.503: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 14 01:50:25.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 14 01:50:26.164: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 14 01:50:26.164: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 14 01:50:26.164: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 14 01:50:26.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 14 01:50:27.138: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 14 01:50:27.138: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 14 01:50:27.138: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 14 01:50:27.138: INFO: Waiting for statefulset status.replicas updated to 0
May 14 01:50:27.152: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
May 14 01:50:37.171: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 14 01:50:37.172: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 14 01:50:37.172: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 14 01:50:37.194: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999046s
May 14 01:50:38.650: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989633287s
May 14 01:50:39.659: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.53670334s
May 14 01:50:40.665: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.528275841s
May 14 01:50:41.679: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.52177392s
May 14 01:50:42.686: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.507956166s
May 14 01:50:46.370: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.500065001s
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8843
May 14 01:50:47.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:50:48.054: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 14 01:50:48.055: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 14 01:50:48.055: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 14 01:50:48.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:50:48.881: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 14 01:50:48.881: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 14 01:50:48.881: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 14 01:50:48.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:50:49.712: INFO: rc: 1
May 14 01:50:49.713: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server: 

error:
exit status 1
May 14 01:50:59.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:51:00.067: INFO: rc: 1
May 14 01:51:00.067: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
May 14 01:51:10.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:51:10.427: INFO: rc: 1
May 14 01:51:10.428: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:51:20.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:51:20.754: INFO: rc: 1
May 14 01:51:20.754: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:51:30.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:51:31.028: INFO: rc: 1
May 14 01:51:31.029: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:51:41.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:51:41.249: INFO: rc: 1
May 14 01:51:41.249: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:51:51.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:51:51.502: INFO: rc: 1
May 14 01:51:51.502: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:52:01.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:52:01.892: INFO: rc: 1
May 14 01:52:01.892: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:52:11.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:52:12.147: INFO: rc: 1
May 14 01:52:12.147: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:52:22.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:52:22.362: INFO: rc: 1
May 14 01:52:22.363: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:52:32.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:52:32.697: INFO: rc: 1
May 14 01:52:32.698: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:52:42.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:52:43.557: INFO: rc: 1
May 14 01:52:43.557: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:52:53.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:52:53.861: INFO: rc: 1
May 14 01:52:53.861: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:53:03.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:53:04.166: INFO: rc: 1
May 14 01:53:04.166: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:53:14.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:53:14.450: INFO: rc: 1
May 14 01:53:14.450: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:53:24.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:53:25.501: INFO: rc: 1
May 14 01:53:25.501: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:53:35.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:53:35.750: INFO: rc: 1
May 14 01:53:35.750: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:53:45.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:53:45.984: INFO: rc: 1
May 14 01:53:45.984: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:53:55.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:53:56.252: INFO: rc: 1
May 14 01:53:56.252: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:54:06.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:54:06.482: INFO: rc: 1
May 14 01:54:06.482: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:54:16.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:54:16.998: INFO: rc: 1
May 14 01:54:16.998: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:54:26.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:54:27.257: INFO: rc: 1
May 14 01:54:27.257: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:54:37.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:54:37.496: INFO: rc: 1
May 14 01:54:37.496: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:54:47.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:54:47.768: INFO: rc: 1
May 14 01:54:47.769: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:54:57.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:54:58.006: INFO: rc: 1
May 14 01:54:58.007: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:55:08.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:55:08.343: INFO: rc: 1
May 14 01:55:08.343: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:55:18.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:55:18.743: INFO: rc: 1
May 14 01:55:18.744: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:55:28.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:55:29.157: INFO: rc: 1
May 14 01:55:29.157: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:55:39.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:55:39.498: INFO: rc: 1
May 14 01:55:39.498: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 14 01:55:49.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-8843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 01:55:49.759: INFO: rc: 1
May 14 01:55:49.759: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
May 14 01:55:49.759: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May 14 01:55:49.789: INFO: Deleting all statefulset in ns statefulset-8843
May 14 01:55:49.796: INFO: Scaling statefulset ss to 0
May 14 01:55:49.815: INFO: Waiting for statefulset status.replicas updated to 0
May 14 01:55:49.821: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 01:55:49.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8843" for this suite.
May 14 01:55:57.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:55:58.247: INFO: namespace statefulset-8843 deletion completed in 8.380994486s

â€¢ [SLOW TEST:438.395 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 01:55:58.257: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-103
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
May 14 01:55:58.554: INFO: Waiting up to 5m0s for pod "client-containers-341ebf34-893d-46fa-9b02-4d074a145a6a" in namespace "containers-103" to be "success or failure"
May 14 01:55:58.565: INFO: Pod "client-containers-341ebf34-893d-46fa-9b02-4d074a145a6a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.455634ms
May 14 01:56:00.574: INFO: Pod "client-containers-341ebf34-893d-46fa-9b02-4d074a145a6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018626043s
May 14 01:56:02.603: INFO: Pod "client-containers-341ebf34-893d-46fa-9b02-4d074a145a6a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048333384s
May 14 01:56:04.614: INFO: Pod "client-containers-341ebf34-893d-46fa-9b02-4d074a145a6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.058501552s
STEP: Saw pod success
May 14 01:56:04.614: INFO: Pod "client-containers-341ebf34-893d-46fa-9b02-4d074a145a6a" satisfied condition "success or failure"
May 14 01:56:04.618: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod client-containers-341ebf34-893d-46fa-9b02-4d074a145a6a container test-container: <nil>
STEP: delete the pod
May 14 01:56:05.837: INFO: Waiting for pod client-containers-341ebf34-893d-46fa-9b02-4d074a145a6a to disappear
May 14 01:56:05.846: INFO: Pod client-containers-341ebf34-893d-46fa-9b02-4d074a145a6a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 01:56:05.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-103" for this suite.
May 14 01:56:11.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:56:12.201: INFO: namespace containers-103 deletion completed in 6.349488681s

â€¢ [SLOW TEST:13.944 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 01:56:12.202: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8908
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
May 14 01:56:12.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 create -f - --namespace=kubectl-8908'
May 14 01:56:12.989: INFO: stderr: ""
May 14 01:56:12.989: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 14 01:56:12.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8908'
May 14 01:56:14.483: INFO: stderr: ""
May 14 01:56:14.483: INFO: stdout: ""
STEP: Replicas for name=update-demo: expected=2 actual=0
May 14 01:56:19.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8908'
May 14 01:56:19.994: INFO: stderr: ""
May 14 01:56:19.995: INFO: stdout: "update-demo-nautilus-gbd7w update-demo-nautilus-gwdn4 "
May 14 01:56:20.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-nautilus-gbd7w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8908'
May 14 01:56:20.343: INFO: stderr: ""
May 14 01:56:20.343: INFO: stdout: ""
May 14 01:56:20.343: INFO: update-demo-nautilus-gbd7w is created but not running
May 14 01:56:25.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8908'
May 14 01:56:25.598: INFO: stderr: ""
May 14 01:56:25.598: INFO: stdout: "update-demo-nautilus-gbd7w update-demo-nautilus-gwdn4 "
May 14 01:56:25.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-nautilus-gbd7w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8908'
May 14 01:56:25.898: INFO: stderr: ""
May 14 01:56:25.898: INFO: stdout: "true"
May 14 01:56:25.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-nautilus-gbd7w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8908'
May 14 01:56:26.122: INFO: stderr: ""
May 14 01:56:26.122: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 14 01:56:26.122: INFO: validating pod update-demo-nautilus-gbd7w
May 14 01:56:26.137: INFO: got data: {
  "image": "nautilus.jpg"
}

May 14 01:56:26.138: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 14 01:56:26.138: INFO: update-demo-nautilus-gbd7w is verified up and running
May 14 01:56:26.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-nautilus-gwdn4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8908'
May 14 01:56:27.289: INFO: stderr: ""
May 14 01:56:27.289: INFO: stdout: "true"
May 14 01:56:27.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-nautilus-gwdn4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8908'
May 14 01:56:32.073: INFO: stderr: ""
May 14 01:56:32.073: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 14 01:56:32.074: INFO: validating pod update-demo-nautilus-gwdn4
May 14 01:56:33.292: INFO: got data: {
  "image": "nautilus.jpg"
}

May 14 01:56:33.292: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 14 01:56:33.292: INFO: update-demo-nautilus-gwdn4 is verified up and running
STEP: using delete to clean up resources
May 14 01:56:33.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 delete --grace-period=0 --force -f - --namespace=kubectl-8908'
May 14 01:56:33.528: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 14 01:56:33.528: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 14 01:56:33.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8908'
May 14 01:56:33.921: INFO: stderr: "No resources found in kubectl-8908 namespace.\n"
May 14 01:56:33.921: INFO: stdout: ""
May 14 01:56:33.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods -l name=update-demo --namespace=kubectl-8908 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 14 01:56:34.164: INFO: stderr: ""
May 14 01:56:34.164: INFO: stdout: "update-demo-nautilus-gbd7w\nupdate-demo-nautilus-gwdn4\n"
May 14 01:56:34.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8908'
May 14 01:56:35.149: INFO: stderr: "No resources found in kubectl-8908 namespace.\n"
May 14 01:56:35.149: INFO: stdout: ""
May 14 01:56:35.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods -l name=update-demo --namespace=kubectl-8908 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 14 01:56:35.677: INFO: stderr: ""
May 14 01:56:35.677: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 01:56:35.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8908" for this suite.
May 14 01:56:43.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:56:44.317: INFO: namespace kubectl-8908 deletion completed in 8.621965195s

â€¢ [SLOW TEST:32.116 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 01:56:44.325: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5488
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 01:57:47.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5488" for this suite.
May 14 01:57:59.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:57:59.708: INFO: namespace container-probe-5488 deletion completed in 12.451649246s

â€¢ [SLOW TEST:75.384 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 01:57:59.713: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-288
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 01:57:59.917: INFO: Creating ReplicaSet my-hostname-basic-fc1a8f43-e7ae-4d08-85c5-1e17d4529ea8
May 14 01:57:59.939: INFO: Pod name my-hostname-basic-fc1a8f43-e7ae-4d08-85c5-1e17d4529ea8: Found 0 pods out of 1
May 14 01:58:04.958: INFO: Pod name my-hostname-basic-fc1a8f43-e7ae-4d08-85c5-1e17d4529ea8: Found 1 pods out of 1
May 14 01:58:04.958: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-fc1a8f43-e7ae-4d08-85c5-1e17d4529ea8" is running
May 14 01:58:04.963: INFO: Pod "my-hostname-basic-fc1a8f43-e7ae-4d08-85c5-1e17d4529ea8-n2lzx" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-14 01:57:59 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-14 01:58:04 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-14 01:58:04 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-14 01:57:59 +0000 UTC Reason: Message:}])
May 14 01:58:04.964: INFO: Trying to dial the pod
May 14 01:58:10.006: INFO: Controller my-hostname-basic-fc1a8f43-e7ae-4d08-85c5-1e17d4529ea8: Got expected result from replica 1 [my-hostname-basic-fc1a8f43-e7ae-4d08-85c5-1e17d4529ea8-n2lzx]: "my-hostname-basic-fc1a8f43-e7ae-4d08-85c5-1e17d4529ea8-n2lzx", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 01:58:10.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-288" for this suite.
May 14 01:58:16.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:58:16.468: INFO: namespace replicaset-288 deletion completed in 6.430354013s

â€¢ [SLOW TEST:16.755 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 01:58:16.478: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8720
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-fe7761d8-d417-459a-b756-19979bc176b7
STEP: Creating a pod to test consume secrets
May 14 01:58:16.761: INFO: Waiting up to 5m0s for pod "pod-secrets-ebee1c8b-5039-4deb-8664-9437044808a4" in namespace "secrets-8720" to be "success or failure"
May 14 01:58:16.787: INFO: Pod "pod-secrets-ebee1c8b-5039-4deb-8664-9437044808a4": Phase="Pending", Reason="", readiness=false. Elapsed: 24.901371ms
May 14 01:58:18.803: INFO: Pod "pod-secrets-ebee1c8b-5039-4deb-8664-9437044808a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040793634s
May 14 01:58:20.890: INFO: Pod "pod-secrets-ebee1c8b-5039-4deb-8664-9437044808a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.128341638s
May 14 01:58:22.894: INFO: Pod "pod-secrets-ebee1c8b-5039-4deb-8664-9437044808a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.132613371s
STEP: Saw pod success
May 14 01:58:22.895: INFO: Pod "pod-secrets-ebee1c8b-5039-4deb-8664-9437044808a4" satisfied condition "success or failure"
May 14 01:58:22.898: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-secrets-ebee1c8b-5039-4deb-8664-9437044808a4 container secret-volume-test: <nil>
STEP: delete the pod
May 14 01:58:22.996: INFO: Waiting for pod pod-secrets-ebee1c8b-5039-4deb-8664-9437044808a4 to disappear
May 14 01:58:23.002: INFO: Pod pod-secrets-ebee1c8b-5039-4deb-8664-9437044808a4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 01:58:23.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8720" for this suite.
May 14 01:58:29.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:58:29.790: INFO: namespace secrets-8720 deletion completed in 6.77359862s

â€¢ [SLOW TEST:13.313 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 01:58:29.823: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1443
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1443, will wait for the garbage collector to delete the pods
May 14 01:58:34.170: INFO: Deleting Job.batch foo took: 16.128574ms
May 14 01:58:35.170: INFO: Terminating Job.batch foo pods took: 1.00052193s
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 01:59:12.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1443" for this suite.
May 14 01:59:18.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:59:19.514: INFO: namespace job-1443 deletion completed in 6.824601158s

â€¢ [SLOW TEST:49.692 seconds]
[sig-apps] Job
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 01:59:19.521: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8194
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
May 14 01:59:20.555: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
May 14 01:59:27.257: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 01:59:53.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8194" for this suite.
May 14 01:59:59.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:59:59.978: INFO: namespace crd-publish-openapi-8194 deletion completed in 6.495996681s

â€¢ [SLOW TEST:40.458 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 01:59:59.989: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7780
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 02:00:00.188: INFO: Waiting up to 5m0s for pod "busybox-user-65534-3b76f8a7-d42d-4d95-a5fb-b43b169d4688" in namespace "security-context-test-7780" to be "success or failure"
May 14 02:00:00.212: INFO: Pod "busybox-user-65534-3b76f8a7-d42d-4d95-a5fb-b43b169d4688": Phase="Pending", Reason="", readiness=false. Elapsed: 23.520118ms
May 14 02:00:02.310: INFO: Pod "busybox-user-65534-3b76f8a7-d42d-4d95-a5fb-b43b169d4688": Phase="Pending", Reason="", readiness=false. Elapsed: 2.121772628s
May 14 02:00:04.327: INFO: Pod "busybox-user-65534-3b76f8a7-d42d-4d95-a5fb-b43b169d4688": Phase="Pending", Reason="", readiness=false. Elapsed: 4.138928567s
May 14 02:00:06.344: INFO: Pod "busybox-user-65534-3b76f8a7-d42d-4d95-a5fb-b43b169d4688": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.155456584s
May 14 02:00:06.344: INFO: Pod "busybox-user-65534-3b76f8a7-d42d-4d95-a5fb-b43b169d4688" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:00:06.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7780" for this suite.
May 14 02:00:14.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:00:14.823: INFO: namespace security-context-test-7780 deletion completed in 8.469786931s

â€¢ [SLOW TEST:14.835 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:00:14.827: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6163
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May 14 02:00:15.081: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 14 02:00:15.098: INFO: Waiting for terminating namespaces to be deleted...
May 14 02:00:15.110: INFO: 
Logging pods the kubelet thinks is on node k8s-fcos-flwang-pyqjxt4oox23-node-0 before test
May 14 02:00:15.201: INFO: calico-node-d8klf from kube-system started at 2020-05-13 10:17:39 +0000 UTC (1 container statuses recorded)
May 14 02:00:15.201: INFO: 	Container calico-node ready: true, restart count 0
May 14 02:00:15.201: INFO: prometheus-operator-kube-state-metrics-54bd6c856f-v6wkd from kube-system started at 2020-05-13 10:20:16 +0000 UTC (1 container statuses recorded)
May 14 02:00:15.201: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 14 02:00:15.201: INFO: prometheus-prometheus-prometheus-0 from kube-system started at 2020-05-13 10:21:12 +0000 UTC (3 container statuses recorded)
May 14 02:00:15.201: INFO: 	Container prometheus ready: true, restart count 1
May 14 02:00:15.202: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May 14 02:00:15.202: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May 14 02:00:15.202: INFO: npd-w82fn from kube-system started at 2020-05-13 10:18:10 +0000 UTC (1 container statuses recorded)
May 14 02:00:15.202: INFO: 	Container node-problem-detector ready: true, restart count 0
May 14 02:00:15.202: INFO: metrics-server-649fdb57b9-jw8v2 from kube-system started at 2020-05-13 10:19:49 +0000 UTC (1 container statuses recorded)
May 14 02:00:15.202: INFO: 	Container metrics-server ready: true, restart count 0
May 14 02:00:15.202: INFO: sonobuoy-systemd-logs-daemon-set-2dde1b3f033a4765-q8j6w from sonobuoy started at 2020-05-14 01:43:10 +0000 UTC (2 container statuses recorded)
May 14 02:00:15.203: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 14 02:00:15.203: INFO: 	Container systemd-logs ready: true, restart count 0
May 14 02:00:15.203: INFO: kube-dns-autoscaler-78d89dc59b-tjwjl from kube-system started at 2020-05-13 10:18:10 +0000 UTC (1 container statuses recorded)
May 14 02:00:15.203: INFO: 	Container autoscaler ready: true, restart count 0
May 14 02:00:15.204: INFO: prometheus-operator-prometheus-node-exporter-dsbl2 from kube-system started at 2020-05-13 10:20:16 +0000 UTC (1 container statuses recorded)
May 14 02:00:15.204: INFO: 	Container node-exporter ready: true, restart count 0
May 14 02:00:15.204: INFO: prometheus-operator-grafana-597f5fdffc-2r2zm from kube-system started at 2020-05-13 10:20:16 +0000 UTC (2 container statuses recorded)
May 14 02:00:15.204: INFO: 	Container grafana ready: true, restart count 0
May 14 02:00:15.204: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
May 14 02:00:15.205: INFO: prometheus-operator-855b67c86c-m4rg8 from kube-system started at 2020-05-13 10:20:16 +0000 UTC (2 container statuses recorded)
May 14 02:00:15.205: INFO: 	Container prometheus ready: true, restart count 0
May 14 02:00:15.205: INFO: 	Container tls-proxy ready: true, restart count 0
May 14 02:00:15.205: INFO: alertmanager-prometheus-alertmanager-0 from kube-system started at 2020-05-13 10:21:01 +0000 UTC (2 container statuses recorded)
May 14 02:00:15.205: INFO: 	Container alertmanager ready: true, restart count 0
May 14 02:00:15.206: INFO: 	Container config-reloader ready: true, restart count 0
May 14 02:00:15.206: INFO: install-prometheus-operator-job-kz4gd from magnum-tiller started at 2020-05-13 10:18:10 +0000 UTC (1 container statuses recorded)
May 14 02:00:15.206: INFO: 	Container config-helm ready: false, restart count 0
May 14 02:00:15.206: INFO: install-prometheus-adapter-job-pmgx5 from magnum-tiller started at 2020-05-13 10:18:10 +0000 UTC (1 container statuses recorded)
May 14 02:00:15.206: INFO: 	Container config-helm ready: false, restart count 0
May 14 02:00:15.207: INFO: install-metrics-server-job-r22dn from magnum-tiller started at 2020-05-13 10:18:20 +0000 UTC (1 container statuses recorded)
May 14 02:00:15.207: INFO: 	Container config-helm ready: false, restart count 0
May 14 02:00:15.207: INFO: prometheus-adapter-5c56dfd965-bfskz from kube-system started at 2020-05-13 10:19:49 +0000 UTC (1 container statuses recorded)
May 14 02:00:15.207: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 14 02:00:15.207: INFO: 
Logging pods the kubelet thinks is on node k8s-fcos-flwang-pyqjxt4oox23-node-1 before test
May 14 02:00:15.283: INFO: sonobuoy from sonobuoy started at 2020-05-14 01:43:05 +0000 UTC (1 container statuses recorded)
May 14 02:00:15.284: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 14 02:00:15.284: INFO: sonobuoy-systemd-logs-daemon-set-2dde1b3f033a4765-4wkjn from sonobuoy started at 2020-05-14 01:43:10 +0000 UTC (2 container statuses recorded)
May 14 02:00:15.284: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 14 02:00:15.284: INFO: 	Container systemd-logs ready: true, restart count 0
May 14 02:00:15.284: INFO: calico-node-cjvph from kube-system started at 2020-05-13 20:00:24 +0000 UTC (1 container statuses recorded)
May 14 02:00:15.284: INFO: 	Container calico-node ready: true, restart count 0
May 14 02:00:15.284: INFO: npd-v8wfq from kube-system started at 2020-05-13 20:01:37 +0000 UTC (1 container statuses recorded)
May 14 02:00:15.284: INFO: 	Container node-problem-detector ready: true, restart count 0
May 14 02:00:15.284: INFO: prometheus-operator-prometheus-node-exporter-64t57 from kube-system started at 2020-05-13 20:00:24 +0000 UTC (1 container statuses recorded)
May 14 02:00:15.284: INFO: 	Container node-exporter ready: true, restart count 0
May 14 02:00:15.284: INFO: sonobuoy-e2e-job-cf5c047f756b45c4 from sonobuoy started at 2020-05-14 01:43:09 +0000 UTC (2 container statuses recorded)
May 14 02:00:15.284: INFO: 	Container e2e ready: true, restart count 0
May 14 02:00:15.284: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node k8s-fcos-flwang-pyqjxt4oox23-node-0
STEP: verifying the node has the label node k8s-fcos-flwang-pyqjxt4oox23-node-1
May 14 02:00:15.405: INFO: Pod alertmanager-prometheus-alertmanager-0 requesting resource cpu=100m on Node k8s-fcos-flwang-pyqjxt4oox23-node-0
May 14 02:00:15.406: INFO: Pod calico-node-cjvph requesting resource cpu=250m on Node k8s-fcos-flwang-pyqjxt4oox23-node-1
May 14 02:00:15.407: INFO: Pod calico-node-d8klf requesting resource cpu=250m on Node k8s-fcos-flwang-pyqjxt4oox23-node-0
May 14 02:00:15.407: INFO: Pod kube-dns-autoscaler-78d89dc59b-tjwjl requesting resource cpu=20m on Node k8s-fcos-flwang-pyqjxt4oox23-node-0
May 14 02:00:15.408: INFO: Pod metrics-server-649fdb57b9-jw8v2 requesting resource cpu=0m on Node k8s-fcos-flwang-pyqjxt4oox23-node-0
May 14 02:00:15.408: INFO: Pod npd-v8wfq requesting resource cpu=20m on Node k8s-fcos-flwang-pyqjxt4oox23-node-1
May 14 02:00:15.409: INFO: Pod npd-w82fn requesting resource cpu=20m on Node k8s-fcos-flwang-pyqjxt4oox23-node-0
May 14 02:00:15.409: INFO: Pod prometheus-adapter-5c56dfd965-bfskz requesting resource cpu=150m on Node k8s-fcos-flwang-pyqjxt4oox23-node-0
May 14 02:00:15.409: INFO: Pod prometheus-operator-855b67c86c-m4rg8 requesting resource cpu=0m on Node k8s-fcos-flwang-pyqjxt4oox23-node-0
May 14 02:00:15.410: INFO: Pod prometheus-operator-grafana-597f5fdffc-2r2zm requesting resource cpu=100m on Node k8s-fcos-flwang-pyqjxt4oox23-node-0
May 14 02:00:15.410: INFO: Pod prometheus-operator-kube-state-metrics-54bd6c856f-v6wkd requesting resource cpu=50m on Node k8s-fcos-flwang-pyqjxt4oox23-node-0
May 14 02:00:15.411: INFO: Pod prometheus-operator-prometheus-node-exporter-64t57 requesting resource cpu=20m on Node k8s-fcos-flwang-pyqjxt4oox23-node-1
May 14 02:00:15.411: INFO: Pod prometheus-operator-prometheus-node-exporter-dsbl2 requesting resource cpu=20m on Node k8s-fcos-flwang-pyqjxt4oox23-node-0
May 14 02:00:15.434: INFO: Pod prometheus-prometheus-prometheus-0 requesting resource cpu=342m on Node k8s-fcos-flwang-pyqjxt4oox23-node-0
May 14 02:00:15.434: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-fcos-flwang-pyqjxt4oox23-node-1
May 14 02:00:15.434: INFO: Pod sonobuoy-e2e-job-cf5c047f756b45c4 requesting resource cpu=0m on Node k8s-fcos-flwang-pyqjxt4oox23-node-1
May 14 02:00:15.434: INFO: Pod sonobuoy-systemd-logs-daemon-set-2dde1b3f033a4765-4wkjn requesting resource cpu=0m on Node k8s-fcos-flwang-pyqjxt4oox23-node-1
May 14 02:00:15.434: INFO: Pod sonobuoy-systemd-logs-daemon-set-2dde1b3f033a4765-q8j6w requesting resource cpu=0m on Node k8s-fcos-flwang-pyqjxt4oox23-node-0
STEP: Starting Pods to consume most of the cluster CPU.
May 14 02:00:15.434: INFO: Creating a pod which consumes cpu=663m on Node k8s-fcos-flwang-pyqjxt4oox23-node-0
May 14 02:00:15.468: INFO: Creating a pod which consumes cpu=1197m on Node k8s-fcos-flwang-pyqjxt4oox23-node-1
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-73e44200-30bf-4b8e-9edf-2f28eac51a71.160ec289c3d4305b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6163/filler-pod-73e44200-30bf-4b8e-9edf-2f28eac51a71 to k8s-fcos-flwang-pyqjxt4oox23-node-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-73e44200-30bf-4b8e-9edf-2f28eac51a71.160ec28b1f65f783], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-73e44200-30bf-4b8e-9edf-2f28eac51a71.160ec28b2c9c1c06], Reason = [Created], Message = [Created container filler-pod-73e44200-30bf-4b8e-9edf-2f28eac51a71]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-73e44200-30bf-4b8e-9edf-2f28eac51a71.160ec28b4582fdc6], Reason = [Started], Message = [Started container filler-pod-73e44200-30bf-4b8e-9edf-2f28eac51a71]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a8672a74-1ef4-4e1e-a539-63726cfa5830.160ec289c5e52a1b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6163/filler-pod-a8672a74-1ef4-4e1e-a539-63726cfa5830 to k8s-fcos-flwang-pyqjxt4oox23-node-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a8672a74-1ef4-4e1e-a539-63726cfa5830.160ec28ac5928b29], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a8672a74-1ef4-4e1e-a539-63726cfa5830.160ec28ac9f66c3e], Reason = [Created], Message = [Created container filler-pod-a8672a74-1ef4-4e1e-a539-63726cfa5830]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a8672a74-1ef4-4e1e-a539-63726cfa5830.160ec28adceda0dd], Reason = [Started], Message = [Started container filler-pod-a8672a74-1ef4-4e1e-a539-63726cfa5830]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.160ec28ba582dae0], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: removing the label node off the node k8s-fcos-flwang-pyqjxt4oox23-node-0
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-fcos-flwang-pyqjxt4oox23-node-1
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:00:24.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6163" for this suite.
May 14 02:00:41.540: INFO: Error while waiting for namespace to be terminated: etcdserver: request timed out
May 14 02:00:42.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:00:44.007: INFO: namespace sched-pred-6163 deletion completed in 19.298146263s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:29.183 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:00:44.017: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1456
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-1e9537ca-6706-4246-9adb-9a2e542616cf in namespace container-probe-1456
May 14 02:00:52.288: INFO: Started pod busybox-1e9537ca-6706-4246-9adb-9a2e542616cf in namespace container-probe-1456
STEP: checking the pod's current state and verifying that restartCount is present
May 14 02:00:52.292: INFO: Initial restart count of pod busybox-1e9537ca-6706-4246-9adb-9a2e542616cf is 0
May 14 02:01:42.545: INFO: Restart count of pod container-probe-1456/busybox-1e9537ca-6706-4246-9adb-9a2e542616cf is now 1 (50.253037688s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:01:42.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1456" for this suite.
May 14 02:01:48.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:01:49.374: INFO: namespace container-probe-1456 deletion completed in 6.743881912s

â€¢ [SLOW TEST:65.358 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:01:49.376: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7162
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
May 14 02:01:58.443: INFO: Successfully updated pod "labelsupdate638f2a3b-07ca-472d-bb09-7d1d54f998df"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:02:00.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7162" for this suite.
May 14 02:03:26.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:03:27.201: INFO: namespace downward-api-7162 deletion completed in 1m26.511054635s

â€¢ [SLOW TEST:97.826 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:03:27.203: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2091
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-70b68e30-663b-4a94-bb0f-131e34561c8d
STEP: Creating a pod to test consume configMaps
May 14 02:03:27.483: INFO: Waiting up to 5m0s for pod "pod-configmaps-94a7d484-413f-41ce-881a-fa15c68152a2" in namespace "configmap-2091" to be "success or failure"
May 14 02:03:27.523: INFO: Pod "pod-configmaps-94a7d484-413f-41ce-881a-fa15c68152a2": Phase="Pending", Reason="", readiness=false. Elapsed: 39.699368ms
May 14 02:03:29.535: INFO: Pod "pod-configmaps-94a7d484-413f-41ce-881a-fa15c68152a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051707796s
May 14 02:03:31.543: INFO: Pod "pod-configmaps-94a7d484-413f-41ce-881a-fa15c68152a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059614955s
May 14 02:03:33.553: INFO: Pod "pod-configmaps-94a7d484-413f-41ce-881a-fa15c68152a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.070029175s
STEP: Saw pod success
May 14 02:03:33.554: INFO: Pod "pod-configmaps-94a7d484-413f-41ce-881a-fa15c68152a2" satisfied condition "success or failure"
May 14 02:03:33.559: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-configmaps-94a7d484-413f-41ce-881a-fa15c68152a2 container configmap-volume-test: <nil>
STEP: delete the pod
May 14 02:03:33.671: INFO: Waiting for pod pod-configmaps-94a7d484-413f-41ce-881a-fa15c68152a2 to disappear
May 14 02:03:33.680: INFO: Pod pod-configmaps-94a7d484-413f-41ce-881a-fa15c68152a2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:03:33.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2091" for this suite.
May 14 02:03:42.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:03:42.622: INFO: namespace configmap-2091 deletion completed in 8.919601717s

â€¢ [SLOW TEST:15.419 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:03:42.623: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8928
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 14 02:03:44.702: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34839ea1-9d44-491c-8eb3-b2d3abf67fac" in namespace "downward-api-8928" to be "success or failure"
May 14 02:03:44.751: INFO: Pod "downwardapi-volume-34839ea1-9d44-491c-8eb3-b2d3abf67fac": Phase="Pending", Reason="", readiness=false. Elapsed: 48.885196ms
May 14 02:03:46.788: INFO: Pod "downwardapi-volume-34839ea1-9d44-491c-8eb3-b2d3abf67fac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.086043836s
May 14 02:03:49.049: INFO: Pod "downwardapi-volume-34839ea1-9d44-491c-8eb3-b2d3abf67fac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.346865815s
May 14 02:03:51.059: INFO: Pod "downwardapi-volume-34839ea1-9d44-491c-8eb3-b2d3abf67fac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.356746758s
STEP: Saw pod success
May 14 02:03:51.059: INFO: Pod "downwardapi-volume-34839ea1-9d44-491c-8eb3-b2d3abf67fac" satisfied condition "success or failure"
May 14 02:03:51.075: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod downwardapi-volume-34839ea1-9d44-491c-8eb3-b2d3abf67fac container client-container: <nil>
STEP: delete the pod
May 14 02:03:52.256: INFO: Waiting for pod downwardapi-volume-34839ea1-9d44-491c-8eb3-b2d3abf67fac to disappear
May 14 02:03:52.401: INFO: Pod downwardapi-volume-34839ea1-9d44-491c-8eb3-b2d3abf67fac no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:03:52.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8928" for this suite.
May 14 02:03:58.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:03:59.260: INFO: namespace downward-api-8928 deletion completed in 6.846042984s

â€¢ [SLOW TEST:16.637 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:03:59.262: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-272
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-dea47795-732c-43de-b988-a1b0449959a9
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:03:59.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-272" for this suite.
May 14 02:04:06.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:04:06.513: INFO: namespace secrets-272 deletion completed in 7.013925175s

â€¢ [SLOW TEST:7.252 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:04:06.521: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2438
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 14 02:04:06.880: INFO: Waiting up to 5m0s for pod "downwardapi-volume-345bacfd-8845-4333-9bfc-56c992249a11" in namespace "projected-2438" to be "success or failure"
May 14 02:04:06.891: INFO: Pod "downwardapi-volume-345bacfd-8845-4333-9bfc-56c992249a11": Phase="Pending", Reason="", readiness=false. Elapsed: 10.148963ms
May 14 02:04:08.902: INFO: Pod "downwardapi-volume-345bacfd-8845-4333-9bfc-56c992249a11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021109369s
May 14 02:04:10.910: INFO: Pod "downwardapi-volume-345bacfd-8845-4333-9bfc-56c992249a11": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029319913s
May 14 02:04:12.917: INFO: Pod "downwardapi-volume-345bacfd-8845-4333-9bfc-56c992249a11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03648782s
STEP: Saw pod success
May 14 02:04:12.922: INFO: Pod "downwardapi-volume-345bacfd-8845-4333-9bfc-56c992249a11" satisfied condition "success or failure"
May 14 02:04:12.926: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod downwardapi-volume-345bacfd-8845-4333-9bfc-56c992249a11 container client-container: <nil>
STEP: delete the pod
May 14 02:04:13.361: INFO: Waiting for pod downwardapi-volume-345bacfd-8845-4333-9bfc-56c992249a11 to disappear
May 14 02:04:13.371: INFO: Pod downwardapi-volume-345bacfd-8845-4333-9bfc-56c992249a11 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:04:13.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2438" for this suite.
May 14 02:04:19.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:04:19.958: INFO: namespace projected-2438 deletion completed in 6.523834525s

â€¢ [SLOW TEST:13.438 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:04:19.961: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-213
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-213.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-213.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-213.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-213.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 14 02:04:28.249: INFO: DNS probes using dns-test-715ade88-6cb7-49ec-87a4-a159518b4289 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-213.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-213.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-213.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-213.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 14 02:04:36.561: INFO: DNS probes using dns-test-07e3b3c2-6158-463d-8d8f-9f16f2424e4e succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-213.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-213.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-213.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-213.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 14 02:04:43.835: INFO: DNS probes using dns-test-b2b74936-03b5-40a3-a80e-33e745d93057 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:04:43.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-213" for this suite.
May 14 02:04:52.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:04:52.619: INFO: namespace dns-213 deletion completed in 8.596463101s

â€¢ [SLOW TEST:32.658 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:04:52.626: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5372
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 14 02:04:55.034: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 14 02:04:57.214: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018695, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018695, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018695, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018695, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 14 02:05:00.334: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 02:05:00.347: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8920-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:05:01.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5372" for this suite.
May 14 02:05:08.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:05:09.406: INFO: namespace webhook-5372 deletion completed in 6.663912055s
STEP: Destroying namespace "webhook-5372-markers" for this suite.
May 14 02:05:17.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:05:17.786: INFO: namespace webhook-5372-markers deletion completed in 8.379762019s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:25.197 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:05:17.825: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6352
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 14 02:05:19.848: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018719, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018719, loc:(*time.Location)(0x78a2900)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-86d95b659d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018719, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018719, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
May 14 02:05:21.860: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018719, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018719, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018719, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018719, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 02:05:23.891: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018719, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018719, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018719, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018719, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 14 02:05:26.989: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:05:27.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6352" for this suite.
May 14 02:05:41.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:05:42.122: INFO: namespace webhook-6352 deletion completed in 14.957020775s
STEP: Destroying namespace "webhook-6352-markers" for this suite.
May 14 02:05:48.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:05:48.797: INFO: namespace webhook-6352-markers deletion completed in 6.669337531s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:31.035 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:05:48.869: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7968
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7968.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7968.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7968.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7968.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7968.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7968.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7968.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7968.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7968.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7968.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7968.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7968.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7968.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 88.56.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.56.88_udp@PTR;check="$$(dig +tcp +noall +answer +search 88.56.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.56.88_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7968.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7968.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7968.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7968.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7968.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7968.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7968.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7968.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7968.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7968.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7968.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7968.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7968.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 88.56.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.56.88_udp@PTR;check="$$(dig +tcp +noall +answer +search 88.56.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.56.88_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 14 02:05:58.868: INFO: Unable to read wheezy_udp@dns-test-service.dns-7968.svc.cluster.local from pod dns-7968/dns-test-3919a2af-82d7-428a-8862-51d35892c6eb: the server could not find the requested resource (get pods dns-test-3919a2af-82d7-428a-8862-51d35892c6eb)
May 14 02:05:58.881: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7968.svc.cluster.local from pod dns-7968/dns-test-3919a2af-82d7-428a-8862-51d35892c6eb: the server could not find the requested resource (get pods dns-test-3919a2af-82d7-428a-8862-51d35892c6eb)
May 14 02:05:58.940: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7968.svc.cluster.local from pod dns-7968/dns-test-3919a2af-82d7-428a-8862-51d35892c6eb: the server could not find the requested resource (get pods dns-test-3919a2af-82d7-428a-8862-51d35892c6eb)
May 14 02:05:59.278: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7968.svc.cluster.local from pod dns-7968/dns-test-3919a2af-82d7-428a-8862-51d35892c6eb: the server could not find the requested resource (get pods dns-test-3919a2af-82d7-428a-8862-51d35892c6eb)
May 14 02:05:59.338: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7968/dns-test-3919a2af-82d7-428a-8862-51d35892c6eb: the server could not find the requested resource (get pods dns-test-3919a2af-82d7-428a-8862-51d35892c6eb)
May 14 02:05:59.344: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7968/dns-test-3919a2af-82d7-428a-8862-51d35892c6eb: the server could not find the requested resource (get pods dns-test-3919a2af-82d7-428a-8862-51d35892c6eb)
May 14 02:05:59.382: INFO: Unable to read jessie_udp@dns-test-service.dns-7968.svc.cluster.local from pod dns-7968/dns-test-3919a2af-82d7-428a-8862-51d35892c6eb: the server could not find the requested resource (get pods dns-test-3919a2af-82d7-428a-8862-51d35892c6eb)
May 14 02:05:59.392: INFO: Unable to read jessie_tcp@dns-test-service.dns-7968.svc.cluster.local from pod dns-7968/dns-test-3919a2af-82d7-428a-8862-51d35892c6eb: the server could not find the requested resource (get pods dns-test-3919a2af-82d7-428a-8862-51d35892c6eb)
May 14 02:05:59.399: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7968.svc.cluster.local from pod dns-7968/dns-test-3919a2af-82d7-428a-8862-51d35892c6eb: the server could not find the requested resource (get pods dns-test-3919a2af-82d7-428a-8862-51d35892c6eb)
May 14 02:05:59.423: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7968.svc.cluster.local from pod dns-7968/dns-test-3919a2af-82d7-428a-8862-51d35892c6eb: the server could not find the requested resource (get pods dns-test-3919a2af-82d7-428a-8862-51d35892c6eb)
May 14 02:05:59.441: INFO: Unable to read jessie_udp@PodARecord from pod dns-7968/dns-test-3919a2af-82d7-428a-8862-51d35892c6eb: the server could not find the requested resource (get pods dns-test-3919a2af-82d7-428a-8862-51d35892c6eb)
May 14 02:05:59.450: INFO: Unable to read jessie_tcp@PodARecord from pod dns-7968/dns-test-3919a2af-82d7-428a-8862-51d35892c6eb: the server could not find the requested resource (get pods dns-test-3919a2af-82d7-428a-8862-51d35892c6eb)
May 14 02:05:59.466: INFO: Lookups using dns-7968/dns-test-3919a2af-82d7-428a-8862-51d35892c6eb failed for: [wheezy_udp@dns-test-service.dns-7968.svc.cluster.local wheezy_tcp@dns-test-service.dns-7968.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7968.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7968.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-7968.svc.cluster.local jessie_tcp@dns-test-service.dns-7968.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7968.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7968.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

May 14 02:06:04.662: INFO: DNS probes using dns-7968/dns-test-3919a2af-82d7-428a-8862-51d35892c6eb succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:06:04.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7968" for this suite.
May 14 02:06:14.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:06:15.302: INFO: namespace dns-7968 deletion completed in 10.433575342s

â€¢ [SLOW TEST:26.441 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:06:15.304: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3704
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
May 14 02:06:15.542: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-817715921 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:06:15.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3704" for this suite.
May 14 02:06:21.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:06:22.106: INFO: namespace kubectl-3704 deletion completed in 6.326183007s

â€¢ [SLOW TEST:6.802 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:06:22.107: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5484
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:06:39.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5484" for this suite.
May 14 02:06:46.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:06:46.627: INFO: namespace resourcequota-5484 deletion completed in 6.939518487s

â€¢ [SLOW TEST:24.520 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:06:46.629: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5451
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 14 02:06:47.389: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5451 /api/v1/namespaces/watch-5451/configmaps/e2e-watch-test-watch-closed ce106e52-a217-4cdc-98c6-64200fc86196 183598 0 2020-05-14 02:06:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 14 02:06:47.392: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5451 /api/v1/namespaces/watch-5451/configmaps/e2e-watch-test-watch-closed ce106e52-a217-4cdc-98c6-64200fc86196 183599 0 2020-05-14 02:06:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 14 02:06:47.436: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5451 /api/v1/namespaces/watch-5451/configmaps/e2e-watch-test-watch-closed ce106e52-a217-4cdc-98c6-64200fc86196 183600 0 2020-05-14 02:06:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 14 02:06:47.436: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5451 /api/v1/namespaces/watch-5451/configmaps/e2e-watch-test-watch-closed ce106e52-a217-4cdc-98c6-64200fc86196 183601 0 2020-05-14 02:06:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:06:47.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5451" for this suite.
May 14 02:06:53.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:06:53.946: INFO: namespace watch-5451 deletion completed in 6.488472035s

â€¢ [SLOW TEST:7.322 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:06:53.970: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1845
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-034af1b7-df3c-4770-b81e-b73d6aaba0b9
STEP: Creating a pod to test consume secrets
May 14 02:06:54.226: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4d29c040-aade-4d78-8300-205af977061f" in namespace "projected-1845" to be "success or failure"
May 14 02:06:54.406: INFO: Pod "pod-projected-secrets-4d29c040-aade-4d78-8300-205af977061f": Phase="Pending", Reason="", readiness=false. Elapsed: 179.219468ms
May 14 02:06:56.415: INFO: Pod "pod-projected-secrets-4d29c040-aade-4d78-8300-205af977061f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.188999808s
May 14 02:06:58.435: INFO: Pod "pod-projected-secrets-4d29c040-aade-4d78-8300-205af977061f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.208566921s
STEP: Saw pod success
May 14 02:06:58.436: INFO: Pod "pod-projected-secrets-4d29c040-aade-4d78-8300-205af977061f" satisfied condition "success or failure"
May 14 02:06:58.443: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-projected-secrets-4d29c040-aade-4d78-8300-205af977061f container projected-secret-volume-test: <nil>
STEP: delete the pod
May 14 02:06:58.586: INFO: Waiting for pod pod-projected-secrets-4d29c040-aade-4d78-8300-205af977061f to disappear
May 14 02:06:58.593: INFO: Pod pod-projected-secrets-4d29c040-aade-4d78-8300-205af977061f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:06:58.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1845" for this suite.
May 14 02:07:04.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:07:05.123: INFO: namespace projected-1845 deletion completed in 6.522613279s

â€¢ [SLOW TEST:11.153 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:07:05.123: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3764
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 14 02:07:11.202: INFO: Successfully updated pod "pod-update-activedeadlineseconds-d7ef239b-fad9-4301-a334-19e159f70ab8"
May 14 02:07:11.202: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d7ef239b-fad9-4301-a334-19e159f70ab8" in namespace "pods-3764" to be "terminated due to deadline exceeded"
May 14 02:07:11.234: INFO: Pod "pod-update-activedeadlineseconds-d7ef239b-fad9-4301-a334-19e159f70ab8": Phase="Running", Reason="", readiness=true. Elapsed: 31.839629ms
May 14 02:07:13.240: INFO: Pod "pod-update-activedeadlineseconds-d7ef239b-fad9-4301-a334-19e159f70ab8": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.037443585s
May 14 02:07:13.240: INFO: Pod "pod-update-activedeadlineseconds-d7ef239b-fad9-4301-a334-19e159f70ab8" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:07:13.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3764" for this suite.
May 14 02:07:19.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:07:19.823: INFO: namespace pods-3764 deletion completed in 6.572313822s

â€¢ [SLOW TEST:14.700 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:07:19.830: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8934
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May 14 02:07:20.047: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 14 02:07:20.096: INFO: Waiting for terminating namespaces to be deleted...
May 14 02:07:20.104: INFO: 
Logging pods the kubelet thinks is on node k8s-fcos-flwang-pyqjxt4oox23-node-0 before test
May 14 02:07:20.240: INFO: calico-node-d8klf from kube-system started at 2020-05-13 10:17:39 +0000 UTC (1 container statuses recorded)
May 14 02:07:20.240: INFO: 	Container calico-node ready: true, restart count 0
May 14 02:07:20.240: INFO: prometheus-operator-kube-state-metrics-54bd6c856f-v6wkd from kube-system started at 2020-05-13 10:20:16 +0000 UTC (1 container statuses recorded)
May 14 02:07:20.240: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 14 02:07:20.240: INFO: prometheus-prometheus-prometheus-0 from kube-system started at 2020-05-13 10:21:12 +0000 UTC (3 container statuses recorded)
May 14 02:07:20.240: INFO: 	Container prometheus ready: true, restart count 1
May 14 02:07:20.240: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May 14 02:07:20.240: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May 14 02:07:20.240: INFO: npd-w82fn from kube-system started at 2020-05-13 10:18:10 +0000 UTC (1 container statuses recorded)
May 14 02:07:20.241: INFO: 	Container node-problem-detector ready: true, restart count 0
May 14 02:07:20.241: INFO: metrics-server-649fdb57b9-jw8v2 from kube-system started at 2020-05-13 10:19:49 +0000 UTC (1 container statuses recorded)
May 14 02:07:20.241: INFO: 	Container metrics-server ready: true, restart count 0
May 14 02:07:20.241: INFO: kube-dns-autoscaler-78d89dc59b-tjwjl from kube-system started at 2020-05-13 10:18:10 +0000 UTC (1 container statuses recorded)
May 14 02:07:20.241: INFO: 	Container autoscaler ready: true, restart count 0
May 14 02:07:20.241: INFO: prometheus-operator-prometheus-node-exporter-dsbl2 from kube-system started at 2020-05-13 10:20:16 +0000 UTC (1 container statuses recorded)
May 14 02:07:20.241: INFO: 	Container node-exporter ready: true, restart count 0
May 14 02:07:20.241: INFO: prometheus-operator-grafana-597f5fdffc-2r2zm from kube-system started at 2020-05-13 10:20:16 +0000 UTC (2 container statuses recorded)
May 14 02:07:20.241: INFO: 	Container grafana ready: true, restart count 0
May 14 02:07:20.241: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
May 14 02:07:20.241: INFO: prometheus-operator-855b67c86c-m4rg8 from kube-system started at 2020-05-13 10:20:16 +0000 UTC (2 container statuses recorded)
May 14 02:07:20.241: INFO: 	Container prometheus ready: true, restart count 0
May 14 02:07:20.241: INFO: 	Container tls-proxy ready: true, restart count 0
May 14 02:07:20.241: INFO: sonobuoy-systemd-logs-daemon-set-2dde1b3f033a4765-q8j6w from sonobuoy started at 2020-05-14 01:43:10 +0000 UTC (2 container statuses recorded)
May 14 02:07:20.241: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 14 02:07:20.241: INFO: 	Container systemd-logs ready: true, restart count 0
May 14 02:07:20.241: INFO: install-prometheus-operator-job-kz4gd from magnum-tiller started at 2020-05-13 10:18:10 +0000 UTC (1 container statuses recorded)
May 14 02:07:20.241: INFO: 	Container config-helm ready: false, restart count 0
May 14 02:07:20.241: INFO: install-prometheus-adapter-job-pmgx5 from magnum-tiller started at 2020-05-13 10:18:10 +0000 UTC (1 container statuses recorded)
May 14 02:07:20.242: INFO: 	Container config-helm ready: false, restart count 0
May 14 02:07:20.242: INFO: install-metrics-server-job-r22dn from magnum-tiller started at 2020-05-13 10:18:20 +0000 UTC (1 container statuses recorded)
May 14 02:07:20.242: INFO: 	Container config-helm ready: false, restart count 0
May 14 02:07:20.242: INFO: prometheus-adapter-5c56dfd965-bfskz from kube-system started at 2020-05-13 10:19:49 +0000 UTC (1 container statuses recorded)
May 14 02:07:20.242: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 14 02:07:20.242: INFO: alertmanager-prometheus-alertmanager-0 from kube-system started at 2020-05-13 10:21:01 +0000 UTC (2 container statuses recorded)
May 14 02:07:20.242: INFO: 	Container alertmanager ready: true, restart count 0
May 14 02:07:20.242: INFO: 	Container config-reloader ready: true, restart count 0
May 14 02:07:20.242: INFO: 
Logging pods the kubelet thinks is on node k8s-fcos-flwang-pyqjxt4oox23-node-1 before test
May 14 02:07:20.260: INFO: sonobuoy-systemd-logs-daemon-set-2dde1b3f033a4765-4wkjn from sonobuoy started at 2020-05-14 01:43:10 +0000 UTC (2 container statuses recorded)
May 14 02:07:20.260: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 14 02:07:20.260: INFO: 	Container systemd-logs ready: true, restart count 0
May 14 02:07:20.260: INFO: sonobuoy from sonobuoy started at 2020-05-14 01:43:05 +0000 UTC (1 container statuses recorded)
May 14 02:07:20.260: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 14 02:07:20.260: INFO: npd-v8wfq from kube-system started at 2020-05-13 20:01:37 +0000 UTC (1 container statuses recorded)
May 14 02:07:20.260: INFO: 	Container node-problem-detector ready: true, restart count 0
May 14 02:07:20.261: INFO: calico-node-cjvph from kube-system started at 2020-05-13 20:00:24 +0000 UTC (1 container statuses recorded)
May 14 02:07:20.261: INFO: 	Container calico-node ready: true, restart count 0
May 14 02:07:20.261: INFO: sonobuoy-e2e-job-cf5c047f756b45c4 from sonobuoy started at 2020-05-14 01:43:09 +0000 UTC (2 container statuses recorded)
May 14 02:07:20.261: INFO: 	Container e2e ready: true, restart count 0
May 14 02:07:20.262: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 14 02:07:20.262: INFO: prometheus-operator-prometheus-node-exporter-64t57 from kube-system started at 2020-05-13 20:00:24 +0000 UTC (1 container statuses recorded)
May 14 02:07:20.262: INFO: 	Container node-exporter ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c185dc8b-faba-45d2-9619-c0d5ed58c854 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-c185dc8b-faba-45d2-9619-c0d5ed58c854 off the node k8s-fcos-flwang-pyqjxt4oox23-node-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c185dc8b-faba-45d2-9619-c0d5ed58c854
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:07:45.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8934" for this suite.
May 14 02:08:23.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:08:23.585: INFO: namespace sched-pred-8934 deletion completed in 38.417670029s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:63.756 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:08:23.589: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6387
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-hwsf
STEP: Creating a pod to test atomic-volume-subpath
May 14 02:08:23.879: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-hwsf" in namespace "subpath-6387" to be "success or failure"
May 14 02:08:23.907: INFO: Pod "pod-subpath-test-secret-hwsf": Phase="Pending", Reason="", readiness=false. Elapsed: 27.277009ms
May 14 02:08:26.733: INFO: Pod "pod-subpath-test-secret-hwsf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.853186066s
May 14 02:08:29.094: INFO: Pod "pod-subpath-test-secret-hwsf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.214156029s
May 14 02:08:31.379: INFO: Pod "pod-subpath-test-secret-hwsf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.498918241s
May 14 02:08:33.844: INFO: Pod "pod-subpath-test-secret-hwsf": Phase="Running", Reason="", readiness=true. Elapsed: 9.964218251s
May 14 02:08:37.850: INFO: Pod "pod-subpath-test-secret-hwsf": Phase="Running", Reason="", readiness=true. Elapsed: 13.970791312s
May 14 02:08:39.861: INFO: Pod "pod-subpath-test-secret-hwsf": Phase="Running", Reason="", readiness=true. Elapsed: 15.981162584s
May 14 02:08:41.869: INFO: Pod "pod-subpath-test-secret-hwsf": Phase="Running", Reason="", readiness=true. Elapsed: 17.989084027s
May 14 02:08:44.187: INFO: Pod "pod-subpath-test-secret-hwsf": Phase="Running", Reason="", readiness=true. Elapsed: 20.307742702s
May 14 02:08:46.364: INFO: Pod "pod-subpath-test-secret-hwsf": Phase="Running", Reason="", readiness=true. Elapsed: 22.483887309s
May 14 02:08:48.372: INFO: Pod "pod-subpath-test-secret-hwsf": Phase="Running", Reason="", readiness=true. Elapsed: 24.492669809s
May 14 02:08:52.215: INFO: Pod "pod-subpath-test-secret-hwsf": Phase="Running", Reason="", readiness=true. Elapsed: 28.335602529s
May 14 02:08:54.227: INFO: Pod "pod-subpath-test-secret-hwsf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.347842972s
STEP: Saw pod success
May 14 02:08:54.228: INFO: Pod "pod-subpath-test-secret-hwsf" satisfied condition "success or failure"
May 14 02:08:54.233: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-subpath-test-secret-hwsf container test-container-subpath-secret-hwsf: <nil>
STEP: delete the pod
May 14 02:08:54.337: INFO: Waiting for pod pod-subpath-test-secret-hwsf to disappear
May 14 02:08:54.356: INFO: Pod pod-subpath-test-secret-hwsf no longer exists
STEP: Deleting pod pod-subpath-test-secret-hwsf
May 14 02:08:54.356: INFO: Deleting pod "pod-subpath-test-secret-hwsf" in namespace "subpath-6387"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:08:54.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6387" for this suite.
May 14 02:09:00.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:09:00.760: INFO: namespace subpath-6387 deletion completed in 6.385710145s

â€¢ [SLOW TEST:37.171 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:09:00.763: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7807
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 14 02:09:01.747: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 14 02:09:03.780: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018941, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018941, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018941, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018941, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 02:09:06.192: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018941, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018941, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018941, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018941, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 14 02:09:08.878: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:09:09.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7807" for this suite.
May 14 02:09:15.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:09:15.907: INFO: namespace webhook-7807 deletion completed in 6.455481174s
STEP: Destroying namespace "webhook-7807-markers" for this suite.
May 14 02:09:22.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:09:22.678: INFO: namespace webhook-7807-markers deletion completed in 6.769456966s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:21.943 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:09:22.709: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-7374
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:09:27.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7374" for this suite.
May 14 02:09:33.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:09:33.620: INFO: namespace emptydir-wrapper-7374 deletion completed in 6.41984182s

â€¢ [SLOW TEST:10.911 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:09:33.620: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8549
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 14 02:09:35.859: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 14 02:09:42.474: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018975, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018975, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018976, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018975, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 02:09:44.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018975, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018975, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018976, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725018975, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 14 02:09:47.535: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:09:47.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8549" for this suite.
May 14 02:09:55.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:09:56.145: INFO: namespace webhook-8549 deletion completed in 8.457557292s
STEP: Destroying namespace "webhook-8549-markers" for this suite.
May 14 02:10:02.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:10:02.563: INFO: namespace webhook-8549-markers deletion completed in 6.416976708s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:28.995 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:10:02.617: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6223
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:10:16.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6223" for this suite.
May 14 02:10:29.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:10:29.745: INFO: namespace resourcequota-6223 deletion completed in 12.511153263s

â€¢ [SLOW TEST:27.129 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:10:29.750: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2999
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 14 02:10:29.974: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1ada4daf-ca59-4f7d-8c4a-0e6bd32fb160" in namespace "downward-api-2999" to be "success or failure"
May 14 02:10:29.982: INFO: Pod "downwardapi-volume-1ada4daf-ca59-4f7d-8c4a-0e6bd32fb160": Phase="Pending", Reason="", readiness=false. Elapsed: 8.05626ms
May 14 02:10:32.031: INFO: Pod "downwardapi-volume-1ada4daf-ca59-4f7d-8c4a-0e6bd32fb160": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057508154s
May 14 02:10:34.157: INFO: Pod "downwardapi-volume-1ada4daf-ca59-4f7d-8c4a-0e6bd32fb160": Phase="Pending", Reason="", readiness=false. Elapsed: 4.183600187s
May 14 02:10:36.363: INFO: Pod "downwardapi-volume-1ada4daf-ca59-4f7d-8c4a-0e6bd32fb160": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.389252125s
STEP: Saw pod success
May 14 02:10:36.363: INFO: Pod "downwardapi-volume-1ada4daf-ca59-4f7d-8c4a-0e6bd32fb160" satisfied condition "success or failure"
May 14 02:10:36.373: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod downwardapi-volume-1ada4daf-ca59-4f7d-8c4a-0e6bd32fb160 container client-container: <nil>
STEP: delete the pod
May 14 02:10:37.449: INFO: Waiting for pod downwardapi-volume-1ada4daf-ca59-4f7d-8c4a-0e6bd32fb160 to disappear
May 14 02:10:37.634: INFO: Pod downwardapi-volume-1ada4daf-ca59-4f7d-8c4a-0e6bd32fb160 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:10:37.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2999" for this suite.
May 14 02:10:47.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:10:48.904: INFO: namespace downward-api-2999 deletion completed in 11.25353587s

â€¢ [SLOW TEST:19.155 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:10:48.910: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7189
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 14 02:10:50.117: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:10:52.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7189" for this suite.
May 14 02:11:02.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:11:03.126: INFO: namespace replication-controller-7189 deletion completed in 10.496161436s

â€¢ [SLOW TEST:14.216 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:11:03.128: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4869
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 02:11:31.560: INFO: Container started at 2020-05-14 02:11:06 +0000 UTC, pod became ready at 2020-05-14 02:11:30 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:11:31.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4869" for this suite.
May 14 02:12:03.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:12:04.390: INFO: namespace container-probe-4869 deletion completed in 32.811770418s

â€¢ [SLOW TEST:61.263 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:12:04.406: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7901
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 14 02:12:11.608: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:12:11.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7901" for this suite.
May 14 02:12:17.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:12:17.991: INFO: namespace container-runtime-7901 deletion completed in 6.354463791s

â€¢ [SLOW TEST:13.585 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:12:17.992: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1425
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-1425
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-1425
May 14 02:12:18.865: INFO: Found 0 stateful pods, waiting for 1
May 14 02:12:28.880: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May 14 02:12:28.990: INFO: Deleting all statefulset in ns statefulset-1425
May 14 02:12:29.115: INFO: Scaling statefulset ss to 0
May 14 02:12:49.176: INFO: Waiting for statefulset status.replicas updated to 0
May 14 02:12:49.181: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:12:49.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1425" for this suite.
May 14 02:12:57.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:12:57.670: INFO: namespace statefulset-1425 deletion completed in 8.45115483s

â€¢ [SLOW TEST:39.680 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:12:57.682: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4810
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:12:57.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4810" for this suite.
May 14 02:13:04.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:13:04.304: INFO: namespace custom-resource-definition-4810 deletion completed in 6.312824589s

â€¢ [SLOW TEST:6.623 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:13:04.325: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2326
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May 14 02:13:04.696: INFO: Waiting up to 5m0s for pod "downward-api-a8bd0977-f8f8-494f-903c-88ac2106b4c1" in namespace "downward-api-2326" to be "success or failure"
May 14 02:13:04.725: INFO: Pod "downward-api-a8bd0977-f8f8-494f-903c-88ac2106b4c1": Phase="Pending", Reason="", readiness=false. Elapsed: 28.827877ms
May 14 02:13:06.733: INFO: Pod "downward-api-a8bd0977-f8f8-494f-903c-88ac2106b4c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037041479s
May 14 02:13:08.740: INFO: Pod "downward-api-a8bd0977-f8f8-494f-903c-88ac2106b4c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043949402s
STEP: Saw pod success
May 14 02:13:08.741: INFO: Pod "downward-api-a8bd0977-f8f8-494f-903c-88ac2106b4c1" satisfied condition "success or failure"
May 14 02:13:08.743: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod downward-api-a8bd0977-f8f8-494f-903c-88ac2106b4c1 container dapi-container: <nil>
STEP: delete the pod
May 14 02:13:08.978: INFO: Waiting for pod downward-api-a8bd0977-f8f8-494f-903c-88ac2106b4c1 to disappear
May 14 02:13:08.986: INFO: Pod downward-api-a8bd0977-f8f8-494f-903c-88ac2106b4c1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:13:08.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2326" for this suite.
May 14 02:13:17.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:13:17.439: INFO: namespace downward-api-2326 deletion completed in 8.40936849s

â€¢ [SLOW TEST:13.114 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:13:17.447: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9680
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 02:13:17.703: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 14 02:13:24.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 --namespace=crd-publish-openapi-9680 create -f -'
May 14 02:13:28.951: INFO: stderr: ""
May 14 02:13:28.951: INFO: stdout: "e2e-test-crd-publish-openapi-3664-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 14 02:13:28.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 --namespace=crd-publish-openapi-9680 delete e2e-test-crd-publish-openapi-3664-crds test-cr'
May 14 02:13:29.518: INFO: stderr: ""
May 14 02:13:29.519: INFO: stdout: "e2e-test-crd-publish-openapi-3664-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May 14 02:13:29.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 --namespace=crd-publish-openapi-9680 apply -f -'
May 14 02:13:30.328: INFO: stderr: ""
May 14 02:13:30.330: INFO: stdout: "e2e-test-crd-publish-openapi-3664-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 14 02:13:30.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 --namespace=crd-publish-openapi-9680 delete e2e-test-crd-publish-openapi-3664-crds test-cr'
May 14 02:13:30.621: INFO: stderr: ""
May 14 02:13:30.621: INFO: stdout: "e2e-test-crd-publish-openapi-3664-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
May 14 02:13:30.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 explain e2e-test-crd-publish-openapi-3664-crds'
May 14 02:13:31.401: INFO: stderr: ""
May 14 02:13:31.401: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3664-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:13:38.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9680" for this suite.
May 14 02:13:44.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:13:46.058: INFO: namespace crd-publish-openapi-9680 deletion completed in 7.671707784s

â€¢ [SLOW TEST:28.612 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:13:46.059: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6826
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
May 14 02:13:52.935: INFO: Successfully updated pod "annotationupdate0669ac3c-1a89-4e82-b398-6b7b86a578ea"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:13:55.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6826" for this suite.
May 14 02:14:08.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:14:08.569: INFO: namespace projected-6826 deletion completed in 12.391544169s

â€¢ [SLOW TEST:22.512 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:14:08.573: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3612
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-2f971f1e-d683-44aa-9ae5-a567435c0f2f
May 14 02:14:08.857: INFO: Pod name my-hostname-basic-2f971f1e-d683-44aa-9ae5-a567435c0f2f: Found 1 pods out of 1
May 14 02:14:08.858: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-2f971f1e-d683-44aa-9ae5-a567435c0f2f" are running
May 14 02:14:14.919: INFO: Pod "my-hostname-basic-2f971f1e-d683-44aa-9ae5-a567435c0f2f-wn4qs" is running (conditions: [])
May 14 02:14:14.919: INFO: Trying to dial the pod
May 14 02:14:21.460: INFO: Controller my-hostname-basic-2f971f1e-d683-44aa-9ae5-a567435c0f2f: Got expected result from replica 1 [my-hostname-basic-2f971f1e-d683-44aa-9ae5-a567435c0f2f-wn4qs]: "my-hostname-basic-2f971f1e-d683-44aa-9ae5-a567435c0f2f-wn4qs", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:14:21.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3612" for this suite.
May 14 02:14:27.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:14:27.885: INFO: namespace replication-controller-3612 deletion completed in 6.414437478s

â€¢ [SLOW TEST:19.313 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:14:27.889: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3876
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May 14 02:14:28.103: INFO: Waiting up to 5m0s for pod "downward-api-64a83f0d-3821-40fd-b66b-a45c3360aff2" in namespace "downward-api-3876" to be "success or failure"
May 14 02:14:28.111: INFO: Pod "downward-api-64a83f0d-3821-40fd-b66b-a45c3360aff2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.7478ms
May 14 02:14:30.510: INFO: Pod "downward-api-64a83f0d-3821-40fd-b66b-a45c3360aff2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.407569295s
May 14 02:14:32.519: INFO: Pod "downward-api-64a83f0d-3821-40fd-b66b-a45c3360aff2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.416296621s
May 14 02:14:34.526: INFO: Pod "downward-api-64a83f0d-3821-40fd-b66b-a45c3360aff2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.423635766s
STEP: Saw pod success
May 14 02:14:34.527: INFO: Pod "downward-api-64a83f0d-3821-40fd-b66b-a45c3360aff2" satisfied condition "success or failure"
May 14 02:14:34.531: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod downward-api-64a83f0d-3821-40fd-b66b-a45c3360aff2 container dapi-container: <nil>
STEP: delete the pod
May 14 02:14:34.603: INFO: Waiting for pod downward-api-64a83f0d-3821-40fd-b66b-a45c3360aff2 to disappear
May 14 02:14:34.614: INFO: Pod downward-api-64a83f0d-3821-40fd-b66b-a45c3360aff2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:14:34.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3876" for this suite.
May 14 02:14:40.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:14:41.031: INFO: namespace downward-api-3876 deletion completed in 6.397133526s

â€¢ [SLOW TEST:13.142 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:14:41.033: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-1929
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
May 14 02:14:44.527: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
May 14 02:14:45.242: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May 14 02:14:47.405: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725019285, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725019285, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725019285, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725019285, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 02:14:51.694: INFO: Waited 2.264314118s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:14:52.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1929" for this suite.
May 14 02:14:58.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:14:59.263: INFO: namespace aggregator-1929 deletion completed in 6.47931917s

â€¢ [SLOW TEST:18.230 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:14:59.265: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9091
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 02:14:59.528: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 14 02:15:06.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 --namespace=crd-publish-openapi-9091 create -f -'
May 14 02:15:08.542: INFO: stderr: ""
May 14 02:15:08.542: INFO: stdout: "e2e-test-crd-publish-openapi-8405-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 14 02:15:08.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 --namespace=crd-publish-openapi-9091 delete e2e-test-crd-publish-openapi-8405-crds test-cr'
May 14 02:15:09.206: INFO: stderr: ""
May 14 02:15:09.206: INFO: stdout: "e2e-test-crd-publish-openapi-8405-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May 14 02:15:09.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 --namespace=crd-publish-openapi-9091 apply -f -'
May 14 02:15:09.867: INFO: stderr: ""
May 14 02:15:09.867: INFO: stdout: "e2e-test-crd-publish-openapi-8405-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 14 02:15:09.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 --namespace=crd-publish-openapi-9091 delete e2e-test-crd-publish-openapi-8405-crds test-cr'
May 14 02:15:10.278: INFO: stderr: ""
May 14 02:15:10.278: INFO: stdout: "e2e-test-crd-publish-openapi-8405-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 14 02:15:10.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 explain e2e-test-crd-publish-openapi-8405-crds'
May 14 02:15:10.887: INFO: stderr: ""
May 14 02:15:10.887: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8405-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:15:18.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9091" for this suite.
May 14 02:15:24.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:15:24.970: INFO: namespace crd-publish-openapi-9091 deletion completed in 6.59559015s

â€¢ [SLOW TEST:25.706 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:15:24.985: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4587
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:15:44.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4587" for this suite.
May 14 02:15:51.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:15:51.455: INFO: namespace resourcequota-4587 deletion completed in 6.387088923s

â€¢ [SLOW TEST:26.476 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:15:51.456: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8945
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 14 02:15:51.696: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8945 /api/v1/namespaces/watch-8945/configmaps/e2e-watch-test-label-changed 8efc40ed-2d6f-401d-832c-b2eda776df78 185898 0 2020-05-14 02:15:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 14 02:15:51.696: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8945 /api/v1/namespaces/watch-8945/configmaps/e2e-watch-test-label-changed 8efc40ed-2d6f-401d-832c-b2eda776df78 185899 0 2020-05-14 02:15:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 14 02:15:51.697: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8945 /api/v1/namespaces/watch-8945/configmaps/e2e-watch-test-label-changed 8efc40ed-2d6f-401d-832c-b2eda776df78 185900 0 2020-05-14 02:15:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 14 02:16:01.768: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8945 /api/v1/namespaces/watch-8945/configmaps/e2e-watch-test-label-changed 8efc40ed-2d6f-401d-832c-b2eda776df78 185928 0 2020-05-14 02:15:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 14 02:16:01.772: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8945 /api/v1/namespaces/watch-8945/configmaps/e2e-watch-test-label-changed 8efc40ed-2d6f-401d-832c-b2eda776df78 185929 0 2020-05-14 02:15:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
May 14 02:16:01.773: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8945 /api/v1/namespaces/watch-8945/configmaps/e2e-watch-test-label-changed 8efc40ed-2d6f-401d-832c-b2eda776df78 185930 0 2020-05-14 02:15:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:16:01.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8945" for this suite.
May 14 02:16:09.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:16:10.710: INFO: namespace watch-8945 deletion completed in 8.926062322s

â€¢ [SLOW TEST:19.254 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:16:10.711: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5458
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 02:16:10.910: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
May 14 02:16:17.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 --namespace=crd-publish-openapi-5458 create -f -'
May 14 02:16:19.535: INFO: stderr: ""
May 14 02:16:19.535: INFO: stdout: "e2e-test-crd-publish-openapi-8926-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 14 02:16:19.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 --namespace=crd-publish-openapi-5458 delete e2e-test-crd-publish-openapi-8926-crds test-foo'
May 14 02:16:22.120: INFO: stderr: ""
May 14 02:16:22.120: INFO: stdout: "e2e-test-crd-publish-openapi-8926-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May 14 02:16:22.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 --namespace=crd-publish-openapi-5458 apply -f -'
May 14 02:16:22.743: INFO: stderr: ""
May 14 02:16:22.743: INFO: stdout: "e2e-test-crd-publish-openapi-8926-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 14 02:16:22.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 --namespace=crd-publish-openapi-5458 delete e2e-test-crd-publish-openapi-8926-crds test-foo'
May 14 02:16:23.097: INFO: stderr: ""
May 14 02:16:23.097: INFO: stdout: "e2e-test-crd-publish-openapi-8926-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
May 14 02:16:23.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 --namespace=crd-publish-openapi-5458 create -f -'
May 14 02:16:23.401: INFO: rc: 1
May 14 02:16:23.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 --namespace=crd-publish-openapi-5458 apply -f -'
May 14 02:16:23.918: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
May 14 02:16:23.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 --namespace=crd-publish-openapi-5458 create -f -'
May 14 02:16:24.455: INFO: rc: 1
May 14 02:16:24.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 --namespace=crd-publish-openapi-5458 apply -f -'
May 14 02:16:24.818: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
May 14 02:16:24.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 explain e2e-test-crd-publish-openapi-8926-crds'
May 14 02:16:25.236: INFO: stderr: ""
May 14 02:16:25.236: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8926-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
May 14 02:16:25.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 explain e2e-test-crd-publish-openapi-8926-crds.metadata'
May 14 02:16:25.765: INFO: stderr: ""
May 14 02:16:25.765: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8926-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May 14 02:16:25.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 explain e2e-test-crd-publish-openapi-8926-crds.spec'
May 14 02:16:26.488: INFO: stderr: ""
May 14 02:16:26.488: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8926-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May 14 02:16:26.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 explain e2e-test-crd-publish-openapi-8926-crds.spec.bars'
May 14 02:16:27.660: INFO: stderr: ""
May 14 02:16:27.660: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8926-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
May 14 02:16:27.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 explain e2e-test-crd-publish-openapi-8926-crds.spec.bars2'
May 14 02:16:28.111: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:16:34.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5458" for this suite.
May 14 02:16:40.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:16:41.232: INFO: namespace crd-publish-openapi-5458 deletion completed in 6.447799014s

â€¢ [SLOW TEST:30.521 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:16:41.234: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5506
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-wpl2g in namespace proxy-5506
I0514 02:16:41.499995      20 runners.go:184] Created replication controller with name: proxy-service-wpl2g, namespace: proxy-5506, replica count: 1
I0514 02:16:42.567769      20 runners.go:184] proxy-service-wpl2g Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0514 02:16:43.568583      20 runners.go:184] proxy-service-wpl2g Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0514 02:16:44.571302      20 runners.go:184] proxy-service-wpl2g Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0514 02:16:45.571712      20 runners.go:184] proxy-service-wpl2g Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0514 02:16:46.574832      20 runners.go:184] proxy-service-wpl2g Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0514 02:16:47.593946      20 runners.go:184] proxy-service-wpl2g Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0514 02:16:48.594560      20 runners.go:184] proxy-service-wpl2g Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0514 02:16:49.594844      20 runners.go:184] proxy-service-wpl2g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0514 02:16:50.595234      20 runners.go:184] proxy-service-wpl2g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0514 02:16:51.595611      20 runners.go:184] proxy-service-wpl2g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0514 02:16:52.596884      20 runners.go:184] proxy-service-wpl2g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0514 02:16:53.597364      20 runners.go:184] proxy-service-wpl2g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0514 02:16:54.598363      20 runners.go:184] proxy-service-wpl2g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0514 02:16:55.598763      20 runners.go:184] proxy-service-wpl2g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0514 02:16:56.599122      20 runners.go:184] proxy-service-wpl2g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0514 02:16:57.599622      20 runners.go:184] proxy-service-wpl2g Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 14 02:16:57.608: INFO: setup took 16.16901554s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 14 02:16:57.639: INFO: (0) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">test<... (200; 30.463362ms)
May 14 02:16:57.643: INFO: (0) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 34.297114ms)
May 14 02:16:57.644: INFO: (0) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 33.989399ms)
May 14 02:16:57.645: INFO: (0) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname2/proxy/: bar (200; 35.00281ms)
May 14 02:16:57.648: INFO: (0) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 38.726294ms)
May 14 02:16:57.651: INFO: (0) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">... (200; 41.355157ms)
May 14 02:16:57.661: INFO: (0) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 51.557337ms)
May 14 02:16:57.665: INFO: (0) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/rewriteme">test</a> (200; 55.54598ms)
May 14 02:16:57.678: INFO: (0) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname1/proxy/: foo (200; 68.147006ms)
May 14 02:16:57.678: INFO: (0) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname1/proxy/: foo (200; 67.605747ms)
May 14 02:16:57.678: INFO: (0) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname2/proxy/: bar (200; 67.613921ms)
May 14 02:16:57.680: INFO: (0) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:460/proxy/: tls baz (200; 69.490914ms)
May 14 02:16:57.680: INFO: (0) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname1/proxy/: tls baz (200; 70.838879ms)
May 14 02:16:57.691: INFO: (0) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/tlsrewritem... (200; 80.662317ms)
May 14 02:16:57.693: INFO: (0) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:462/proxy/: tls qux (200; 82.219145ms)
May 14 02:16:57.694: INFO: (0) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname2/proxy/: tls qux (200; 82.156625ms)
May 14 02:16:57.708: INFO: (1) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 12.42286ms)
May 14 02:16:57.708: INFO: (1) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/tlsrewritem... (200; 13.282042ms)
May 14 02:16:57.709: INFO: (1) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">test<... (200; 13.189362ms)
May 14 02:16:57.719: INFO: (1) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname1/proxy/: foo (200; 22.629587ms)
May 14 02:16:57.722: INFO: (1) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 24.898099ms)
May 14 02:16:57.723: INFO: (1) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname1/proxy/: foo (200; 24.666837ms)
May 14 02:16:57.724: INFO: (1) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 26.034986ms)
May 14 02:16:57.725: INFO: (1) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname2/proxy/: tls qux (200; 29.005655ms)
May 14 02:16:57.726: INFO: (1) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:462/proxy/: tls qux (200; 27.267325ms)
May 14 02:16:57.726: INFO: (1) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">... (200; 28.274937ms)
May 14 02:16:57.726: INFO: (1) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname1/proxy/: tls baz (200; 29.034441ms)
May 14 02:16:57.727: INFO: (1) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname2/proxy/: bar (200; 30.232052ms)
May 14 02:16:57.727: INFO: (1) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname2/proxy/: bar (200; 28.26147ms)
May 14 02:16:57.727: INFO: (1) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/rewriteme">test</a> (200; 28.193947ms)
May 14 02:16:57.727: INFO: (1) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 30.055323ms)
May 14 02:16:57.727: INFO: (1) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:460/proxy/: tls baz (200; 29.105715ms)
May 14 02:16:57.749: INFO: (2) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 21.848242ms)
May 14 02:16:57.754: INFO: (2) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/rewriteme">test</a> (200; 26.407459ms)
May 14 02:16:57.756: INFO: (2) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname2/proxy/: bar (200; 28.178459ms)
May 14 02:16:57.776: INFO: (2) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 46.490024ms)
May 14 02:16:57.776: INFO: (2) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:462/proxy/: tls qux (200; 47.399286ms)
May 14 02:16:57.776: INFO: (2) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname1/proxy/: foo (200; 48.917302ms)
May 14 02:16:57.776: INFO: (2) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:460/proxy/: tls baz (200; 47.64209ms)
May 14 02:16:57.776: INFO: (2) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">... (200; 48.110277ms)
May 14 02:16:57.776: INFO: (2) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 47.816737ms)
May 14 02:16:57.776: INFO: (2) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/tlsrewritem... (200; 47.180754ms)
May 14 02:16:57.776: INFO: (2) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname1/proxy/: tls baz (200; 48.69324ms)
May 14 02:16:57.776: INFO: (2) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 48.047008ms)
May 14 02:16:57.776: INFO: (2) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname1/proxy/: foo (200; 47.440199ms)
May 14 02:16:57.776: INFO: (2) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">test<... (200; 47.10337ms)
May 14 02:16:57.776: INFO: (2) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname2/proxy/: tls qux (200; 47.200471ms)
May 14 02:16:57.776: INFO: (2) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname2/proxy/: bar (200; 47.378911ms)
May 14 02:16:57.796: INFO: (3) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 18.766518ms)
May 14 02:16:57.798: INFO: (3) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:460/proxy/: tls baz (200; 18.739631ms)
May 14 02:16:57.798: INFO: (3) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">... (200; 19.314384ms)
May 14 02:16:57.798: INFO: (3) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:462/proxy/: tls qux (200; 19.449724ms)
May 14 02:16:57.798: INFO: (3) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 19.877803ms)
May 14 02:16:57.798: INFO: (3) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/tlsrewritem... (200; 20.992371ms)
May 14 02:16:57.799: INFO: (3) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">test<... (200; 21.653968ms)
May 14 02:16:57.800: INFO: (3) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname2/proxy/: bar (200; 22.439207ms)
May 14 02:16:57.800: INFO: (3) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/rewriteme">test</a> (200; 22.119335ms)
May 14 02:16:57.800: INFO: (3) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 22.304169ms)
May 14 02:16:57.801: INFO: (3) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 22.820925ms)
May 14 02:16:57.807: INFO: (3) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname2/proxy/: bar (200; 30.248535ms)
May 14 02:16:57.807: INFO: (3) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname1/proxy/: foo (200; 29.56006ms)
May 14 02:16:57.807: INFO: (3) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname1/proxy/: tls baz (200; 29.408224ms)
May 14 02:16:57.807: INFO: (3) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname2/proxy/: tls qux (200; 29.914385ms)
May 14 02:16:57.808: INFO: (3) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname1/proxy/: foo (200; 28.483284ms)
May 14 02:16:57.826: INFO: (4) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 14.970753ms)
May 14 02:16:57.831: INFO: (4) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/rewriteme">test</a> (200; 20.901125ms)
May 14 02:16:57.831: INFO: (4) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname2/proxy/: bar (200; 20.719216ms)
May 14 02:16:57.836: INFO: (4) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/tlsrewritem... (200; 25.884008ms)
May 14 02:16:57.836: INFO: (4) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname1/proxy/: foo (200; 25.857915ms)
May 14 02:16:57.836: INFO: (4) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:460/proxy/: tls baz (200; 24.853788ms)
May 14 02:16:57.837: INFO: (4) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 27.01745ms)
May 14 02:16:57.837: INFO: (4) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:462/proxy/: tls qux (200; 27.158538ms)
May 14 02:16:57.838: INFO: (4) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname2/proxy/: tls qux (200; 28.107591ms)
May 14 02:16:57.839: INFO: (4) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">test<... (200; 28.429675ms)
May 14 02:16:57.839: INFO: (4) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">... (200; 27.891534ms)
May 14 02:16:57.839: INFO: (4) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname2/proxy/: bar (200; 29.805253ms)
May 14 02:16:57.839: INFO: (4) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 28.723712ms)
May 14 02:16:57.840: INFO: (4) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname1/proxy/: foo (200; 29.145338ms)
May 14 02:16:57.841: INFO: (4) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname1/proxy/: tls baz (200; 30.183292ms)
May 14 02:16:57.841: INFO: (4) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 30.35877ms)
May 14 02:16:57.855: INFO: (5) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 13.766887ms)
May 14 02:16:57.858: INFO: (5) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:460/proxy/: tls baz (200; 16.348719ms)
May 14 02:16:57.861: INFO: (5) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">... (200; 17.512238ms)
May 14 02:16:57.861: INFO: (5) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname2/proxy/: bar (200; 17.943796ms)
May 14 02:16:57.861: INFO: (5) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname1/proxy/: foo (200; 19.359091ms)
May 14 02:16:57.864: INFO: (5) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/rewriteme">test</a> (200; 20.231713ms)
May 14 02:16:57.865: INFO: (5) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname1/proxy/: tls baz (200; 20.091965ms)
May 14 02:16:57.865: INFO: (5) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname2/proxy/: bar (200; 21.593799ms)
May 14 02:16:57.872: INFO: (5) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 28.42848ms)
May 14 02:16:57.872: INFO: (5) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname2/proxy/: tls qux (200; 29.130499ms)
May 14 02:16:57.872: INFO: (5) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:462/proxy/: tls qux (200; 29.858443ms)
May 14 02:16:57.873: INFO: (5) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname1/proxy/: foo (200; 29.807267ms)
May 14 02:16:57.873: INFO: (5) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 28.621643ms)
May 14 02:16:57.873: INFO: (5) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">test<... (200; 28.071834ms)
May 14 02:16:57.873: INFO: (5) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/tlsrewritem... (200; 30.906338ms)
May 14 02:16:57.873: INFO: (5) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 31.013989ms)
May 14 02:16:57.952: INFO: (6) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 77.134464ms)
May 14 02:16:57.952: INFO: (6) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/rewriteme">test</a> (200; 77.064966ms)
May 14 02:16:57.952: INFO: (6) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/tlsrewritem... (200; 77.032919ms)
May 14 02:16:57.952: INFO: (6) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:462/proxy/: tls qux (200; 77.25129ms)
May 14 02:16:57.953: INFO: (6) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">test<... (200; 77.956204ms)
May 14 02:16:57.955: INFO: (6) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 81.265836ms)
May 14 02:16:57.956: INFO: (6) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">... (200; 81.897521ms)
May 14 02:16:57.957: INFO: (6) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 84.067849ms)
May 14 02:16:57.958: INFO: (6) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname2/proxy/: bar (200; 82.492107ms)
May 14 02:16:57.958: INFO: (6) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname2/proxy/: tls qux (200; 84.693283ms)
May 14 02:16:57.958: INFO: (6) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 83.870279ms)
May 14 02:16:57.958: INFO: (6) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname1/proxy/: tls baz (200; 84.576738ms)
May 14 02:16:57.958: INFO: (6) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:460/proxy/: tls baz (200; 83.50595ms)
May 14 02:16:57.958: INFO: (6) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname2/proxy/: bar (200; 83.153532ms)
May 14 02:16:57.959: INFO: (6) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname1/proxy/: foo (200; 83.521333ms)
May 14 02:16:57.959: INFO: (6) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname1/proxy/: foo (200; 84.234793ms)
May 14 02:16:57.970: INFO: (7) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">test<... (200; 10.355758ms)
May 14 02:16:57.976: INFO: (7) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 15.986434ms)
May 14 02:16:57.976: INFO: (7) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname1/proxy/: foo (200; 16.26055ms)
May 14 02:16:57.976: INFO: (7) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 15.204431ms)
May 14 02:16:57.976: INFO: (7) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/tlsrewritem... (200; 16.312092ms)
May 14 02:16:57.981: INFO: (7) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname2/proxy/: tls qux (200; 20.489641ms)
May 14 02:16:57.981: INFO: (7) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/rewriteme">test</a> (200; 20.138185ms)
May 14 02:16:57.983: INFO: (7) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname1/proxy/: tls baz (200; 22.500429ms)
May 14 02:16:57.983: INFO: (7) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname2/proxy/: bar (200; 22.939584ms)
May 14 02:16:57.984: INFO: (7) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 22.320844ms)
May 14 02:16:57.984: INFO: (7) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:460/proxy/: tls baz (200; 22.96147ms)
May 14 02:16:57.984: INFO: (7) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">... (200; 23.535401ms)
May 14 02:16:57.985: INFO: (7) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 23.75119ms)
May 14 02:16:57.985: INFO: (7) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:462/proxy/: tls qux (200; 23.594619ms)
May 14 02:16:57.987: INFO: (7) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname2/proxy/: bar (200; 24.740018ms)
May 14 02:16:57.987: INFO: (7) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname1/proxy/: foo (200; 25.472321ms)
May 14 02:16:58.005: INFO: (8) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/rewriteme">test</a> (200; 16.777039ms)
May 14 02:16:58.005: INFO: (8) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 17.72163ms)
May 14 02:16:58.009: INFO: (8) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname2/proxy/: bar (200; 21.485115ms)
May 14 02:16:58.010: INFO: (8) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:462/proxy/: tls qux (200; 21.904157ms)
May 14 02:16:58.010: INFO: (8) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 22.66554ms)
May 14 02:16:58.010: INFO: (8) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">... (200; 22.923565ms)
May 14 02:16:58.010: INFO: (8) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname1/proxy/: tls baz (200; 22.604338ms)
May 14 02:16:58.011: INFO: (8) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:460/proxy/: tls baz (200; 22.979278ms)
May 14 02:16:58.011: INFO: (8) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/tlsrewritem... (200; 21.889624ms)
May 14 02:16:58.011: INFO: (8) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname2/proxy/: bar (200; 21.690902ms)
May 14 02:16:58.011: INFO: (8) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 23.425351ms)
May 14 02:16:58.011: INFO: (8) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 23.267488ms)
May 14 02:16:58.011: INFO: (8) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname1/proxy/: foo (200; 23.348126ms)
May 14 02:16:58.011: INFO: (8) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname2/proxy/: tls qux (200; 22.40976ms)
May 14 02:16:58.011: INFO: (8) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname1/proxy/: foo (200; 22.570587ms)
May 14 02:16:58.012: INFO: (8) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">test<... (200; 24.486453ms)
May 14 02:16:58.021: INFO: (9) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 9.142012ms)
May 14 02:16:58.034: INFO: (9) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 21.875987ms)
May 14 02:16:58.035: INFO: (9) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 20.732581ms)
May 14 02:16:58.035: INFO: (9) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 22.011242ms)
May 14 02:16:58.036: INFO: (9) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:460/proxy/: tls baz (200; 23.712905ms)
May 14 02:16:58.036: INFO: (9) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:462/proxy/: tls qux (200; 23.940407ms)
May 14 02:16:58.037: INFO: (9) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname1/proxy/: tls baz (200; 22.659965ms)
May 14 02:16:58.037: INFO: (9) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/rewriteme">test</a> (200; 23.854208ms)
May 14 02:16:58.037: INFO: (9) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname2/proxy/: bar (200; 24.082789ms)
May 14 02:16:58.037: INFO: (9) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">test<... (200; 23.322718ms)
May 14 02:16:58.037: INFO: (9) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname2/proxy/: bar (200; 23.950998ms)
May 14 02:16:58.037: INFO: (9) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname1/proxy/: foo (200; 24.566458ms)
May 14 02:16:58.037: INFO: (9) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/tlsrewritem... (200; 23.755973ms)
May 14 02:16:58.037: INFO: (9) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">... (200; 24.266528ms)
May 14 02:16:58.037: INFO: (9) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname2/proxy/: tls qux (200; 23.482166ms)
May 14 02:16:58.038: INFO: (9) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname1/proxy/: foo (200; 24.376789ms)
May 14 02:16:58.057: INFO: (10) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname1/proxy/: tls baz (200; 17.172465ms)
May 14 02:16:58.060: INFO: (10) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname2/proxy/: bar (200; 19.609231ms)
May 14 02:16:58.061: INFO: (10) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/tlsrewritem... (200; 19.987327ms)
May 14 02:16:58.068: INFO: (10) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 26.718183ms)
May 14 02:16:58.089: INFO: (10) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname2/proxy/: tls qux (200; 47.522812ms)
May 14 02:16:58.089: INFO: (10) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 48.169431ms)
May 14 02:16:58.090: INFO: (10) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:460/proxy/: tls baz (200; 48.457153ms)
May 14 02:16:58.091: INFO: (10) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">test<... (200; 49.412602ms)
May 14 02:16:58.091: INFO: (10) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/rewriteme">test</a> (200; 50.03606ms)
May 14 02:16:58.092: INFO: (10) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 50.483179ms)
May 14 02:16:58.093: INFO: (10) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname2/proxy/: bar (200; 51.732759ms)
May 14 02:16:58.093: INFO: (10) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">... (200; 51.59441ms)
May 14 02:16:58.096: INFO: (10) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname1/proxy/: foo (200; 55.560549ms)
May 14 02:16:58.097: INFO: (10) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:462/proxy/: tls qux (200; 55.07028ms)
May 14 02:16:58.097: INFO: (10) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 55.252842ms)
May 14 02:16:58.098: INFO: (10) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname1/proxy/: foo (200; 56.368459ms)
May 14 02:16:58.115: INFO: (11) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 16.342893ms)
May 14 02:16:58.119: INFO: (11) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/tlsrewritem... (200; 19.39626ms)
May 14 02:16:58.122: INFO: (11) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 21.73213ms)
May 14 02:16:58.122: INFO: (11) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">test<... (200; 21.905446ms)
May 14 02:16:58.124: INFO: (11) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/rewriteme">test</a> (200; 24.417072ms)
May 14 02:16:58.124: INFO: (11) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 25.821616ms)
May 14 02:16:58.124: INFO: (11) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 25.696393ms)
May 14 02:16:58.125: INFO: (11) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:460/proxy/: tls baz (200; 25.324376ms)
May 14 02:16:58.125: INFO: (11) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">... (200; 24.990172ms)
May 14 02:16:58.125: INFO: (11) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname2/proxy/: tls qux (200; 25.402222ms)
May 14 02:16:58.125: INFO: (11) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:462/proxy/: tls qux (200; 25.581265ms)
May 14 02:16:58.125: INFO: (11) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname2/proxy/: bar (200; 25.459694ms)
May 14 02:16:58.127: INFO: (11) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname2/proxy/: bar (200; 27.744041ms)
May 14 02:16:58.127: INFO: (11) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname1/proxy/: foo (200; 27.858408ms)
May 14 02:16:58.127: INFO: (11) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname1/proxy/: foo (200; 27.723474ms)
May 14 02:16:58.130: INFO: (11) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname1/proxy/: tls baz (200; 30.48013ms)
May 14 02:16:58.138: INFO: (12) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 7.854121ms)
May 14 02:16:58.156: INFO: (12) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/tlsrewritem... (200; 25.442882ms)
May 14 02:16:58.156: INFO: (12) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 25.712887ms)
May 14 02:16:58.163: INFO: (12) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 30.274165ms)
May 14 02:16:58.164: INFO: (12) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname2/proxy/: bar (200; 32.362863ms)
May 14 02:16:58.164: INFO: (12) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname2/proxy/: bar (200; 30.31089ms)
May 14 02:16:58.165: INFO: (12) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">... (200; 31.218941ms)
May 14 02:16:58.166: INFO: (12) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname2/proxy/: tls qux (200; 34.33915ms)
May 14 02:16:58.166: INFO: (12) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname1/proxy/: foo (200; 35.142775ms)
May 14 02:16:58.166: INFO: (12) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:460/proxy/: tls baz (200; 33.14673ms)
May 14 02:16:58.169: INFO: (12) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/rewriteme">test</a> (200; 35.350571ms)
May 14 02:16:58.170: INFO: (12) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">test<... (200; 37.737704ms)
May 14 02:16:58.171: INFO: (12) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname1/proxy/: tls baz (200; 38.649682ms)
May 14 02:16:58.171: INFO: (12) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 38.32847ms)
May 14 02:16:58.171: INFO: (12) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname1/proxy/: foo (200; 37.077826ms)
May 14 02:16:58.171: INFO: (12) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:462/proxy/: tls qux (200; 37.526412ms)
May 14 02:16:58.200: INFO: (13) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">... (200; 26.89898ms)
May 14 02:16:58.200: INFO: (13) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 29.070348ms)
May 14 02:16:58.208: INFO: (13) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname1/proxy/: foo (200; 34.348636ms)
May 14 02:16:58.208: INFO: (13) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname2/proxy/: bar (200; 34.312613ms)
May 14 02:16:58.210: INFO: (13) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname2/proxy/: tls qux (200; 36.503981ms)
May 14 02:16:58.210: INFO: (13) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname2/proxy/: bar (200; 37.868138ms)
May 14 02:16:58.210: INFO: (13) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">test<... (200; 36.347195ms)
May 14 02:16:58.210: INFO: (13) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname1/proxy/: foo (200; 38.11665ms)
May 14 02:16:58.210: INFO: (13) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 39.068063ms)
May 14 02:16:58.210: INFO: (13) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:462/proxy/: tls qux (200; 38.408747ms)
May 14 02:16:58.210: INFO: (13) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 37.546267ms)
May 14 02:16:58.210: INFO: (13) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/tlsrewritem... (200; 37.409604ms)
May 14 02:16:58.211: INFO: (13) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 39.013823ms)
May 14 02:16:58.211: INFO: (13) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:460/proxy/: tls baz (200; 38.840026ms)
May 14 02:16:58.211: INFO: (13) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname1/proxy/: tls baz (200; 36.44183ms)
May 14 02:16:58.211: INFO: (13) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/rewriteme">test</a> (200; 38.096556ms)
May 14 02:16:58.217: INFO: (14) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname1/proxy/: foo (200; 6.485213ms)
May 14 02:16:58.218: INFO: (14) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/tlsrewritem... (200; 7.312128ms)
May 14 02:16:58.224: INFO: (14) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 12.894745ms)
May 14 02:16:58.225: INFO: (14) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">test<... (200; 13.9271ms)
May 14 02:16:58.229: INFO: (14) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname2/proxy/: tls qux (200; 17.435699ms)
May 14 02:16:58.231: INFO: (14) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname2/proxy/: bar (200; 20.026635ms)
May 14 02:16:58.231: INFO: (14) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname1/proxy/: foo (200; 19.668005ms)
May 14 02:16:58.232: INFO: (14) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 20.236765ms)
May 14 02:16:58.233: INFO: (14) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">... (200; 20.852023ms)
May 14 02:16:58.237: INFO: (14) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 24.332903ms)
May 14 02:16:58.241: INFO: (14) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 27.43604ms)
May 14 02:16:58.241: INFO: (14) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:462/proxy/: tls qux (200; 28.953026ms)
May 14 02:16:58.241: INFO: (14) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:460/proxy/: tls baz (200; 28.994309ms)
May 14 02:16:58.242: INFO: (14) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname2/proxy/: bar (200; 29.35893ms)
May 14 02:16:58.242: INFO: (14) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/rewriteme">test</a> (200; 29.623705ms)
May 14 02:16:58.243: INFO: (14) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname1/proxy/: tls baz (200; 30.087409ms)
May 14 02:16:58.248: INFO: (15) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 5.116147ms)
May 14 02:16:58.250: INFO: (15) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/tlsrewritem... (200; 7.237171ms)
May 14 02:16:58.252: INFO: (15) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 9.575073ms)
May 14 02:16:58.272: INFO: (15) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:462/proxy/: tls qux (200; 27.287153ms)
May 14 02:16:58.277: INFO: (15) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 32.789923ms)
May 14 02:16:58.277: INFO: (15) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 33.083548ms)
May 14 02:16:58.278: INFO: (15) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">test<... (200; 33.920369ms)
May 14 02:16:58.279: INFO: (15) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/rewriteme">test</a> (200; 33.175669ms)
May 14 02:16:58.280: INFO: (15) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname2/proxy/: bar (200; 36.083599ms)
May 14 02:16:58.280: INFO: (15) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:460/proxy/: tls baz (200; 34.959574ms)
May 14 02:16:58.280: INFO: (15) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname1/proxy/: foo (200; 35.217187ms)
May 14 02:16:58.281: INFO: (15) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname2/proxy/: tls qux (200; 37.071027ms)
May 14 02:16:58.281: INFO: (15) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname1/proxy/: tls baz (200; 36.719382ms)
May 14 02:16:58.281: INFO: (15) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">... (200; 35.65808ms)
May 14 02:16:58.282: INFO: (15) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname1/proxy/: foo (200; 38.611877ms)
May 14 02:16:58.282: INFO: (15) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname2/proxy/: bar (200; 36.963647ms)
May 14 02:16:58.300: INFO: (16) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:462/proxy/: tls qux (200; 17.615339ms)
May 14 02:16:58.302: INFO: (16) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 19.018913ms)
May 14 02:16:58.305: INFO: (16) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">test<... (200; 20.990733ms)
May 14 02:16:58.325: INFO: (16) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname2/proxy/: tls qux (200; 41.04364ms)
May 14 02:16:58.325: INFO: (16) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:460/proxy/: tls baz (200; 39.11492ms)
May 14 02:16:58.325: INFO: (16) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 39.941282ms)
May 14 02:16:58.325: INFO: (16) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname2/proxy/: bar (200; 40.60933ms)
May 14 02:16:58.326: INFO: (16) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/rewriteme">test</a> (200; 40.58099ms)
May 14 02:16:58.326: INFO: (16) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname1/proxy/: foo (200; 42.34707ms)
May 14 02:16:58.326: INFO: (16) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname1/proxy/: foo (200; 41.421726ms)
May 14 02:16:58.327: INFO: (16) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 42.148736ms)
May 14 02:16:58.327: INFO: (16) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">... (200; 41.517899ms)
May 14 02:16:58.327: INFO: (16) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/tlsrewritem... (200; 43.612999ms)
May 14 02:16:58.327: INFO: (16) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname2/proxy/: bar (200; 43.480051ms)
May 14 02:16:58.327: INFO: (16) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname1/proxy/: tls baz (200; 43.133255ms)
May 14 02:16:58.327: INFO: (16) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 41.944237ms)
May 14 02:16:58.336: INFO: (17) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">... (200; 7.683765ms)
May 14 02:16:58.337: INFO: (17) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/tlsrewritem... (200; 7.776842ms)
May 14 02:16:58.345: INFO: (17) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 13.10534ms)
May 14 02:16:58.348: INFO: (17) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname1/proxy/: tls baz (200; 16.216374ms)
May 14 02:16:58.348: INFO: (17) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:460/proxy/: tls baz (200; 17.606827ms)
May 14 02:16:58.353: INFO: (17) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:462/proxy/: tls qux (200; 22.353627ms)
May 14 02:16:58.354: INFO: (17) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 23.164817ms)
May 14 02:16:58.354: INFO: (17) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 23.729806ms)
May 14 02:16:58.354: INFO: (17) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname2/proxy/: tls qux (200; 24.781544ms)
May 14 02:16:58.354: INFO: (17) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname2/proxy/: bar (200; 22.189791ms)
May 14 02:16:58.354: INFO: (17) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 24.016746ms)
May 14 02:16:58.355: INFO: (17) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/rewriteme">test</a> (200; 22.609621ms)
May 14 02:16:58.355: INFO: (17) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname2/proxy/: bar (200; 24.710362ms)
May 14 02:16:58.356: INFO: (17) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname1/proxy/: foo (200; 25.387457ms)
May 14 02:16:58.356: INFO: (17) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname1/proxy/: foo (200; 26.214816ms)
May 14 02:16:58.355: INFO: (17) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">test<... (200; 25.038841ms)
May 14 02:16:58.365: INFO: (18) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 7.38689ms)
May 14 02:16:58.369: INFO: (18) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:462/proxy/: tls qux (200; 11.427939ms)
May 14 02:16:58.371: INFO: (18) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 13.709338ms)
May 14 02:16:58.371: INFO: (18) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">... (200; 13.566598ms)
May 14 02:16:58.390: INFO: (18) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 32.137ms)
May 14 02:16:58.391: INFO: (18) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/rewriteme">test</a> (200; 33.04644ms)
May 14 02:16:58.391: INFO: (18) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname1/proxy/: foo (200; 33.53704ms)
May 14 02:16:58.391: INFO: (18) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:460/proxy/: tls baz (200; 32.476979ms)
May 14 02:16:58.391: INFO: (18) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/tlsrewritem... (200; 33.857569ms)
May 14 02:16:58.391: INFO: (18) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname2/proxy/: bar (200; 34.051573ms)
May 14 02:16:58.393: INFO: (18) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">test<... (200; 35.144607ms)
May 14 02:16:58.393: INFO: (18) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 34.996353ms)
May 14 02:16:58.393: INFO: (18) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname2/proxy/: tls qux (200; 35.279881ms)
May 14 02:16:58.393: INFO: (18) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname2/proxy/: bar (200; 35.484977ms)
May 14 02:16:58.393: INFO: (18) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname1/proxy/: tls baz (200; 35.64892ms)
May 14 02:16:58.393: INFO: (18) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname1/proxy/: foo (200; 35.627745ms)
May 14 02:16:58.409: INFO: (19) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:443/proxy/tlsrewritem... (200; 15.532514ms)
May 14 02:16:58.410: INFO: (19) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname2/proxy/: tls qux (200; 16.011291ms)
May 14 02:16:58.410: INFO: (19) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname1/proxy/: foo (200; 16.894686ms)
May 14 02:16:58.411: INFO: (19) /api/v1/namespaces/proxy-5506/services/proxy-service-wpl2g:portname2/proxy/: bar (200; 17.211257ms)
May 14 02:16:58.416: INFO: (19) /api/v1/namespaces/proxy-5506/services/https:proxy-service-wpl2g:tlsportname1/proxy/: tls baz (200; 22.308394ms)
May 14 02:16:58.421: INFO: (19) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:462/proxy/: tls qux (200; 26.676624ms)
May 14 02:16:58.422: INFO: (19) /api/v1/namespaces/proxy-5506/pods/https:proxy-service-wpl2g-bjws6:460/proxy/: tls baz (200; 27.537709ms)
May 14 02:16:58.421: INFO: (19) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 27.289219ms)
May 14 02:16:58.423: INFO: (19) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 28.397451ms)
May 14 02:16:58.423: INFO: (19) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">... (200; 28.637428ms)
May 14 02:16:58.423: INFO: (19) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6/proxy/rewriteme">test</a> (200; 29.036705ms)
May 14 02:16:58.423: INFO: (19) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname1/proxy/: foo (200; 29.258299ms)
May 14 02:16:58.424: INFO: (19) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:1080/proxy/rewriteme">test<... (200; 30.482249ms)
May 14 02:16:58.425: INFO: (19) /api/v1/namespaces/proxy-5506/pods/proxy-service-wpl2g-bjws6:162/proxy/: bar (200; 30.258499ms)
May 14 02:16:58.432: INFO: (19) /api/v1/namespaces/proxy-5506/pods/http:proxy-service-wpl2g-bjws6:160/proxy/: foo (200; 36.930074ms)
May 14 02:16:58.432: INFO: (19) /api/v1/namespaces/proxy-5506/services/http:proxy-service-wpl2g:portname2/proxy/: bar (200; 38.685556ms)
STEP: deleting ReplicationController proxy-service-wpl2g in namespace proxy-5506, will wait for the garbage collector to delete the pods
May 14 02:16:58.504: INFO: Deleting ReplicationController proxy-service-wpl2g took: 11.985815ms
May 14 02:16:59.504: INFO: Terminating ReplicationController proxy-service-wpl2g pods took: 1.000374711s
[AfterEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:17:12.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5506" for this suite.
May 14 02:17:22.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:17:23.527: INFO: namespace proxy-5506 deletion completed in 10.608679382s

â€¢ [SLOW TEST:42.294 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:17:23.530: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2944
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-2944
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2944 to expose endpoints map[]
May 14 02:17:23.842: INFO: successfully validated that service endpoint-test2 in namespace services-2944 exposes endpoints map[] (8.092598ms elapsed)
STEP: Creating pod pod1 in namespace services-2944
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2944 to expose endpoints map[pod1:[80]]
May 14 02:17:29.080: INFO: successfully validated that service endpoint-test2 in namespace services-2944 exposes endpoints map[pod1:[80]] (5.214240357s elapsed)
STEP: Creating pod pod2 in namespace services-2944
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2944 to expose endpoints map[pod1:[80] pod2:[80]]
May 14 02:17:32.156: INFO: successfully validated that service endpoint-test2 in namespace services-2944 exposes endpoints map[pod1:[80] pod2:[80]] (3.06343943s elapsed)
STEP: Deleting pod pod1 in namespace services-2944
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2944 to expose endpoints map[pod2:[80]]
May 14 02:17:32.202: INFO: successfully validated that service endpoint-test2 in namespace services-2944 exposes endpoints map[pod2:[80]] (22.68231ms elapsed)
STEP: Deleting pod pod2 in namespace services-2944
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2944 to expose endpoints map[]
May 14 02:17:33.362: INFO: successfully validated that service endpoint-test2 in namespace services-2944 exposes endpoints map[] (1.148500162s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:17:35.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2944" for this suite.
May 14 02:18:03.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:18:03.948: INFO: namespace services-2944 deletion completed in 28.529252897s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:40.419 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:18:03.958: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9276
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
May 14 02:18:05.216: INFO: Waiting up to 5m0s for pod "client-containers-e532b4c7-6256-49d4-b78c-b6a402a75b22" in namespace "containers-9276" to be "success or failure"
May 14 02:18:05.229: INFO: Pod "client-containers-e532b4c7-6256-49d4-b78c-b6a402a75b22": Phase="Pending", Reason="", readiness=false. Elapsed: 12.413513ms
May 14 02:18:07.236: INFO: Pod "client-containers-e532b4c7-6256-49d4-b78c-b6a402a75b22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019888045s
May 14 02:18:09.604: INFO: Pod "client-containers-e532b4c7-6256-49d4-b78c-b6a402a75b22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.387280535s
May 14 02:18:11.612: INFO: Pod "client-containers-e532b4c7-6256-49d4-b78c-b6a402a75b22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.396173295s
STEP: Saw pod success
May 14 02:18:11.613: INFO: Pod "client-containers-e532b4c7-6256-49d4-b78c-b6a402a75b22" satisfied condition "success or failure"
May 14 02:18:11.619: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod client-containers-e532b4c7-6256-49d4-b78c-b6a402a75b22 container test-container: <nil>
STEP: delete the pod
May 14 02:18:11.759: INFO: Waiting for pod client-containers-e532b4c7-6256-49d4-b78c-b6a402a75b22 to disappear
May 14 02:18:11.764: INFO: Pod client-containers-e532b4c7-6256-49d4-b78c-b6a402a75b22 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:18:11.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9276" for this suite.
May 14 02:18:19.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:18:20.230: INFO: namespace containers-9276 deletion completed in 8.453201571s

â€¢ [SLOW TEST:16.274 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:18:20.235: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1608
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-e646f114-af56-4934-8273-a9b3d05ea327
STEP: Creating a pod to test consume secrets
May 14 02:18:20.952: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-103d0e63-4095-4870-bef2-a1594228dfcc" in namespace "projected-1608" to be "success or failure"
May 14 02:18:20.970: INFO: Pod "pod-projected-secrets-103d0e63-4095-4870-bef2-a1594228dfcc": Phase="Pending", Reason="", readiness=false. Elapsed: 17.445271ms
May 14 02:18:22.975: INFO: Pod "pod-projected-secrets-103d0e63-4095-4870-bef2-a1594228dfcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022270321s
May 14 02:18:24.981: INFO: Pod "pod-projected-secrets-103d0e63-4095-4870-bef2-a1594228dfcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028720214s
STEP: Saw pod success
May 14 02:18:24.982: INFO: Pod "pod-projected-secrets-103d0e63-4095-4870-bef2-a1594228dfcc" satisfied condition "success or failure"
May 14 02:18:24.991: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-projected-secrets-103d0e63-4095-4870-bef2-a1594228dfcc container projected-secret-volume-test: <nil>
STEP: delete the pod
May 14 02:18:25.056: INFO: Waiting for pod pod-projected-secrets-103d0e63-4095-4870-bef2-a1594228dfcc to disappear
May 14 02:18:25.067: INFO: Pod pod-projected-secrets-103d0e63-4095-4870-bef2-a1594228dfcc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:18:25.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1608" for this suite.
May 14 02:18:31.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:18:31.503: INFO: namespace projected-1608 deletion completed in 6.425461465s

â€¢ [SLOW TEST:11.269 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:18:31.511: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4962
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 14 02:18:41.881: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 14 02:18:41.896: INFO: Pod pod-with-poststart-http-hook still exists
May 14 02:18:43.898: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 14 02:18:43.908: INFO: Pod pod-with-poststart-http-hook still exists
May 14 02:18:45.897: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 14 02:18:45.904: INFO: Pod pod-with-poststart-http-hook still exists
May 14 02:18:47.897: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 14 02:18:47.903: INFO: Pod pod-with-poststart-http-hook still exists
May 14 02:18:49.897: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 14 02:18:55.767: INFO: Pod pod-with-poststart-http-hook still exists
May 14 02:18:55.897: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 14 02:18:58.055: INFO: Pod pod-with-poststart-http-hook still exists
May 14 02:18:59.897: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 14 02:18:59.903: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:18:59.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4962" for this suite.
May 14 02:19:12.245: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:19:12.703: INFO: namespace container-lifecycle-hook-4962 deletion completed in 12.79194012s

â€¢ [SLOW TEST:41.193 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:19:12.714: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2384
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2384.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2384.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2384.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2384.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2384.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2384.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2384.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2384.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2384.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2384.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2384.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2384.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2384.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2384.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2384.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2384.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2384.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2384.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 14 02:19:19.038: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2384.svc.cluster.local from pod dns-2384/dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd: the server could not find the requested resource (get pods dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd)
May 14 02:19:19.044: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2384.svc.cluster.local from pod dns-2384/dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd: the server could not find the requested resource (get pods dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd)
May 14 02:19:19.053: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2384.svc.cluster.local from pod dns-2384/dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd: the server could not find the requested resource (get pods dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd)
May 14 02:19:19.059: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2384.svc.cluster.local from pod dns-2384/dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd: the server could not find the requested resource (get pods dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd)
May 14 02:19:19.064: INFO: Unable to read wheezy_udp@PodARecord from pod dns-2384/dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd: the server could not find the requested resource (get pods dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd)
May 14 02:19:19.071: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-2384/dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd: the server could not find the requested resource (get pods dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd)
May 14 02:19:19.080: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2384.svc.cluster.local from pod dns-2384/dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd: the server could not find the requested resource (get pods dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd)
May 14 02:19:19.099: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2384.svc.cluster.local from pod dns-2384/dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd: the server could not find the requested resource (get pods dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd)
May 14 02:19:19.106: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2384.svc.cluster.local from pod dns-2384/dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd: the server could not find the requested resource (get pods dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd)
May 14 02:19:19.115: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2384.svc.cluster.local from pod dns-2384/dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd: the server could not find the requested resource (get pods dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd)
May 14 02:19:19.121: INFO: Unable to read jessie_udp@PodARecord from pod dns-2384/dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd: the server could not find the requested resource (get pods dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd)
May 14 02:19:19.137: INFO: Unable to read jessie_tcp@PodARecord from pod dns-2384/dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd: the server could not find the requested resource (get pods dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd)
May 14 02:19:19.137: INFO: Lookups using dns-2384/dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2384.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2384.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2384.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2384.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-querier-2.dns-test-service-2.dns-2384.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2384.svc.cluster.local jessie_udp@dns-test-service-2.dns-2384.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2384.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

May 14 02:19:24.224: INFO: DNS probes using dns-2384/dns-test-c0c35387-1154-4f22-b8ea-9a5988fcebfd succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:19:24.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2384" for this suite.
May 14 02:19:34.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:19:34.929: INFO: namespace dns-2384 deletion completed in 10.478865688s

â€¢ [SLOW TEST:22.216 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:19:34.942: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6720
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-a5b27a06-9e56-429c-9a9c-63dec80bb23a
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:19:37.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6720" for this suite.
May 14 02:19:43.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:19:43.695: INFO: namespace configmap-6720 deletion completed in 6.560307679s

â€¢ [SLOW TEST:8.754 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:19:43.706: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4381
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-c11e7254-ea98-493f-a4ce-13a1f95dacad
STEP: Creating a pod to test consume secrets
May 14 02:19:44.000: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e56d38fc-62bd-42bd-8fa1-4cedfc3e507b" in namespace "projected-4381" to be "success or failure"
May 14 02:19:44.019: INFO: Pod "pod-projected-secrets-e56d38fc-62bd-42bd-8fa1-4cedfc3e507b": Phase="Pending", Reason="", readiness=false. Elapsed: 19.628007ms
May 14 02:19:46.233: INFO: Pod "pod-projected-secrets-e56d38fc-62bd-42bd-8fa1-4cedfc3e507b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.232931278s
May 14 02:19:48.642: INFO: Pod "pod-projected-secrets-e56d38fc-62bd-42bd-8fa1-4cedfc3e507b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.641967296s
STEP: Saw pod success
May 14 02:19:48.642: INFO: Pod "pod-projected-secrets-e56d38fc-62bd-42bd-8fa1-4cedfc3e507b" satisfied condition "success or failure"
May 14 02:19:48.647: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-projected-secrets-e56d38fc-62bd-42bd-8fa1-4cedfc3e507b container projected-secret-volume-test: <nil>
STEP: delete the pod
May 14 02:19:48.764: INFO: Waiting for pod pod-projected-secrets-e56d38fc-62bd-42bd-8fa1-4cedfc3e507b to disappear
May 14 02:19:48.770: INFO: Pod pod-projected-secrets-e56d38fc-62bd-42bd-8fa1-4cedfc3e507b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:19:48.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4381" for this suite.
May 14 02:19:54.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:19:55.180: INFO: namespace projected-4381 deletion completed in 6.404561185s

â€¢ [SLOW TEST:11.476 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:19:55.197: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2300
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-2300
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 14 02:19:55.432: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 14 02:20:20.099: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.136.79:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2300 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 02:20:20.100: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
May 14 02:20:20.506: INFO: Found all expected endpoints: [netserver-0]
May 14 02:20:20.518: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.230.32:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2300 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 02:20:20.518: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
May 14 02:20:20.880: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:20:20.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2300" for this suite.
May 14 02:20:32.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:20:33.315: INFO: namespace pod-network-test-2300 deletion completed in 12.424644698s

â€¢ [SLOW TEST:38.119 seconds]
[sig-network] Networking
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:20:33.317: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9502
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
May 14 02:20:33.512: INFO: Waiting up to 5m0s for pod "pod-3befcc3b-a2ba-41d4-906f-4f92f3f89ce2" in namespace "emptydir-9502" to be "success or failure"
May 14 02:20:33.524: INFO: Pod "pod-3befcc3b-a2ba-41d4-906f-4f92f3f89ce2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.010196ms
May 14 02:20:35.919: INFO: Pod "pod-3befcc3b-a2ba-41d4-906f-4f92f3f89ce2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.407595741s
May 14 02:20:41.092: INFO: Pod "pod-3befcc3b-a2ba-41d4-906f-4f92f3f89ce2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.580367575s
May 14 02:20:43.110: INFO: Pod "pod-3befcc3b-a2ba-41d4-906f-4f92f3f89ce2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.597921306s
STEP: Saw pod success
May 14 02:20:43.110: INFO: Pod "pod-3befcc3b-a2ba-41d4-906f-4f92f3f89ce2" satisfied condition "success or failure"
May 14 02:20:43.152: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-3befcc3b-a2ba-41d4-906f-4f92f3f89ce2 container test-container: <nil>
STEP: delete the pod
May 14 02:20:43.195: INFO: Waiting for pod pod-3befcc3b-a2ba-41d4-906f-4f92f3f89ce2 to disappear
May 14 02:20:43.971: INFO: Pod pod-3befcc3b-a2ba-41d4-906f-4f92f3f89ce2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:20:43.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9502" for this suite.
May 14 02:20:50.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:20:50.500: INFO: namespace emptydir-9502 deletion completed in 6.515808361s

â€¢ [SLOW TEST:17.183 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:20:50.505: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7171
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May 14 02:20:50.758: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 14 02:20:50.786: INFO: Waiting for terminating namespaces to be deleted...
May 14 02:20:50.792: INFO: 
Logging pods the kubelet thinks is on node k8s-fcos-flwang-pyqjxt4oox23-node-0 before test
May 14 02:20:50.895: INFO: calico-node-d8klf from kube-system started at 2020-05-13 10:17:39 +0000 UTC (1 container statuses recorded)
May 14 02:20:50.895: INFO: 	Container calico-node ready: true, restart count 0
May 14 02:20:50.895: INFO: prometheus-operator-kube-state-metrics-54bd6c856f-v6wkd from kube-system started at 2020-05-13 10:20:16 +0000 UTC (1 container statuses recorded)
May 14 02:20:50.895: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 14 02:20:50.895: INFO: prometheus-prometheus-prometheus-0 from kube-system started at 2020-05-13 10:21:12 +0000 UTC (3 container statuses recorded)
May 14 02:20:50.895: INFO: 	Container prometheus ready: true, restart count 1
May 14 02:20:50.895: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May 14 02:20:50.895: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May 14 02:20:50.895: INFO: npd-w82fn from kube-system started at 2020-05-13 10:18:10 +0000 UTC (1 container statuses recorded)
May 14 02:20:50.895: INFO: 	Container node-problem-detector ready: true, restart count 0
May 14 02:20:50.895: INFO: metrics-server-649fdb57b9-jw8v2 from kube-system started at 2020-05-13 10:19:49 +0000 UTC (1 container statuses recorded)
May 14 02:20:50.895: INFO: 	Container metrics-server ready: true, restart count 0
May 14 02:20:50.895: INFO: kube-dns-autoscaler-78d89dc59b-tjwjl from kube-system started at 2020-05-13 10:18:10 +0000 UTC (1 container statuses recorded)
May 14 02:20:50.895: INFO: 	Container autoscaler ready: true, restart count 0
May 14 02:20:50.895: INFO: prometheus-operator-prometheus-node-exporter-dsbl2 from kube-system started at 2020-05-13 10:20:16 +0000 UTC (1 container statuses recorded)
May 14 02:20:50.895: INFO: 	Container node-exporter ready: true, restart count 0
May 14 02:20:50.895: INFO: prometheus-operator-grafana-597f5fdffc-2r2zm from kube-system started at 2020-05-13 10:20:16 +0000 UTC (2 container statuses recorded)
May 14 02:20:50.895: INFO: 	Container grafana ready: true, restart count 0
May 14 02:20:50.895: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
May 14 02:20:50.895: INFO: prometheus-operator-855b67c86c-m4rg8 from kube-system started at 2020-05-13 10:20:16 +0000 UTC (2 container statuses recorded)
May 14 02:20:50.895: INFO: 	Container prometheus ready: true, restart count 0
May 14 02:20:50.895: INFO: 	Container tls-proxy ready: true, restart count 0
May 14 02:20:50.895: INFO: sonobuoy-systemd-logs-daemon-set-2dde1b3f033a4765-q8j6w from sonobuoy started at 2020-05-14 01:43:10 +0000 UTC (2 container statuses recorded)
May 14 02:20:50.895: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 14 02:20:50.895: INFO: 	Container systemd-logs ready: true, restart count 0
May 14 02:20:50.895: INFO: install-prometheus-operator-job-kz4gd from magnum-tiller started at 2020-05-13 10:18:10 +0000 UTC (1 container statuses recorded)
May 14 02:20:50.895: INFO: 	Container config-helm ready: false, restart count 0
May 14 02:20:50.895: INFO: install-prometheus-adapter-job-pmgx5 from magnum-tiller started at 2020-05-13 10:18:10 +0000 UTC (1 container statuses recorded)
May 14 02:20:50.895: INFO: 	Container config-helm ready: false, restart count 0
May 14 02:20:50.895: INFO: install-metrics-server-job-r22dn from magnum-tiller started at 2020-05-13 10:18:20 +0000 UTC (1 container statuses recorded)
May 14 02:20:50.896: INFO: 	Container config-helm ready: false, restart count 0
May 14 02:20:50.896: INFO: prometheus-adapter-5c56dfd965-bfskz from kube-system started at 2020-05-13 10:19:49 +0000 UTC (1 container statuses recorded)
May 14 02:20:50.896: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 14 02:20:50.896: INFO: alertmanager-prometheus-alertmanager-0 from kube-system started at 2020-05-13 10:21:01 +0000 UTC (2 container statuses recorded)
May 14 02:20:50.896: INFO: 	Container alertmanager ready: true, restart count 0
May 14 02:20:50.896: INFO: 	Container config-reloader ready: true, restart count 0
May 14 02:20:50.896: INFO: 
Logging pods the kubelet thinks is on node k8s-fcos-flwang-pyqjxt4oox23-node-1 before test
May 14 02:20:50.924: INFO: sonobuoy from sonobuoy started at 2020-05-14 01:43:05 +0000 UTC (1 container statuses recorded)
May 14 02:20:50.924: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 14 02:20:50.925: INFO: sonobuoy-systemd-logs-daemon-set-2dde1b3f033a4765-4wkjn from sonobuoy started at 2020-05-14 01:43:10 +0000 UTC (2 container statuses recorded)
May 14 02:20:50.925: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 14 02:20:50.926: INFO: 	Container systemd-logs ready: true, restart count 0
May 14 02:20:50.926: INFO: calico-node-cjvph from kube-system started at 2020-05-13 20:00:24 +0000 UTC (1 container statuses recorded)
May 14 02:20:50.927: INFO: 	Container calico-node ready: true, restart count 0
May 14 02:20:50.927: INFO: npd-v8wfq from kube-system started at 2020-05-13 20:01:37 +0000 UTC (1 container statuses recorded)
May 14 02:20:50.928: INFO: 	Container node-problem-detector ready: true, restart count 0
May 14 02:20:50.929: INFO: prometheus-operator-prometheus-node-exporter-64t57 from kube-system started at 2020-05-13 20:00:24 +0000 UTC (1 container statuses recorded)
May 14 02:20:50.929: INFO: 	Container node-exporter ready: true, restart count 0
May 14 02:20:50.930: INFO: sonobuoy-e2e-job-cf5c047f756b45c4 from sonobuoy started at 2020-05-14 01:43:09 +0000 UTC (2 container statuses recorded)
May 14 02:20:50.931: INFO: 	Container e2e ready: true, restart count 0
May 14 02:20:50.931: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c8e035b5-941e-4544-aa94-bc537fea8d39 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-c8e035b5-941e-4544-aa94-bc537fea8d39 off the node k8s-fcos-flwang-pyqjxt4oox23-node-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c8e035b5-941e-4544-aa94-bc537fea8d39
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:21:01.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7171" for this suite.
May 14 02:21:15.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:21:15.779: INFO: namespace sched-pred-7171 deletion completed in 14.407089874s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:25.276 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:21:15.789: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5625
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-5625
STEP: creating replication controller nodeport-test in namespace services-5625
I0514 02:21:16.389045      20 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-5625, replica count: 2
I0514 02:21:19.441366      20 runners.go:184] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0514 02:21:22.442456      20 runners.go:184] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0514 02:21:25.443058      20 runners.go:184] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 14 02:21:28.444: INFO: Creating new exec pod
I0514 02:21:28.444085      20 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 14 02:21:35.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=services-5625 execpodlbldz -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
May 14 02:21:36.278: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 14 02:21:36.278: INFO: stdout: ""
May 14 02:21:36.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=services-5625 execpodlbldz -- /bin/sh -x -c nc -zv -t -w 2 10.254.144.33 80'
May 14 02:21:37.045: INFO: stderr: "+ nc -zv -t -w 2 10.254.144.33 80\nConnection to 10.254.144.33 80 port [tcp/http] succeeded!\n"
May 14 02:21:37.045: INFO: stdout: ""
May 14 02:21:37.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=services-5625 execpodlbldz -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.11 32028'
May 14 02:21:37.663: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.11 32028\nConnection to 10.0.0.11 32028 port [tcp/32028] succeeded!\n"
May 14 02:21:37.663: INFO: stdout: ""
May 14 02:21:37.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=services-5625 execpodlbldz -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.13 32028'
May 14 02:21:38.264: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.13 32028\nConnection to 10.0.0.13 32028 port [tcp/32028] succeeded!\n"
May 14 02:21:38.264: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:21:38.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5625" for this suite.
May 14 02:21:45.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:21:45.659: INFO: namespace services-5625 deletion completed in 6.947986056s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:29.871 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:21:45.664: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1306
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 02:21:45.949: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 14 02:21:46.009: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:21:46.070: INFO: Number of nodes with available pods: 0
May 14 02:21:46.070: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 02:21:47.098: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:21:47.104: INFO: Number of nodes with available pods: 0
May 14 02:21:47.104: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 02:21:48.090: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:21:48.095: INFO: Number of nodes with available pods: 0
May 14 02:21:48.095: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 02:21:49.078: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:21:49.084: INFO: Number of nodes with available pods: 0
May 14 02:21:49.084: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 02:21:50.079: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:21:50.085: INFO: Number of nodes with available pods: 0
May 14 02:21:50.085: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 02:21:51.081: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:21:51.087: INFO: Number of nodes with available pods: 1
May 14 02:21:51.088: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-1 is running more than one daemon pod
May 14 02:21:52.084: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:21:52.090: INFO: Number of nodes with available pods: 2
May 14 02:21:52.090: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 14 02:21:52.208: INFO: Wrong image for pod: daemon-set-gmbmg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:21:52.208: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:21:52.213: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:21:53.220: INFO: Wrong image for pod: daemon-set-gmbmg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:21:53.220: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:21:53.225: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:21:54.235: INFO: Wrong image for pod: daemon-set-gmbmg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:21:54.236: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:21:54.244: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:21:55.299: INFO: Wrong image for pod: daemon-set-gmbmg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:21:55.299: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:21:55.309: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:21:56.219: INFO: Wrong image for pod: daemon-set-gmbmg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:21:56.219: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:21:56.224: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:21:57.224: INFO: Wrong image for pod: daemon-set-gmbmg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:21:57.224: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:21:57.233: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:21:58.970: INFO: Wrong image for pod: daemon-set-gmbmg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:21:58.970: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:21:58.989: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:21:59.225: INFO: Wrong image for pod: daemon-set-gmbmg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:21:59.225: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:21:59.233: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:00.236: INFO: Wrong image for pod: daemon-set-gmbmg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:00.236: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:00.245: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:01.222: INFO: Wrong image for pod: daemon-set-gmbmg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:01.222: INFO: Pod daemon-set-gmbmg is not available
May 14 02:22:01.222: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:01.228: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:02.222: INFO: Pod daemon-set-kdpn4 is not available
May 14 02:22:02.222: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:02.229: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:03.222: INFO: Pod daemon-set-kdpn4 is not available
May 14 02:22:03.222: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:03.234: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:04.220: INFO: Pod daemon-set-kdpn4 is not available
May 14 02:22:04.221: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:04.227: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:05.219: INFO: Pod daemon-set-kdpn4 is not available
May 14 02:22:05.220: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:05.229: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:06.807: INFO: Pod daemon-set-kdpn4 is not available
May 14 02:22:06.807: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:06.820: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:07.642: INFO: Pod daemon-set-kdpn4 is not available
May 14 02:22:07.642: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:07.700: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:08.219: INFO: Pod daemon-set-kdpn4 is not available
May 14 02:22:08.219: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:08.224: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:09.226: INFO: Pod daemon-set-kdpn4 is not available
May 14 02:22:09.226: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:09.234: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:10.651: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:10.663: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:11.224: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:11.228: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:12.222: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:12.231: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:13.218: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:13.224: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:14.221: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:14.227: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:15.240: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:15.240: INFO: Pod daemon-set-xdxbw is not available
May 14 02:22:15.246: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:16.429: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:16.429: INFO: Pod daemon-set-xdxbw is not available
May 14 02:22:16.436: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:17.219: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:17.219: INFO: Pod daemon-set-xdxbw is not available
May 14 02:22:17.223: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:18.219: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:18.219: INFO: Pod daemon-set-xdxbw is not available
May 14 02:22:18.226: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:19.221: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:19.221: INFO: Pod daemon-set-xdxbw is not available
May 14 02:22:19.230: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:20.221: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:20.221: INFO: Pod daemon-set-xdxbw is not available
May 14 02:22:20.226: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:21.574: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:21.574: INFO: Pod daemon-set-xdxbw is not available
May 14 02:22:21.587: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:22.218: INFO: Wrong image for pod: daemon-set-xdxbw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 14 02:22:22.218: INFO: Pod daemon-set-xdxbw is not available
May 14 02:22:22.223: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:23.221: INFO: Pod daemon-set-tsw5p is not available
May 14 02:22:23.228: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
May 14 02:22:23.237: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:23.253: INFO: Number of nodes with available pods: 1
May 14 02:22:23.253: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-1 is running more than one daemon pod
May 14 02:22:24.259: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:24.263: INFO: Number of nodes with available pods: 1
May 14 02:22:24.263: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-1 is running more than one daemon pod
May 14 02:22:25.277: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:25.283: INFO: Number of nodes with available pods: 1
May 14 02:22:25.284: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-1 is running more than one daemon pod
May 14 02:22:26.265: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:26.272: INFO: Number of nodes with available pods: 1
May 14 02:22:26.272: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-1 is running more than one daemon pod
May 14 02:22:27.263: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:22:27.268: INFO: Number of nodes with available pods: 2
May 14 02:22:27.268: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1306, will wait for the garbage collector to delete the pods
May 14 02:22:27.395: INFO: Deleting DaemonSet.extensions daemon-set took: 12.362506ms
May 14 02:22:30.296: INFO: Terminating DaemonSet.extensions daemon-set pods took: 2.900365075s
May 14 02:22:48.207: INFO: Number of nodes with available pods: 0
May 14 02:22:48.207: INFO: Number of running nodes: 0, number of available pods: 0
May 14 02:22:48.221: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1306/daemonsets","resourceVersion":"187677"},"items":null}

May 14 02:22:48.227: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1306/pods","resourceVersion":"187677"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:22:48.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1306" for this suite.
May 14 02:22:56.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:22:57.666: INFO: namespace daemonsets-1306 deletion completed in 9.408183091s

â€¢ [SLOW TEST:72.002 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:22:57.677: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6324
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-6324
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 14 02:22:57.892: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 14 02:23:29.379: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.136.81:8080/dial?request=hostName&protocol=http&host=10.100.136.75&port=8080&tries=1'] Namespace:pod-network-test-6324 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 02:23:29.380: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
May 14 02:23:29.852: INFO: Waiting for endpoints: map[]
May 14 02:23:29.891: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.136.81:8080/dial?request=hostName&protocol=http&host=10.100.230.34&port=8080&tries=1'] Namespace:pod-network-test-6324 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 02:23:29.891: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
May 14 02:23:30.268: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:23:30.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6324" for this suite.
May 14 02:23:44.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:23:44.685: INFO: namespace pod-network-test-6324 deletion completed in 14.407752295s

â€¢ [SLOW TEST:47.010 seconds]
[sig-network] Networking
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:23:44.693: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3368
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-b83e13c8-9adb-410f-8099-6ffcdffe57da
STEP: Creating secret with name secret-projected-all-test-volume-18294b94-e486-4c09-a14c-841c890b15b1
STEP: Creating a pod to test Check all projections for projected volume plugin
May 14 02:23:45.166: INFO: Waiting up to 5m0s for pod "projected-volume-d59f6775-498e-4116-b35d-49119b18f2ac" in namespace "projected-3368" to be "success or failure"
May 14 02:23:45.179: INFO: Pod "projected-volume-d59f6775-498e-4116-b35d-49119b18f2ac": Phase="Pending", Reason="", readiness=false. Elapsed: 11.896869ms
May 14 02:23:47.187: INFO: Pod "projected-volume-d59f6775-498e-4116-b35d-49119b18f2ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020465688s
May 14 02:23:49.193: INFO: Pod "projected-volume-d59f6775-498e-4116-b35d-49119b18f2ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026851669s
STEP: Saw pod success
May 14 02:23:49.194: INFO: Pod "projected-volume-d59f6775-498e-4116-b35d-49119b18f2ac" satisfied condition "success or failure"
May 14 02:23:49.197: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod projected-volume-d59f6775-498e-4116-b35d-49119b18f2ac container projected-all-volume-test: <nil>
STEP: delete the pod
May 14 02:23:49.317: INFO: Waiting for pod projected-volume-d59f6775-498e-4116-b35d-49119b18f2ac to disappear
May 14 02:23:49.322: INFO: Pod projected-volume-d59f6775-498e-4116-b35d-49119b18f2ac no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:23:49.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3368" for this suite.
May 14 02:23:57.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:23:57.686: INFO: namespace projected-3368 deletion completed in 8.355055047s

â€¢ [SLOW TEST:12.994 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:23:57.690: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-7009
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 14 02:23:58.440: INFO: Pod name wrapped-volume-race-dec55698-1a70-4ffd-8bad-3cb693f15b9e: Found 0 pods out of 5
May 14 02:24:03.458: INFO: Pod name wrapped-volume-race-dec55698-1a70-4ffd-8bad-3cb693f15b9e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-dec55698-1a70-4ffd-8bad-3cb693f15b9e in namespace emptydir-wrapper-7009, will wait for the garbage collector to delete the pods
May 14 02:24:17.610: INFO: Deleting ReplicationController wrapped-volume-race-dec55698-1a70-4ffd-8bad-3cb693f15b9e took: 15.654036ms
May 14 02:24:19.811: INFO: Terminating ReplicationController wrapped-volume-race-dec55698-1a70-4ffd-8bad-3cb693f15b9e pods took: 2.200737609s
STEP: Creating RC which spawns configmap-volume pods
May 14 02:25:09.251: INFO: Pod name wrapped-volume-race-0393b30c-f88f-4be7-bc38-ca3798a4aae5: Found 0 pods out of 5
May 14 02:25:14.309: INFO: Pod name wrapped-volume-race-0393b30c-f88f-4be7-bc38-ca3798a4aae5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-0393b30c-f88f-4be7-bc38-ca3798a4aae5 in namespace emptydir-wrapper-7009, will wait for the garbage collector to delete the pods
May 14 02:25:40.647: INFO: Deleting ReplicationController wrapped-volume-race-0393b30c-f88f-4be7-bc38-ca3798a4aae5 took: 10.142831ms
May 14 02:25:41.748: INFO: Terminating ReplicationController wrapped-volume-race-0393b30c-f88f-4be7-bc38-ca3798a4aae5 pods took: 1.100871294s
STEP: Creating RC which spawns configmap-volume pods
May 14 02:26:30.214: INFO: Pod name wrapped-volume-race-04bb5e61-37bd-4b62-bc4a-ee513dd7b7de: Found 0 pods out of 5
May 14 02:26:35.252: INFO: Pod name wrapped-volume-race-04bb5e61-37bd-4b62-bc4a-ee513dd7b7de: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-04bb5e61-37bd-4b62-bc4a-ee513dd7b7de in namespace emptydir-wrapper-7009, will wait for the garbage collector to delete the pods
May 14 02:26:53.128: INFO: Deleting ReplicationController wrapped-volume-race-04bb5e61-37bd-4b62-bc4a-ee513dd7b7de took: 10.129276ms
May 14 02:26:54.130: INFO: Terminating ReplicationController wrapped-volume-race-04bb5e61-37bd-4b62-bc4a-ee513dd7b7de pods took: 1.002402553s
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:27:39.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7009" for this suite.
May 14 02:27:49.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:27:50.339: INFO: namespace emptydir-wrapper-7009 deletion completed in 10.389894618s

â€¢ [SLOW TEST:232.650 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:27:50.340: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1629
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 14 02:27:51.627: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 14 02:27:53.643: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725020071, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725020071, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725020071, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725020071, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 02:27:55.650: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725020071, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725020071, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725020071, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725020071, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 14 02:27:59.420: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:27:59.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1629" for this suite.
May 14 02:28:05.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:28:05.887: INFO: namespace webhook-1629 deletion completed in 6.442362268s
STEP: Destroying namespace "webhook-1629-markers" for this suite.
May 14 02:28:11.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:28:12.262: INFO: namespace webhook-1629-markers deletion completed in 6.3751501s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:21.947 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:28:12.302: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4732
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:28:12.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4732" for this suite.
May 14 02:28:18.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:28:18.812: INFO: namespace services-4732 deletion completed in 6.305189258s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:6.512 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:28:18.835: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1751
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-1751
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1751
STEP: Creating statefulset with conflicting port in namespace statefulset-1751
STEP: Waiting until pod test-pod will start running in namespace statefulset-1751
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1751
May 14 02:28:25.223: INFO: Observed stateful pod in namespace: statefulset-1751, name: ss-0, uid: 7c038252-907c-4f2c-b6c4-034654dadc89, status phase: Pending. Waiting for statefulset controller to delete.
May 14 02:28:25.381: INFO: Observed stateful pod in namespace: statefulset-1751, name: ss-0, uid: 7c038252-907c-4f2c-b6c4-034654dadc89, status phase: Failed. Waiting for statefulset controller to delete.
May 14 02:28:28.019: INFO: Observed stateful pod in namespace: statefulset-1751, name: ss-0, uid: 7c038252-907c-4f2c-b6c4-034654dadc89, status phase: Failed. Waiting for statefulset controller to delete.
May 14 02:28:28.029: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1751
STEP: Removing pod with conflicting port in namespace statefulset-1751
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1751 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May 14 02:28:32.415: INFO: Deleting all statefulset in ns statefulset-1751
May 14 02:28:32.429: INFO: Scaling statefulset ss to 0
May 14 02:28:42.468: INFO: Waiting for statefulset status.replicas updated to 0
May 14 02:28:42.474: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:28:42.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1751" for this suite.
May 14 02:28:50.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:28:50.933: INFO: namespace statefulset-1751 deletion completed in 8.386746377s

â€¢ [SLOW TEST:32.099 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:28:50.941: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4192
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
May 14 02:28:51.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 api-versions'
May 14 02:28:51.596: INFO: stderr: ""
May 14 02:28:51.596: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauditregistration.k8s.io/v1alpha1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ncustom.metrics.k8s.io/v1beta1\ndiscovery.k8s.io/v1alpha1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1alpha1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1alpha1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nsettings.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1alpha1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:28:51.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4192" for this suite.
May 14 02:28:57.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:28:57.979: INFO: namespace kubectl-4192 deletion completed in 6.370452721s

â€¢ [SLOW TEST:7.040 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:28:57.985: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1086
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 14 02:28:58.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-1086'
May 14 02:29:00.078: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 14 02:29:00.078: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
May 14 02:29:02.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 delete deployment e2e-test-httpd-deployment --namespace=kubectl-1086'
May 14 02:29:02.656: INFO: stderr: ""
May 14 02:29:02.656: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:29:02.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1086" for this suite.
May 14 02:30:58.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:30:59.739: INFO: namespace kubectl-1086 deletion completed in 1m57.006840828s

â€¢ [SLOW TEST:121.755 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:30:59.754: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5153
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-4cc4d287-e3ff-4b2c-ad8b-5aee94e4bfaf
STEP: Creating configMap with name cm-test-opt-upd-c317869c-6cd5-4522-9895-e06c14b4c126
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-4cc4d287-e3ff-4b2c-ad8b-5aee94e4bfaf
STEP: Updating configmap cm-test-opt-upd-c317869c-6cd5-4522-9895-e06c14b4c126
STEP: Creating configMap with name cm-test-opt-create-44c2eb0b-6730-428e-b239-f0ccd1386ccb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:31:08.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5153" for this suite.
May 14 02:31:20.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:31:21.005: INFO: namespace projected-5153 deletion completed in 12.602899001s

â€¢ [SLOW TEST:21.252 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:31:21.007: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9997
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6276
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8151
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:31:29.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9997" for this suite.
May 14 02:31:35.979: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:31:36.303: INFO: namespace namespaces-9997 deletion completed in 6.366213227s
STEP: Destroying namespace "nsdeletetest-6276" for this suite.
May 14 02:31:36.306: INFO: Namespace nsdeletetest-6276 was already deleted
STEP: Destroying namespace "nsdeletetest-8151" for this suite.
May 14 02:31:42.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:31:42.658: INFO: namespace nsdeletetest-8151 deletion completed in 6.35255104s

â€¢ [SLOW TEST:21.652 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:31:42.660: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-568
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-68c95982-bbd1-454b-9727-e4344c37695e
STEP: Creating a pod to test consume configMaps
May 14 02:31:42.886: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-482bb26b-f2a9-4e29-a70d-be1539b34613" in namespace "projected-568" to be "success or failure"
May 14 02:31:42.895: INFO: Pod "pod-projected-configmaps-482bb26b-f2a9-4e29-a70d-be1539b34613": Phase="Pending", Reason="", readiness=false. Elapsed: 8.918972ms
May 14 02:31:44.901: INFO: Pod "pod-projected-configmaps-482bb26b-f2a9-4e29-a70d-be1539b34613": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015297931s
May 14 02:31:46.911: INFO: Pod "pod-projected-configmaps-482bb26b-f2a9-4e29-a70d-be1539b34613": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025013004s
STEP: Saw pod success
May 14 02:31:46.911: INFO: Pod "pod-projected-configmaps-482bb26b-f2a9-4e29-a70d-be1539b34613" satisfied condition "success or failure"
May 14 02:31:46.914: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-projected-configmaps-482bb26b-f2a9-4e29-a70d-be1539b34613 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 14 02:31:46.962: INFO: Waiting for pod pod-projected-configmaps-482bb26b-f2a9-4e29-a70d-be1539b34613 to disappear
May 14 02:31:46.977: INFO: Pod pod-projected-configmaps-482bb26b-f2a9-4e29-a70d-be1539b34613 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:31:46.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-568" for this suite.
May 14 02:31:53.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:31:53.411: INFO: namespace projected-568 deletion completed in 6.42229037s

â€¢ [SLOW TEST:10.751 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:31:53.412: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3515
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-55c9de7f-85b3-40db-b961-e783101695c3
STEP: Creating a pod to test consume secrets
May 14 02:31:53.633: INFO: Waiting up to 5m0s for pod "pod-secrets-398d6e2c-0ad6-4e0c-bfff-7d58e73fde5f" in namespace "secrets-3515" to be "success or failure"
May 14 02:31:53.956: INFO: Pod "pod-secrets-398d6e2c-0ad6-4e0c-bfff-7d58e73fde5f": Phase="Pending", Reason="", readiness=false. Elapsed: 320.53971ms
May 14 02:31:55.964: INFO: Pod "pod-secrets-398d6e2c-0ad6-4e0c-bfff-7d58e73fde5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.328814202s
May 14 02:31:57.972: INFO: Pod "pod-secrets-398d6e2c-0ad6-4e0c-bfff-7d58e73fde5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.336618683s
STEP: Saw pod success
May 14 02:31:57.972: INFO: Pod "pod-secrets-398d6e2c-0ad6-4e0c-bfff-7d58e73fde5f" satisfied condition "success or failure"
May 14 02:31:57.975: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-secrets-398d6e2c-0ad6-4e0c-bfff-7d58e73fde5f container secret-env-test: <nil>
STEP: delete the pod
May 14 02:31:58.007: INFO: Waiting for pod pod-secrets-398d6e2c-0ad6-4e0c-bfff-7d58e73fde5f to disappear
May 14 02:31:58.013: INFO: Pod pod-secrets-398d6e2c-0ad6-4e0c-bfff-7d58e73fde5f no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:31:58.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3515" for this suite.
May 14 02:32:04.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:32:04.612: INFO: namespace secrets-3515 deletion completed in 6.583178958s

â€¢ [SLOW TEST:11.202 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:32:04.633: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8689
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 02:32:04.891: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:32:09.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8689" for this suite.
May 14 02:32:53.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:32:53.572: INFO: namespace pods-8689 deletion completed in 44.469481439s

â€¢ [SLOW TEST:48.940 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:32:53.577: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2663
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-d0f606c1-c389-4c7c-8b19-cacaac75e73e
STEP: Creating a pod to test consume secrets
May 14 02:32:53.800: INFO: Waiting up to 5m0s for pod "pod-secrets-847651fb-64ed-4da2-8ba0-19e4bceef56c" in namespace "secrets-2663" to be "success or failure"
May 14 02:32:53.817: INFO: Pod "pod-secrets-847651fb-64ed-4da2-8ba0-19e4bceef56c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.630028ms
May 14 02:32:55.821: INFO: Pod "pod-secrets-847651fb-64ed-4da2-8ba0-19e4bceef56c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020930657s
May 14 02:32:57.828: INFO: Pod "pod-secrets-847651fb-64ed-4da2-8ba0-19e4bceef56c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027603463s
STEP: Saw pod success
May 14 02:32:57.828: INFO: Pod "pod-secrets-847651fb-64ed-4da2-8ba0-19e4bceef56c" satisfied condition "success or failure"
May 14 02:32:57.831: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-secrets-847651fb-64ed-4da2-8ba0-19e4bceef56c container secret-volume-test: <nil>
STEP: delete the pod
May 14 02:32:57.868: INFO: Waiting for pod pod-secrets-847651fb-64ed-4da2-8ba0-19e4bceef56c to disappear
May 14 02:32:57.876: INFO: Pod pod-secrets-847651fb-64ed-4da2-8ba0-19e4bceef56c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:32:57.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2663" for this suite.
May 14 02:33:03.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:33:04.227: INFO: namespace secrets-2663 deletion completed in 6.341446703s

â€¢ [SLOW TEST:10.650 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:33:04.232: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6885
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-2f646cc3-eee7-41c8-b6e9-914f18b606db
STEP: Creating configMap with name cm-test-opt-upd-e77a72b3-132c-4dbf-b1f1-09e1dfb720e1
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-2f646cc3-eee7-41c8-b6e9-914f18b606db
STEP: Updating configmap cm-test-opt-upd-e77a72b3-132c-4dbf-b1f1-09e1dfb720e1
STEP: Creating configMap with name cm-test-opt-create-821acae6-e255-435b-9be1-ecc89b2c8871
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:34:34.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6885" for this suite.
May 14 02:34:46.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:34:46.916: INFO: namespace configmap-6885 deletion completed in 12.38298194s

â€¢ [SLOW TEST:102.685 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:34:46.926: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1657
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 14 02:34:51.264: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:34:51.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1657" for this suite.
May 14 02:34:57.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:34:57.599: INFO: namespace container-runtime-1657 deletion completed in 6.298986048s

â€¢ [SLOW TEST:10.673 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:34:57.602: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6012
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 14 02:34:58.962: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 14 02:35:00.994: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725020498, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725020498, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725020499, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725020498, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 14 02:35:04.030: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 02:35:04.038: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:35:05.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6012" for this suite.
May 14 02:35:12.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:35:12.655: INFO: namespace webhook-6012 deletion completed in 6.941112257s
STEP: Destroying namespace "webhook-6012-markers" for this suite.
May 14 02:35:18.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:35:19.150: INFO: namespace webhook-6012-markers deletion completed in 6.494449642s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:21.579 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:35:19.181: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-801
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:35:26.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-801" for this suite.
May 14 02:35:38.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:35:38.960: INFO: namespace replication-controller-801 deletion completed in 12.461533791s

â€¢ [SLOW TEST:19.779 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:35:38.975: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9965
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9965.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9965.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9965.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9965.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9965.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9965.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 14 02:35:47.319: INFO: Unable to read wheezy_udp@PodARecord from pod dns-9965/dns-test-132477e8-3893-4504-a58c-4c544c32e24a: the server could not find the requested resource (get pods dns-test-132477e8-3893-4504-a58c-4c544c32e24a)
May 14 02:35:47.330: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-9965/dns-test-132477e8-3893-4504-a58c-4c544c32e24a: the server could not find the requested resource (get pods dns-test-132477e8-3893-4504-a58c-4c544c32e24a)
May 14 02:35:47.350: INFO: Unable to read jessie_udp@PodARecord from pod dns-9965/dns-test-132477e8-3893-4504-a58c-4c544c32e24a: the server could not find the requested resource (get pods dns-test-132477e8-3893-4504-a58c-4c544c32e24a)
May 14 02:35:47.356: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9965/dns-test-132477e8-3893-4504-a58c-4c544c32e24a: the server could not find the requested resource (get pods dns-test-132477e8-3893-4504-a58c-4c544c32e24a)
May 14 02:35:47.356: INFO: Lookups using dns-9965/dns-test-132477e8-3893-4504-a58c-4c544c32e24a failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

May 14 02:35:52.417: INFO: DNS probes using dns-9965/dns-test-132477e8-3893-4504-a58c-4c544c32e24a succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:35:52.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9965" for this suite.
May 14 02:35:58.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:35:58.991: INFO: namespace dns-9965 deletion completed in 6.446379706s

â€¢ [SLOW TEST:20.017 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:35:58.995: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5310
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 02:35:59.249: INFO: Create a RollingUpdate DaemonSet
May 14 02:35:59.261: INFO: Check that daemon pods launch on every node of the cluster
May 14 02:35:59.284: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:35:59.342: INFO: Number of nodes with available pods: 0
May 14 02:35:59.343: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 02:36:00.354: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:36:00.359: INFO: Number of nodes with available pods: 0
May 14 02:36:00.359: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 02:36:01.353: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:36:01.359: INFO: Number of nodes with available pods: 0
May 14 02:36:01.359: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 02:36:02.349: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:36:02.356: INFO: Number of nodes with available pods: 0
May 14 02:36:02.356: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 02:36:03.352: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:36:03.361: INFO: Number of nodes with available pods: 1
May 14 02:36:03.361: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 02:36:04.352: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:36:04.366: INFO: Number of nodes with available pods: 1
May 14 02:36:04.366: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 02:36:05.349: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:36:05.354: INFO: Number of nodes with available pods: 1
May 14 02:36:05.354: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 02:36:06.352: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:36:06.359: INFO: Number of nodes with available pods: 1
May 14 02:36:06.359: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 02:36:07.349: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:36:07.354: INFO: Number of nodes with available pods: 1
May 14 02:36:07.354: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 02:36:08.351: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:36:08.355: INFO: Number of nodes with available pods: 2
May 14 02:36:08.355: INFO: Number of running nodes: 2, number of available pods: 2
May 14 02:36:08.356: INFO: Update the DaemonSet to trigger a rollout
May 14 02:36:08.380: INFO: Updating DaemonSet daemon-set
May 14 02:36:14.419: INFO: Roll back the DaemonSet before rollout is complete
May 14 02:36:14.431: INFO: Updating DaemonSet daemon-set
May 14 02:36:14.431: INFO: Make sure DaemonSet rollback is complete
May 14 02:36:14.438: INFO: Wrong image for pod: daemon-set-mrn9c. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 14 02:36:14.439: INFO: Pod daemon-set-mrn9c is not available
May 14 02:36:14.449: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 02:36:15.456: INFO: Pod daemon-set-hbqch is not available
May 14 02:36:15.461: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5310, will wait for the garbage collector to delete the pods
May 14 02:36:15.546: INFO: Deleting DaemonSet.extensions daemon-set took: 15.45107ms
May 14 02:36:16.546: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.000684238s
May 14 02:36:22.658: INFO: Number of nodes with available pods: 0
May 14 02:36:22.658: INFO: Number of running nodes: 0, number of available pods: 0
May 14 02:36:22.661: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5310/daemonsets","resourceVersion":"191760"},"items":null}

May 14 02:36:22.686: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5310/pods","resourceVersion":"191760"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:36:22.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5310" for this suite.
May 14 02:36:28.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:36:30.073: INFO: namespace daemonsets-5310 deletion completed in 7.355494829s

â€¢ [SLOW TEST:31.078 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:36:30.085: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8361
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May 14 02:36:30.281: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 14 02:36:30.302: INFO: Waiting for terminating namespaces to be deleted...
May 14 02:36:30.316: INFO: 
Logging pods the kubelet thinks is on node k8s-fcos-flwang-pyqjxt4oox23-node-0 before test
May 14 02:36:30.429: INFO: kube-dns-autoscaler-78d89dc59b-tjwjl from kube-system started at 2020-05-13 10:18:10 +0000 UTC (1 container statuses recorded)
May 14 02:36:30.430: INFO: 	Container autoscaler ready: true, restart count 0
May 14 02:36:30.430: INFO: prometheus-operator-prometheus-node-exporter-dsbl2 from kube-system started at 2020-05-13 10:20:16 +0000 UTC (1 container statuses recorded)
May 14 02:36:30.430: INFO: 	Container node-exporter ready: true, restart count 0
May 14 02:36:30.430: INFO: prometheus-operator-grafana-597f5fdffc-2r2zm from kube-system started at 2020-05-13 10:20:16 +0000 UTC (2 container statuses recorded)
May 14 02:36:30.430: INFO: 	Container grafana ready: true, restart count 0
May 14 02:36:30.430: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
May 14 02:36:30.431: INFO: prometheus-operator-855b67c86c-m4rg8 from kube-system started at 2020-05-13 10:20:16 +0000 UTC (2 container statuses recorded)
May 14 02:36:30.433: INFO: 	Container prometheus ready: true, restart count 0
May 14 02:36:30.433: INFO: 	Container tls-proxy ready: true, restart count 0
May 14 02:36:30.433: INFO: sonobuoy-systemd-logs-daemon-set-2dde1b3f033a4765-q8j6w from sonobuoy started at 2020-05-14 01:43:10 +0000 UTC (2 container statuses recorded)
May 14 02:36:30.433: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 14 02:36:30.433: INFO: 	Container systemd-logs ready: true, restart count 0
May 14 02:36:30.433: INFO: install-prometheus-operator-job-kz4gd from magnum-tiller started at 2020-05-13 10:18:10 +0000 UTC (1 container statuses recorded)
May 14 02:36:30.434: INFO: 	Container config-helm ready: false, restart count 0
May 14 02:36:30.434: INFO: install-prometheus-adapter-job-pmgx5 from magnum-tiller started at 2020-05-13 10:18:10 +0000 UTC (1 container statuses recorded)
May 14 02:36:30.434: INFO: 	Container config-helm ready: false, restart count 0
May 14 02:36:30.434: INFO: install-metrics-server-job-r22dn from magnum-tiller started at 2020-05-13 10:18:20 +0000 UTC (1 container statuses recorded)
May 14 02:36:30.434: INFO: 	Container config-helm ready: false, restart count 0
May 14 02:36:30.434: INFO: prometheus-adapter-5c56dfd965-bfskz from kube-system started at 2020-05-13 10:19:49 +0000 UTC (1 container statuses recorded)
May 14 02:36:30.435: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 14 02:36:30.435: INFO: alertmanager-prometheus-alertmanager-0 from kube-system started at 2020-05-13 10:21:01 +0000 UTC (2 container statuses recorded)
May 14 02:36:30.435: INFO: 	Container alertmanager ready: true, restart count 0
May 14 02:36:30.435: INFO: 	Container config-reloader ready: true, restart count 0
May 14 02:36:30.435: INFO: calico-node-d8klf from kube-system started at 2020-05-13 10:17:39 +0000 UTC (1 container statuses recorded)
May 14 02:36:30.436: INFO: 	Container calico-node ready: true, restart count 0
May 14 02:36:30.436: INFO: prometheus-operator-kube-state-metrics-54bd6c856f-v6wkd from kube-system started at 2020-05-13 10:20:16 +0000 UTC (1 container statuses recorded)
May 14 02:36:30.436: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 14 02:36:30.436: INFO: prometheus-prometheus-prometheus-0 from kube-system started at 2020-05-13 10:21:12 +0000 UTC (3 container statuses recorded)
May 14 02:36:30.437: INFO: 	Container prometheus ready: true, restart count 1
May 14 02:36:30.437: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May 14 02:36:30.437: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May 14 02:36:30.437: INFO: npd-w82fn from kube-system started at 2020-05-13 10:18:10 +0000 UTC (1 container statuses recorded)
May 14 02:36:30.437: INFO: 	Container node-problem-detector ready: true, restart count 0
May 14 02:36:30.438: INFO: metrics-server-649fdb57b9-jw8v2 from kube-system started at 2020-05-13 10:19:49 +0000 UTC (1 container statuses recorded)
May 14 02:36:30.438: INFO: 	Container metrics-server ready: true, restart count 0
May 14 02:36:30.438: INFO: 
Logging pods the kubelet thinks is on node k8s-fcos-flwang-pyqjxt4oox23-node-1 before test
May 14 02:36:30.530: INFO: calico-node-cjvph from kube-system started at 2020-05-13 20:00:24 +0000 UTC (1 container statuses recorded)
May 14 02:36:30.530: INFO: 	Container calico-node ready: true, restart count 0
May 14 02:36:30.530: INFO: npd-v8wfq from kube-system started at 2020-05-13 20:01:37 +0000 UTC (1 container statuses recorded)
May 14 02:36:30.530: INFO: 	Container node-problem-detector ready: true, restart count 0
May 14 02:36:30.530: INFO: prometheus-operator-prometheus-node-exporter-64t57 from kube-system started at 2020-05-13 20:00:24 +0000 UTC (1 container statuses recorded)
May 14 02:36:30.530: INFO: 	Container node-exporter ready: true, restart count 0
May 14 02:36:30.531: INFO: sonobuoy-e2e-job-cf5c047f756b45c4 from sonobuoy started at 2020-05-14 01:43:09 +0000 UTC (2 container statuses recorded)
May 14 02:36:30.531: INFO: 	Container e2e ready: true, restart count 0
May 14 02:36:30.531: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 14 02:36:30.531: INFO: sonobuoy from sonobuoy started at 2020-05-14 01:43:05 +0000 UTC (1 container statuses recorded)
May 14 02:36:30.531: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 14 02:36:30.531: INFO: sonobuoy-systemd-logs-daemon-set-2dde1b3f033a4765-4wkjn from sonobuoy started at 2020-05-14 01:43:10 +0000 UTC (2 container statuses recorded)
May 14 02:36:30.531: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 14 02:36:30.531: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.160ec484336ec3e2], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:36:31.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8361" for this suite.
May 14 02:36:37.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:36:38.063: INFO: namespace sched-pred-8361 deletion completed in 6.428511121s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:7.979 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:36:38.065: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7775
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:36:42.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7775" for this suite.
May 14 02:37:28.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:37:28.731: INFO: namespace kubelet-test-7775 deletion completed in 46.371840294s

â€¢ [SLOW TEST:50.667 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:37:28.751: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-428
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
May 14 02:37:29.780: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
May 14 02:37:35.910: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:38:01.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-428" for this suite.
May 14 02:38:08.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:38:08.728: INFO: namespace crd-publish-openapi-428 deletion completed in 6.565059664s

â€¢ [SLOW TEST:39.978 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:38:08.732: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2359
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:38:09.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2359" for this suite.
May 14 02:38:15.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:38:15.440: INFO: namespace resourcequota-2359 deletion completed in 6.31881942s

â€¢ [SLOW TEST:6.708 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:38:15.441: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4318
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
May 14 02:38:15.658: INFO: Waiting up to 5m0s for pod "pod-eb73feff-f74e-4e9f-8d76-e98606108146" in namespace "emptydir-4318" to be "success or failure"
May 14 02:38:15.673: INFO: Pod "pod-eb73feff-f74e-4e9f-8d76-e98606108146": Phase="Pending", Reason="", readiness=false. Elapsed: 14.610001ms
May 14 02:38:17.710: INFO: Pod "pod-eb73feff-f74e-4e9f-8d76-e98606108146": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051480194s
May 14 02:38:19.737: INFO: Pod "pod-eb73feff-f74e-4e9f-8d76-e98606108146": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.078519474s
STEP: Saw pod success
May 14 02:38:19.738: INFO: Pod "pod-eb73feff-f74e-4e9f-8d76-e98606108146" satisfied condition "success or failure"
May 14 02:38:19.744: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-eb73feff-f74e-4e9f-8d76-e98606108146 container test-container: <nil>
STEP: delete the pod
May 14 02:38:19.856: INFO: Waiting for pod pod-eb73feff-f74e-4e9f-8d76-e98606108146 to disappear
May 14 02:38:19.864: INFO: Pod pod-eb73feff-f74e-4e9f-8d76-e98606108146 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:38:19.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4318" for this suite.
May 14 02:38:25.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:38:26.176: INFO: namespace emptydir-4318 deletion completed in 6.298888243s

â€¢ [SLOW TEST:10.735 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:38:26.178: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3483
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 14 02:38:26.489: INFO: Waiting up to 5m0s for pod "downwardapi-volume-19de2b5c-8f64-4a2d-bea4-2fd24d1dc301" in namespace "downward-api-3483" to be "success or failure"
May 14 02:38:26.515: INFO: Pod "downwardapi-volume-19de2b5c-8f64-4a2d-bea4-2fd24d1dc301": Phase="Pending", Reason="", readiness=false. Elapsed: 25.651943ms
May 14 02:38:28.523: INFO: Pod "downwardapi-volume-19de2b5c-8f64-4a2d-bea4-2fd24d1dc301": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033485891s
May 14 02:38:30.540: INFO: Pod "downwardapi-volume-19de2b5c-8f64-4a2d-bea4-2fd24d1dc301": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050107554s
May 14 02:38:32.545: INFO: Pod "downwardapi-volume-19de2b5c-8f64-4a2d-bea4-2fd24d1dc301": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055786845s
STEP: Saw pod success
May 14 02:38:32.545: INFO: Pod "downwardapi-volume-19de2b5c-8f64-4a2d-bea4-2fd24d1dc301" satisfied condition "success or failure"
May 14 02:38:32.550: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod downwardapi-volume-19de2b5c-8f64-4a2d-bea4-2fd24d1dc301 container client-container: <nil>
STEP: delete the pod
May 14 02:38:32.588: INFO: Waiting for pod downwardapi-volume-19de2b5c-8f64-4a2d-bea4-2fd24d1dc301 to disappear
May 14 02:38:32.594: INFO: Pod downwardapi-volume-19de2b5c-8f64-4a2d-bea4-2fd24d1dc301 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:38:32.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3483" for this suite.
May 14 02:38:38.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:38:39.580: INFO: namespace downward-api-3483 deletion completed in 6.977988489s

â€¢ [SLOW TEST:13.403 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:38:39.594: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3315
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-3315/configmap-test-44212cd7-9dfa-406d-9506-fe4ad58a1669
STEP: Creating a pod to test consume configMaps
May 14 02:38:39.968: INFO: Waiting up to 5m0s for pod "pod-configmaps-2e4a65b3-2b58-4d1e-aca3-9c7587397557" in namespace "configmap-3315" to be "success or failure"
May 14 02:38:39.986: INFO: Pod "pod-configmaps-2e4a65b3-2b58-4d1e-aca3-9c7587397557": Phase="Pending", Reason="", readiness=false. Elapsed: 17.925772ms
May 14 02:38:41.994: INFO: Pod "pod-configmaps-2e4a65b3-2b58-4d1e-aca3-9c7587397557": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025838948s
May 14 02:38:43.999: INFO: Pod "pod-configmaps-2e4a65b3-2b58-4d1e-aca3-9c7587397557": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031401042s
STEP: Saw pod success
May 14 02:38:44.000: INFO: Pod "pod-configmaps-2e4a65b3-2b58-4d1e-aca3-9c7587397557" satisfied condition "success or failure"
May 14 02:38:44.002: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-configmaps-2e4a65b3-2b58-4d1e-aca3-9c7587397557 container env-test: <nil>
STEP: delete the pod
May 14 02:38:44.045: INFO: Waiting for pod pod-configmaps-2e4a65b3-2b58-4d1e-aca3-9c7587397557 to disappear
May 14 02:38:44.052: INFO: Pod pod-configmaps-2e4a65b3-2b58-4d1e-aca3-9c7587397557 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:38:44.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3315" for this suite.
May 14 02:38:50.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:38:50.472: INFO: namespace configmap-3315 deletion completed in 6.412196309s

â€¢ [SLOW TEST:10.879 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:38:50.485: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-1589
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 02:38:51.010: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Creating first CR 
May 14 02:38:52.711: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-14T02:38:52Z generation:1 name:name1 resourceVersion:192384 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:d2dcf1d3-478c-41ad-a85a-0e1bb74eb60d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
May 14 02:39:02.735: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-14T02:39:02Z generation:1 name:name2 resourceVersion:192410 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:8204c1bb-2c8d-43ec-9c5b-75a1bba2060d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
May 14 02:39:12.759: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-14T02:38:52Z generation:2 name:name1 resourceVersion:192434 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:d2dcf1d3-478c-41ad-a85a-0e1bb74eb60d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
May 14 02:39:22.782: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-14T02:39:02Z generation:2 name:name2 resourceVersion:192461 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:8204c1bb-2c8d-43ec-9c5b-75a1bba2060d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
May 14 02:39:32.802: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-14T02:38:52Z generation:2 name:name1 resourceVersion:192488 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:d2dcf1d3-478c-41ad-a85a-0e1bb74eb60d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
May 14 02:39:42.817: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-14T02:39:02Z generation:2 name:name2 resourceVersion:192515 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:8204c1bb-2c8d-43ec-9c5b-75a1bba2060d] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:39:53.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-1589" for this suite.
May 14 02:39:59.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:39:59.824: INFO: namespace crd-watch-1589 deletion completed in 6.443179537s

â€¢ [SLOW TEST:69.339 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:39:59.826: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-2700
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2700
I0514 02:40:00.122205      20 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2700, replica count: 1
I0514 02:40:01.174455      20 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0514 02:40:02.174762      20 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0514 02:40:03.177722      20 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0514 02:40:04.178614      20 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0514 02:40:05.179039      20 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 14 02:40:05.421: INFO: Created: latency-svc-b62pl
May 14 02:40:05.442: INFO: Got endpoints: latency-svc-b62pl [161.534252ms]
May 14 02:40:05.487: INFO: Created: latency-svc-tpd29
May 14 02:40:05.492: INFO: Created: latency-svc-pj9hs
May 14 02:40:05.493: INFO: Got endpoints: latency-svc-pj9hs [49.774251ms]
May 14 02:40:05.499: INFO: Created: latency-svc-q27nc
May 14 02:40:05.509: INFO: Got endpoints: latency-svc-tpd29 [61.406117ms]
May 14 02:40:05.529: INFO: Got endpoints: latency-svc-q27nc [83.176777ms]
May 14 02:40:05.552: INFO: Created: latency-svc-tlfhc
May 14 02:40:05.552: INFO: Created: latency-svc-4w52r
May 14 02:40:05.552: INFO: Got endpoints: latency-svc-4w52r [59.81353ms]
May 14 02:40:05.566: INFO: Created: latency-svc-nwv9k
May 14 02:40:05.569: INFO: Got endpoints: latency-svc-tlfhc [120.379955ms]
May 14 02:40:05.576: INFO: Got endpoints: latency-svc-nwv9k [129.738684ms]
May 14 02:40:05.585: INFO: Created: latency-svc-h9b5x
May 14 02:40:05.593: INFO: Created: latency-svc-jbw8j
May 14 02:40:05.611: INFO: Got endpoints: latency-svc-h9b5x [163.73163ms]
May 14 02:40:05.612: INFO: Got endpoints: latency-svc-jbw8j [164.012721ms]
May 14 02:40:05.619: INFO: Created: latency-svc-cgxnm
May 14 02:40:05.646: INFO: Created: latency-svc-5mzhz
May 14 02:40:05.647: INFO: Created: latency-svc-2xfkk
May 14 02:40:05.655: INFO: Got endpoints: latency-svc-5mzhz [207.052612ms]
May 14 02:40:05.656: INFO: Got endpoints: latency-svc-cgxnm [209.318656ms]
May 14 02:40:05.669: INFO: Created: latency-svc-6tfm4
May 14 02:40:05.679: INFO: Got endpoints: latency-svc-2xfkk [232.006395ms]
May 14 02:40:05.682: INFO: Created: latency-svc-hzxrq
May 14 02:40:05.699: INFO: Got endpoints: latency-svc-6tfm4 [251.481831ms]
May 14 02:40:05.703: INFO: Created: latency-svc-d7gb5
May 14 02:40:05.710: INFO: Got endpoints: latency-svc-hzxrq [260.53682ms]
May 14 02:40:05.725: INFO: Got endpoints: latency-svc-d7gb5 [275.645708ms]
May 14 02:40:05.725: INFO: Created: latency-svc-wvcp5
May 14 02:40:05.730: INFO: Got endpoints: latency-svc-wvcp5 [280.762476ms]
May 14 02:40:05.740: INFO: Created: latency-svc-b8f86
May 14 02:40:05.767: INFO: Created: latency-svc-fsb6d
May 14 02:40:05.768: INFO: Got endpoints: latency-svc-b8f86 [323.050757ms]
May 14 02:40:05.775: INFO: Got endpoints: latency-svc-fsb6d [265.400517ms]
May 14 02:40:05.775: INFO: Created: latency-svc-2blz9
May 14 02:40:05.794: INFO: Created: latency-svc-wbj2x
May 14 02:40:05.796: INFO: Got endpoints: latency-svc-2blz9 [266.707081ms]
May 14 02:40:05.811: INFO: Got endpoints: latency-svc-wbj2x [258.708634ms]
May 14 02:40:05.812: INFO: Created: latency-svc-gdzk7
May 14 02:40:05.821: INFO: Got endpoints: latency-svc-gdzk7 [252.133575ms]
May 14 02:40:05.834: INFO: Created: latency-svc-ltxwd
May 14 02:40:05.849: INFO: Got endpoints: latency-svc-ltxwd [272.780426ms]
May 14 02:40:05.851: INFO: Created: latency-svc-5tbmg
May 14 02:40:05.863: INFO: Got endpoints: latency-svc-5tbmg [251.461951ms]
May 14 02:40:05.897: INFO: Created: latency-svc-vkk2l
May 14 02:40:05.907: INFO: Got endpoints: latency-svc-vkk2l [294.679871ms]
May 14 02:40:05.920: INFO: Created: latency-svc-ns748
May 14 02:40:05.974: INFO: Got endpoints: latency-svc-ns748 [318.074525ms]
May 14 02:40:05.979: INFO: Created: latency-svc-xcf2p
May 14 02:40:06.011: INFO: Created: latency-svc-w7dxd
May 14 02:40:06.019: INFO: Got endpoints: latency-svc-xcf2p [362.991125ms]
May 14 02:40:06.024: INFO: Created: latency-svc-h8m4g
May 14 02:40:06.033: INFO: Got endpoints: latency-svc-w7dxd [353.743987ms]
May 14 02:40:06.036: INFO: Got endpoints: latency-svc-h8m4g [336.946912ms]
May 14 02:40:06.037: INFO: Created: latency-svc-dpqw2
May 14 02:40:06.050: INFO: Created: latency-svc-kdcvm
May 14 02:40:06.053: INFO: Got endpoints: latency-svc-dpqw2 [342.829699ms]
May 14 02:40:06.062: INFO: Created: latency-svc-cqrj2
May 14 02:40:06.066: INFO: Got endpoints: latency-svc-kdcvm [341.29107ms]
May 14 02:40:06.075: INFO: Got endpoints: latency-svc-cqrj2 [344.914447ms]
May 14 02:40:06.076: INFO: Created: latency-svc-qpddx
May 14 02:40:06.093: INFO: Created: latency-svc-t76jj
May 14 02:40:06.094: INFO: Got endpoints: latency-svc-qpddx [325.348834ms]
May 14 02:40:06.100: INFO: Got endpoints: latency-svc-t76jj [324.86578ms]
May 14 02:40:06.102: INFO: Created: latency-svc-4pgrt
May 14 02:40:06.132: INFO: Got endpoints: latency-svc-4pgrt [336.168541ms]
May 14 02:40:06.148: INFO: Created: latency-svc-95jfk
May 14 02:40:06.157: INFO: Created: latency-svc-qcxcf
May 14 02:40:06.161: INFO: Got endpoints: latency-svc-95jfk [350.049254ms]
May 14 02:40:06.173: INFO: Got endpoints: latency-svc-qcxcf [351.433981ms]
May 14 02:40:06.178: INFO: Created: latency-svc-nqq7w
May 14 02:40:06.195: INFO: Got endpoints: latency-svc-nqq7w [345.817938ms]
May 14 02:40:06.197: INFO: Created: latency-svc-42czs
May 14 02:40:06.202: INFO: Created: latency-svc-x4mvr
May 14 02:40:06.219: INFO: Created: latency-svc-wznl7
May 14 02:40:06.230: INFO: Got endpoints: latency-svc-x4mvr [322.462516ms]
May 14 02:40:06.230: INFO: Got endpoints: latency-svc-42czs [367.111341ms]
May 14 02:40:06.249: INFO: Got endpoints: latency-svc-wznl7 [275.285928ms]
May 14 02:40:06.250: INFO: Created: latency-svc-x69wp
May 14 02:40:06.262: INFO: Created: latency-svc-q52jg
May 14 02:40:06.269: INFO: Got endpoints: latency-svc-x69wp [249.234104ms]
May 14 02:40:06.283: INFO: Created: latency-svc-82nbc
May 14 02:40:06.286: INFO: Got endpoints: latency-svc-q52jg [252.688186ms]
May 14 02:40:06.305: INFO: Created: latency-svc-6ghlp
May 14 02:40:06.314: INFO: Got endpoints: latency-svc-6ghlp [260.998097ms]
May 14 02:40:06.318: INFO: Got endpoints: latency-svc-82nbc [281.927184ms]
May 14 02:40:06.323: INFO: Created: latency-svc-gh9gx
May 14 02:40:06.342: INFO: Got endpoints: latency-svc-gh9gx [276.094589ms]
May 14 02:40:06.352: INFO: Created: latency-svc-lvmnw
May 14 02:40:06.365: INFO: Created: latency-svc-cnf5h
May 14 02:40:06.370: INFO: Got endpoints: latency-svc-lvmnw [294.763339ms]
May 14 02:40:06.379: INFO: Got endpoints: latency-svc-cnf5h [284.759588ms]
May 14 02:40:06.385: INFO: Created: latency-svc-mvzdz
May 14 02:40:06.388: INFO: Created: latency-svc-qm6r2
May 14 02:40:06.399: INFO: Got endpoints: latency-svc-qm6r2 [266.737637ms]
May 14 02:40:06.399: INFO: Got endpoints: latency-svc-mvzdz [298.043631ms]
May 14 02:40:06.408: INFO: Created: latency-svc-zfsn9
May 14 02:40:06.431: INFO: Created: latency-svc-hs6qr
May 14 02:40:06.431: INFO: Created: latency-svc-pn5mh
May 14 02:40:06.439: INFO: Got endpoints: latency-svc-zfsn9 [277.627882ms]
May 14 02:40:06.447: INFO: Created: latency-svc-q5xph
May 14 02:40:06.473: INFO: Created: latency-svc-jhqc9
May 14 02:40:06.478: INFO: Got endpoints: latency-svc-pn5mh [305.496981ms]
May 14 02:40:06.483: INFO: Created: latency-svc-k5w6h
May 14 02:40:06.496: INFO: Created: latency-svc-qhjrx
May 14 02:40:06.509: INFO: Created: latency-svc-2fkjl
May 14 02:40:06.518: INFO: Created: latency-svc-vnrqn
May 14 02:40:06.527: INFO: Created: latency-svc-2fk7d
May 14 02:40:06.528: INFO: Got endpoints: latency-svc-hs6qr [333.15398ms]
May 14 02:40:06.539: INFO: Created: latency-svc-47x78
May 14 02:40:06.555: INFO: Created: latency-svc-nzhn7
May 14 02:40:06.565: INFO: Created: latency-svc-btbtf
May 14 02:40:06.586: INFO: Created: latency-svc-rwrmh
May 14 02:40:06.592: INFO: Got endpoints: latency-svc-q5xph [362.184968ms]
May 14 02:40:06.600: INFO: Created: latency-svc-q94lb
May 14 02:40:06.617: INFO: Created: latency-svc-sfx24
May 14 02:40:06.639: INFO: Created: latency-svc-dk5kp
May 14 02:40:06.639: INFO: Created: latency-svc-2khsm
May 14 02:40:06.639: INFO: Got endpoints: latency-svc-jhqc9 [408.704285ms]
May 14 02:40:06.651: INFO: Created: latency-svc-8wnsw
May 14 02:40:06.663: INFO: Created: latency-svc-89kgd
May 14 02:40:06.691: INFO: Got endpoints: latency-svc-k5w6h [441.845222ms]
May 14 02:40:06.704: INFO: Created: latency-svc-ccctf
May 14 02:40:06.742: INFO: Got endpoints: latency-svc-qhjrx [473.411389ms]
May 14 02:40:06.798: INFO: Got endpoints: latency-svc-2fkjl [511.578449ms]
May 14 02:40:06.859: INFO: Created: latency-svc-tvbsz
May 14 02:40:06.859: INFO: Got endpoints: latency-svc-vnrqn [544.839211ms]
May 14 02:40:06.867: INFO: Created: latency-svc-tggtl
May 14 02:40:06.876: INFO: Created: latency-svc-cvxkx
May 14 02:40:06.883: INFO: Got endpoints: latency-svc-2fk7d [564.882994ms]
May 14 02:40:06.960: INFO: Created: latency-svc-w82rv
May 14 02:40:06.960: INFO: Got endpoints: latency-svc-47x78 [617.640353ms]
May 14 02:40:06.977: INFO: Created: latency-svc-pq8vj
May 14 02:40:06.980: INFO: Got endpoints: latency-svc-nzhn7 [609.97055ms]
May 14 02:40:06.997: INFO: Created: latency-svc-75lz9
May 14 02:40:07.033: INFO: Got endpoints: latency-svc-btbtf [654.176831ms]
May 14 02:40:07.060: INFO: Created: latency-svc-gh6hq
May 14 02:40:07.078: INFO: Got endpoints: latency-svc-rwrmh [678.642231ms]
May 14 02:40:07.090: INFO: Created: latency-svc-zqz56
May 14 02:40:07.131: INFO: Got endpoints: latency-svc-q94lb [731.626473ms]
May 14 02:40:07.144: INFO: Created: latency-svc-t5jgb
May 14 02:40:07.179: INFO: Got endpoints: latency-svc-sfx24 [739.399951ms]
May 14 02:40:07.198: INFO: Created: latency-svc-7lm6w
May 14 02:40:07.232: INFO: Got endpoints: latency-svc-2khsm [753.703357ms]
May 14 02:40:07.246: INFO: Created: latency-svc-55z8h
May 14 02:40:07.299: INFO: Got endpoints: latency-svc-dk5kp [770.657985ms]
May 14 02:40:07.314: INFO: Created: latency-svc-zc485
May 14 02:40:07.328: INFO: Got endpoints: latency-svc-8wnsw [736.132797ms]
May 14 02:40:07.343: INFO: Created: latency-svc-nhhss
May 14 02:40:07.381: INFO: Got endpoints: latency-svc-89kgd [741.445885ms]
May 14 02:40:07.394: INFO: Created: latency-svc-s45st
May 14 02:40:07.429: INFO: Got endpoints: latency-svc-ccctf [737.550271ms]
May 14 02:40:07.452: INFO: Created: latency-svc-7q4q6
May 14 02:40:07.481: INFO: Got endpoints: latency-svc-tvbsz [712.160299ms]
May 14 02:40:07.495: INFO: Created: latency-svc-crnrj
May 14 02:40:07.540: INFO: Got endpoints: latency-svc-tggtl [741.825499ms]
May 14 02:40:07.556: INFO: Created: latency-svc-42lkm
May 14 02:40:07.581: INFO: Got endpoints: latency-svc-cvxkx [722.155983ms]
May 14 02:40:07.600: INFO: Created: latency-svc-2l7zr
May 14 02:40:07.635: INFO: Got endpoints: latency-svc-w82rv [751.739027ms]
May 14 02:40:07.658: INFO: Created: latency-svc-qkshs
May 14 02:40:07.685: INFO: Got endpoints: latency-svc-pq8vj [724.481571ms]
May 14 02:40:07.705: INFO: Created: latency-svc-jpzln
May 14 02:40:07.731: INFO: Got endpoints: latency-svc-75lz9 [750.817738ms]
May 14 02:40:07.744: INFO: Created: latency-svc-526wl
May 14 02:40:07.779: INFO: Got endpoints: latency-svc-gh6hq [746.284662ms]
May 14 02:40:07.851: INFO: Got endpoints: latency-svc-zqz56 [772.730754ms]
May 14 02:40:07.865: INFO: Created: latency-svc-b8h2g
May 14 02:40:08.171: INFO: Got endpoints: latency-svc-nhhss [842.976884ms]
May 14 02:40:08.171: INFO: Got endpoints: latency-svc-7lm6w [992.526085ms]
May 14 02:40:08.172: INFO: Got endpoints: latency-svc-55z8h [939.799322ms]
May 14 02:40:08.172: INFO: Got endpoints: latency-svc-zc485 [872.755075ms]
May 14 02:40:08.172: INFO: Got endpoints: latency-svc-t5jgb [1.0411166s]
May 14 02:40:08.659: INFO: Got endpoints: latency-svc-7q4q6 [1.230001053s]
May 14 02:40:08.659: INFO: Got endpoints: latency-svc-s45st [1.278298719s]
May 14 02:40:08.660: INFO: Got endpoints: latency-svc-2l7zr [1.078644582s]
May 14 02:40:08.661: INFO: Got endpoints: latency-svc-crnrj [1.179759431s]
May 14 02:40:08.661: INFO: Got endpoints: latency-svc-42lkm [1.120965716s]
May 14 02:40:08.663: INFO: Got endpoints: latency-svc-qkshs [1.028095165s]
May 14 02:40:08.687: INFO: Got endpoints: latency-svc-jpzln [1.002580378s]
May 14 02:40:08.692: INFO: Created: latency-svc-zzvxh
May 14 02:40:08.700: INFO: Got endpoints: latency-svc-b8h2g [920.104141ms]
May 14 02:40:08.703: INFO: Got endpoints: latency-svc-526wl [971.261361ms]
May 14 02:40:08.703: INFO: Got endpoints: latency-svc-zzvxh [852.27756ms]
May 14 02:40:08.705: INFO: Created: latency-svc-rlj9z
May 14 02:40:08.715: INFO: Created: latency-svc-74cq6
May 14 02:40:08.724: INFO: Got endpoints: latency-svc-rlj9z [552.297561ms]
May 14 02:40:08.733: INFO: Got endpoints: latency-svc-74cq6 [560.325802ms]
May 14 02:40:08.741: INFO: Created: latency-svc-2hrts
May 14 02:40:08.757: INFO: Created: latency-svc-45s7x
May 14 02:40:08.757: INFO: Got endpoints: latency-svc-2hrts [585.714699ms]
May 14 02:40:08.774: INFO: Created: latency-svc-7q8n4
May 14 02:40:08.782: INFO: Got endpoints: latency-svc-45s7x [610.532613ms]
May 14 02:40:08.786: INFO: Created: latency-svc-pbhqd
May 14 02:40:08.797: INFO: Created: latency-svc-hw4lz
May 14 02:40:08.819: INFO: Created: latency-svc-6l2rc
May 14 02:40:08.831: INFO: Created: latency-svc-jq4zj
May 14 02:40:08.839: INFO: Created: latency-svc-wbhmw
May 14 02:40:08.839: INFO: Got endpoints: latency-svc-7q8n4 [667.292379ms]
May 14 02:40:08.885: INFO: Got endpoints: latency-svc-pbhqd [225.886017ms]
May 14 02:40:08.899: INFO: Created: latency-svc-s7gtn
May 14 02:40:08.899: INFO: Created: latency-svc-cnjws
May 14 02:40:08.899: INFO: Created: latency-svc-cpgvq
May 14 02:40:08.900: INFO: Created: latency-svc-8jwpd
May 14 02:40:08.941: INFO: Created: latency-svc-sjd7q
May 14 02:40:08.947: INFO: Created: latency-svc-cjhjn
May 14 02:40:08.962: INFO: Got endpoints: latency-svc-hw4lz [299.191838ms]
May 14 02:40:08.976: INFO: Created: latency-svc-pr78v
May 14 02:40:09.032: INFO: Got endpoints: latency-svc-6l2rc [371.645903ms]
May 14 02:40:09.035: INFO: Created: latency-svc-kwf8q
May 14 02:40:09.090: INFO: Created: latency-svc-bpzhb
May 14 02:40:09.090: INFO: Got endpoints: latency-svc-jq4zj [429.982515ms]
May 14 02:40:09.122: INFO: Created: latency-svc-cnw2l
May 14 02:40:09.122: INFO: Got endpoints: latency-svc-wbhmw [460.992958ms]
May 14 02:40:09.149: INFO: Created: latency-svc-vbsqt
May 14 02:40:09.149: INFO: Got endpoints: latency-svc-cnjws [487.866642ms]
May 14 02:40:09.159: INFO: Created: latency-svc-nsxfx
May 14 02:40:09.191: INFO: Got endpoints: latency-svc-cpgvq [503.425645ms]
May 14 02:40:09.194: INFO: Created: latency-svc-b72xf
May 14 02:40:09.206: INFO: Created: latency-svc-fz8ll
May 14 02:40:09.218: INFO: Created: latency-svc-79hpm
May 14 02:40:09.239: INFO: Created: latency-svc-9r2xg
May 14 02:40:09.266: INFO: Got endpoints: latency-svc-8jwpd [565.810207ms]
May 14 02:40:09.273: INFO: Created: latency-svc-8fdrq
May 14 02:40:09.314: INFO: Got endpoints: latency-svc-s7gtn [610.471756ms]
May 14 02:40:09.332: INFO: Created: latency-svc-88j8q
May 14 02:40:09.335: INFO: Got endpoints: latency-svc-sjd7q [632.250544ms]
May 14 02:40:09.344: INFO: Created: latency-svc-ktczt
May 14 02:40:09.373: INFO: Created: latency-svc-m8f9f
May 14 02:40:09.385: INFO: Got endpoints: latency-svc-cjhjn [660.838629ms]
May 14 02:40:09.419: INFO: Created: latency-svc-7xxph
May 14 02:40:09.462: INFO: Got endpoints: latency-svc-pr78v [576.944228ms]
May 14 02:40:09.492: INFO: Got endpoints: latency-svc-kwf8q [759.125081ms]
May 14 02:40:09.525: INFO: Created: latency-svc-nhbzb
May 14 02:40:09.540: INFO: Got endpoints: latency-svc-bpzhb [782.300903ms]
May 14 02:40:09.563: INFO: Created: latency-svc-m2lzd
May 14 02:40:09.577: INFO: Created: latency-svc-9j5nj
May 14 02:40:09.593: INFO: Got endpoints: latency-svc-cnw2l [810.807555ms]
May 14 02:40:09.640: INFO: Created: latency-svc-cfc6z
May 14 02:40:09.653: INFO: Got endpoints: latency-svc-vbsqt [813.404781ms]
May 14 02:40:09.687: INFO: Got endpoints: latency-svc-nsxfx [716.86044ms]
May 14 02:40:09.727: INFO: Created: latency-svc-m2hvp
May 14 02:40:09.738: INFO: Created: latency-svc-w7fk6
May 14 02:40:09.744: INFO: Got endpoints: latency-svc-b72xf [712.160563ms]
May 14 02:40:09.773: INFO: Created: latency-svc-5gxfg
May 14 02:40:09.792: INFO: Got endpoints: latency-svc-fz8ll [701.390499ms]
May 14 02:40:09.828: INFO: Created: latency-svc-4jxp8
May 14 02:40:09.836: INFO: Got endpoints: latency-svc-79hpm [713.015772ms]
May 14 02:40:09.856: INFO: Created: latency-svc-vmfv7
May 14 02:40:09.885: INFO: Got endpoints: latency-svc-9r2xg [735.294199ms]
May 14 02:40:09.911: INFO: Created: latency-svc-pzs7c
May 14 02:40:09.933: INFO: Got endpoints: latency-svc-8fdrq [742.044095ms]
May 14 02:40:09.956: INFO: Created: latency-svc-gfpnq
May 14 02:40:09.984: INFO: Got endpoints: latency-svc-88j8q [718.001851ms]
May 14 02:40:10.002: INFO: Created: latency-svc-mtjt7
May 14 02:40:10.036: INFO: Got endpoints: latency-svc-ktczt [721.730652ms]
May 14 02:40:10.058: INFO: Created: latency-svc-xsk9r
May 14 02:40:10.087: INFO: Got endpoints: latency-svc-m8f9f [751.982693ms]
May 14 02:40:10.110: INFO: Created: latency-svc-jhr47
May 14 02:40:10.130: INFO: Got endpoints: latency-svc-7xxph [745.513623ms]
May 14 02:40:10.154: INFO: Created: latency-svc-gzjjg
May 14 02:40:10.180: INFO: Got endpoints: latency-svc-nhbzb [687.542959ms]
May 14 02:40:10.192: INFO: Created: latency-svc-ssb9l
May 14 02:40:10.229: INFO: Got endpoints: latency-svc-m2lzd [765.885126ms]
May 14 02:40:10.247: INFO: Created: latency-svc-4c85c
May 14 02:40:10.291: INFO: Got endpoints: latency-svc-9j5nj [751.135587ms]
May 14 02:40:10.312: INFO: Created: latency-svc-jjt9z
May 14 02:40:10.335: INFO: Got endpoints: latency-svc-cfc6z [739.516523ms]
May 14 02:40:10.352: INFO: Created: latency-svc-m7zkm
May 14 02:40:10.378: INFO: Got endpoints: latency-svc-m2hvp [724.674052ms]
May 14 02:40:10.397: INFO: Created: latency-svc-wtzzx
May 14 02:40:10.436: INFO: Got endpoints: latency-svc-w7fk6 [749.153727ms]
May 14 02:40:10.452: INFO: Created: latency-svc-84x6l
May 14 02:40:10.483: INFO: Got endpoints: latency-svc-5gxfg [738.185479ms]
May 14 02:40:10.495: INFO: Created: latency-svc-wkbz4
May 14 02:40:10.527: INFO: Got endpoints: latency-svc-4jxp8 [735.17485ms]
May 14 02:40:10.545: INFO: Created: latency-svc-wkcrk
May 14 02:40:10.592: INFO: Got endpoints: latency-svc-vmfv7 [756.069582ms]
May 14 02:40:10.611: INFO: Created: latency-svc-4frsr
May 14 02:40:10.634: INFO: Got endpoints: latency-svc-pzs7c [749.248867ms]
May 14 02:40:10.655: INFO: Created: latency-svc-x46rt
May 14 02:40:10.684: INFO: Got endpoints: latency-svc-gfpnq [750.922807ms]
May 14 02:40:10.712: INFO: Created: latency-svc-t4n74
May 14 02:40:10.735: INFO: Got endpoints: latency-svc-mtjt7 [751.247256ms]
May 14 02:40:10.748: INFO: Created: latency-svc-5srgd
May 14 02:40:10.785: INFO: Got endpoints: latency-svc-xsk9r [748.899247ms]
May 14 02:40:10.809: INFO: Created: latency-svc-llp2g
May 14 02:40:10.846: INFO: Got endpoints: latency-svc-jhr47 [758.56996ms]
May 14 02:40:10.873: INFO: Created: latency-svc-hqrm6
May 14 02:40:10.881: INFO: Got endpoints: latency-svc-gzjjg [749.37993ms]
May 14 02:40:10.901: INFO: Created: latency-svc-28wlm
May 14 02:40:10.930: INFO: Got endpoints: latency-svc-ssb9l [750.019734ms]
May 14 02:40:10.951: INFO: Created: latency-svc-z29dk
May 14 02:40:10.985: INFO: Got endpoints: latency-svc-4c85c [754.168018ms]
May 14 02:40:11.011: INFO: Created: latency-svc-75dpc
May 14 02:40:11.037: INFO: Got endpoints: latency-svc-jjt9z [746.291646ms]
May 14 02:40:11.055: INFO: Created: latency-svc-lzqzp
May 14 02:40:11.083: INFO: Got endpoints: latency-svc-m7zkm [747.218805ms]
May 14 02:40:11.103: INFO: Created: latency-svc-lfgdw
May 14 02:40:11.137: INFO: Got endpoints: latency-svc-wtzzx [758.938019ms]
May 14 02:40:11.158: INFO: Created: latency-svc-rq6w7
May 14 02:40:11.233: INFO: Got endpoints: latency-svc-wkbz4 [750.501113ms]
May 14 02:40:11.234: INFO: Got endpoints: latency-svc-84x6l [797.690311ms]
May 14 02:40:11.267: INFO: Created: latency-svc-wk9sx
May 14 02:40:11.268: INFO: Created: latency-svc-5pp27
May 14 02:40:11.280: INFO: Got endpoints: latency-svc-wkcrk [752.989201ms]
May 14 02:40:11.314: INFO: Created: latency-svc-zm9wz
May 14 02:40:11.344: INFO: Got endpoints: latency-svc-4frsr [751.60935ms]
May 14 02:40:11.359: INFO: Created: latency-svc-wmngb
May 14 02:40:11.380: INFO: Got endpoints: latency-svc-x46rt [746.388502ms]
May 14 02:40:11.409: INFO: Created: latency-svc-jqjht
May 14 02:40:11.515: INFO: Got endpoints: latency-svc-t4n74 [830.629918ms]
May 14 02:40:11.545: INFO: Got endpoints: latency-svc-5srgd [809.657584ms]
May 14 02:40:11.926: INFO: Created: latency-svc-vwqt5
May 14 02:40:11.937: INFO: Got endpoints: latency-svc-z29dk [1.006266253s]
May 14 02:40:11.938: INFO: Got endpoints: latency-svc-llp2g [1.152491928s]
May 14 02:40:11.939: INFO: Got endpoints: latency-svc-hqrm6 [1.092375803s]
May 14 02:40:11.939: INFO: Got endpoints: latency-svc-28wlm [1.057982666s]
May 14 02:40:11.945: INFO: Got endpoints: latency-svc-75dpc [960.044745ms]
May 14 02:40:11.950: INFO: Got endpoints: latency-svc-rq6w7 [812.28501ms]
May 14 02:40:11.950: INFO: Got endpoints: latency-svc-lzqzp [912.645432ms]
May 14 02:40:11.951: INFO: Got endpoints: latency-svc-wk9sx [716.561793ms]
May 14 02:40:11.952: INFO: Got endpoints: latency-svc-lfgdw [868.592442ms]
May 14 02:40:11.957: INFO: Created: latency-svc-dlgsd
May 14 02:40:12.003: INFO: Got endpoints: latency-svc-5pp27 [769.02423ms]
May 14 02:40:12.017: INFO: Created: latency-svc-qggj8
May 14 02:40:12.026: INFO: Created: latency-svc-hxxll
May 14 02:40:12.031: INFO: Got endpoints: latency-svc-zm9wz [750.448763ms]
May 14 02:40:12.039: INFO: Created: latency-svc-t6bq4
May 14 02:40:12.069: INFO: Created: latency-svc-b59ht
May 14 02:40:12.094: INFO: Created: latency-svc-nkj4l
May 14 02:40:12.101: INFO: Got endpoints: latency-svc-wmngb [756.957362ms]
May 14 02:40:12.114: INFO: Created: latency-svc-nstcj
May 14 02:40:12.136: INFO: Got endpoints: latency-svc-jqjht [755.703997ms]
May 14 02:40:12.146: INFO: Created: latency-svc-stfgd
May 14 02:40:12.160: INFO: Created: latency-svc-krklj
May 14 02:40:12.170: INFO: Created: latency-svc-zx67n
May 14 02:40:12.188: INFO: Created: latency-svc-khkxg
May 14 02:40:12.210: INFO: Got endpoints: latency-svc-vwqt5 [695.454768ms]
May 14 02:40:12.217: INFO: Created: latency-svc-8b6dg
May 14 02:40:12.218: INFO: Created: latency-svc-lqkbw
May 14 02:40:12.223: INFO: Created: latency-svc-flsh8
May 14 02:40:12.235: INFO: Created: latency-svc-rj6x9
May 14 02:40:12.238: INFO: Got endpoints: latency-svc-dlgsd [692.212638ms]
May 14 02:40:12.256: INFO: Created: latency-svc-tvbct
May 14 02:40:12.309: INFO: Got endpoints: latency-svc-qggj8 [372.392526ms]
May 14 02:40:12.353: INFO: Got endpoints: latency-svc-hxxll [400.841783ms]
May 14 02:40:12.354: INFO: Created: latency-svc-pwcfw
May 14 02:40:12.370: INFO: Created: latency-svc-dxd2m
May 14 02:40:12.380: INFO: Got endpoints: latency-svc-t6bq4 [442.009734ms]
May 14 02:40:12.399: INFO: Created: latency-svc-tlwpk
May 14 02:40:12.433: INFO: Got endpoints: latency-svc-b59ht [494.135207ms]
May 14 02:40:12.449: INFO: Created: latency-svc-4wwff
May 14 02:40:12.483: INFO: Got endpoints: latency-svc-nkj4l [479.597572ms]
May 14 02:40:12.500: INFO: Created: latency-svc-9whlx
May 14 02:40:12.539: INFO: Got endpoints: latency-svc-nstcj [599.654652ms]
May 14 02:40:12.558: INFO: Created: latency-svc-fr2ks
May 14 02:40:12.578: INFO: Got endpoints: latency-svc-stfgd [632.617856ms]
May 14 02:40:12.588: INFO: Created: latency-svc-v78g7
May 14 02:40:12.629: INFO: Got endpoints: latency-svc-krklj [679.47942ms]
May 14 02:40:12.648: INFO: Created: latency-svc-d7ktm
May 14 02:40:12.682: INFO: Got endpoints: latency-svc-zx67n [731.545588ms]
May 14 02:40:12.715: INFO: Created: latency-svc-7l7qf
May 14 02:40:12.837: INFO: Got endpoints: latency-svc-8b6dg [805.536914ms]
May 14 02:40:12.839: INFO: Got endpoints: latency-svc-lqkbw [738.44785ms]
May 14 02:40:12.837: INFO: Got endpoints: latency-svc-khkxg [885.435667ms]
May 14 02:40:12.867: INFO: Created: latency-svc-md8pl
May 14 02:40:12.952: INFO: Got endpoints: latency-svc-rj6x9 [741.303028ms]
May 14 02:40:12.952: INFO: Got endpoints: latency-svc-flsh8 [815.441721ms]
May 14 02:40:12.960: INFO: Created: latency-svc-xxwwk
May 14 02:40:12.974: INFO: Created: latency-svc-frp4q
May 14 02:40:12.981: INFO: Got endpoints: latency-svc-tvbct [743.166909ms]
May 14 02:40:12.987: INFO: Created: latency-svc-h4q8w
May 14 02:40:13.005: INFO: Created: latency-svc-l29k9
May 14 02:40:13.019: INFO: Created: latency-svc-s58mf
May 14 02:40:13.033: INFO: Got endpoints: latency-svc-pwcfw [723.532068ms]
May 14 02:40:13.043: INFO: Created: latency-svc-sdd6v
May 14 02:40:13.087: INFO: Got endpoints: latency-svc-dxd2m [734.24143ms]
May 14 02:40:13.107: INFO: Created: latency-svc-rxt22
May 14 02:40:13.139: INFO: Got endpoints: latency-svc-tlwpk [759.002476ms]
May 14 02:40:13.163: INFO: Created: latency-svc-kfblx
May 14 02:40:13.183: INFO: Got endpoints: latency-svc-4wwff [749.780305ms]
May 14 02:40:13.211: INFO: Created: latency-svc-cf8hz
May 14 02:40:13.230: INFO: Got endpoints: latency-svc-9whlx [747.202924ms]
May 14 02:40:13.249: INFO: Created: latency-svc-gpz6l
May 14 02:40:13.289: INFO: Got endpoints: latency-svc-fr2ks [749.555981ms]
May 14 02:40:13.330: INFO: Got endpoints: latency-svc-v78g7 [751.817339ms]
May 14 02:40:13.379: INFO: Got endpoints: latency-svc-d7ktm [749.856945ms]
May 14 02:40:13.430: INFO: Got endpoints: latency-svc-7l7qf [747.450186ms]
May 14 02:40:13.931: INFO: Got endpoints: latency-svc-frp4q [1.091565454s]
May 14 02:40:13.931: INFO: Got endpoints: latency-svc-md8pl [1.094521805s]
May 14 02:40:13.931: INFO: Got endpoints: latency-svc-xxwwk [1.092111045s]
May 14 02:40:13.934: INFO: Got endpoints: latency-svc-l29k9 [982.367461ms]
May 14 02:40:13.935: INFO: Got endpoints: latency-svc-h4q8w [982.349471ms]
May 14 02:40:13.940: INFO: Got endpoints: latency-svc-s58mf [958.992455ms]
May 14 02:40:13.947: INFO: Got endpoints: latency-svc-kfblx [807.491045ms]
May 14 02:40:13.947: INFO: Got endpoints: latency-svc-cf8hz [764.008597ms]
May 14 02:40:13.952: INFO: Got endpoints: latency-svc-rxt22 [863.888136ms]
May 14 02:40:13.957: INFO: Got endpoints: latency-svc-sdd6v [923.923428ms]
May 14 02:40:13.983: INFO: Got endpoints: latency-svc-gpz6l [752.669693ms]
May 14 02:40:13.983: INFO: Latencies: [49.774251ms 59.81353ms 61.406117ms 83.176777ms 120.379955ms 129.738684ms 163.73163ms 164.012721ms 207.052612ms 209.318656ms 225.886017ms 232.006395ms 249.234104ms 251.461951ms 251.481831ms 252.133575ms 252.688186ms 258.708634ms 260.53682ms 260.998097ms 265.400517ms 266.707081ms 266.737637ms 272.780426ms 275.285928ms 275.645708ms 276.094589ms 277.627882ms 280.762476ms 281.927184ms 284.759588ms 294.679871ms 294.763339ms 298.043631ms 299.191838ms 305.496981ms 318.074525ms 322.462516ms 323.050757ms 324.86578ms 325.348834ms 333.15398ms 336.168541ms 336.946912ms 341.29107ms 342.829699ms 344.914447ms 345.817938ms 350.049254ms 351.433981ms 353.743987ms 362.184968ms 362.991125ms 367.111341ms 371.645903ms 372.392526ms 400.841783ms 408.704285ms 429.982515ms 441.845222ms 442.009734ms 460.992958ms 473.411389ms 479.597572ms 487.866642ms 494.135207ms 503.425645ms 511.578449ms 544.839211ms 552.297561ms 560.325802ms 564.882994ms 565.810207ms 576.944228ms 585.714699ms 599.654652ms 609.97055ms 610.471756ms 610.532613ms 617.640353ms 632.250544ms 632.617856ms 654.176831ms 660.838629ms 667.292379ms 678.642231ms 679.47942ms 687.542959ms 692.212638ms 695.454768ms 701.390499ms 712.160299ms 712.160563ms 713.015772ms 716.561793ms 716.86044ms 718.001851ms 721.730652ms 722.155983ms 723.532068ms 724.481571ms 724.674052ms 731.545588ms 731.626473ms 734.24143ms 735.17485ms 735.294199ms 736.132797ms 737.550271ms 738.185479ms 738.44785ms 739.399951ms 739.516523ms 741.303028ms 741.445885ms 741.825499ms 742.044095ms 743.166909ms 745.513623ms 746.284662ms 746.291646ms 746.388502ms 747.202924ms 747.218805ms 747.450186ms 748.899247ms 749.153727ms 749.248867ms 749.37993ms 749.555981ms 749.780305ms 749.856945ms 750.019734ms 750.448763ms 750.501113ms 750.817738ms 750.922807ms 751.135587ms 751.247256ms 751.60935ms 751.739027ms 751.817339ms 751.982693ms 752.669693ms 752.989201ms 753.703357ms 754.168018ms 755.703997ms 756.069582ms 756.957362ms 758.56996ms 758.938019ms 759.002476ms 759.125081ms 764.008597ms 765.885126ms 769.02423ms 770.657985ms 772.730754ms 782.300903ms 797.690311ms 805.536914ms 807.491045ms 809.657584ms 810.807555ms 812.28501ms 813.404781ms 815.441721ms 830.629918ms 842.976884ms 852.27756ms 863.888136ms 868.592442ms 872.755075ms 885.435667ms 912.645432ms 920.104141ms 923.923428ms 939.799322ms 958.992455ms 960.044745ms 971.261361ms 982.349471ms 982.367461ms 992.526085ms 1.002580378s 1.006266253s 1.028095165s 1.0411166s 1.057982666s 1.078644582s 1.091565454s 1.092111045s 1.092375803s 1.094521805s 1.120965716s 1.152491928s 1.179759431s 1.230001053s 1.278298719s]
May 14 02:40:13.984: INFO: 50 %ile: 724.481571ms
May 14 02:40:13.984: INFO: 90 %ile: 960.044745ms
May 14 02:40:13.984: INFO: 99 %ile: 1.230001053s
May 14 02:40:13.984: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:40:13.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2700" for this suite.
May 14 02:40:46.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:40:46.410: INFO: namespace svc-latency-2700 deletion completed in 32.414889008s

â€¢ [SLOW TEST:46.584 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:40:46.425: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1225
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 02:40:46.652: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
May 14 02:40:47.755: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:40:47.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1225" for this suite.
May 14 02:40:53.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:40:54.192: INFO: namespace replication-controller-1225 deletion completed in 6.421764275s

â€¢ [SLOW TEST:7.768 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:40:54.201: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-793
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:41:24.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-793" for this suite.
May 14 02:41:30.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:41:31.021: INFO: namespace container-runtime-793 deletion completed in 6.386444s

â€¢ [SLOW TEST:36.820 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:41:31.026: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3030
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
May 14 02:41:31.211: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-817715921 proxy --unix-socket=/tmp/kubectl-proxy-unix046021520/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:41:31.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3030" for this suite.
May 14 02:41:37.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:41:37.721: INFO: namespace kubectl-3030 deletion completed in 6.364027743s

â€¢ [SLOW TEST:6.696 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:41:37.728: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7609
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 14 02:41:37.974: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7609 /api/v1/namespaces/watch-7609/configmaps/e2e-watch-test-configmap-a 3efce9ff-b40f-45ad-bfd5-1834eb5bdf6b 194254 0 2020-05-14 02:41:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 14 02:41:37.975: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7609 /api/v1/namespaces/watch-7609/configmaps/e2e-watch-test-configmap-a 3efce9ff-b40f-45ad-bfd5-1834eb5bdf6b 194254 0 2020-05-14 02:41:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 14 02:41:47.997: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7609 /api/v1/namespaces/watch-7609/configmaps/e2e-watch-test-configmap-a 3efce9ff-b40f-45ad-bfd5-1834eb5bdf6b 194281 0 2020-05-14 02:41:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 14 02:41:47.997: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7609 /api/v1/namespaces/watch-7609/configmaps/e2e-watch-test-configmap-a 3efce9ff-b40f-45ad-bfd5-1834eb5bdf6b 194281 0 2020-05-14 02:41:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 14 02:41:58.016: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7609 /api/v1/namespaces/watch-7609/configmaps/e2e-watch-test-configmap-a 3efce9ff-b40f-45ad-bfd5-1834eb5bdf6b 194308 0 2020-05-14 02:41:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 14 02:41:58.017: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7609 /api/v1/namespaces/watch-7609/configmaps/e2e-watch-test-configmap-a 3efce9ff-b40f-45ad-bfd5-1834eb5bdf6b 194308 0 2020-05-14 02:41:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 14 02:42:08.046: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7609 /api/v1/namespaces/watch-7609/configmaps/e2e-watch-test-configmap-a 3efce9ff-b40f-45ad-bfd5-1834eb5bdf6b 194337 0 2020-05-14 02:41:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 14 02:42:08.047: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7609 /api/v1/namespaces/watch-7609/configmaps/e2e-watch-test-configmap-a 3efce9ff-b40f-45ad-bfd5-1834eb5bdf6b 194337 0 2020-05-14 02:41:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 14 02:42:18.060: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7609 /api/v1/namespaces/watch-7609/configmaps/e2e-watch-test-configmap-b eaef134a-3503-4197-8073-31cab5ce50d9 194364 0 2020-05-14 02:42:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 14 02:42:18.061: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7609 /api/v1/namespaces/watch-7609/configmaps/e2e-watch-test-configmap-b eaef134a-3503-4197-8073-31cab5ce50d9 194364 0 2020-05-14 02:42:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 14 02:42:28.075: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7609 /api/v1/namespaces/watch-7609/configmaps/e2e-watch-test-configmap-b eaef134a-3503-4197-8073-31cab5ce50d9 194391 0 2020-05-14 02:42:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 14 02:42:28.075: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7609 /api/v1/namespaces/watch-7609/configmaps/e2e-watch-test-configmap-b eaef134a-3503-4197-8073-31cab5ce50d9 194391 0 2020-05-14 02:42:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:42:38.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7609" for this suite.
May 14 02:42:44.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:42:44.451: INFO: namespace watch-7609 deletion completed in 6.362554273s

â€¢ [SLOW TEST:66.724 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:42:44.457: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7369
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 14 02:42:44.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-7369'
May 14 02:42:45.803: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 14 02:42:45.803: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
May 14 02:42:45.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 delete deployment e2e-test-httpd-deployment --namespace=kubectl-7369'
May 14 02:42:46.221: INFO: stderr: ""
May 14 02:42:46.221: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:42:46.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7369" for this suite.
May 14 02:42:52.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:42:52.630: INFO: namespace kubectl-7369 deletion completed in 6.384522507s

â€¢ [SLOW TEST:8.175 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:42:52.653: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9520
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9520.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9520.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9520.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9520.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9520.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9520.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 14 02:42:56.944: INFO: Unable to read wheezy_udp@PodARecord from pod dns-9520/dns-test-1f2736a9-389f-439f-85b3-1fedf5cced7f: the server could not find the requested resource (get pods dns-test-1f2736a9-389f-439f-85b3-1fedf5cced7f)
May 14 02:42:56.959: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-9520/dns-test-1f2736a9-389f-439f-85b3-1fedf5cced7f: the server could not find the requested resource (get pods dns-test-1f2736a9-389f-439f-85b3-1fedf5cced7f)
May 14 02:42:56.980: INFO: Unable to read jessie_udp@PodARecord from pod dns-9520/dns-test-1f2736a9-389f-439f-85b3-1fedf5cced7f: the server could not find the requested resource (get pods dns-test-1f2736a9-389f-439f-85b3-1fedf5cced7f)
May 14 02:42:56.988: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9520/dns-test-1f2736a9-389f-439f-85b3-1fedf5cced7f: the server could not find the requested resource (get pods dns-test-1f2736a9-389f-439f-85b3-1fedf5cced7f)
May 14 02:42:56.988: INFO: Lookups using dns-9520/dns-test-1f2736a9-389f-439f-85b3-1fedf5cced7f failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

May 14 02:43:02.024: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-9520/dns-test-1f2736a9-389f-439f-85b3-1fedf5cced7f: the server could not find the requested resource (get pods dns-test-1f2736a9-389f-439f-85b3-1fedf5cced7f)
May 14 02:43:02.053: INFO: Lookups using dns-9520/dns-test-1f2736a9-389f-439f-85b3-1fedf5cced7f failed for: [wheezy_tcp@PodARecord]

May 14 02:43:07.156: INFO: DNS probes using dns-9520/dns-test-1f2736a9-389f-439f-85b3-1fedf5cced7f succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:43:07.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9520" for this suite.
May 14 02:43:13.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:43:14.097: INFO: namespace dns-9520 deletion completed in 6.682014191s

â€¢ [SLOW TEST:21.446 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:43:14.105: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7332
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 02:43:14.359: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:43:14.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7332" for this suite.
May 14 02:43:20.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:43:20.960: INFO: namespace custom-resource-definition-7332 deletion completed in 6.464887434s

â€¢ [SLOW TEST:6.856 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:43:20.966: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1126
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 14 02:43:21.490: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fee6ad1c-9735-4c0a-82df-b6c13de9982e" in namespace "projected-1126" to be "success or failure"
May 14 02:43:21.547: INFO: Pod "downwardapi-volume-fee6ad1c-9735-4c0a-82df-b6c13de9982e": Phase="Pending", Reason="", readiness=false. Elapsed: 56.934695ms
May 14 02:43:23.558: INFO: Pod "downwardapi-volume-fee6ad1c-9735-4c0a-82df-b6c13de9982e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067607357s
May 14 02:43:25.574: INFO: Pod "downwardapi-volume-fee6ad1c-9735-4c0a-82df-b6c13de9982e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.083469432s
May 14 02:43:27.656: INFO: Pod "downwardapi-volume-fee6ad1c-9735-4c0a-82df-b6c13de9982e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.165494676s
STEP: Saw pod success
May 14 02:43:27.656: INFO: Pod "downwardapi-volume-fee6ad1c-9735-4c0a-82df-b6c13de9982e" satisfied condition "success or failure"
May 14 02:43:27.745: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod downwardapi-volume-fee6ad1c-9735-4c0a-82df-b6c13de9982e container client-container: <nil>
STEP: delete the pod
May 14 02:43:27.933: INFO: Waiting for pod downwardapi-volume-fee6ad1c-9735-4c0a-82df-b6c13de9982e to disappear
May 14 02:43:27.949: INFO: Pod downwardapi-volume-fee6ad1c-9735-4c0a-82df-b6c13de9982e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:43:27.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1126" for this suite.
May 14 02:43:33.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:43:34.367: INFO: namespace projected-1126 deletion completed in 6.40226234s

â€¢ [SLOW TEST:13.402 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:43:34.371: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2228
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 14 02:43:39.844: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:43:39.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2228" for this suite.
May 14 02:43:45.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:43:46.500: INFO: namespace container-runtime-2228 deletion completed in 6.586294527s

â€¢ [SLOW TEST:12.130 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:43:46.506: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7355
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 14 02:43:46.749: INFO: Waiting up to 5m0s for pod "downwardapi-volume-96721416-2a5e-491c-b582-5518fba0e0f9" in namespace "projected-7355" to be "success or failure"
May 14 02:43:46.818: INFO: Pod "downwardapi-volume-96721416-2a5e-491c-b582-5518fba0e0f9": Phase="Pending", Reason="", readiness=false. Elapsed: 68.723136ms
May 14 02:43:48.824: INFO: Pod "downwardapi-volume-96721416-2a5e-491c-b582-5518fba0e0f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.07547301s
May 14 02:43:50.830: INFO: Pod "downwardapi-volume-96721416-2a5e-491c-b582-5518fba0e0f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.080699793s
STEP: Saw pod success
May 14 02:43:50.830: INFO: Pod "downwardapi-volume-96721416-2a5e-491c-b582-5518fba0e0f9" satisfied condition "success or failure"
May 14 02:43:50.832: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod downwardapi-volume-96721416-2a5e-491c-b582-5518fba0e0f9 container client-container: <nil>
STEP: delete the pod
May 14 02:43:50.855: INFO: Waiting for pod downwardapi-volume-96721416-2a5e-491c-b582-5518fba0e0f9 to disappear
May 14 02:43:50.859: INFO: Pod downwardapi-volume-96721416-2a5e-491c-b582-5518fba0e0f9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:43:50.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7355" for this suite.
May 14 02:43:57.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:43:57.710: INFO: namespace projected-7355 deletion completed in 6.842681338s

â€¢ [SLOW TEST:11.204 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:43:57.712: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8895
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 14 02:44:06.830: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 14 02:44:06.851: INFO: Pod pod-with-prestop-exec-hook still exists
May 14 02:44:08.860: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 14 02:44:08.903: INFO: Pod pod-with-prestop-exec-hook still exists
May 14 02:44:10.856: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 14 02:44:10.870: INFO: Pod pod-with-prestop-exec-hook still exists
May 14 02:44:12.856: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 14 02:44:12.863: INFO: Pod pod-with-prestop-exec-hook still exists
May 14 02:44:14.855: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 14 02:44:14.861: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:44:14.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8895" for this suite.
May 14 02:44:44.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:44:45.433: INFO: namespace container-lifecycle-hook-8895 deletion completed in 30.543820849s

â€¢ [SLOW TEST:47.721 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:44:45.443: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8374
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 14 02:44:45.715: INFO: Waiting up to 5m0s for pod "pod-07a4dbe1-1104-4b31-9e0f-eaadb51e44cc" in namespace "emptydir-8374" to be "success or failure"
May 14 02:44:45.723: INFO: Pod "pod-07a4dbe1-1104-4b31-9e0f-eaadb51e44cc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.661676ms
May 14 02:44:47.732: INFO: Pod "pod-07a4dbe1-1104-4b31-9e0f-eaadb51e44cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017045547s
May 14 02:44:49.769: INFO: Pod "pod-07a4dbe1-1104-4b31-9e0f-eaadb51e44cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053263574s
STEP: Saw pod success
May 14 02:44:49.769: INFO: Pod "pod-07a4dbe1-1104-4b31-9e0f-eaadb51e44cc" satisfied condition "success or failure"
May 14 02:44:49.773: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-07a4dbe1-1104-4b31-9e0f-eaadb51e44cc container test-container: <nil>
STEP: delete the pod
May 14 02:44:49.807: INFO: Waiting for pod pod-07a4dbe1-1104-4b31-9e0f-eaadb51e44cc to disappear
May 14 02:44:49.813: INFO: Pod pod-07a4dbe1-1104-4b31-9e0f-eaadb51e44cc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:44:49.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8374" for this suite.
May 14 02:44:57.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:44:58.348: INFO: namespace emptydir-8374 deletion completed in 8.52241063s

â€¢ [SLOW TEST:12.906 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:44:58.356: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8041
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-89bb6488-ab7b-462c-9016-3607a4b627d6
STEP: Creating a pod to test consume configMaps
May 14 02:44:59.088: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7fa7ee71-f286-4685-8bfc-4c6b60210ab3" in namespace "projected-8041" to be "success or failure"
May 14 02:44:59.123: INFO: Pod "pod-projected-configmaps-7fa7ee71-f286-4685-8bfc-4c6b60210ab3": Phase="Pending", Reason="", readiness=false. Elapsed: 30.222439ms
May 14 02:45:01.661: INFO: Pod "pod-projected-configmaps-7fa7ee71-f286-4685-8bfc-4c6b60210ab3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.568072509s
May 14 02:45:04.057: INFO: Pod "pod-projected-configmaps-7fa7ee71-f286-4685-8bfc-4c6b60210ab3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.964113405s
STEP: Saw pod success
May 14 02:45:04.057: INFO: Pod "pod-projected-configmaps-7fa7ee71-f286-4685-8bfc-4c6b60210ab3" satisfied condition "success or failure"
May 14 02:45:04.217: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-projected-configmaps-7fa7ee71-f286-4685-8bfc-4c6b60210ab3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 14 02:45:04.264: INFO: Waiting for pod pod-projected-configmaps-7fa7ee71-f286-4685-8bfc-4c6b60210ab3 to disappear
May 14 02:45:04.274: INFO: Pod pod-projected-configmaps-7fa7ee71-f286-4685-8bfc-4c6b60210ab3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:45:04.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8041" for this suite.
May 14 02:45:10.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:45:10.884: INFO: namespace projected-8041 deletion completed in 6.602015745s

â€¢ [SLOW TEST:12.529 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:45:10.894: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8271
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 14 02:45:11.136: INFO: Waiting up to 5m0s for pod "pod-76e4aae1-0664-4629-bdcc-e7a073c82dd4" in namespace "emptydir-8271" to be "success or failure"
May 14 02:45:11.870: INFO: Pod "pod-76e4aae1-0664-4629-bdcc-e7a073c82dd4": Phase="Pending", Reason="", readiness=false. Elapsed: 734.078918ms
May 14 02:45:13.879: INFO: Pod "pod-76e4aae1-0664-4629-bdcc-e7a073c82dd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.743190572s
May 14 02:45:15.885: INFO: Pod "pod-76e4aae1-0664-4629-bdcc-e7a073c82dd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.7491691s
STEP: Saw pod success
May 14 02:45:15.886: INFO: Pod "pod-76e4aae1-0664-4629-bdcc-e7a073c82dd4" satisfied condition "success or failure"
May 14 02:45:15.895: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-76e4aae1-0664-4629-bdcc-e7a073c82dd4 container test-container: <nil>
STEP: delete the pod
May 14 02:45:15.942: INFO: Waiting for pod pod-76e4aae1-0664-4629-bdcc-e7a073c82dd4 to disappear
May 14 02:45:15.950: INFO: Pod pod-76e4aae1-0664-4629-bdcc-e7a073c82dd4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:45:15.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8271" for this suite.
May 14 02:45:21.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:45:22.374: INFO: namespace emptydir-8271 deletion completed in 6.418152291s

â€¢ [SLOW TEST:11.481 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:45:22.383: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9115
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
May 14 02:45:22.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-9115 -- logs-generator --log-lines-total 100 --run-duration 20s'
May 14 02:45:23.175: INFO: stderr: ""
May 14 02:45:23.175: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
May 14 02:45:23.176: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May 14 02:45:23.176: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9115" to be "running and ready, or succeeded"
May 14 02:45:23.188: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004282ms
May 14 02:45:25.215: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038529086s
May 14 02:45:28.236: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 5.059609901s
May 14 02:45:28.236: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May 14 02:45:28.236: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
May 14 02:45:28.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 logs logs-generator logs-generator --namespace=kubectl-9115'
May 14 02:45:28.581: INFO: stderr: ""
May 14 02:45:28.581: INFO: stdout: "I0514 02:45:25.674458       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/wgd 498\nI0514 02:45:25.875092       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/687 512\nI0514 02:45:26.075697       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/hhm 451\nI0514 02:45:26.274648       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/jgp 354\nI0514 02:45:26.474993       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/tsc8 331\nI0514 02:45:26.674810       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/ffpt 201\nI0514 02:45:26.874764       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/njf 414\nI0514 02:45:27.075613       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/t48p 559\nI0514 02:45:27.274919       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/9gd9 202\nI0514 02:45:27.478916       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/h5z 221\nI0514 02:45:27.675632       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/pdp 408\nI0514 02:45:27.875188       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/cjr 217\nI0514 02:45:28.075005       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/jvq 545\nI0514 02:45:28.275218       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/zm8f 372\nI0514 02:45:28.474738       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/jtlr 261\n"
STEP: limiting log lines
May 14 02:45:28.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 logs logs-generator logs-generator --namespace=kubectl-9115 --tail=1'
May 14 02:45:29.210: INFO: stderr: ""
May 14 02:45:29.210: INFO: stdout: "I0514 02:45:29.078476       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/tqw 527\n"
STEP: limiting log bytes
May 14 02:45:29.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 logs logs-generator logs-generator --namespace=kubectl-9115 --limit-bytes=1'
May 14 02:45:29.511: INFO: stderr: ""
May 14 02:45:29.511: INFO: stdout: "I"
STEP: exposing timestamps
May 14 02:45:29.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 logs logs-generator logs-generator --namespace=kubectl-9115 --tail=1 --timestamps'
May 14 02:45:29.742: INFO: stderr: ""
May 14 02:45:29.742: INFO: stdout: "2020-05-14T02:45:29.675138095Z I0514 02:45:29.674953       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/t8l 302\n"
STEP: restricting to a time range
May 14 02:45:32.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 logs logs-generator logs-generator --namespace=kubectl-9115 --since=1s'
May 14 02:45:38.168: INFO: stderr: ""
May 14 02:45:38.168: INFO: stdout: "I0514 02:45:37.274629       1 logs_generator.go:76] 58 POST /api/v1/namespaces/ns/pods/m49 386\nI0514 02:45:37.474579       1 logs_generator.go:76] 59 POST /api/v1/namespaces/ns/pods/dlms 420\nI0514 02:45:37.674591       1 logs_generator.go:76] 60 GET /api/v1/namespaces/default/pods/nvf 537\nI0514 02:45:37.874629       1 logs_generator.go:76] 61 POST /api/v1/namespaces/kube-system/pods/psk 525\nI0514 02:45:38.074629       1 logs_generator.go:76] 62 PUT /api/v1/namespaces/default/pods/ns7p 581\n"
May 14 02:45:38.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 logs logs-generator logs-generator --namespace=kubectl-9115 --since=24h'
May 14 02:45:38.617: INFO: stderr: ""
May 14 02:45:38.617: INFO: stdout: "I0514 02:45:25.674458       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/wgd 498\nI0514 02:45:25.875092       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/687 512\nI0514 02:45:26.075697       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/hhm 451\nI0514 02:45:26.274648       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/jgp 354\nI0514 02:45:26.474993       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/tsc8 331\nI0514 02:45:26.674810       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/ffpt 201\nI0514 02:45:26.874764       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/njf 414\nI0514 02:45:27.075613       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/t48p 559\nI0514 02:45:27.274919       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/9gd9 202\nI0514 02:45:27.478916       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/h5z 221\nI0514 02:45:27.675632       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/pdp 408\nI0514 02:45:27.875188       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/cjr 217\nI0514 02:45:28.075005       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/jvq 545\nI0514 02:45:28.275218       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/zm8f 372\nI0514 02:45:28.474738       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/jtlr 261\nI0514 02:45:28.675933       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/7qdq 313\nI0514 02:45:28.925563       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/kbj 345\nI0514 02:45:29.078476       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/tqw 527\nI0514 02:45:29.274595       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/z6h 243\nI0514 02:45:29.474959       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/pd9 230\nI0514 02:45:29.674953       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/t8l 302\nI0514 02:45:29.874703       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/gkv 305\nI0514 02:45:30.074828       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/rh75 580\nI0514 02:45:30.287214       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/ns/pods/wbr 362\nI0514 02:45:30.474907       1 logs_generator.go:76] 24 GET /api/v1/namespaces/kube-system/pods/nw4s 457\nI0514 02:45:30.674881       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/njmr 238\nI0514 02:45:30.874826       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/kq4 348\nI0514 02:45:31.076288       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/kube-system/pods/v9qd 556\nI0514 02:45:31.274849       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/ns/pods/5rq 403\nI0514 02:45:31.474500       1 logs_generator.go:76] 29 GET /api/v1/namespaces/ns/pods/h8pv 568\nI0514 02:45:31.674567       1 logs_generator.go:76] 30 GET /api/v1/namespaces/default/pods/zdt 549\nI0514 02:45:31.874600       1 logs_generator.go:76] 31 GET /api/v1/namespaces/ns/pods/k8c 473\nI0514 02:45:32.074648       1 logs_generator.go:76] 32 GET /api/v1/namespaces/kube-system/pods/frfr 443\nI0514 02:45:32.274607       1 logs_generator.go:76] 33 GET /api/v1/namespaces/default/pods/8sf 282\nI0514 02:45:32.474706       1 logs_generator.go:76] 34 PUT /api/v1/namespaces/ns/pods/c8lr 425\nI0514 02:45:32.677366       1 logs_generator.go:76] 35 POST /api/v1/namespaces/default/pods/6fml 489\nI0514 02:45:32.874714       1 logs_generator.go:76] 36 POST /api/v1/namespaces/kube-system/pods/d24b 251\nI0514 02:45:33.074638       1 logs_generator.go:76] 37 POST /api/v1/namespaces/kube-system/pods/f2p 581\nI0514 02:45:33.274633       1 logs_generator.go:76] 38 POST /api/v1/namespaces/ns/pods/8jbl 497\nI0514 02:45:33.474604       1 logs_generator.go:76] 39 POST /api/v1/namespaces/default/pods/9x4c 290\nI0514 02:45:33.674622       1 logs_generator.go:76] 40 POST /api/v1/namespaces/ns/pods/dcj 410\nI0514 02:45:33.874622       1 logs_generator.go:76] 41 POST /api/v1/namespaces/ns/pods/h44c 220\nI0514 02:45:34.074611       1 logs_generator.go:76] 42 PUT /api/v1/namespaces/ns/pods/p6g 521\nI0514 02:45:34.274631       1 logs_generator.go:76] 43 PUT /api/v1/namespaces/default/pods/vlm4 374\nI0514 02:45:34.474617       1 logs_generator.go:76] 44 PUT /api/v1/namespaces/kube-system/pods/sgrg 401\nI0514 02:45:34.674747       1 logs_generator.go:76] 45 PUT /api/v1/namespaces/ns/pods/bg9b 579\nI0514 02:45:34.874637       1 logs_generator.go:76] 46 GET /api/v1/namespaces/default/pods/tkn 235\nI0514 02:45:35.074735       1 logs_generator.go:76] 47 POST /api/v1/namespaces/kube-system/pods/gsz9 277\nI0514 02:45:35.274756       1 logs_generator.go:76] 48 POST /api/v1/namespaces/ns/pods/rcs 441\nI0514 02:45:35.474597       1 logs_generator.go:76] 49 POST /api/v1/namespaces/ns/pods/p5d 201\nI0514 02:45:35.674610       1 logs_generator.go:76] 50 GET /api/v1/namespaces/ns/pods/rtzc 453\nI0514 02:45:35.874610       1 logs_generator.go:76] 51 PUT /api/v1/namespaces/kube-system/pods/nzlm 259\nI0514 02:45:36.074686       1 logs_generator.go:76] 52 PUT /api/v1/namespaces/ns/pods/n5ch 430\nI0514 02:45:36.274705       1 logs_generator.go:76] 53 POST /api/v1/namespaces/ns/pods/sftz 339\nI0514 02:45:36.474778       1 logs_generator.go:76] 54 POST /api/v1/namespaces/ns/pods/5c5 563\nI0514 02:45:36.674639       1 logs_generator.go:76] 55 GET /api/v1/namespaces/kube-system/pods/stgm 514\nI0514 02:45:36.874623       1 logs_generator.go:76] 56 PUT /api/v1/namespaces/ns/pods/gjkx 512\nI0514 02:45:37.074640       1 logs_generator.go:76] 57 GET /api/v1/namespaces/kube-system/pods/9x9s 205\nI0514 02:45:37.274629       1 logs_generator.go:76] 58 POST /api/v1/namespaces/ns/pods/m49 386\nI0514 02:45:37.474579       1 logs_generator.go:76] 59 POST /api/v1/namespaces/ns/pods/dlms 420\nI0514 02:45:37.674591       1 logs_generator.go:76] 60 GET /api/v1/namespaces/default/pods/nvf 537\nI0514 02:45:37.874629       1 logs_generator.go:76] 61 POST /api/v1/namespaces/kube-system/pods/psk 525\nI0514 02:45:38.074629       1 logs_generator.go:76] 62 PUT /api/v1/namespaces/default/pods/ns7p 581\nI0514 02:45:38.277068       1 logs_generator.go:76] 63 GET /api/v1/namespaces/default/pods/6bp 300\nI0514 02:45:38.474617       1 logs_generator.go:76] 64 PUT /api/v1/namespaces/default/pods/jr2 227\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
May 14 02:45:38.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 delete pod logs-generator --namespace=kubectl-9115'
May 14 02:45:52.689: INFO: stderr: ""
May 14 02:45:52.691: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:45:52.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9115" for this suite.
May 14 02:45:58.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:45:59.452: INFO: namespace kubectl-9115 deletion completed in 6.736860153s

â€¢ [SLOW TEST:37.070 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:45:59.454: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9308
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 14 02:46:00.829: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 14 02:46:02.881: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725021160, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725021160, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725021161, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725021160, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 02:46:04.890: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725021160, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725021160, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725021161, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725021160, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 14 02:46:07.999: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:46:18.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9308" for this suite.
May 14 02:46:24.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:46:25.044: INFO: namespace webhook-9308 deletion completed in 6.596389931s
STEP: Destroying namespace "webhook-9308-markers" for this suite.
May 14 02:46:31.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:46:31.411: INFO: namespace webhook-9308-markers deletion completed in 6.36611208s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:31.981 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:46:31.438: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5414
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 02:46:36.315: INFO: Waiting up to 5m0s for pod "client-envvars-1501f619-5383-45de-9ab7-4865273089db" in namespace "pods-5414" to be "success or failure"
May 14 02:46:36.332: INFO: Pod "client-envvars-1501f619-5383-45de-9ab7-4865273089db": Phase="Pending", Reason="", readiness=false. Elapsed: 16.697102ms
May 14 02:46:38.353: INFO: Pod "client-envvars-1501f619-5383-45de-9ab7-4865273089db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037735358s
May 14 02:46:40.362: INFO: Pod "client-envvars-1501f619-5383-45de-9ab7-4865273089db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046991575s
STEP: Saw pod success
May 14 02:46:40.363: INFO: Pod "client-envvars-1501f619-5383-45de-9ab7-4865273089db" satisfied condition "success or failure"
May 14 02:46:40.373: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod client-envvars-1501f619-5383-45de-9ab7-4865273089db container env3cont: <nil>
STEP: delete the pod
May 14 02:46:40.445: INFO: Waiting for pod client-envvars-1501f619-5383-45de-9ab7-4865273089db to disappear
May 14 02:46:40.454: INFO: Pod client-envvars-1501f619-5383-45de-9ab7-4865273089db no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:46:40.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5414" for this suite.
May 14 02:47:10.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:47:10.855: INFO: namespace pods-5414 deletion completed in 30.393099262s

â€¢ [SLOW TEST:39.418 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:47:10.866: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-141
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-ea286b51-7c4a-48cc-99f2-853864569317
STEP: Creating a pod to test consume secrets
May 14 02:47:11.072: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-06f6256e-62af-4109-97aa-320f7284bc0b" in namespace "projected-141" to be "success or failure"
May 14 02:47:11.083: INFO: Pod "pod-projected-secrets-06f6256e-62af-4109-97aa-320f7284bc0b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.340262ms
May 14 02:47:18.578: INFO: Pod "pod-projected-secrets-06f6256e-62af-4109-97aa-320f7284bc0b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.506119846s
May 14 02:47:20.967: INFO: Pod "pod-projected-secrets-06f6256e-62af-4109-97aa-320f7284bc0b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.894874579s
May 14 02:47:22.972: INFO: Pod "pod-projected-secrets-06f6256e-62af-4109-97aa-320f7284bc0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.899949312s
STEP: Saw pod success
May 14 02:47:22.972: INFO: Pod "pod-projected-secrets-06f6256e-62af-4109-97aa-320f7284bc0b" satisfied condition "success or failure"
May 14 02:47:22.976: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-projected-secrets-06f6256e-62af-4109-97aa-320f7284bc0b container projected-secret-volume-test: <nil>
STEP: delete the pod
May 14 02:47:23.038: INFO: Waiting for pod pod-projected-secrets-06f6256e-62af-4109-97aa-320f7284bc0b to disappear
May 14 02:47:23.068: INFO: Pod pod-projected-secrets-06f6256e-62af-4109-97aa-320f7284bc0b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:47:23.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-141" for this suite.
May 14 02:47:29.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:47:29.393: INFO: namespace projected-141 deletion completed in 6.313554768s

â€¢ [SLOW TEST:18.528 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:47:29.395: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-451
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-5ca46c84-3f0d-4cf2-8c7f-dfde6231416c
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:47:35.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-451" for this suite.
May 14 02:48:03.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:48:04.209: INFO: namespace configmap-451 deletion completed in 28.433571841s

â€¢ [SLOW TEST:34.816 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:48:04.227: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-1468
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:48:04.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1468" for this suite.
May 14 02:48:10.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:48:11.702: INFO: namespace tables-1468 deletion completed in 7.22482032s

â€¢ [SLOW TEST:7.475 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:48:11.703: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4475
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 14 02:48:16.187: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:48:16.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4475" for this suite.
May 14 02:48:22.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:48:22.518: INFO: namespace container-runtime-4475 deletion completed in 6.30029777s

â€¢ [SLOW TEST:10.816 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:48:22.527: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-3492
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 14 02:48:27.132: INFO: &Pod{ObjectMeta:{send-events-4b11d9ab-cf55-4b96-ad75-8df5e37c0e51  events-3492 /api/v1/namespaces/events-3492/pods/send-events-4b11d9ab-cf55-4b96-ad75-8df5e37c0e51 dcdcd3e5-41db-4022-b5ca-007bbe5d024f 196004 0 2020-05-14 02:48:22 +0000 UTC <nil> <nil> map[name:foo time:752585570] map[cni.projectcalico.org/podIP:10.100.136.121/32 cni.projectcalico.org/podIPs:10.100.136.121/32] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-n7xgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-n7xgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-n7xgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 02:48:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 02:48:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 02:48:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 02:48:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.13,PodIP:10.100.136.121,StartTime:2020-05-14 02:48:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-14 02:48:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:docker://5968075f12930652f962af0a50f26fa7f370f4cfe5c83192ea1ba36f7b0ee323,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.136.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
May 14 02:48:29.148: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 14 02:48:31.156: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:48:31.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3492" for this suite.
May 14 02:49:15.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:49:15.827: INFO: namespace events-3492 deletion completed in 44.334518075s

â€¢ [SLOW TEST:53.301 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:49:15.829: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-789
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 14 02:49:16.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-789'
May 14 02:49:16.386: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 14 02:49:16.386: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
May 14 02:49:16.752: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
May 14 02:49:16.808: INFO: scanned /root for discovery docs: <nil>
May 14 02:49:16.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-789'
May 14 02:49:31.321: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 14 02:49:31.321: INFO: stdout: "Created e2e-test-httpd-rc-c338aa3ad6f458d24b1b40fdd62fd088\nScaling up e2e-test-httpd-rc-c338aa3ad6f458d24b1b40fdd62fd088 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-c338aa3ad6f458d24b1b40fdd62fd088 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-c338aa3ad6f458d24b1b40fdd62fd088 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
May 14 02:49:31.321: INFO: stdout: "Created e2e-test-httpd-rc-c338aa3ad6f458d24b1b40fdd62fd088\nScaling up e2e-test-httpd-rc-c338aa3ad6f458d24b1b40fdd62fd088 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-c338aa3ad6f458d24b1b40fdd62fd088 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-c338aa3ad6f458d24b1b40fdd62fd088 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
May 14 02:49:31.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-789'
May 14 02:49:31.592: INFO: stderr: ""
May 14 02:49:31.593: INFO: stdout: "e2e-test-httpd-rc-c338aa3ad6f458d24b1b40fdd62fd088-tq9xf e2e-test-httpd-rc-vngv5 "
STEP: Replicas for run=e2e-test-httpd-rc: expected=1 actual=2
May 14 02:49:36.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-789'
May 14 02:49:37.039: INFO: stderr: ""
May 14 02:49:37.039: INFO: stdout: "e2e-test-httpd-rc-c338aa3ad6f458d24b1b40fdd62fd088-tq9xf "
May 14 02:49:37.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods e2e-test-httpd-rc-c338aa3ad6f458d24b1b40fdd62fd088-tq9xf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-789'
May 14 02:49:37.322: INFO: stderr: ""
May 14 02:49:37.322: INFO: stdout: "true"
May 14 02:49:37.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods e2e-test-httpd-rc-c338aa3ad6f458d24b1b40fdd62fd088-tq9xf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-789'
May 14 02:49:37.564: INFO: stderr: ""
May 14 02:49:37.564: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
May 14 02:49:37.564: INFO: e2e-test-httpd-rc-c338aa3ad6f458d24b1b40fdd62fd088-tq9xf is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
May 14 02:49:37.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 delete rc e2e-test-httpd-rc --namespace=kubectl-789'
May 14 02:49:37.973: INFO: stderr: ""
May 14 02:49:37.974: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:49:37.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-789" for this suite.
May 14 02:50:06.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:50:06.628: INFO: namespace kubectl-789 deletion completed in 28.635117726s

â€¢ [SLOW TEST:50.800 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:50:06.640: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5917
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
May 14 02:50:36.958: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:50:36.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0514 02:50:36.958518      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-5917" for this suite.
May 14 02:50:42.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:50:43.390: INFO: namespace gc-5917 deletion completed in 6.425648718s

â€¢ [SLOW TEST:36.754 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:50:43.400: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1311
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-d64cc887-016a-4e63-9caa-485e5dc29c8f
STEP: Creating a pod to test consume configMaps
May 14 02:50:43.768: INFO: Waiting up to 5m0s for pod "pod-configmaps-6ada0f5e-cedf-4423-becd-78ffb3a5cac7" in namespace "configmap-1311" to be "success or failure"
May 14 02:50:43.793: INFO: Pod "pod-configmaps-6ada0f5e-cedf-4423-becd-78ffb3a5cac7": Phase="Pending", Reason="", readiness=false. Elapsed: 24.664434ms
May 14 02:50:45.879: INFO: Pod "pod-configmaps-6ada0f5e-cedf-4423-becd-78ffb3a5cac7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.110711083s
May 14 02:50:47.885: INFO: Pod "pod-configmaps-6ada0f5e-cedf-4423-becd-78ffb3a5cac7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.116838848s
May 14 02:50:49.894: INFO: Pod "pod-configmaps-6ada0f5e-cedf-4423-becd-78ffb3a5cac7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.125674744s
STEP: Saw pod success
May 14 02:50:49.894: INFO: Pod "pod-configmaps-6ada0f5e-cedf-4423-becd-78ffb3a5cac7" satisfied condition "success or failure"
May 14 02:50:49.931: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-configmaps-6ada0f5e-cedf-4423-becd-78ffb3a5cac7 container configmap-volume-test: <nil>
STEP: delete the pod
May 14 02:50:50.278: INFO: Waiting for pod pod-configmaps-6ada0f5e-cedf-4423-becd-78ffb3a5cac7 to disappear
May 14 02:50:51.571: INFO: Pod pod-configmaps-6ada0f5e-cedf-4423-becd-78ffb3a5cac7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:50:51.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1311" for this suite.
May 14 02:51:05.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:51:06.052: INFO: namespace configmap-1311 deletion completed in 14.382702119s

â€¢ [SLOW TEST:22.652 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:51:06.063: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7540
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
May 14 02:51:07.134: INFO: Waiting up to 5m0s for pod "var-expansion-92d7e467-4002-4781-abdd-bcf140cd0f0e" in namespace "var-expansion-7540" to be "success or failure"
May 14 02:51:07.149: INFO: Pod "var-expansion-92d7e467-4002-4781-abdd-bcf140cd0f0e": Phase="Pending", Reason="", readiness=false. Elapsed: 15.109811ms
May 14 02:51:09.420: INFO: Pod "var-expansion-92d7e467-4002-4781-abdd-bcf140cd0f0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.2866016s
May 14 02:51:11.426: INFO: Pod "var-expansion-92d7e467-4002-4781-abdd-bcf140cd0f0e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.292308475s
May 14 02:51:13.431: INFO: Pod "var-expansion-92d7e467-4002-4781-abdd-bcf140cd0f0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.297063725s
STEP: Saw pod success
May 14 02:51:13.431: INFO: Pod "var-expansion-92d7e467-4002-4781-abdd-bcf140cd0f0e" satisfied condition "success or failure"
May 14 02:51:13.443: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod var-expansion-92d7e467-4002-4781-abdd-bcf140cd0f0e container dapi-container: <nil>
STEP: delete the pod
May 14 02:51:13.489: INFO: Waiting for pod var-expansion-92d7e467-4002-4781-abdd-bcf140cd0f0e to disappear
May 14 02:51:13.499: INFO: Pod var-expansion-92d7e467-4002-4781-abdd-bcf140cd0f0e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:51:13.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7540" for this suite.
May 14 02:51:22.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:51:23.110: INFO: namespace var-expansion-7540 deletion completed in 9.603663148s

â€¢ [SLOW TEST:17.048 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:51:23.118: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2812
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:51:47.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2812" for this suite.
May 14 02:52:46.127: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:52:46.481: INFO: namespace kubelet-test-2812 deletion completed in 58.793475347s

â€¢ [SLOW TEST:83.364 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:52:46.487: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7785
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7785
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-7785
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7785
May 14 02:52:46.784: INFO: Found 0 stateful pods, waiting for 1
May 14 02:52:56.874: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 14 02:52:56.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 14 02:53:00.047: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 14 02:53:00.047: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 14 02:53:00.047: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 14 02:53:00.061: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 14 02:53:10.069: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 14 02:53:10.069: INFO: Waiting for statefulset status.replicas updated to 0
May 14 02:53:10.111: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
May 14 02:53:10.111: INFO: ss-0  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:52:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:52:46 +0000 UTC  }]
May 14 02:53:10.111: INFO: ss-1                                       Pending         []
May 14 02:53:10.111: INFO: 
May 14 02:53:10.111: INFO: StatefulSet ss has not reached scale 3, at 2
May 14 02:53:11.143: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.973110299s
May 14 02:53:12.149: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.95070511s
May 14 02:53:13.461: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.945443377s
May 14 02:53:14.843: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.627563131s
May 14 02:53:16.059: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.249471376s
May 14 02:53:17.782: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.034716607s
May 14 02:53:18.793: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.310783157s
May 14 02:53:21.188: INFO: Verifying statefulset ss doesn't scale past 3 for another 300.943082ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7785
May 14 02:53:22.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:53:22.952: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 14 02:53:22.952: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 14 02:53:22.952: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 14 02:53:22.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:53:23.701: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 14 02:53:23.702: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 14 02:53:23.702: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 14 02:53:23.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:53:24.332: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 14 02:53:24.334: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 14 02:53:24.334: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 14 02:53:24.346: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 14 02:53:24.346: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 14 02:53:24.346: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 14 02:53:24.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 14 02:53:24.961: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 14 02:53:24.962: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 14 02:53:24.962: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 14 02:53:24.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 14 02:53:25.537: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 14 02:53:25.537: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 14 02:53:25.537: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 14 02:53:25.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 14 02:53:26.300: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 14 02:53:26.300: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 14 02:53:26.300: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 14 02:53:26.300: INFO: Waiting for statefulset status.replicas updated to 0
May 14 02:53:27.129: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
May 14 02:53:38.262: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 14 02:53:38.263: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 14 02:53:38.263: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 14 02:53:38.381: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
May 14 02:53:38.381: INFO: ss-0  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:52:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:52:46 +0000 UTC  }]
May 14 02:53:38.382: INFO: ss-1  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  }]
May 14 02:53:38.382: INFO: ss-2  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  }]
May 14 02:53:38.382: INFO: 
May 14 02:53:38.383: INFO: StatefulSet ss has not reached scale 0, at 3
May 14 02:53:39.393: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
May 14 02:53:39.393: INFO: ss-0  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:52:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:52:46 +0000 UTC  }]
May 14 02:53:39.393: INFO: ss-1  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  }]
May 14 02:53:39.393: INFO: ss-2  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  }]
May 14 02:53:39.393: INFO: 
May 14 02:53:39.393: INFO: StatefulSet ss has not reached scale 0, at 3
May 14 02:53:40.987: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
May 14 02:53:40.987: INFO: ss-0  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:52:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:52:46 +0000 UTC  }]
May 14 02:53:40.988: INFO: ss-1  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  }]
May 14 02:53:40.988: INFO: ss-2  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  }]
May 14 02:53:40.988: INFO: 
May 14 02:53:40.988: INFO: StatefulSet ss has not reached scale 0, at 3
May 14 02:53:42.003: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
May 14 02:53:42.003: INFO: ss-0  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:52:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:52:46 +0000 UTC  }]
May 14 02:53:42.003: INFO: ss-1  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  }]
May 14 02:53:42.003: INFO: ss-2  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  }]
May 14 02:53:42.003: INFO: 
May 14 02:53:42.003: INFO: StatefulSet ss has not reached scale 0, at 3
May 14 02:53:43.058: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
May 14 02:53:43.058: INFO: ss-0  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:52:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:52:46 +0000 UTC  }]
May 14 02:53:43.058: INFO: ss-1  k8s-fcos-flwang-pyqjxt4oox23-node-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  }]
May 14 02:53:43.058: INFO: ss-2  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  }]
May 14 02:53:43.058: INFO: 
May 14 02:53:43.058: INFO: StatefulSet ss has not reached scale 0, at 3
May 14 02:53:44.074: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
May 14 02:53:44.076: INFO: ss-0  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:52:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:52:46 +0000 UTC  }]
May 14 02:53:44.076: INFO: ss-1  k8s-fcos-flwang-pyqjxt4oox23-node-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  }]
May 14 02:53:44.076: INFO: ss-2  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  }]
May 14 02:53:44.077: INFO: 
May 14 02:53:44.077: INFO: StatefulSet ss has not reached scale 0, at 3
May 14 02:53:45.087: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
May 14 02:53:45.087: INFO: ss-0  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:52:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:52:46 +0000 UTC  }]
May 14 02:53:45.087: INFO: ss-1  k8s-fcos-flwang-pyqjxt4oox23-node-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  }]
May 14 02:53:45.087: INFO: ss-2  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  }]
May 14 02:53:45.087: INFO: 
May 14 02:53:45.087: INFO: StatefulSet ss has not reached scale 0, at 3
May 14 02:53:46.101: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
May 14 02:53:46.101: INFO: ss-0  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:52:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:52:46 +0000 UTC  }]
May 14 02:53:46.101: INFO: ss-1  k8s-fcos-flwang-pyqjxt4oox23-node-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  }]
May 14 02:53:46.101: INFO: ss-2  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  }]
May 14 02:53:46.101: INFO: 
May 14 02:53:46.101: INFO: StatefulSet ss has not reached scale 0, at 3
May 14 02:53:47.133: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
May 14 02:53:47.133: INFO: ss-0  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:52:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:52:46 +0000 UTC  }]
May 14 02:53:47.133: INFO: ss-1  k8s-fcos-flwang-pyqjxt4oox23-node-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  }]
May 14 02:53:47.134: INFO: ss-2  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  }]
May 14 02:53:47.134: INFO: 
May 14 02:53:47.134: INFO: StatefulSet ss has not reached scale 0, at 3
May 14 02:53:48.141: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
May 14 02:53:48.141: INFO: ss-0  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:52:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:52:46 +0000 UTC  }]
May 14 02:53:48.141: INFO: ss-1  k8s-fcos-flwang-pyqjxt4oox23-node-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  }]
May 14 02:53:48.142: INFO: ss-2  k8s-fcos-flwang-pyqjxt4oox23-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-14 02:53:10 +0000 UTC  }]
May 14 02:53:48.142: INFO: 
May 14 02:53:48.142: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7785
May 14 02:53:50.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:53:50.770: INFO: rc: 1
May 14 02:53:50.770: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
May 14 02:54:00.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:54:01.191: INFO: rc: 1
May 14 02:54:01.191: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:54:11.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:54:12.048: INFO: rc: 1
May 14 02:54:12.049: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:54:22.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:54:22.430: INFO: rc: 1
May 14 02:54:22.431: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:54:32.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:54:32.786: INFO: rc: 1
May 14 02:54:32.786: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:54:42.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:54:43.208: INFO: rc: 1
May 14 02:54:43.209: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:54:53.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:54:55.843: INFO: rc: 1
May 14 02:54:55.844: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:55:05.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:55:06.127: INFO: rc: 1
May 14 02:55:06.128: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:55:16.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:55:16.400: INFO: rc: 1
May 14 02:55:16.400: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:55:26.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:55:26.710: INFO: rc: 1
May 14 02:55:26.710: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:55:36.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:55:37.087: INFO: rc: 1
May 14 02:55:37.087: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:55:47.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:55:47.396: INFO: rc: 1
May 14 02:55:47.397: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:55:57.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:55:57.615: INFO: rc: 1
May 14 02:55:57.615: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:56:07.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:56:07.914: INFO: rc: 1
May 14 02:56:07.914: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:56:17.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:56:18.364: INFO: rc: 1
May 14 02:56:18.364: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:56:28.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:56:28.617: INFO: rc: 1
May 14 02:56:28.617: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:56:38.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:56:39.541: INFO: rc: 1
May 14 02:56:39.542: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:56:49.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:56:49.802: INFO: rc: 1
May 14 02:56:49.803: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:56:59.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:57:00.126: INFO: rc: 1
May 14 02:57:00.126: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:57:10.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:57:10.451: INFO: rc: 1
May 14 02:57:10.451: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:57:20.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:57:20.700: INFO: rc: 1
May 14 02:57:20.700: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:57:30.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:57:31.082: INFO: rc: 1
May 14 02:57:31.082: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:57:41.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:57:41.306: INFO: rc: 1
May 14 02:57:41.306: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:57:51.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:57:51.576: INFO: rc: 1
May 14 02:57:51.576: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:58:01.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:58:01.832: INFO: rc: 1
May 14 02:58:01.832: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:58:11.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:58:12.241: INFO: rc: 1
May 14 02:58:12.241: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:58:22.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:58:22.493: INFO: rc: 1
May 14 02:58:22.493: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:58:32.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:58:32.835: INFO: rc: 1
May 14 02:58:32.836: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:58:42.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:58:43.073: INFO: rc: 1
May 14 02:58:43.074: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 14 02:58:53.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-7785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 02:58:53.330: INFO: rc: 1
May 14 02:58:53.330: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
May 14 02:58:53.330: INFO: Scaling statefulset ss to 0
May 14 02:58:53.388: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May 14 02:58:53.392: INFO: Deleting all statefulset in ns statefulset-7785
May 14 02:58:53.398: INFO: Scaling statefulset ss to 0
May 14 02:58:53.435: INFO: Waiting for statefulset status.replicas updated to 0
May 14 02:58:53.439: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:58:53.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7785" for this suite.
May 14 02:58:59.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:58:59.958: INFO: namespace statefulset-7785 deletion completed in 6.438681305s

â€¢ [SLOW TEST:373.471 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:58:59.963: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6430
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0514 02:59:10.192070      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 14 02:59:10.193: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:59:10.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6430" for this suite.
May 14 02:59:16.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:59:16.776: INFO: namespace gc-6430 deletion completed in 6.574845612s

â€¢ [SLOW TEST:16.813 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:59:16.794: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7658
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:59:28.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7658" for this suite.
May 14 02:59:36.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:59:37.077: INFO: namespace resourcequota-7658 deletion completed in 8.417940829s

â€¢ [SLOW TEST:20.283 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 02:59:37.080: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6630
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 14 02:59:39.224: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 14 02:59:41.268: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725021979, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725021979, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725021979, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725021979, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 14 02:59:44.298: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 02:59:46.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6630" for this suite.
May 14 02:59:52.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 02:59:52.516: INFO: namespace webhook-6630 deletion completed in 6.450608466s
STEP: Destroying namespace "webhook-6630-markers" for this suite.
May 14 02:59:58.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:00:00.431: INFO: namespace webhook-6630-markers deletion completed in 7.9146177s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:23.394 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:00:00.495: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-9675
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 14 03:00:05.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725022004, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725022004, loc:(*time.Location)(0x78a2900)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-crd-conversion-webhook-deployment-64d485d9bb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725022005, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725022005, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
May 14 03:00:07.446: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725022005, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725022005, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725022005, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725022004, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 03:00:09.907: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725022005, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725022005, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725022005, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725022004, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 14 03:00:12.985: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 03:00:12.991: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:00:15.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9675" for this suite.
May 14 03:00:23.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:00:25.334: INFO: namespace crd-webhook-9675 deletion completed in 9.864413129s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:24.868 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:00:25.366: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2610
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 14 03:00:25.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-2610'
May 14 03:00:25.987: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 14 03:00:25.987: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
May 14 03:00:25.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 delete jobs e2e-test-httpd-job --namespace=kubectl-2610'
May 14 03:00:26.267: INFO: stderr: ""
May 14 03:00:26.267: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:00:26.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2610" for this suite.
May 14 03:00:32.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:00:32.917: INFO: namespace kubectl-2610 deletion completed in 6.637072445s

â€¢ [SLOW TEST:7.552 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:00:32.919: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3596
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
May 14 03:00:33.089: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:01:09.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3596" for this suite.
May 14 03:01:15.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:01:15.848: INFO: namespace crd-publish-openapi-3596 deletion completed in 6.390800327s

â€¢ [SLOW TEST:42.930 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:01:15.849: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6241
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 14 03:01:16.078: INFO: Waiting up to 5m0s for pod "downwardapi-volume-74f991ee-b2e3-42d8-a6e5-5ac1a901a963" in namespace "downward-api-6241" to be "success or failure"
May 14 03:01:16.088: INFO: Pod "downwardapi-volume-74f991ee-b2e3-42d8-a6e5-5ac1a901a963": Phase="Pending", Reason="", readiness=false. Elapsed: 9.245567ms
May 14 03:01:18.095: INFO: Pod "downwardapi-volume-74f991ee-b2e3-42d8-a6e5-5ac1a901a963": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017098776s
May 14 03:01:20.104: INFO: Pod "downwardapi-volume-74f991ee-b2e3-42d8-a6e5-5ac1a901a963": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026132138s
May 14 03:01:22.586: INFO: Pod "downwardapi-volume-74f991ee-b2e3-42d8-a6e5-5ac1a901a963": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.507357186s
STEP: Saw pod success
May 14 03:01:22.586: INFO: Pod "downwardapi-volume-74f991ee-b2e3-42d8-a6e5-5ac1a901a963" satisfied condition "success or failure"
May 14 03:01:22.594: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod downwardapi-volume-74f991ee-b2e3-42d8-a6e5-5ac1a901a963 container client-container: <nil>
STEP: delete the pod
May 14 03:01:22.709: INFO: Waiting for pod downwardapi-volume-74f991ee-b2e3-42d8-a6e5-5ac1a901a963 to disappear
May 14 03:01:22.716: INFO: Pod downwardapi-volume-74f991ee-b2e3-42d8-a6e5-5ac1a901a963 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:01:22.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6241" for this suite.
May 14 03:01:28.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:01:29.154: INFO: namespace downward-api-6241 deletion completed in 6.428380965s

â€¢ [SLOW TEST:13.306 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:01:29.166: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9613
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 03:01:30.950: INFO: (0) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 530.541735ms)
May 14 03:01:30.991: INFO: (1) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 40.482727ms)
May 14 03:01:31.003: INFO: (2) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 10.039878ms)
May 14 03:01:31.039: INFO: (3) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 36.189495ms)
May 14 03:01:31.052: INFO: (4) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 12.825024ms)
May 14 03:01:31.061: INFO: (5) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 9.046514ms)
May 14 03:01:31.080: INFO: (6) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 18.25398ms)
May 14 03:01:31.104: INFO: (7) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 23.909006ms)
May 14 03:01:31.114: INFO: (8) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 10.36753ms)
May 14 03:01:31.122: INFO: (9) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 7.524712ms)
May 14 03:01:31.129: INFO: (10) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 6.79651ms)
May 14 03:01:31.137: INFO: (11) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 7.234601ms)
May 14 03:01:31.146: INFO: (12) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 8.862009ms)
May 14 03:01:31.156: INFO: (13) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 10.74317ms)
May 14 03:01:31.164: INFO: (14) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 7.538373ms)
May 14 03:01:31.171: INFO: (15) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 6.935872ms)
May 14 03:01:31.178: INFO: (16) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 7.209413ms)
May 14 03:01:31.185: INFO: (17) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 6.654596ms)
May 14 03:01:31.215: INFO: (18) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 29.602184ms)
May 14 03:01:31.225: INFO: (19) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 9.582194ms)
[AfterEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:01:31.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9613" for this suite.
May 14 03:01:37.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:01:37.603: INFO: namespace proxy-9613 deletion completed in 6.367979408s

â€¢ [SLOW TEST:8.437 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:01:37.607: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-8998
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
May 14 03:01:37.821: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-8998" to be "success or failure"
May 14 03:01:37.837: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 10.401514ms
May 14 03:01:39.856: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02877786s
May 14 03:01:41.881: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053703844s
May 14 03:01:43.889: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.061793008s
STEP: Saw pod success
May 14 03:01:43.889: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
May 14 03:01:43.894: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
May 14 03:01:43.947: INFO: Waiting for pod pod-host-path-test to disappear
May 14 03:01:44.038: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:01:44.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-8998" for this suite.
May 14 03:01:50.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:01:50.454: INFO: namespace hostpath-8998 deletion completed in 6.404943558s

â€¢ [SLOW TEST:12.848 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:01:50.458: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2299
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-45e8694d-8ca2-4103-99df-734f0f708300
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-45e8694d-8ca2-4103-99df-734f0f708300
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:03:08.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2299" for this suite.
May 14 03:03:20.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:03:21.082: INFO: namespace projected-2299 deletion completed in 12.435152329s

â€¢ [SLOW TEST:90.624 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:03:21.084: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-571
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-2a8596c3-7fba-4ee8-97ad-eda80cbbeb9b in namespace container-probe-571
May 14 03:03:25.378: INFO: Started pod liveness-2a8596c3-7fba-4ee8-97ad-eda80cbbeb9b in namespace container-probe-571
STEP: checking the pod's current state and verifying that restartCount is present
May 14 03:03:25.386: INFO: Initial restart count of pod liveness-2a8596c3-7fba-4ee8-97ad-eda80cbbeb9b is 0
May 14 03:03:41.478: INFO: Restart count of pod container-probe-571/liveness-2a8596c3-7fba-4ee8-97ad-eda80cbbeb9b is now 1 (16.091593213s elapsed)
May 14 03:04:01.863: INFO: Restart count of pod container-probe-571/liveness-2a8596c3-7fba-4ee8-97ad-eda80cbbeb9b is now 2 (36.476379206s elapsed)
May 14 03:04:21.960: INFO: Restart count of pod container-probe-571/liveness-2a8596c3-7fba-4ee8-97ad-eda80cbbeb9b is now 3 (56.573614154s elapsed)
May 14 03:04:41.033: INFO: Restart count of pod container-probe-571/liveness-2a8596c3-7fba-4ee8-97ad-eda80cbbeb9b is now 4 (1m15.646050921s elapsed)
May 14 03:05:44.573: INFO: Restart count of pod container-probe-571/liveness-2a8596c3-7fba-4ee8-97ad-eda80cbbeb9b is now 5 (2m19.186036594s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:05:44.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-571" for this suite.
May 14 03:05:52.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:05:52.992: INFO: namespace container-probe-571 deletion completed in 8.356333241s

â€¢ [SLOW TEST:151.908 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:05:52.997: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4235
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 03:05:53.463: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"0aef35d6-974f-432f-82b1-f923dcf82628", Controller:(*bool)(0xc0031c283a), BlockOwnerDeletion:(*bool)(0xc0031c283b)}}
May 14 03:05:53.519: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"beebc975-fa2f-4fd6-9c82-07ac39a53fd8", Controller:(*bool)(0xc0036be04a), BlockOwnerDeletion:(*bool)(0xc0036be04b)}}
May 14 03:05:53.561: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"61432aa8-4750-4d52-bdd7-a83cd103b319", Controller:(*bool)(0xc0036be1d2), BlockOwnerDeletion:(*bool)(0xc0036be1d3)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:06:00.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4235" for this suite.
May 14 03:06:07.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:06:07.365: INFO: namespace gc-4235 deletion completed in 6.36129784s

â€¢ [SLOW TEST:14.368 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:06:07.369: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2937
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-2937
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 14 03:06:07.544: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 14 03:06:33.903: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.136.84:8080/dial?request=hostName&protocol=udp&host=10.100.230.52&port=8081&tries=1'] Namespace:pod-network-test-2937 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 03:06:33.904: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
May 14 03:06:34.641: INFO: Waiting for endpoints: map[]
May 14 03:06:34.647: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.136.84:8080/dial?request=hostName&protocol=udp&host=10.100.136.73&port=8081&tries=1'] Namespace:pod-network-test-2937 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 03:06:34.647: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
May 14 03:06:34.976: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:06:34.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2937" for this suite.
May 14 03:06:47.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:06:47.463: INFO: namespace pod-network-test-2937 deletion completed in 12.474561646s

â€¢ [SLOW TEST:40.095 seconds]
[sig-network] Networking
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:06:47.469: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8437
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
May 14 03:06:47.723: INFO: Waiting up to 5m0s for pod "pod-84dccbb9-3810-4ea6-a54e-a5e710ffde48" in namespace "emptydir-8437" to be "success or failure"
May 14 03:06:47.741: INFO: Pod "pod-84dccbb9-3810-4ea6-a54e-a5e710ffde48": Phase="Pending", Reason="", readiness=false. Elapsed: 17.753896ms
May 14 03:06:49.748: INFO: Pod "pod-84dccbb9-3810-4ea6-a54e-a5e710ffde48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024607915s
May 14 03:06:51.765: INFO: Pod "pod-84dccbb9-3810-4ea6-a54e-a5e710ffde48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041484473s
STEP: Saw pod success
May 14 03:06:51.765: INFO: Pod "pod-84dccbb9-3810-4ea6-a54e-a5e710ffde48" satisfied condition "success or failure"
May 14 03:06:51.773: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-84dccbb9-3810-4ea6-a54e-a5e710ffde48 container test-container: <nil>
STEP: delete the pod
May 14 03:06:51.968: INFO: Waiting for pod pod-84dccbb9-3810-4ea6-a54e-a5e710ffde48 to disappear
May 14 03:06:51.976: INFO: Pod pod-84dccbb9-3810-4ea6-a54e-a5e710ffde48 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:06:51.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8437" for this suite.
May 14 03:07:00.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:07:00.299: INFO: namespace emptydir-8437 deletion completed in 8.314829901s

â€¢ [SLOW TEST:12.831 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:07:00.306: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1761
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-499c1f13-a265-48dd-8246-e6dd136c5c5e
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-499c1f13-a265-48dd-8246-e6dd136c5c5e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:08:25.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1761" for this suite.
May 14 03:08:40.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:08:40.359: INFO: namespace configmap-1761 deletion completed in 14.380985836s

â€¢ [SLOW TEST:100.054 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:08:40.364: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1583
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 03:08:40.882: INFO: (0) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 88.015336ms)
May 14 03:08:40.893: INFO: (1) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 10.088429ms)
May 14 03:08:40.899: INFO: (2) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 6.226102ms)
May 14 03:08:40.907: INFO: (3) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 8.139803ms)
May 14 03:08:40.915: INFO: (4) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 6.998848ms)
May 14 03:08:40.926: INFO: (5) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 9.947869ms)
May 14 03:08:40.932: INFO: (6) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 6.096234ms)
May 14 03:08:40.939: INFO: (7) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 7.148992ms)
May 14 03:08:40.955: INFO: (8) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 15.695341ms)
May 14 03:08:40.971: INFO: (9) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 14.714954ms)
May 14 03:08:40.985: INFO: (10) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 12.833121ms)
May 14 03:08:40.995: INFO: (11) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 9.735193ms)
May 14 03:08:41.005: INFO: (12) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 9.500486ms)
May 14 03:08:41.013: INFO: (13) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 7.537716ms)
May 14 03:08:41.022: INFO: (14) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 8.223367ms)
May 14 03:08:41.030: INFO: (15) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 7.320562ms)
May 14 03:08:41.040: INFO: (16) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 9.828339ms)
May 14 03:08:41.061: INFO: (17) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 20.97957ms)
May 14 03:08:41.072: INFO: (18) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 10.708234ms)
May 14 03:08:41.083: INFO: (19) /api/v1/nodes/k8s-fcos-flwang-pyqjxt4oox23-node-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="containers/">containers/</a>... (200; 10.552033ms)
[AfterEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:08:41.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1583" for this suite.
May 14 03:08:47.127: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:08:47.456: INFO: namespace proxy-1583 deletion completed in 6.365696132s

â€¢ [SLOW TEST:7.092 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:08:47.470: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8053
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
May 14 03:08:58.210: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8053 pod-service-account-e4ed6419-aef0-408c-b21d-1207aa990dae -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 14 03:09:01.304: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8053 pod-service-account-e4ed6419-aef0-408c-b21d-1207aa990dae -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 14 03:09:01.963: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8053 pod-service-account-e4ed6419-aef0-408c-b21d-1207aa990dae -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:09:02.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8053" for this suite.
May 14 03:09:09.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:09:10.302: INFO: namespace svcaccounts-8053 deletion completed in 7.723491512s

â€¢ [SLOW TEST:22.833 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:09:10.303: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6302
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
May 14 03:09:10.504: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:09:18.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6302" for this suite.
May 14 03:09:24.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:09:25.125: INFO: namespace init-container-6302 deletion completed in 6.435161284s

â€¢ [SLOW TEST:14.822 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:09:25.136: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5989
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 03:09:25.417: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 14 03:09:25.433: INFO: Number of nodes with available pods: 0
May 14 03:09:25.433: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 14 03:09:25.475: INFO: Number of nodes with available pods: 0
May 14 03:09:25.475: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:09:26.480: INFO: Number of nodes with available pods: 0
May 14 03:09:26.480: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:09:27.488: INFO: Number of nodes with available pods: 0
May 14 03:09:27.488: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:09:28.485: INFO: Number of nodes with available pods: 0
May 14 03:09:28.485: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:09:29.480: INFO: Number of nodes with available pods: 0
May 14 03:09:29.480: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:09:30.480: INFO: Number of nodes with available pods: 0
May 14 03:09:30.480: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:09:31.482: INFO: Number of nodes with available pods: 1
May 14 03:09:31.482: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 14 03:09:31.546: INFO: Number of nodes with available pods: 0
May 14 03:09:31.546: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 14 03:09:31.565: INFO: Number of nodes with available pods: 0
May 14 03:09:31.565: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:09:32.572: INFO: Number of nodes with available pods: 0
May 14 03:09:32.573: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:09:33.571: INFO: Number of nodes with available pods: 0
May 14 03:09:33.571: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:09:34.569: INFO: Number of nodes with available pods: 0
May 14 03:09:34.569: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:09:35.998: INFO: Number of nodes with available pods: 0
May 14 03:09:36.000: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:09:36.876: INFO: Number of nodes with available pods: 0
May 14 03:09:36.876: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:09:37.572: INFO: Number of nodes with available pods: 0
May 14 03:09:37.572: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:09:38.570: INFO: Number of nodes with available pods: 0
May 14 03:09:38.570: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:09:39.574: INFO: Number of nodes with available pods: 0
May 14 03:09:39.577: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:09:40.586: INFO: Number of nodes with available pods: 1
May 14 03:09:40.586: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5989, will wait for the garbage collector to delete the pods
May 14 03:09:40.683: INFO: Deleting DaemonSet.extensions daemon-set took: 11.860764ms
May 14 03:09:41.684: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.0010874s
May 14 03:09:49.195: INFO: Number of nodes with available pods: 0
May 14 03:09:49.195: INFO: Number of running nodes: 0, number of available pods: 0
May 14 03:09:49.199: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5989/daemonsets","resourceVersion":"200535"},"items":null}

May 14 03:09:49.201: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5989/pods","resourceVersion":"200535"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:09:49.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5989" for this suite.
May 14 03:09:55.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:09:55.685: INFO: namespace daemonsets-5989 deletion completed in 6.410455213s

â€¢ [SLOW TEST:30.551 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:09:55.692: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-864
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 03:09:55.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 version'
May 14 03:09:58.269: INFO: stderr: ""
May 14 03:09:58.269: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.9\", GitCommit:\"a17149e1a189050796ced469dbd78d380f2ed5ef\", GitTreeState:\"clean\", BuildDate:\"2020-04-16T11:44:51Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.9\", GitCommit:\"a17149e1a189050796ced469dbd78d380f2ed5ef\", GitTreeState:\"clean\", BuildDate:\"2020-04-16T11:36:15Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:09:58.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-864" for this suite.
May 14 03:10:05.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:10:06.664: INFO: namespace kubectl-864 deletion completed in 8.385668269s

â€¢ [SLOW TEST:10.973 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:10:06.667: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9200
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
May 14 03:10:07.325: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
May 14 03:10:33.899: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
May 14 03:10:39.723: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:11:04.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9200" for this suite.
May 14 03:11:10.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:11:11.195: INFO: namespace crd-publish-openapi-9200 deletion completed in 6.383781157s

â€¢ [SLOW TEST:64.528 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:11:11.197: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8650
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 14 03:11:11.443: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:11:11.452: INFO: Number of nodes with available pods: 0
May 14 03:11:11.452: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:11:12.458: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:11:12.463: INFO: Number of nodes with available pods: 0
May 14 03:11:12.463: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:11:13.465: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:11:13.476: INFO: Number of nodes with available pods: 0
May 14 03:11:13.477: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:11:14.467: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:11:14.479: INFO: Number of nodes with available pods: 0
May 14 03:11:14.479: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:11:15.464: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:11:15.474: INFO: Number of nodes with available pods: 1
May 14 03:11:15.474: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-1 is running more than one daemon pod
May 14 03:11:16.459: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:11:16.464: INFO: Number of nodes with available pods: 2
May 14 03:11:16.464: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 14 03:11:16.489: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:11:16.504: INFO: Number of nodes with available pods: 2
May 14 03:11:16.504: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8650, will wait for the garbage collector to delete the pods
May 14 03:11:16.680: INFO: Deleting DaemonSet.extensions daemon-set took: 67.025904ms
May 14 03:11:17.683: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.003244908s
May 14 03:11:21.392: INFO: Number of nodes with available pods: 0
May 14 03:11:21.393: INFO: Number of running nodes: 0, number of available pods: 0
May 14 03:11:21.404: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8650/daemonsets","resourceVersion":"200901"},"items":null}

May 14 03:11:21.418: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8650/pods","resourceVersion":"200901"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:11:21.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8650" for this suite.
May 14 03:11:29.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:11:29.828: INFO: namespace daemonsets-8650 deletion completed in 8.369253863s

â€¢ [SLOW TEST:18.631 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:11:29.834: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9647
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 14 03:11:31.570: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 14 03:11:33.599: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725022691, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725022691, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725022691, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725022691, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 03:11:35.608: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725022691, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725022691, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725022691, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725022691, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 14 03:11:38.636: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 03:11:38.643: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6617-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:11:40.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9647" for this suite.
May 14 03:11:48.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:11:50.367: INFO: namespace webhook-9647 deletion completed in 10.059217546s
STEP: Destroying namespace "webhook-9647-markers" for this suite.
May 14 03:11:56.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:11:56.751: INFO: namespace webhook-9647-markers deletion completed in 6.382411431s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:26.959 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:11:56.809: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9826
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
May 14 03:11:57.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 create -f - --namespace=kubectl-9826'
May 14 03:11:58.238: INFO: stderr: ""
May 14 03:11:58.238: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 14 03:11:59.250: INFO: Selector matched 1 pods for map[app:redis]
May 14 03:11:59.251: INFO: Found 0 / 1
May 14 03:12:00.249: INFO: Selector matched 1 pods for map[app:redis]
May 14 03:12:00.249: INFO: Found 0 / 1
May 14 03:12:01.260: INFO: Selector matched 1 pods for map[app:redis]
May 14 03:12:01.260: INFO: Found 1 / 1
May 14 03:12:01.260: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 14 03:12:01.267: INFO: Selector matched 1 pods for map[app:redis]
May 14 03:12:01.267: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 14 03:12:01.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 patch pod redis-master-42hxs --namespace=kubectl-9826 -p {"metadata":{"annotations":{"x":"y"}}}'
May 14 03:12:01.606: INFO: stderr: ""
May 14 03:12:01.606: INFO: stdout: "pod/redis-master-42hxs patched\n"
STEP: checking annotations
May 14 03:12:01.612: INFO: Selector matched 1 pods for map[app:redis]
May 14 03:12:01.612: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:12:01.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9826" for this suite.
May 14 03:12:13.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:12:14.151: INFO: namespace kubectl-9826 deletion completed in 12.520183952s

â€¢ [SLOW TEST:17.356 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:12:14.158: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8127
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-7008b8a6-e389-4cff-9c26-442a6a046390
STEP: Creating secret with name s-test-opt-upd-ec0d13f2-aa9e-45d0-a078-799ab3fe6258
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-7008b8a6-e389-4cff-9c26-442a6a046390
STEP: Updating secret s-test-opt-upd-ec0d13f2-aa9e-45d0-a078-799ab3fe6258
STEP: Creating secret with name s-test-opt-create-ae559707-2c12-4d24-85fc-41531a297bfc
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:13:43.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8127" for this suite.
May 14 03:14:13.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:14:13.911: INFO: namespace secrets-8127 deletion completed in 30.479964493s

â€¢ [SLOW TEST:119.753 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:14:13.912: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5556
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
May 14 03:14:20.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec pod-sharedvolume-ac88424c-86ef-4f4e-b41b-35e11ea24431 -c busybox-main-container --namespace=emptydir-5556 -- cat /usr/share/volumeshare/shareddata.txt'
May 14 03:14:21.280: INFO: stderr: ""
May 14 03:14:21.280: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:14:21.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5556" for this suite.
May 14 03:14:27.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:14:28.347: INFO: namespace emptydir-5556 deletion completed in 7.05725051s

â€¢ [SLOW TEST:14.436 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:14:28.353: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9304
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-09a424fc-c3fd-4d9d-80e4-68ce32e23096 in namespace container-probe-9304
May 14 03:14:32.664: INFO: Started pod test-webserver-09a424fc-c3fd-4d9d-80e4-68ce32e23096 in namespace container-probe-9304
STEP: checking the pod's current state and verifying that restartCount is present
May 14 03:14:32.669: INFO: Initial restart count of pod test-webserver-09a424fc-c3fd-4d9d-80e4-68ce32e23096 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:18:34.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9304" for this suite.
May 14 03:18:41.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:18:41.954: INFO: namespace container-probe-9304 deletion completed in 6.480928584s

â€¢ [SLOW TEST:253.602 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:18:41.959: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2658
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 14 03:18:44.232: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023124, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023124, loc:(*time.Location)(0x78a2900)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-86d95b659d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023124, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023124, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
May 14 03:18:47.898: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023124, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023124, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023124, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023124, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 03:18:48.244: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023124, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023124, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023124, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023124, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 14 03:18:51.260: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 03:18:51.272: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1957-crds.webhook.example.com via the AdmissionRegistration API
May 14 03:18:52.384: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:18:53.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2658" for this suite.
May 14 03:18:59.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:19:00.086: INFO: namespace webhook-2658 deletion completed in 6.359383718s
STEP: Destroying namespace "webhook-2658-markers" for this suite.
May 14 03:19:06.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:19:06.482: INFO: namespace webhook-2658-markers deletion completed in 6.39543009s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:24.548 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:19:06.516: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4382
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May 14 03:19:06.929: INFO: Waiting up to 5m0s for pod "downward-api-ee160e36-33f7-438b-9f52-2370d4c3aa04" in namespace "downward-api-4382" to be "success or failure"
May 14 03:19:06.945: INFO: Pod "downward-api-ee160e36-33f7-438b-9f52-2370d4c3aa04": Phase="Pending", Reason="", readiness=false. Elapsed: 15.386024ms
May 14 03:19:08.955: INFO: Pod "downward-api-ee160e36-33f7-438b-9f52-2370d4c3aa04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025551741s
May 14 03:19:10.972: INFO: Pod "downward-api-ee160e36-33f7-438b-9f52-2370d4c3aa04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042441984s
STEP: Saw pod success
May 14 03:19:10.972: INFO: Pod "downward-api-ee160e36-33f7-438b-9f52-2370d4c3aa04" satisfied condition "success or failure"
May 14 03:19:10.981: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod downward-api-ee160e36-33f7-438b-9f52-2370d4c3aa04 container dapi-container: <nil>
STEP: delete the pod
May 14 03:19:11.132: INFO: Waiting for pod downward-api-ee160e36-33f7-438b-9f52-2370d4c3aa04 to disappear
May 14 03:19:11.152: INFO: Pod downward-api-ee160e36-33f7-438b-9f52-2370d4c3aa04 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:19:11.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4382" for this suite.
May 14 03:19:17.186: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:19:17.628: INFO: namespace downward-api-4382 deletion completed in 6.460135086s

â€¢ [SLOW TEST:11.113 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:19:17.636: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-3286
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:19:34.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3286" for this suite.
May 14 03:19:40.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:19:40.599: INFO: namespace job-3286 deletion completed in 6.455512822s

â€¢ [SLOW TEST:22.965 seconds]
[sig-apps] Job
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:19:40.604: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-834
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-834.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-834.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 14 03:19:46.861: INFO: Unable to read wheezy_udp@PodARecord from pod dns-834/dns-test-9cfcfea1-b8b5-4add-9997-7307c440bb74: the server could not find the requested resource (get pods dns-test-9cfcfea1-b8b5-4add-9997-7307c440bb74)
May 14 03:19:46.867: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-834/dns-test-9cfcfea1-b8b5-4add-9997-7307c440bb74: the server could not find the requested resource (get pods dns-test-9cfcfea1-b8b5-4add-9997-7307c440bb74)
May 14 03:19:46.886: INFO: Unable to read jessie_udp@PodARecord from pod dns-834/dns-test-9cfcfea1-b8b5-4add-9997-7307c440bb74: the server could not find the requested resource (get pods dns-test-9cfcfea1-b8b5-4add-9997-7307c440bb74)
May 14 03:19:46.925: INFO: Unable to read jessie_tcp@PodARecord from pod dns-834/dns-test-9cfcfea1-b8b5-4add-9997-7307c440bb74: the server could not find the requested resource (get pods dns-test-9cfcfea1-b8b5-4add-9997-7307c440bb74)
May 14 03:19:46.925: INFO: Lookups using dns-834/dns-test-9cfcfea1-b8b5-4add-9997-7307c440bb74 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

May 14 03:19:52.010: INFO: DNS probes using dns-834/dns-test-9cfcfea1-b8b5-4add-9997-7307c440bb74 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:19:52.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-834" for this suite.
May 14 03:19:58.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:19:58.470: INFO: namespace dns-834 deletion completed in 6.31334767s

â€¢ [SLOW TEST:17.867 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:19:58.472: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2184
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-033b82fd-7cb2-4137-b250-b946d728619e
STEP: Creating a pod to test consume secrets
May 14 03:19:58.739: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-be4a2bd0-34ac-4c4c-b216-875d1cc251cb" in namespace "projected-2184" to be "success or failure"
May 14 03:19:58.755: INFO: Pod "pod-projected-secrets-be4a2bd0-34ac-4c4c-b216-875d1cc251cb": Phase="Pending", Reason="", readiness=false. Elapsed: 15.650372ms
May 14 03:20:00.858: INFO: Pod "pod-projected-secrets-be4a2bd0-34ac-4c4c-b216-875d1cc251cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.118714606s
May 14 03:20:02.865: INFO: Pod "pod-projected-secrets-be4a2bd0-34ac-4c4c-b216-875d1cc251cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.125409544s
STEP: Saw pod success
May 14 03:20:02.865: INFO: Pod "pod-projected-secrets-be4a2bd0-34ac-4c4c-b216-875d1cc251cb" satisfied condition "success or failure"
May 14 03:20:02.868: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-projected-secrets-be4a2bd0-34ac-4c4c-b216-875d1cc251cb container projected-secret-volume-test: <nil>
STEP: delete the pod
May 14 03:20:02.910: INFO: Waiting for pod pod-projected-secrets-be4a2bd0-34ac-4c4c-b216-875d1cc251cb to disappear
May 14 03:20:02.923: INFO: Pod pod-projected-secrets-be4a2bd0-34ac-4c4c-b216-875d1cc251cb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:20:02.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2184" for this suite.
May 14 03:20:09.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:20:09.515: INFO: namespace projected-2184 deletion completed in 6.575860685s

â€¢ [SLOW TEST:11.043 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:20:09.516: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6589
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 14 03:20:10.983: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023210, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023210, loc:(*time.Location)(0x78a2900)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-86d95b659d\""}}, CollisionCount:(*int32)(nil)}
May 14 03:20:12.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023211, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023211, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023211, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023210, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 14 03:20:16.024: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:20:16.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6589" for this suite.
May 14 03:20:28.186: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:20:29.447: INFO: namespace webhook-6589 deletion completed in 13.306124473s
STEP: Destroying namespace "webhook-6589-markers" for this suite.
May 14 03:20:35.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:20:35.862: INFO: namespace webhook-6589-markers deletion completed in 6.412906925s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:26.385 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:20:35.902: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6653
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:20:40.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6653" for this suite.
May 14 03:20:46.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:20:46.426: INFO: namespace kubelet-test-6653 deletion completed in 6.306565966s

â€¢ [SLOW TEST:10.525 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:20:46.433: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9432
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 14 03:20:46.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-9432'
May 14 03:20:47.868: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 14 03:20:47.868: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
May 14 03:20:47.898: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-hlrbp]
May 14 03:20:47.898: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-hlrbp" in namespace "kubectl-9432" to be "running and ready"
May 14 03:20:47.914: INFO: Pod "e2e-test-httpd-rc-hlrbp": Phase="Pending", Reason="", readiness=false. Elapsed: 15.556052ms
May 14 03:20:49.920: INFO: Pod "e2e-test-httpd-rc-hlrbp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021687771s
May 14 03:20:51.927: INFO: Pod "e2e-test-httpd-rc-hlrbp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028426187s
May 14 03:20:53.936: INFO: Pod "e2e-test-httpd-rc-hlrbp": Phase="Running", Reason="", readiness=true. Elapsed: 6.037475985s
May 14 03:20:53.936: INFO: Pod "e2e-test-httpd-rc-hlrbp" satisfied condition "running and ready"
May 14 03:20:53.936: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-hlrbp]
May 14 03:20:53.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 logs rc/e2e-test-httpd-rc --namespace=kubectl-9432'
May 14 03:20:54.230: INFO: stderr: ""
May 14 03:20:54.231: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.100.136.107. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.100.136.107. Set the 'ServerName' directive globally to suppress this message\n[Thu May 14 03:20:51.217663 2020] [mpm_event:notice] [pid 1:tid 139974415666024] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Thu May 14 03:20:51.217772 2020] [core:notice] [pid 1:tid 139974415666024] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
May 14 03:20:54.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 delete rc e2e-test-httpd-rc --namespace=kubectl-9432'
May 14 03:20:54.488: INFO: stderr: ""
May 14 03:20:54.488: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:20:54.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9432" for this suite.
May 14 03:21:00.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:21:00.878: INFO: namespace kubectl-9432 deletion completed in 6.380826753s

â€¢ [SLOW TEST:14.447 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:21:00.884: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-1894
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 14 03:21:02.797: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May 14 03:21:04.843: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023262, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023262, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023262, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023262, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 14 03:21:07.876: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 03:21:07.964: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:21:10.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1894" for this suite.
May 14 03:21:16.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:21:16.560: INFO: namespace crd-webhook-1894 deletion completed in 6.516919015s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:15.756 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:21:16.655: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9594
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-8e248ba1-2764-4aba-b1e6-44709199b1c0
STEP: Creating a pod to test consume configMaps
May 14 03:21:16.940: INFO: Waiting up to 5m0s for pod "pod-configmaps-752021d4-94f8-4444-90fa-2de93e07e146" in namespace "configmap-9594" to be "success or failure"
May 14 03:21:16.958: INFO: Pod "pod-configmaps-752021d4-94f8-4444-90fa-2de93e07e146": Phase="Pending", Reason="", readiness=false. Elapsed: 17.431667ms
May 14 03:21:18.981: INFO: Pod "pod-configmaps-752021d4-94f8-4444-90fa-2de93e07e146": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040368129s
May 14 03:21:20.987: INFO: Pod "pod-configmaps-752021d4-94f8-4444-90fa-2de93e07e146": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04608932s
STEP: Saw pod success
May 14 03:21:20.987: INFO: Pod "pod-configmaps-752021d4-94f8-4444-90fa-2de93e07e146" satisfied condition "success or failure"
May 14 03:21:20.991: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-configmaps-752021d4-94f8-4444-90fa-2de93e07e146 container configmap-volume-test: <nil>
STEP: delete the pod
May 14 03:21:21.034: INFO: Waiting for pod pod-configmaps-752021d4-94f8-4444-90fa-2de93e07e146 to disappear
May 14 03:21:21.045: INFO: Pod pod-configmaps-752021d4-94f8-4444-90fa-2de93e07e146 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:21:21.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9594" for this suite.
May 14 03:21:27.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:21:27.483: INFO: namespace configmap-9594 deletion completed in 6.427774327s

â€¢ [SLOW TEST:10.834 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:21:27.486: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9407
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 03:21:27.662: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 14 03:21:27.685: INFO: Pod name sample-pod: Found 0 pods out of 1
May 14 03:21:32.693: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 14 03:21:32.693: INFO: Creating deployment "test-rolling-update-deployment"
May 14 03:21:32.710: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 14 03:21:32.756: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
May 14 03:21:34.765: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 14 03:21:34.770: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023292, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023292, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023292, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725023292, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 03:21:36.789: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May 14 03:21:36.841: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9407 /apis/apps/v1/namespaces/deployment-9407/deployments/test-rolling-update-deployment a38bf4cc-5534-4e7c-9b35-50386023e7d0 203278 1 2020-05-14 03:21:32 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002692818 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-05-14 03:21:32 +0000 UTC,LastTransitionTime:2020-05-14 03:21:32 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-05-14 03:21:35 +0000 UTC,LastTransitionTime:2020-05-14 03:21:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 14 03:21:36.852: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-9407 /apis/apps/v1/namespaces/deployment-9407/replicasets/test-rolling-update-deployment-55d946486 af6f5399-1114-489b-b2bc-487f920f5247 203268 1 2020-05-14 03:21:32 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment a38bf4cc-5534-4e7c-9b35-50386023e7d0 0xc002692cf0 0xc002692cf1}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002692d58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 14 03:21:36.853: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 14 03:21:36.853: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9407 /apis/apps/v1/namespaces/deployment-9407/replicasets/test-rolling-update-controller 35f90cfc-e2d1-4428-b9ab-809e52766f76 203277 2 2020-05-14 03:21:27 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment a38bf4cc-5534-4e7c-9b35-50386023e7d0 0xc002692c27 0xc002692c28}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002692c88 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 14 03:21:36.868: INFO: Pod "test-rolling-update-deployment-55d946486-fmdsb" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-fmdsb test-rolling-update-deployment-55d946486- deployment-9407 /api/v1/namespaces/deployment-9407/pods/test-rolling-update-deployment-55d946486-fmdsb aacd1632-d81b-4c92-9231-e96cab4ca6fb 203267 0 2020-05-14 03:21:32 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[cni.projectcalico.org/podIP:10.100.136.83/32 cni.projectcalico.org/podIPs:10.100.136.83/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 af6f5399-1114-489b-b2bc-487f920f5247 0xc002366e70 0xc002366e71}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2k492,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2k492,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2k492,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:21:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:21:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:21:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:21:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.13,PodIP:10.100.136.83,StartTime:2020-05-14 03:21:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-14 03:21:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://f095cca02b3faf77333581a0b70e3ed73ae8e0fe512507cd158eb5436f1b0ff2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.136.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:21:36.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9407" for this suite.
May 14 03:21:43.543: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:21:44.073: INFO: namespace deployment-9407 deletion completed in 7.196091124s

â€¢ [SLOW TEST:16.587 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:21:44.074: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9628
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:21:55.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9628" for this suite.
May 14 03:22:01.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:22:01.902: INFO: namespace resourcequota-9628 deletion completed in 6.460660776s

â€¢ [SLOW TEST:17.829 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:22:01.909: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8329
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-778ab4a5-1031-410a-b8b0-38b0cfd71fed in namespace container-probe-8329
May 14 03:22:06.218: INFO: Started pod busybox-778ab4a5-1031-410a-b8b0-38b0cfd71fed in namespace container-probe-8329
STEP: checking the pod's current state and verifying that restartCount is present
May 14 03:22:06.225: INFO: Initial restart count of pod busybox-778ab4a5-1031-410a-b8b0-38b0cfd71fed is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:26:07.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8329" for this suite.
May 14 03:26:13.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:26:13.611: INFO: namespace container-probe-8329 deletion completed in 6.503596732s

â€¢ [SLOW TEST:251.704 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:26:13.613: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1251
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 14 03:26:13.927: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e5524355-1451-4f1c-9284-6682a8e13b92" in namespace "downward-api-1251" to be "success or failure"
May 14 03:26:13.952: INFO: Pod "downwardapi-volume-e5524355-1451-4f1c-9284-6682a8e13b92": Phase="Pending", Reason="", readiness=false. Elapsed: 25.252484ms
May 14 03:26:17.017: INFO: Pod "downwardapi-volume-e5524355-1451-4f1c-9284-6682a8e13b92": Phase="Pending", Reason="", readiness=false. Elapsed: 3.090245354s
May 14 03:26:19.026: INFO: Pod "downwardapi-volume-e5524355-1451-4f1c-9284-6682a8e13b92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 5.09949978s
STEP: Saw pod success
May 14 03:26:19.026: INFO: Pod "downwardapi-volume-e5524355-1451-4f1c-9284-6682a8e13b92" satisfied condition "success or failure"
May 14 03:26:19.036: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod downwardapi-volume-e5524355-1451-4f1c-9284-6682a8e13b92 container client-container: <nil>
STEP: delete the pod
May 14 03:26:19.227: INFO: Waiting for pod downwardapi-volume-e5524355-1451-4f1c-9284-6682a8e13b92 to disappear
May 14 03:26:19.240: INFO: Pod downwardapi-volume-e5524355-1451-4f1c-9284-6682a8e13b92 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:26:19.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1251" for this suite.
May 14 03:26:25.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:26:25.660: INFO: namespace downward-api-1251 deletion completed in 6.412432296s

â€¢ [SLOW TEST:12.048 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:26:25.667: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2090
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
May 14 03:26:31.968: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-817715921 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
May 14 03:26:37.154: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:26:37.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2090" for this suite.
May 14 03:26:43.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:26:43.503: INFO: namespace pods-2090 deletion completed in 6.336421272s

â€¢ [SLOW TEST:17.837 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:26:43.511: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-7850
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 14 03:26:49.001: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:26:50.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7850" for this suite.
May 14 03:27:18.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:27:18.500: INFO: namespace replicaset-7850 deletion completed in 28.452737869s

â€¢ [SLOW TEST:34.989 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:27:18.507: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6995
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
May 14 03:27:23.438: INFO: Successfully updated pod "annotationupdate4ec66bae-5095-42f6-81f7-e31c1e509248"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:27:25.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6995" for this suite.
May 14 03:27:53.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:27:54.068: INFO: namespace downward-api-6995 deletion completed in 28.478289116s

â€¢ [SLOW TEST:35.562 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:27:54.078: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-787
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
May 14 03:27:54.348: INFO: Waiting up to 5m0s for pod "var-expansion-e2e5dda3-6e9e-4ca3-823c-fe4f1c79a930" in namespace "var-expansion-787" to be "success or failure"
May 14 03:27:54.364: INFO: Pod "var-expansion-e2e5dda3-6e9e-4ca3-823c-fe4f1c79a930": Phase="Pending", Reason="", readiness=false. Elapsed: 16.556697ms
May 14 03:27:56.371: INFO: Pod "var-expansion-e2e5dda3-6e9e-4ca3-823c-fe4f1c79a930": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023075356s
May 14 03:27:58.380: INFO: Pod "var-expansion-e2e5dda3-6e9e-4ca3-823c-fe4f1c79a930": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031792521s
STEP: Saw pod success
May 14 03:27:58.380: INFO: Pod "var-expansion-e2e5dda3-6e9e-4ca3-823c-fe4f1c79a930" satisfied condition "success or failure"
May 14 03:27:58.391: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod var-expansion-e2e5dda3-6e9e-4ca3-823c-fe4f1c79a930 container dapi-container: <nil>
STEP: delete the pod
May 14 03:27:58.437: INFO: Waiting for pod var-expansion-e2e5dda3-6e9e-4ca3-823c-fe4f1c79a930 to disappear
May 14 03:27:58.453: INFO: Pod var-expansion-e2e5dda3-6e9e-4ca3-823c-fe4f1c79a930 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:27:58.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-787" for this suite.
May 14 03:28:04.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:28:04.866: INFO: namespace var-expansion-787 deletion completed in 6.402364382s

â€¢ [SLOW TEST:10.790 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:28:04.873: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2279
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 03:28:05.259: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 14 03:28:12.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 --namespace=crd-publish-openapi-2279 create -f -'
May 14 03:28:14.852: INFO: stderr: ""
May 14 03:28:14.852: INFO: stdout: "e2e-test-crd-publish-openapi-1953-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 14 03:28:14.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 --namespace=crd-publish-openapi-2279 delete e2e-test-crd-publish-openapi-1953-crds test-cr'
May 14 03:28:15.167: INFO: stderr: ""
May 14 03:28:15.167: INFO: stdout: "e2e-test-crd-publish-openapi-1953-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May 14 03:28:15.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 --namespace=crd-publish-openapi-2279 apply -f -'
May 14 03:28:16.018: INFO: stderr: ""
May 14 03:28:16.018: INFO: stdout: "e2e-test-crd-publish-openapi-1953-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 14 03:28:16.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 --namespace=crd-publish-openapi-2279 delete e2e-test-crd-publish-openapi-1953-crds test-cr'
May 14 03:28:16.332: INFO: stderr: ""
May 14 03:28:16.332: INFO: stdout: "e2e-test-crd-publish-openapi-1953-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 14 03:28:16.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 explain e2e-test-crd-publish-openapi-1953-crds'
May 14 03:28:16.971: INFO: stderr: ""
May 14 03:28:16.971: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1953-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:28:23.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2279" for this suite.
May 14 03:28:29.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:28:29.707: INFO: namespace crd-publish-openapi-2279 deletion completed in 6.599028828s

â€¢ [SLOW TEST:24.835 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:28:29.709: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4827
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-4827
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 14 03:28:29.924: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 14 03:28:56.135: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.230.55 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4827 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 03:28:56.136: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
May 14 03:28:57.515: INFO: Found all expected endpoints: [netserver-0]
May 14 03:28:57.520: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.136.117 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4827 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 03:28:57.520: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
May 14 03:28:58.928: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:28:58.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4827" for this suite.
May 14 03:29:10.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:29:11.458: INFO: namespace pod-network-test-4827 deletion completed in 12.521553692s

â€¢ [SLOW TEST:41.750 seconds]
[sig-network] Networking
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:29:11.468: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6265
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
May 14 03:29:11.645: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:29:51.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6265" for this suite.
May 14 03:29:57.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:29:57.648: INFO: namespace crd-publish-openapi-6265 deletion completed in 6.441318173s

â€¢ [SLOW TEST:46.181 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:29:57.664: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1102
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May 14 03:29:57.951: INFO: Waiting up to 5m0s for pod "downward-api-e7be5342-d7f2-4510-b3b3-5118f774f479" in namespace "downward-api-1102" to be "success or failure"
May 14 03:29:57.964: INFO: Pod "downward-api-e7be5342-d7f2-4510-b3b3-5118f774f479": Phase="Pending", Reason="", readiness=false. Elapsed: 12.811415ms
May 14 03:29:59.981: INFO: Pod "downward-api-e7be5342-d7f2-4510-b3b3-5118f774f479": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029991762s
May 14 03:30:01.988: INFO: Pod "downward-api-e7be5342-d7f2-4510-b3b3-5118f774f479": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037075935s
STEP: Saw pod success
May 14 03:30:01.988: INFO: Pod "downward-api-e7be5342-d7f2-4510-b3b3-5118f774f479" satisfied condition "success or failure"
May 14 03:30:01.992: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod downward-api-e7be5342-d7f2-4510-b3b3-5118f774f479 container dapi-container: <nil>
STEP: delete the pod
May 14 03:30:02.141: INFO: Waiting for pod downward-api-e7be5342-d7f2-4510-b3b3-5118f774f479 to disappear
May 14 03:30:02.149: INFO: Pod downward-api-e7be5342-d7f2-4510-b3b3-5118f774f479 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:30:02.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1102" for this suite.
May 14 03:30:08.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:30:08.499: INFO: namespace downward-api-1102 deletion completed in 6.335932944s

â€¢ [SLOW TEST:10.837 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:30:08.510: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1182
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
May 14 03:30:08.680: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:30:14.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1182" for this suite.
May 14 03:30:20.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:30:21.173: INFO: namespace init-container-1182 deletion completed in 6.511128012s

â€¢ [SLOW TEST:12.664 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:30:21.174: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9178
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 14 03:30:21.474: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9178 /api/v1/namespaces/watch-9178/configmaps/e2e-watch-test-resource-version cf98a34a-ce89-43a1-ac97-8dddeaf51fc2 205151 0 2020-05-14 03:30:21 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 14 03:30:21.475: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9178 /api/v1/namespaces/watch-9178/configmaps/e2e-watch-test-resource-version cf98a34a-ce89-43a1-ac97-8dddeaf51fc2 205152 0 2020-05-14 03:30:21 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:30:21.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9178" for this suite.
May 14 03:30:27.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:30:28.208: INFO: namespace watch-9178 deletion completed in 6.726032136s

â€¢ [SLOW TEST:7.034 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:30:28.210: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-648
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
May 14 03:30:28.783: INFO: Waiting up to 5m0s for pod "var-expansion-5342dcf3-11e3-4800-bd1d-7bfd9d312e14" in namespace "var-expansion-648" to be "success or failure"
May 14 03:30:28.799: INFO: Pod "var-expansion-5342dcf3-11e3-4800-bd1d-7bfd9d312e14": Phase="Pending", Reason="", readiness=false. Elapsed: 15.947301ms
May 14 03:30:30.809: INFO: Pod "var-expansion-5342dcf3-11e3-4800-bd1d-7bfd9d312e14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025757104s
May 14 03:30:32.819: INFO: Pod "var-expansion-5342dcf3-11e3-4800-bd1d-7bfd9d312e14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035746863s
STEP: Saw pod success
May 14 03:30:32.819: INFO: Pod "var-expansion-5342dcf3-11e3-4800-bd1d-7bfd9d312e14" satisfied condition "success or failure"
May 14 03:30:32.824: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod var-expansion-5342dcf3-11e3-4800-bd1d-7bfd9d312e14 container dapi-container: <nil>
STEP: delete the pod
May 14 03:30:32.881: INFO: Waiting for pod var-expansion-5342dcf3-11e3-4800-bd1d-7bfd9d312e14 to disappear
May 14 03:30:32.888: INFO: Pod var-expansion-5342dcf3-11e3-4800-bd1d-7bfd9d312e14 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:30:32.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-648" for this suite.
May 14 03:30:38.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:30:39.430: INFO: namespace var-expansion-648 deletion completed in 6.535227995s

â€¢ [SLOW TEST:11.222 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:30:39.442: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1564
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 14 03:30:39.682: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83b0c05d-e65d-4945-919e-c3a6f842ee3b" in namespace "downward-api-1564" to be "success or failure"
May 14 03:30:39.721: INFO: Pod "downwardapi-volume-83b0c05d-e65d-4945-919e-c3a6f842ee3b": Phase="Pending", Reason="", readiness=false. Elapsed: 39.779491ms
May 14 03:30:41.727: INFO: Pod "downwardapi-volume-83b0c05d-e65d-4945-919e-c3a6f842ee3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04535597s
May 14 03:30:43.743: INFO: Pod "downwardapi-volume-83b0c05d-e65d-4945-919e-c3a6f842ee3b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060895197s
May 14 03:30:45.756: INFO: Pod "downwardapi-volume-83b0c05d-e65d-4945-919e-c3a6f842ee3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.074596734s
STEP: Saw pod success
May 14 03:30:45.757: INFO: Pod "downwardapi-volume-83b0c05d-e65d-4945-919e-c3a6f842ee3b" satisfied condition "success or failure"
May 14 03:30:45.779: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod downwardapi-volume-83b0c05d-e65d-4945-919e-c3a6f842ee3b container client-container: <nil>
STEP: delete the pod
May 14 03:30:45.832: INFO: Waiting for pod downwardapi-volume-83b0c05d-e65d-4945-919e-c3a6f842ee3b to disappear
May 14 03:30:45.840: INFO: Pod downwardapi-volume-83b0c05d-e65d-4945-919e-c3a6f842ee3b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:30:45.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1564" for this suite.
May 14 03:30:51.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:30:52.311: INFO: namespace downward-api-1564 deletion completed in 6.463069644s

â€¢ [SLOW TEST:12.869 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:30:52.314: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4550
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May 14 03:30:53.272: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 14 03:30:54.343: INFO: Waiting for terminating namespaces to be deleted...
May 14 03:30:54.354: INFO: 
Logging pods the kubelet thinks is on node k8s-fcos-flwang-pyqjxt4oox23-node-0 before test
May 14 03:30:54.575: INFO: kube-dns-autoscaler-78d89dc59b-tjwjl from kube-system started at 2020-05-13 10:18:10 +0000 UTC (1 container statuses recorded)
May 14 03:30:54.575: INFO: 	Container autoscaler ready: true, restart count 0
May 14 03:30:54.575: INFO: prometheus-operator-prometheus-node-exporter-dsbl2 from kube-system started at 2020-05-13 10:20:16 +0000 UTC (1 container statuses recorded)
May 14 03:30:54.575: INFO: 	Container node-exporter ready: true, restart count 0
May 14 03:30:54.575: INFO: prometheus-operator-grafana-597f5fdffc-2r2zm from kube-system started at 2020-05-13 10:20:16 +0000 UTC (2 container statuses recorded)
May 14 03:30:54.575: INFO: 	Container grafana ready: true, restart count 0
May 14 03:30:54.575: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
May 14 03:30:54.575: INFO: prometheus-operator-855b67c86c-m4rg8 from kube-system started at 2020-05-13 10:20:16 +0000 UTC (2 container statuses recorded)
May 14 03:30:54.575: INFO: 	Container prometheus ready: true, restart count 0
May 14 03:30:54.575: INFO: 	Container tls-proxy ready: true, restart count 0
May 14 03:30:54.575: INFO: sonobuoy-systemd-logs-daemon-set-2dde1b3f033a4765-q8j6w from sonobuoy started at 2020-05-14 01:43:10 +0000 UTC (2 container statuses recorded)
May 14 03:30:54.575: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 14 03:30:54.575: INFO: 	Container systemd-logs ready: true, restart count 0
May 14 03:30:54.575: INFO: install-prometheus-operator-job-kz4gd from magnum-tiller started at 2020-05-13 10:18:10 +0000 UTC (1 container statuses recorded)
May 14 03:30:54.576: INFO: 	Container config-helm ready: false, restart count 0
May 14 03:30:54.576: INFO: install-prometheus-adapter-job-pmgx5 from magnum-tiller started at 2020-05-13 10:18:10 +0000 UTC (1 container statuses recorded)
May 14 03:30:54.576: INFO: 	Container config-helm ready: false, restart count 0
May 14 03:30:54.576: INFO: install-metrics-server-job-r22dn from magnum-tiller started at 2020-05-13 10:18:20 +0000 UTC (1 container statuses recorded)
May 14 03:30:54.576: INFO: 	Container config-helm ready: false, restart count 0
May 14 03:30:54.576: INFO: prometheus-adapter-5c56dfd965-bfskz from kube-system started at 2020-05-13 10:19:49 +0000 UTC (1 container statuses recorded)
May 14 03:30:54.576: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 14 03:30:54.576: INFO: alertmanager-prometheus-alertmanager-0 from kube-system started at 2020-05-13 10:21:01 +0000 UTC (2 container statuses recorded)
May 14 03:30:54.576: INFO: 	Container alertmanager ready: true, restart count 0
May 14 03:30:54.576: INFO: 	Container config-reloader ready: true, restart count 0
May 14 03:30:54.576: INFO: calico-node-d8klf from kube-system started at 2020-05-13 10:17:39 +0000 UTC (1 container statuses recorded)
May 14 03:30:54.576: INFO: 	Container calico-node ready: true, restart count 0
May 14 03:30:54.576: INFO: prometheus-operator-kube-state-metrics-54bd6c856f-v6wkd from kube-system started at 2020-05-13 10:20:16 +0000 UTC (1 container statuses recorded)
May 14 03:30:54.576: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 14 03:30:54.576: INFO: prometheus-prometheus-prometheus-0 from kube-system started at 2020-05-13 10:21:12 +0000 UTC (3 container statuses recorded)
May 14 03:30:54.576: INFO: 	Container prometheus ready: true, restart count 1
May 14 03:30:54.576: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May 14 03:30:54.576: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May 14 03:30:54.576: INFO: npd-w82fn from kube-system started at 2020-05-13 10:18:10 +0000 UTC (1 container statuses recorded)
May 14 03:30:54.576: INFO: 	Container node-problem-detector ready: true, restart count 0
May 14 03:30:54.576: INFO: metrics-server-649fdb57b9-jw8v2 from kube-system started at 2020-05-13 10:19:49 +0000 UTC (1 container statuses recorded)
May 14 03:30:54.576: INFO: 	Container metrics-server ready: true, restart count 0
May 14 03:30:54.576: INFO: 
Logging pods the kubelet thinks is on node k8s-fcos-flwang-pyqjxt4oox23-node-1 before test
May 14 03:30:54.620: INFO: prometheus-operator-prometheus-node-exporter-64t57 from kube-system started at 2020-05-13 20:00:24 +0000 UTC (1 container statuses recorded)
May 14 03:30:54.620: INFO: 	Container node-exporter ready: true, restart count 0
May 14 03:30:54.620: INFO: sonobuoy-e2e-job-cf5c047f756b45c4 from sonobuoy started at 2020-05-14 01:43:09 +0000 UTC (2 container statuses recorded)
May 14 03:30:54.621: INFO: 	Container e2e ready: true, restart count 0
May 14 03:30:54.621: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 14 03:30:54.621: INFO: sonobuoy from sonobuoy started at 2020-05-14 01:43:05 +0000 UTC (1 container statuses recorded)
May 14 03:30:54.621: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 14 03:30:54.621: INFO: sonobuoy-systemd-logs-daemon-set-2dde1b3f033a4765-4wkjn from sonobuoy started at 2020-05-14 01:43:10 +0000 UTC (2 container statuses recorded)
May 14 03:30:54.621: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 14 03:30:54.621: INFO: 	Container systemd-logs ready: true, restart count 0
May 14 03:30:54.621: INFO: calico-node-cjvph from kube-system started at 2020-05-13 20:00:24 +0000 UTC (1 container statuses recorded)
May 14 03:30:54.621: INFO: 	Container calico-node ready: true, restart count 0
May 14 03:30:54.621: INFO: npd-v8wfq from kube-system started at 2020-05-13 20:01:37 +0000 UTC (1 container statuses recorded)
May 14 03:30:54.621: INFO: 	Container node-problem-detector ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-be63c6a5-e7ea-40b3-bb08-998e14c95617 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-be63c6a5-e7ea-40b3-bb08-998e14c95617 off the node k8s-fcos-flwang-pyqjxt4oox23-node-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-be63c6a5-e7ea-40b3-bb08-998e14c95617
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:36:07.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4550" for this suite.
May 14 03:36:27.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:36:28.051: INFO: namespace sched-pred-4550 deletion completed in 20.424862161s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:335.739 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:36:28.058: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-7811
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4380
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5807
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:37:02.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7811" for this suite.
May 14 03:37:08.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:37:09.300: INFO: namespace namespaces-7811 deletion completed in 6.543772697s
STEP: Destroying namespace "nsdeletetest-4380" for this suite.
May 14 03:37:09.319: INFO: Namespace nsdeletetest-4380 was already deleted
STEP: Destroying namespace "nsdeletetest-5807" for this suite.
May 14 03:37:15.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:37:15.716: INFO: namespace nsdeletetest-5807 deletion completed in 6.396401968s

â€¢ [SLOW TEST:47.658 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:37:15.725: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-799
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 14 03:37:16.001: INFO: Waiting up to 5m0s for pod "pod-a4735abc-1db7-4330-9959-fc164ffd06b2" in namespace "emptydir-799" to be "success or failure"
May 14 03:37:16.107: INFO: Pod "pod-a4735abc-1db7-4330-9959-fc164ffd06b2": Phase="Pending", Reason="", readiness=false. Elapsed: 104.759273ms
May 14 03:37:18.115: INFO: Pod "pod-a4735abc-1db7-4330-9959-fc164ffd06b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.113274603s
May 14 03:37:21.525: INFO: Pod "pod-a4735abc-1db7-4330-9959-fc164ffd06b2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.523291592s
May 14 03:37:23.531: INFO: Pod "pod-a4735abc-1db7-4330-9959-fc164ffd06b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.528912858s
STEP: Saw pod success
May 14 03:37:23.531: INFO: Pod "pod-a4735abc-1db7-4330-9959-fc164ffd06b2" satisfied condition "success or failure"
May 14 03:37:23.538: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-a4735abc-1db7-4330-9959-fc164ffd06b2 container test-container: <nil>
STEP: delete the pod
May 14 03:37:23.683: INFO: Waiting for pod pod-a4735abc-1db7-4330-9959-fc164ffd06b2 to disappear
May 14 03:37:23.690: INFO: Pod pod-a4735abc-1db7-4330-9959-fc164ffd06b2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:37:23.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-799" for this suite.
May 14 03:37:31.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:37:32.178: INFO: namespace emptydir-799 deletion completed in 8.480831913s

â€¢ [SLOW TEST:16.454 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:37:32.181: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6738
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-2afad4d6-1b37-4396-bb6c-007d6d1dcc64
STEP: Creating a pod to test consume secrets
May 14 03:37:32.418: INFO: Waiting up to 5m0s for pod "pod-secrets-8ecc7101-abc2-4486-b8c0-c7ed11e80085" in namespace "secrets-6738" to be "success or failure"
May 14 03:37:32.443: INFO: Pod "pod-secrets-8ecc7101-abc2-4486-b8c0-c7ed11e80085": Phase="Pending", Reason="", readiness=false. Elapsed: 25.070902ms
May 14 03:37:34.453: INFO: Pod "pod-secrets-8ecc7101-abc2-4486-b8c0-c7ed11e80085": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035234493s
May 14 03:37:36.460: INFO: Pod "pod-secrets-8ecc7101-abc2-4486-b8c0-c7ed11e80085": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04231616s
May 14 03:37:38.466: INFO: Pod "pod-secrets-8ecc7101-abc2-4486-b8c0-c7ed11e80085": Phase="Pending", Reason="", readiness=false. Elapsed: 6.047974249s
May 14 03:37:40.472: INFO: Pod "pod-secrets-8ecc7101-abc2-4486-b8c0-c7ed11e80085": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.053486444s
STEP: Saw pod success
May 14 03:37:40.472: INFO: Pod "pod-secrets-8ecc7101-abc2-4486-b8c0-c7ed11e80085" satisfied condition "success or failure"
May 14 03:37:40.477: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-secrets-8ecc7101-abc2-4486-b8c0-c7ed11e80085 container secret-volume-test: <nil>
STEP: delete the pod
May 14 03:37:40.510: INFO: Waiting for pod pod-secrets-8ecc7101-abc2-4486-b8c0-c7ed11e80085 to disappear
May 14 03:37:40.518: INFO: Pod pod-secrets-8ecc7101-abc2-4486-b8c0-c7ed11e80085 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:37:40.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6738" for this suite.
May 14 03:37:46.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:37:46.959: INFO: namespace secrets-6738 deletion completed in 6.429716256s

â€¢ [SLOW TEST:14.778 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:37:46.961: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9541
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 03:37:47.136: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:37:53.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9541" for this suite.
May 14 03:38:37.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:38:38.209: INFO: namespace pods-9541 deletion completed in 44.600705625s

â€¢ [SLOW TEST:51.249 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:38:38.234: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6686
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 14 03:38:38.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-6686'
May 14 03:38:40.255: INFO: stderr: ""
May 14 03:38:40.255: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
May 14 03:38:45.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pod e2e-test-httpd-pod --namespace=kubectl-6686 -o json'
May 14 03:38:45.600: INFO: stderr: ""
May 14 03:38:45.600: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.100.136.69/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.100.136.69/32\"\n        },\n        \"creationTimestamp\": \"2020-05-14T03:38:40Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6686\",\n        \"resourceVersion\": \"206773\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6686/pods/e2e-test-httpd-pod\",\n        \"uid\": \"ff299f03-4827-47ce-9dc9-280afa1b3fd1\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-mbztt\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-fcos-flwang-pyqjxt4oox23-node-1\",\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-mbztt\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-mbztt\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-05-14T03:38:40Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-05-14T03:38:44Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-05-14T03:38:44Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-05-14T03:38:40Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://c51d2317f8dd4c10a31bb3623e5b61f045a9cdcad6fa1cd7cc5e27b48fc8c4d1\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-05-14T03:38:43Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.0.13\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.100.136.69\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.100.136.69\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-05-14T03:38:40Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 14 03:38:45.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 replace -f - --namespace=kubectl-6686'
May 14 03:38:46.304: INFO: stderr: ""
May 14 03:38:46.304: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
May 14 03:38:46.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 delete pods e2e-test-httpd-pod --namespace=kubectl-6686'
May 14 03:38:50.880: INFO: stderr: ""
May 14 03:38:50.880: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:38:50.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6686" for this suite.
May 14 03:38:56.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:38:57.192: INFO: namespace kubectl-6686 deletion completed in 6.300160998s

â€¢ [SLOW TEST:18.958 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:38:57.199: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9220
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
May 14 03:39:10.836: INFO: 10 pods remaining
May 14 03:39:10.837: INFO: 10 pods has nil DeletionTimestamp
May 14 03:39:10.837: INFO: 
STEP: Gathering metrics
May 14 03:39:14.308: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:39:14.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0514 03:39:14.307661      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-9220" for this suite.
May 14 03:39:22.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:39:23.222: INFO: namespace gc-9220 deletion completed in 8.553297459s

â€¢ [SLOW TEST:26.024 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:39:23.242: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-353
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-61ac8c05-4c7c-4b34-8f60-897bba3d8216 in namespace container-probe-353
May 14 03:39:29.525: INFO: Started pod liveness-61ac8c05-4c7c-4b34-8f60-897bba3d8216 in namespace container-probe-353
STEP: checking the pod's current state and verifying that restartCount is present
May 14 03:39:29.531: INFO: Initial restart count of pod liveness-61ac8c05-4c7c-4b34-8f60-897bba3d8216 is 0
May 14 03:39:50.560: INFO: Restart count of pod container-probe-353/liveness-61ac8c05-4c7c-4b34-8f60-897bba3d8216 is now 1 (21.028893713s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:39:50.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-353" for this suite.
May 14 03:39:59.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:40:01.230: INFO: namespace container-probe-353 deletion completed in 10.59204227s

â€¢ [SLOW TEST:37.989 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:40:01.246: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7053
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
May 14 03:40:01.712: INFO: PodSpec: initContainers in spec.initContainers
May 14 03:41:02.935: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-8da01a2c-b8bf-4fbb-86b7-7111c1d713bf", GenerateName:"", Namespace:"init-container-7053", SelfLink:"/api/v1/namespaces/init-container-7053/pods/pod-init-8da01a2c-b8bf-4fbb-86b7-7111c1d713bf", UID:"8916bc56-0d56-438c-bc24-4df394d8df6a", ResourceVersion:"207263", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63725024402, loc:(*time.Location)(0x78a2900)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"712630644"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.100.136.76/32", "cni.projectcalico.org/podIPs":"10.100.136.76/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-hk8n2", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0015c5740), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hk8n2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hk8n2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hk8n2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0026938a8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-fcos-flwang-pyqjxt4oox23-node-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001f88ba0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002693940)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002693960)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002693968), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725024402, loc:(*time.Location)(0x78a2900)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725024402, loc:(*time.Location)(0x78a2900)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725024402, loc:(*time.Location)(0x78a2900)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725024402, loc:(*time.Location)(0x78a2900)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.0.13", PodIP:"10.100.136.76", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.100.136.76"}}, StartTime:(*v1.Time)(0xc0030d8180), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0001899d0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000189a40)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://335c3f7efab8771a9248f0d45d5f31fdfaf1aebc34a9093c42148de8249c2240", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0030d81c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0030d81a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc0026939ef)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:41:02.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7053" for this suite.
May 14 03:42:15.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:42:15.544: INFO: namespace init-container-7053 deletion completed in 1m12.540644847s

â€¢ [SLOW TEST:134.299 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:42:15.548: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5880
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
May 14 03:42:15.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 create -f - --namespace=kubectl-5880'
May 14 03:42:16.324: INFO: stderr: ""
May 14 03:42:16.324: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 14 03:42:16.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5880'
May 14 03:42:17.317: INFO: stderr: ""
May 14 03:42:17.317: INFO: stdout: ""
STEP: Replicas for name=update-demo: expected=2 actual=0
May 14 03:42:22.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5880'
May 14 03:42:22.738: INFO: stderr: ""
May 14 03:42:22.738: INFO: stdout: "update-demo-nautilus-j2p66 update-demo-nautilus-lkfxm "
May 14 03:42:22.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-nautilus-j2p66 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5880'
May 14 03:42:22.994: INFO: stderr: ""
May 14 03:42:22.994: INFO: stdout: "true"
May 14 03:42:22.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-nautilus-j2p66 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5880'
May 14 03:42:23.339: INFO: stderr: ""
May 14 03:42:23.339: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 14 03:42:23.340: INFO: validating pod update-demo-nautilus-j2p66
May 14 03:42:23.355: INFO: got data: {
  "image": "nautilus.jpg"
}

May 14 03:42:23.356: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 14 03:42:23.356: INFO: update-demo-nautilus-j2p66 is verified up and running
May 14 03:42:23.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-nautilus-lkfxm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5880'
May 14 03:42:23.637: INFO: stderr: ""
May 14 03:42:23.638: INFO: stdout: "true"
May 14 03:42:23.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-nautilus-lkfxm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5880'
May 14 03:42:23.916: INFO: stderr: ""
May 14 03:42:23.916: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 14 03:42:23.916: INFO: validating pod update-demo-nautilus-lkfxm
May 14 03:42:23.932: INFO: got data: {
  "image": "nautilus.jpg"
}

May 14 03:42:23.932: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 14 03:42:23.932: INFO: update-demo-nautilus-lkfxm is verified up and running
STEP: scaling down the replication controller
May 14 03:42:23.961: INFO: scanned /root for discovery docs: <nil>
May 14 03:42:23.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-5880'
May 14 03:42:24.610: INFO: stderr: ""
May 14 03:42:24.611: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 14 03:42:24.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5880'
May 14 03:42:25.065: INFO: stderr: ""
May 14 03:42:25.066: INFO: stdout: "update-demo-nautilus-j2p66 update-demo-nautilus-lkfxm "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 14 03:42:30.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5880'
May 14 03:42:30.390: INFO: stderr: ""
May 14 03:42:30.390: INFO: stdout: "update-demo-nautilus-j2p66 update-demo-nautilus-lkfxm "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 14 03:42:35.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5880'
May 14 03:42:35.698: INFO: stderr: ""
May 14 03:42:35.698: INFO: stdout: "update-demo-nautilus-lkfxm "
May 14 03:42:35.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-nautilus-lkfxm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5880'
May 14 03:42:36.001: INFO: stderr: ""
May 14 03:42:36.001: INFO: stdout: "true"
May 14 03:42:36.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-nautilus-lkfxm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5880'
May 14 03:42:36.359: INFO: stderr: ""
May 14 03:42:36.359: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 14 03:42:36.367: INFO: validating pod update-demo-nautilus-lkfxm
May 14 03:42:36.385: INFO: got data: {
  "image": "nautilus.jpg"
}

May 14 03:42:36.385: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 14 03:42:36.385: INFO: update-demo-nautilus-lkfxm is verified up and running
STEP: scaling up the replication controller
May 14 03:42:36.405: INFO: scanned /root for discovery docs: <nil>
May 14 03:42:36.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-5880'
May 14 03:42:36.980: INFO: stderr: ""
May 14 03:42:36.980: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 14 03:42:36.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5880'
May 14 03:42:37.274: INFO: stderr: ""
May 14 03:42:37.274: INFO: stdout: "update-demo-nautilus-lkfxm update-demo-nautilus-zwgks "
May 14 03:42:37.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-nautilus-lkfxm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5880'
May 14 03:42:37.579: INFO: stderr: ""
May 14 03:42:37.579: INFO: stdout: "true"
May 14 03:42:37.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-nautilus-lkfxm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5880'
May 14 03:42:37.874: INFO: stderr: ""
May 14 03:42:37.874: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 14 03:42:37.874: INFO: validating pod update-demo-nautilus-lkfxm
May 14 03:42:37.889: INFO: got data: {
  "image": "nautilus.jpg"
}

May 14 03:42:37.889: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 14 03:42:37.889: INFO: update-demo-nautilus-lkfxm is verified up and running
May 14 03:42:37.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-nautilus-zwgks -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5880'
May 14 03:42:38.225: INFO: stderr: ""
May 14 03:42:38.225: INFO: stdout: ""
May 14 03:42:38.225: INFO: update-demo-nautilus-zwgks is created but not running
May 14 03:42:43.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5880'
May 14 03:42:43.477: INFO: stderr: ""
May 14 03:42:43.477: INFO: stdout: "update-demo-nautilus-lkfxm update-demo-nautilus-zwgks "
May 14 03:42:43.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-nautilus-lkfxm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5880'
May 14 03:42:43.698: INFO: stderr: ""
May 14 03:42:43.699: INFO: stdout: "true"
May 14 03:42:43.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-nautilus-lkfxm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5880'
May 14 03:42:44.005: INFO: stderr: ""
May 14 03:42:44.005: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 14 03:42:44.005: INFO: validating pod update-demo-nautilus-lkfxm
May 14 03:42:44.018: INFO: got data: {
  "image": "nautilus.jpg"
}

May 14 03:42:44.018: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 14 03:42:44.018: INFO: update-demo-nautilus-lkfxm is verified up and running
May 14 03:42:44.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-nautilus-zwgks -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5880'
May 14 03:42:44.327: INFO: stderr: ""
May 14 03:42:44.328: INFO: stdout: "true"
May 14 03:42:44.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-nautilus-zwgks -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5880'
May 14 03:42:44.697: INFO: stderr: ""
May 14 03:42:44.697: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 14 03:42:44.697: INFO: validating pod update-demo-nautilus-zwgks
May 14 03:42:44.711: INFO: got data: {
  "image": "nautilus.jpg"
}

May 14 03:42:44.711: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 14 03:42:44.711: INFO: update-demo-nautilus-zwgks is verified up and running
STEP: using delete to clean up resources
May 14 03:42:44.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 delete --grace-period=0 --force -f - --namespace=kubectl-5880'
May 14 03:42:44.957: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 14 03:42:44.957: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 14 03:42:44.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5880'
May 14 03:42:45.313: INFO: stderr: "No resources found in kubectl-5880 namespace.\n"
May 14 03:42:45.313: INFO: stdout: ""
May 14 03:42:45.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods -l name=update-demo --namespace=kubectl-5880 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 14 03:42:45.585: INFO: stderr: ""
May 14 03:42:45.585: INFO: stdout: "update-demo-nautilus-lkfxm\nupdate-demo-nautilus-zwgks\n"
May 14 03:42:46.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5880'
May 14 03:42:46.635: INFO: stderr: "No resources found in kubectl-5880 namespace.\n"
May 14 03:42:46.635: INFO: stdout: ""
May 14 03:42:46.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods -l name=update-demo --namespace=kubectl-5880 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 14 03:42:47.044: INFO: stderr: ""
May 14 03:42:47.046: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:42:47.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5880" for this suite.
May 14 03:42:57.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:42:57.479: INFO: namespace kubectl-5880 deletion completed in 10.420435717s

â€¢ [SLOW TEST:41.932 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:42:57.485: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2673
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-5085c366-0f96-4fb6-882a-691a1a0ca024
STEP: Creating a pod to test consume configMaps
May 14 03:42:57.700: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0f60326e-ae04-48a1-b84d-bca14e1a9c23" in namespace "projected-2673" to be "success or failure"
May 14 03:42:57.711: INFO: Pod "pod-projected-configmaps-0f60326e-ae04-48a1-b84d-bca14e1a9c23": Phase="Pending", Reason="", readiness=false. Elapsed: 10.893066ms
May 14 03:42:59.724: INFO: Pod "pod-projected-configmaps-0f60326e-ae04-48a1-b84d-bca14e1a9c23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023757868s
May 14 03:43:01.736: INFO: Pod "pod-projected-configmaps-0f60326e-ae04-48a1-b84d-bca14e1a9c23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036143648s
May 14 03:43:03.743: INFO: Pod "pod-projected-configmaps-0f60326e-ae04-48a1-b84d-bca14e1a9c23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043389427s
STEP: Saw pod success
May 14 03:43:03.744: INFO: Pod "pod-projected-configmaps-0f60326e-ae04-48a1-b84d-bca14e1a9c23" satisfied condition "success or failure"
May 14 03:43:03.747: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-projected-configmaps-0f60326e-ae04-48a1-b84d-bca14e1a9c23 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 14 03:43:04.387: INFO: Waiting for pod pod-projected-configmaps-0f60326e-ae04-48a1-b84d-bca14e1a9c23 to disappear
May 14 03:43:04.401: INFO: Pod pod-projected-configmaps-0f60326e-ae04-48a1-b84d-bca14e1a9c23 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:43:04.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2673" for this suite.
May 14 03:43:10.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:43:10.833: INFO: namespace projected-2673 deletion completed in 6.41795765s

â€¢ [SLOW TEST:13.348 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:43:10.835: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6558
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:43:11.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6558" for this suite.
May 14 03:43:17.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:43:17.609: INFO: namespace kubelet-test-6558 deletion completed in 6.458527997s

â€¢ [SLOW TEST:6.774 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:43:17.615: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9602
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
May 14 03:43:17.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 create -f - --namespace=kubectl-9602'
May 14 03:43:18.433: INFO: stderr: ""
May 14 03:43:18.433: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 14 03:43:18.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9602'
May 14 03:43:18.921: INFO: stderr: ""
May 14 03:43:18.921: INFO: stdout: "update-demo-nautilus-7kmg2 update-demo-nautilus-zj6xx "
May 14 03:43:18.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-nautilus-7kmg2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9602'
May 14 03:43:19.534: INFO: stderr: ""
May 14 03:43:19.534: INFO: stdout: ""
May 14 03:43:19.534: INFO: update-demo-nautilus-7kmg2 is created but not running
May 14 03:43:24.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9602'
May 14 03:43:24.910: INFO: stderr: ""
May 14 03:43:24.910: INFO: stdout: "update-demo-nautilus-7kmg2 update-demo-nautilus-zj6xx "
May 14 03:43:24.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-nautilus-7kmg2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9602'
May 14 03:43:25.408: INFO: stderr: ""
May 14 03:43:25.408: INFO: stdout: "true"
May 14 03:43:25.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-nautilus-7kmg2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9602'
May 14 03:43:25.759: INFO: stderr: ""
May 14 03:43:25.759: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 14 03:43:25.759: INFO: validating pod update-demo-nautilus-7kmg2
May 14 03:43:25.793: INFO: got data: {
  "image": "nautilus.jpg"
}

May 14 03:43:25.793: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 14 03:43:25.794: INFO: update-demo-nautilus-7kmg2 is verified up and running
May 14 03:43:25.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-nautilus-zj6xx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9602'
May 14 03:43:26.152: INFO: stderr: ""
May 14 03:43:26.152: INFO: stdout: "true"
May 14 03:43:26.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-nautilus-zj6xx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9602'
May 14 03:43:26.524: INFO: stderr: ""
May 14 03:43:26.524: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 14 03:43:26.524: INFO: validating pod update-demo-nautilus-zj6xx
May 14 03:43:26.538: INFO: got data: {
  "image": "nautilus.jpg"
}

May 14 03:43:26.538: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 14 03:43:26.538: INFO: update-demo-nautilus-zj6xx is verified up and running
STEP: rolling-update to new replication controller
May 14 03:43:26.551: INFO: scanned /root for discovery docs: <nil>
May 14 03:43:26.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-9602'
May 14 03:43:50.311: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 14 03:43:50.311: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 14 03:43:50.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9602'
May 14 03:43:50.792: INFO: stderr: ""
May 14 03:43:50.793: INFO: stdout: "update-demo-kitten-7pm9z update-demo-kitten-8xhvs update-demo-nautilus-zj6xx "
STEP: Replicas for name=update-demo: expected=2 actual=3
May 14 03:43:55.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9602'
May 14 03:43:56.140: INFO: stderr: ""
May 14 03:43:56.140: INFO: stdout: "update-demo-kitten-7pm9z update-demo-kitten-8xhvs "
May 14 03:43:56.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-kitten-7pm9z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9602'
May 14 03:43:56.942: INFO: stderr: ""
May 14 03:43:56.942: INFO: stdout: "true"
May 14 03:43:56.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-kitten-7pm9z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9602'
May 14 03:43:57.324: INFO: stderr: ""
May 14 03:43:57.324: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 14 03:43:57.324: INFO: validating pod update-demo-kitten-7pm9z
May 14 03:43:57.360: INFO: got data: {
  "image": "kitten.jpg"
}

May 14 03:43:57.360: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 14 03:43:57.361: INFO: update-demo-kitten-7pm9z is verified up and running
May 14 03:43:57.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-kitten-8xhvs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9602'
May 14 03:43:57.750: INFO: stderr: ""
May 14 03:43:57.750: INFO: stdout: "true"
May 14 03:43:57.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods update-demo-kitten-8xhvs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9602'
May 14 03:43:58.053: INFO: stderr: ""
May 14 03:43:58.053: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 14 03:43:58.053: INFO: validating pod update-demo-kitten-8xhvs
May 14 03:43:58.079: INFO: got data: {
  "image": "kitten.jpg"
}

May 14 03:43:58.079: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 14 03:43:58.079: INFO: update-demo-kitten-8xhvs is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:43:58.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9602" for this suite.
May 14 03:44:28.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:44:28.563: INFO: namespace kubectl-9602 deletion completed in 30.47023716s

â€¢ [SLOW TEST:70.948 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:44:28.568: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1556
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 14 03:44:29.344: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d19f6fc-e678-41f6-ac49-aa1cfe15de0f" in namespace "downward-api-1556" to be "success or failure"
May 14 03:44:29.349: INFO: Pod "downwardapi-volume-4d19f6fc-e678-41f6-ac49-aa1cfe15de0f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.056519ms
May 14 03:44:31.379: INFO: Pod "downwardapi-volume-4d19f6fc-e678-41f6-ac49-aa1cfe15de0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03460547s
May 14 03:44:35.663: INFO: Pod "downwardapi-volume-4d19f6fc-e678-41f6-ac49-aa1cfe15de0f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.318533402s
May 14 03:44:37.669: INFO: Pod "downwardapi-volume-4d19f6fc-e678-41f6-ac49-aa1cfe15de0f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.32462088s
May 14 03:44:39.675: INFO: Pod "downwardapi-volume-4d19f6fc-e678-41f6-ac49-aa1cfe15de0f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.331177312s
May 14 03:44:41.682: INFO: Pod "downwardapi-volume-4d19f6fc-e678-41f6-ac49-aa1cfe15de0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.338042588s
STEP: Saw pod success
May 14 03:44:41.682: INFO: Pod "downwardapi-volume-4d19f6fc-e678-41f6-ac49-aa1cfe15de0f" satisfied condition "success or failure"
May 14 03:44:41.694: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod downwardapi-volume-4d19f6fc-e678-41f6-ac49-aa1cfe15de0f container client-container: <nil>
STEP: delete the pod
May 14 03:44:41.840: INFO: Waiting for pod downwardapi-volume-4d19f6fc-e678-41f6-ac49-aa1cfe15de0f to disappear
May 14 03:44:41.849: INFO: Pod downwardapi-volume-4d19f6fc-e678-41f6-ac49-aa1cfe15de0f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:44:41.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1556" for this suite.
May 14 03:44:49.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:44:52.032: INFO: namespace downward-api-1556 deletion completed in 10.173117059s

â€¢ [SLOW TEST:23.466 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:44:52.047: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7551
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 14 03:44:52.708: INFO: Waiting up to 5m0s for pod "pod-2f9ac53f-228c-4aa0-bd2d-17b1f0c94b06" in namespace "emptydir-7551" to be "success or failure"
May 14 03:44:53.028: INFO: Pod "pod-2f9ac53f-228c-4aa0-bd2d-17b1f0c94b06": Phase="Pending", Reason="", readiness=false. Elapsed: 320.429497ms
May 14 03:44:55.035: INFO: Pod "pod-2f9ac53f-228c-4aa0-bd2d-17b1f0c94b06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.327034486s
May 14 03:44:57.125: INFO: Pod "pod-2f9ac53f-228c-4aa0-bd2d-17b1f0c94b06": Phase="Pending", Reason="", readiness=false. Elapsed: 4.417330778s
May 14 03:44:59.272: INFO: Pod "pod-2f9ac53f-228c-4aa0-bd2d-17b1f0c94b06": Phase="Pending", Reason="", readiness=false. Elapsed: 6.563610083s
May 14 03:45:01.278: INFO: Pod "pod-2f9ac53f-228c-4aa0-bd2d-17b1f0c94b06": Phase="Pending", Reason="", readiness=false. Elapsed: 8.569645493s
May 14 03:45:07.128: INFO: Pod "pod-2f9ac53f-228c-4aa0-bd2d-17b1f0c94b06": Phase="Pending", Reason="", readiness=false. Elapsed: 14.419751679s
May 14 03:45:21.754: INFO: Get pod "pod-2f9ac53f-228c-4aa0-bd2d-17b1f0c94b06" in namespace "emptydir-7551" failed, ignoring for 2s. Error: etcdserver: request timed out
May 14 03:45:33.364: INFO: Get pod "pod-2f9ac53f-228c-4aa0-bd2d-17b1f0c94b06" in namespace "emptydir-7551" failed, ignoring for 2s. Error: etcdserver: request timed out
May 14 03:45:41.164: INFO: Pod "pod-2f9ac53f-228c-4aa0-bd2d-17b1f0c94b06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 48.455588797s
STEP: Saw pod success
May 14 03:45:41.164: INFO: Pod "pod-2f9ac53f-228c-4aa0-bd2d-17b1f0c94b06" satisfied condition "success or failure"
May 14 03:45:41.251: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-2f9ac53f-228c-4aa0-bd2d-17b1f0c94b06 container test-container: <nil>
STEP: delete the pod
May 14 03:45:41.555: INFO: Waiting for pod pod-2f9ac53f-228c-4aa0-bd2d-17b1f0c94b06 to disappear
May 14 03:45:41.951: INFO: Pod pod-2f9ac53f-228c-4aa0-bd2d-17b1f0c94b06 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:45:41.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7551" for this suite.
May 14 03:46:50.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:46:50.549: INFO: namespace emptydir-7551 deletion completed in 1m8.556796747s

â€¢ [SLOW TEST:118.508 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:46:50.567: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8626
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 14 03:46:51.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725024811, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725024811, loc:(*time.Location)(0x78a2900)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-86d95b659d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725024811, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725024811, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
May 14 03:46:53.466: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725024811, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725024811, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725024811, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725024811, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 14 03:46:56.506: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:46:56.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8626" for this suite.
May 14 03:47:04.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:47:05.137: INFO: namespace webhook-8626 deletion completed in 8.301219371s
STEP: Destroying namespace "webhook-8626-markers" for this suite.
May 14 03:47:11.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:47:11.501: INFO: namespace webhook-8626-markers deletion completed in 6.364012749s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:20.966 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:47:11.537: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-881
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: Gathering metrics
May 14 03:47:12.134: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:47:12.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0514 03:47:12.133903      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-881" for this suite.
May 14 03:47:18.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:47:19.274: INFO: namespace gc-881 deletion completed in 7.131911361s

â€¢ [SLOW TEST:7.739 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:47:19.277: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6335
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 03:47:19.589: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 14 03:47:24.600: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 14 03:47:24.600: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May 14 03:47:28.754: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6335 /apis/apps/v1/namespaces/deployment-6335/deployments/test-cleanup-deployment 8c75c262-b562-4214-b567-20540da40d19 208680 1 2020-05-14 03:47:24 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0028e19b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-05-14 03:47:24 +0000 UTC,LastTransitionTime:2020-05-14 03:47:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-65db99849b" has successfully progressed.,LastUpdateTime:2020-05-14 03:47:28 +0000 UTC,LastTransitionTime:2020-05-14 03:47:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 14 03:47:28.782: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-6335 /apis/apps/v1/namespaces/deployment-6335/replicasets/test-cleanup-deployment-65db99849b 77f66457-90e5-4c15-89de-487aa2aa950a 208670 1 2020-05-14 03:47:24 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 8c75c262-b562-4214-b567-20540da40d19 0xc0017d4c87 0xc0017d4c88}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0017d4ce8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 14 03:47:28.788: INFO: Pod "test-cleanup-deployment-65db99849b-zr2sd" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-65db99849b-zr2sd test-cleanup-deployment-65db99849b- deployment-6335 /api/v1/namespaces/deployment-6335/pods/test-cleanup-deployment-65db99849b-zr2sd 62781403-0da3-46b9-bb05-d7f076ebf830 208669 0 2020-05-14 03:47:24 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[cni.projectcalico.org/podIP:10.100.136.84/32 cni.projectcalico.org/podIPs:10.100.136.84/32] [{apps/v1 ReplicaSet test-cleanup-deployment-65db99849b 77f66457-90e5-4c15-89de-487aa2aa950a 0xc0028e1e07 0xc0028e1e08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t49vr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t49vr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t49vr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:47:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:47:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:47:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:47:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.13,PodIP:10.100.136.84,StartTime:2020-05-14 03:47:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-14 03:47:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://d3783bc0d1328c69bcdf2c34f5af082ae4b1d6325528da0df5f99e481828823c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.136.84,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:47:28.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6335" for this suite.
May 14 03:47:34.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:47:35.396: INFO: namespace deployment-6335 deletion completed in 6.600301086s

â€¢ [SLOW TEST:16.120 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:47:35.401: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-366
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:47:35.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-366" for this suite.
May 14 03:48:06.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:48:06.921: INFO: namespace pods-366 deletion completed in 31.025335538s

â€¢ [SLOW TEST:31.520 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:48:06.924: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4305
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 14 03:48:07.200: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de86e2b2-a4dc-4757-af3d-5236b933c163" in namespace "projected-4305" to be "success or failure"
May 14 03:48:07.214: INFO: Pod "downwardapi-volume-de86e2b2-a4dc-4757-af3d-5236b933c163": Phase="Pending", Reason="", readiness=false. Elapsed: 13.830788ms
May 14 03:48:09.220: INFO: Pod "downwardapi-volume-de86e2b2-a4dc-4757-af3d-5236b933c163": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019459386s
May 14 03:48:11.227: INFO: Pod "downwardapi-volume-de86e2b2-a4dc-4757-af3d-5236b933c163": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026783283s
May 14 03:48:13.233: INFO: Pod "downwardapi-volume-de86e2b2-a4dc-4757-af3d-5236b933c163": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032507409s
May 14 03:48:15.239: INFO: Pod "downwardapi-volume-de86e2b2-a4dc-4757-af3d-5236b933c163": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.038628067s
STEP: Saw pod success
May 14 03:48:15.239: INFO: Pod "downwardapi-volume-de86e2b2-a4dc-4757-af3d-5236b933c163" satisfied condition "success or failure"
May 14 03:48:15.244: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod downwardapi-volume-de86e2b2-a4dc-4757-af3d-5236b933c163 container client-container: <nil>
STEP: delete the pod
May 14 03:48:15.382: INFO: Waiting for pod downwardapi-volume-de86e2b2-a4dc-4757-af3d-5236b933c163 to disappear
May 14 03:48:15.395: INFO: Pod downwardapi-volume-de86e2b2-a4dc-4757-af3d-5236b933c163 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:48:15.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4305" for this suite.
May 14 03:48:21.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:48:22.460: INFO: namespace projected-4305 deletion completed in 7.048980807s

â€¢ [SLOW TEST:15.538 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:48:22.468: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-183
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
May 14 03:48:22.670: INFO: Waiting up to 5m0s for pod "pod-c2636bd8-f54b-4958-9520-11fa07a6a032" in namespace "emptydir-183" to be "success or failure"
May 14 03:48:22.680: INFO: Pod "pod-c2636bd8-f54b-4958-9520-11fa07a6a032": Phase="Pending", Reason="", readiness=false. Elapsed: 9.967687ms
May 14 03:48:24.747: INFO: Pod "pod-c2636bd8-f54b-4958-9520-11fa07a6a032": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076530725s
May 14 03:48:26.794: INFO: Pod "pod-c2636bd8-f54b-4958-9520-11fa07a6a032": Phase="Pending", Reason="", readiness=false. Elapsed: 4.123782708s
May 14 03:48:28.809: INFO: Pod "pod-c2636bd8-f54b-4958-9520-11fa07a6a032": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.139084004s
STEP: Saw pod success
May 14 03:48:28.809: INFO: Pod "pod-c2636bd8-f54b-4958-9520-11fa07a6a032" satisfied condition "success or failure"
May 14 03:48:28.820: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-c2636bd8-f54b-4958-9520-11fa07a6a032 container test-container: <nil>
STEP: delete the pod
May 14 03:48:28.856: INFO: Waiting for pod pod-c2636bd8-f54b-4958-9520-11fa07a6a032 to disappear
May 14 03:48:28.861: INFO: Pod pod-c2636bd8-f54b-4958-9520-11fa07a6a032 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:48:28.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-183" for this suite.
May 14 03:48:34.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:48:35.255: INFO: namespace emptydir-183 deletion completed in 6.386980847s

â€¢ [SLOW TEST:12.788 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:48:35.256: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-797
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:48:52.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-797" for this suite.
May 14 03:48:58.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:48:59.350: INFO: namespace resourcequota-797 deletion completed in 6.493230546s

â€¢ [SLOW TEST:24.094 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:48:59.352: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-47
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-349b558c-0078-4914-8d13-17f95ef2172a
STEP: Creating a pod to test consume configMaps
May 14 03:48:59.675: INFO: Waiting up to 5m0s for pod "pod-configmaps-fe56c94e-8831-4455-b0d0-b11c92202b13" in namespace "configmap-47" to be "success or failure"
May 14 03:48:59.704: INFO: Pod "pod-configmaps-fe56c94e-8831-4455-b0d0-b11c92202b13": Phase="Pending", Reason="", readiness=false. Elapsed: 28.651095ms
May 14 03:49:01.711: INFO: Pod "pod-configmaps-fe56c94e-8831-4455-b0d0-b11c92202b13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035465733s
May 14 03:49:03.728: INFO: Pod "pod-configmaps-fe56c94e-8831-4455-b0d0-b11c92202b13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052004189s
STEP: Saw pod success
May 14 03:49:03.728: INFO: Pod "pod-configmaps-fe56c94e-8831-4455-b0d0-b11c92202b13" satisfied condition "success or failure"
May 14 03:49:03.733: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-configmaps-fe56c94e-8831-4455-b0d0-b11c92202b13 container configmap-volume-test: <nil>
STEP: delete the pod
May 14 03:49:03.766: INFO: Waiting for pod pod-configmaps-fe56c94e-8831-4455-b0d0-b11c92202b13 to disappear
May 14 03:49:03.774: INFO: Pod pod-configmaps-fe56c94e-8831-4455-b0d0-b11c92202b13 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:49:03.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-47" for this suite.
May 14 03:49:09.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:49:10.232: INFO: namespace configmap-47 deletion completed in 6.436264295s

â€¢ [SLOW TEST:10.882 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:49:10.243: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1683
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-e02c6194-9cf7-459d-b9bc-cb30b6df3e8e
STEP: Creating a pod to test consume configMaps
May 14 03:49:10.488: INFO: Waiting up to 5m0s for pod "pod-configmaps-5ce3f74c-7fa6-43a4-9324-3bb2702367a4" in namespace "configmap-1683" to be "success or failure"
May 14 03:49:10.519: INFO: Pod "pod-configmaps-5ce3f74c-7fa6-43a4-9324-3bb2702367a4": Phase="Pending", Reason="", readiness=false. Elapsed: 31.189582ms
May 14 03:49:12.532: INFO: Pod "pod-configmaps-5ce3f74c-7fa6-43a4-9324-3bb2702367a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044073149s
May 14 03:49:14.548: INFO: Pod "pod-configmaps-5ce3f74c-7fa6-43a4-9324-3bb2702367a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059297805s
STEP: Saw pod success
May 14 03:49:14.548: INFO: Pod "pod-configmaps-5ce3f74c-7fa6-43a4-9324-3bb2702367a4" satisfied condition "success or failure"
May 14 03:49:14.555: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-configmaps-5ce3f74c-7fa6-43a4-9324-3bb2702367a4 container configmap-volume-test: <nil>
STEP: delete the pod
May 14 03:49:14.602: INFO: Waiting for pod pod-configmaps-5ce3f74c-7fa6-43a4-9324-3bb2702367a4 to disappear
May 14 03:49:14.610: INFO: Pod pod-configmaps-5ce3f74c-7fa6-43a4-9324-3bb2702367a4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:49:14.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1683" for this suite.
May 14 03:49:25.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:49:27.987: INFO: namespace configmap-1683 deletion completed in 13.363892748s

â€¢ [SLOW TEST:17.745 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:49:27.991: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7862
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:49:32.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7862" for this suite.
May 14 03:49:38.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:49:38.906: INFO: namespace watch-7862 deletion completed in 6.535197011s

â€¢ [SLOW TEST:10.915 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:49:38.907: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5533
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-5533
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
May 14 03:49:39.617: INFO: Found 0 stateful pods, waiting for 3
May 14 03:49:49.626: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 14 03:49:49.627: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 14 03:49:49.627: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May 14 03:50:00.882: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 14 03:50:00.884: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 14 03:50:00.884: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 14 03:50:00.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-5533 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 14 03:50:08.512: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 14 03:50:08.513: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 14 03:50:08.514: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May 14 03:50:18.569: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 14 03:50:28.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-5533 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 03:50:29.440: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 14 03:50:29.441: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 14 03:50:29.441: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 14 03:50:59.879: INFO: Waiting for StatefulSet statefulset-5533/ss2 to complete update
May 14 03:50:59.880: INFO: Waiting for Pod statefulset-5533/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 14 03:51:09.900: INFO: Waiting for StatefulSet statefulset-5533/ss2 to complete update
May 14 03:51:09.900: INFO: Waiting for Pod statefulset-5533/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
May 14 03:51:19.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-5533 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 14 03:51:20.896: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 14 03:51:20.896: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 14 03:51:20.896: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 14 03:51:20.948: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 14 03:51:31.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=statefulset-5533 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 14 03:51:31.759: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 14 03:51:31.759: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 14 03:51:31.759: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 14 03:51:42.863: INFO: Waiting for StatefulSet statefulset-5533/ss2 to complete update
May 14 03:51:42.864: INFO: Waiting for Pod statefulset-5533/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 14 03:51:42.864: INFO: Waiting for Pod statefulset-5533/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 14 03:51:52.887: INFO: Waiting for StatefulSet statefulset-5533/ss2 to complete update
May 14 03:51:52.888: INFO: Waiting for Pod statefulset-5533/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 14 03:52:02.880: INFO: Waiting for StatefulSet statefulset-5533/ss2 to complete update
May 14 03:52:02.881: INFO: Waiting for Pod statefulset-5533/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 14 03:52:12.889: INFO: Waiting for StatefulSet statefulset-5533/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May 14 03:52:22.883: INFO: Deleting all statefulset in ns statefulset-5533
May 14 03:52:22.900: INFO: Scaling statefulset ss2 to 0
May 14 03:52:43.426: INFO: Waiting for statefulset status.replicas updated to 0
May 14 03:52:43.449: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:52:43.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5533" for this suite.
May 14 03:52:51.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:52:51.912: INFO: namespace statefulset-5533 deletion completed in 8.386864254s

â€¢ [SLOW TEST:193.005 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:52:51.927: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1421
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0514 03:53:04.883905      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 14 03:53:04.884: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:53:04.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1421" for this suite.
May 14 03:53:12.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:53:13.619: INFO: namespace gc-1421 deletion completed in 8.728215622s

â€¢ [SLOW TEST:21.692 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:53:13.625: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-316
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 14 03:53:13.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-316'
May 14 03:53:14.411: INFO: stderr: ""
May 14 03:53:14.411: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
May 14 03:53:14.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 delete pods e2e-test-httpd-pod --namespace=kubectl-316'
May 14 03:53:16.912: INFO: stderr: ""
May 14 03:53:16.912: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:53:16.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-316" for this suite.
May 14 03:53:22.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:53:23.380: INFO: namespace kubectl-316 deletion completed in 6.411158119s

â€¢ [SLOW TEST:9.755 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:53:23.383: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6902
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
May 14 03:53:30.279: INFO: Successfully updated pod "labelsupdatec7f0174f-6de0-4221-893f-e904436fea06"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:53:33.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6902" for this suite.
May 14 03:54:01.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:54:01.856: INFO: namespace projected-6902 deletion completed in 28.70936556s

â€¢ [SLOW TEST:38.473 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:54:01.869: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9440
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-nxnd
STEP: Creating a pod to test atomic-volume-subpath
May 14 03:54:02.174: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-nxnd" in namespace "subpath-9440" to be "success or failure"
May 14 03:54:02.214: INFO: Pod "pod-subpath-test-projected-nxnd": Phase="Pending", Reason="", readiness=false. Elapsed: 39.652187ms
May 14 03:54:04.237: INFO: Pod "pod-subpath-test-projected-nxnd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062795897s
May 14 03:54:06.244: INFO: Pod "pod-subpath-test-projected-nxnd": Phase="Running", Reason="", readiness=true. Elapsed: 4.069764197s
May 14 03:54:08.249: INFO: Pod "pod-subpath-test-projected-nxnd": Phase="Running", Reason="", readiness=true. Elapsed: 6.074793621s
May 14 03:54:10.255: INFO: Pod "pod-subpath-test-projected-nxnd": Phase="Running", Reason="", readiness=true. Elapsed: 8.080143359s
May 14 03:54:12.467: INFO: Pod "pod-subpath-test-projected-nxnd": Phase="Running", Reason="", readiness=true. Elapsed: 10.292363808s
May 14 03:54:14.474: INFO: Pod "pod-subpath-test-projected-nxnd": Phase="Running", Reason="", readiness=true. Elapsed: 12.299627395s
May 14 03:54:16.531: INFO: Pod "pod-subpath-test-projected-nxnd": Phase="Running", Reason="", readiness=true. Elapsed: 14.356637603s
May 14 03:54:18.537: INFO: Pod "pod-subpath-test-projected-nxnd": Phase="Running", Reason="", readiness=true. Elapsed: 16.362728141s
May 14 03:54:20.543: INFO: Pod "pod-subpath-test-projected-nxnd": Phase="Running", Reason="", readiness=true. Elapsed: 18.368764134s
May 14 03:54:22.552: INFO: Pod "pod-subpath-test-projected-nxnd": Phase="Running", Reason="", readiness=true. Elapsed: 20.377611821s
May 14 03:54:24.571: INFO: Pod "pod-subpath-test-projected-nxnd": Phase="Running", Reason="", readiness=true. Elapsed: 22.395943662s
May 14 03:54:26.590: INFO: Pod "pod-subpath-test-projected-nxnd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.415635662s
STEP: Saw pod success
May 14 03:54:26.591: INFO: Pod "pod-subpath-test-projected-nxnd" satisfied condition "success or failure"
May 14 03:54:26.607: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-subpath-test-projected-nxnd container test-container-subpath-projected-nxnd: <nil>
STEP: delete the pod
May 14 03:54:26.654: INFO: Waiting for pod pod-subpath-test-projected-nxnd to disappear
May 14 03:54:26.662: INFO: Pod pod-subpath-test-projected-nxnd no longer exists
STEP: Deleting pod pod-subpath-test-projected-nxnd
May 14 03:54:26.662: INFO: Deleting pod "pod-subpath-test-projected-nxnd" in namespace "subpath-9440"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:54:26.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9440" for this suite.
May 14 03:54:32.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:54:33.093: INFO: namespace subpath-9440 deletion completed in 6.415054619s

â€¢ [SLOW TEST:31.225 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:54:33.098: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3192
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 03:54:33.348: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-2dde3a37-71e8-4def-9c87-1f4f5f20db4e" in namespace "security-context-test-3192" to be "success or failure"
May 14 03:54:33.369: INFO: Pod "busybox-readonly-false-2dde3a37-71e8-4def-9c87-1f4f5f20db4e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.791261ms
May 14 03:54:35.375: INFO: Pod "busybox-readonly-false-2dde3a37-71e8-4def-9c87-1f4f5f20db4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026615789s
May 14 03:54:37.381: INFO: Pod "busybox-readonly-false-2dde3a37-71e8-4def-9c87-1f4f5f20db4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032990895s
May 14 03:54:37.381: INFO: Pod "busybox-readonly-false-2dde3a37-71e8-4def-9c87-1f4f5f20db4e" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:54:37.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3192" for this suite.
May 14 03:54:43.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:54:43.860: INFO: namespace security-context-test-3192 deletion completed in 6.468686022s

â€¢ [SLOW TEST:10.763 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:54:43.865: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-577
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 14 03:54:45.004: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725025284, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725025284, loc:(*time.Location)(0x78a2900)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-86d95b659d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725025284, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725025284, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
May 14 03:54:47.014: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725025284, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725025284, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725025285, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725025284, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 14 03:54:50.031: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
May 14 03:54:50.082: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:54:50.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-577" for this suite.
May 14 03:55:00.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:55:00.600: INFO: namespace webhook-577 deletion completed in 10.476946602s
STEP: Destroying namespace "webhook-577-markers" for this suite.
May 14 03:55:08.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:55:09.108: INFO: namespace webhook-577-markers deletion completed in 8.507407079s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:25.287 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:55:09.154: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3131
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 14 03:55:09.398: INFO: Waiting up to 5m0s for pod "downwardapi-volume-191d3675-d2ca-4f7e-96b4-ad00a184c1de" in namespace "projected-3131" to be "success or failure"
May 14 03:55:09.426: INFO: Pod "downwardapi-volume-191d3675-d2ca-4f7e-96b4-ad00a184c1de": Phase="Pending", Reason="", readiness=false. Elapsed: 27.922072ms
May 14 03:55:11.436: INFO: Pod "downwardapi-volume-191d3675-d2ca-4f7e-96b4-ad00a184c1de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037860586s
May 14 03:55:13.448: INFO: Pod "downwardapi-volume-191d3675-d2ca-4f7e-96b4-ad00a184c1de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049392351s
May 14 03:55:15.455: INFO: Pod "downwardapi-volume-191d3675-d2ca-4f7e-96b4-ad00a184c1de": Phase="Pending", Reason="", readiness=false. Elapsed: 6.056381987s
May 14 03:55:17.460: INFO: Pod "downwardapi-volume-191d3675-d2ca-4f7e-96b4-ad00a184c1de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.061624159s
STEP: Saw pod success
May 14 03:55:17.474: INFO: Pod "downwardapi-volume-191d3675-d2ca-4f7e-96b4-ad00a184c1de" satisfied condition "success or failure"
May 14 03:55:17.480: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod downwardapi-volume-191d3675-d2ca-4f7e-96b4-ad00a184c1de container client-container: <nil>
STEP: delete the pod
May 14 03:55:17.516: INFO: Waiting for pod downwardapi-volume-191d3675-d2ca-4f7e-96b4-ad00a184c1de to disappear
May 14 03:55:17.699: INFO: Pod downwardapi-volume-191d3675-d2ca-4f7e-96b4-ad00a184c1de no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:55:17.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3131" for this suite.
May 14 03:55:25.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:55:26.195: INFO: namespace projected-3131 deletion completed in 8.482695572s

â€¢ [SLOW TEST:17.042 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:55:26.199: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-824
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
May 14 03:55:26.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 --namespace=kubectl-824 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
May 14 03:55:32.596: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
May 14 03:55:32.596: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:55:34.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-824" for this suite.
May 14 03:55:42.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:55:43.163: INFO: namespace kubectl-824 deletion completed in 8.552506731s

â€¢ [SLOW TEST:16.965 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:55:43.165: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1297
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
May 14 03:55:43.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 cluster-info'
May 14 03:55:43.762: INFO: stderr: ""
May 14 03:55:43.762: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:55:43.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1297" for this suite.
May 14 03:55:49.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:55:50.183: INFO: namespace kubectl-1297 deletion completed in 6.414743142s

â€¢ [SLOW TEST:7.018 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:55:50.185: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2031
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-q8hj
STEP: Creating a pod to test atomic-volume-subpath
May 14 03:55:50.557: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-q8hj" in namespace "subpath-2031" to be "success or failure"
May 14 03:55:50.570: INFO: Pod "pod-subpath-test-configmap-q8hj": Phase="Pending", Reason="", readiness=false. Elapsed: 12.596442ms
May 14 03:55:52.601: INFO: Pod "pod-subpath-test-configmap-q8hj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043623524s
May 14 03:55:54.629: INFO: Pod "pod-subpath-test-configmap-q8hj": Phase="Running", Reason="", readiness=true. Elapsed: 4.071454592s
May 14 03:55:56.639: INFO: Pod "pod-subpath-test-configmap-q8hj": Phase="Running", Reason="", readiness=true. Elapsed: 6.081053835s
May 14 03:55:58.647: INFO: Pod "pod-subpath-test-configmap-q8hj": Phase="Running", Reason="", readiness=true. Elapsed: 8.089778713s
May 14 03:56:00.654: INFO: Pod "pod-subpath-test-configmap-q8hj": Phase="Running", Reason="", readiness=true. Elapsed: 10.096610977s
May 14 03:56:03.231: INFO: Pod "pod-subpath-test-configmap-q8hj": Phase="Running", Reason="", readiness=true. Elapsed: 12.673715005s
May 14 03:56:05.238: INFO: Pod "pod-subpath-test-configmap-q8hj": Phase="Running", Reason="", readiness=true. Elapsed: 14.680495681s
May 14 03:56:07.245: INFO: Pod "pod-subpath-test-configmap-q8hj": Phase="Running", Reason="", readiness=true. Elapsed: 16.687193732s
May 14 03:56:09.251: INFO: Pod "pod-subpath-test-configmap-q8hj": Phase="Running", Reason="", readiness=true. Elapsed: 18.693844432s
May 14 03:56:11.258: INFO: Pod "pod-subpath-test-configmap-q8hj": Phase="Running", Reason="", readiness=true. Elapsed: 20.700533887s
May 14 03:56:13.275: INFO: Pod "pod-subpath-test-configmap-q8hj": Phase="Running", Reason="", readiness=true. Elapsed: 22.717246295s
May 14 03:56:15.281: INFO: Pod "pod-subpath-test-configmap-q8hj": Phase="Running", Reason="", readiness=true. Elapsed: 24.723631731s
May 14 03:56:17.295: INFO: Pod "pod-subpath-test-configmap-q8hj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.737853692s
STEP: Saw pod success
May 14 03:56:17.295: INFO: Pod "pod-subpath-test-configmap-q8hj" satisfied condition "success or failure"
May 14 03:56:17.301: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-subpath-test-configmap-q8hj container test-container-subpath-configmap-q8hj: <nil>
STEP: delete the pod
May 14 03:56:18.241: INFO: Waiting for pod pod-subpath-test-configmap-q8hj to disappear
May 14 03:56:18.253: INFO: Pod pod-subpath-test-configmap-q8hj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-q8hj
May 14 03:56:18.253: INFO: Deleting pod "pod-subpath-test-configmap-q8hj" in namespace "subpath-2031"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:56:18.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2031" for this suite.
May 14 03:56:24.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:56:24.837: INFO: namespace subpath-2031 deletion completed in 6.572799683s

â€¢ [SLOW TEST:34.654 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:56:24.846: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2213
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 03:56:25.083: INFO: Creating deployment "test-recreate-deployment"
May 14 03:56:25.098: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 14 03:56:25.139: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 14 03:56:27.157: INFO: Waiting deployment "test-recreate-deployment" to complete
May 14 03:56:27.164: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725025385, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725025385, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725025385, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725025385, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 03:56:29.179: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725025385, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725025385, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725025385, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725025385, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 03:56:31.175: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 14 03:56:31.191: INFO: Updating deployment test-recreate-deployment
May 14 03:56:31.191: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May 14 03:56:33.020: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2213 /apis/apps/v1/namespaces/deployment-2213/deployments/test-recreate-deployment 75a96d30-492d-4d0d-a305-f1d793d0b476 211329 2 2020-05-14 03:56:25 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005d6d9e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-05-14 03:56:32 +0000 UTC,LastTransitionTime:2020-05-14 03:56:32 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-05-14 03:56:32 +0000 UTC,LastTransitionTime:2020-05-14 03:56:25 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May 14 03:56:33.025: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-2213 /apis/apps/v1/namespaces/deployment-2213/replicasets/test-recreate-deployment-5f94c574ff b7b67684-9cfb-43bc-9816-9dc64fd9484d 211326 1 2020-05-14 03:56:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 75a96d30-492d-4d0d-a305-f1d793d0b476 0xc00426ebc7 0xc00426ebc8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00426ec28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 14 03:56:33.025: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 14 03:56:33.026: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-2213 /apis/apps/v1/namespaces/deployment-2213/replicasets/test-recreate-deployment-68fc85c7bb 0fbd1b7f-3fa2-4869-9868-80128aef8e7c 211312 2 2020-05-14 03:56:25 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 75a96d30-492d-4d0d-a305-f1d793d0b476 0xc00426ec97 0xc00426ec98}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00426ecf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 14 03:56:33.033: INFO: Pod "test-recreate-deployment-5f94c574ff-zcvd8" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-zcvd8 test-recreate-deployment-5f94c574ff- deployment-2213 /api/v1/namespaces/deployment-2213/pods/test-recreate-deployment-5f94c574ff-zcvd8 cbf606ad-0e9f-4ec8-9869-25b03d49b499 211328 0 2020-05-14 03:56:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff b7b67684-9cfb-43bc-9816-9dc64fd9484d 0xc005d6dda7 0xc005d6dda8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-plmx4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-plmx4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-plmx4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:56:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:56:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:56:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:56:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.13,PodIP:,StartTime:2020-05-14 03:56:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:56:33.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2213" for this suite.
May 14 03:56:39.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:56:39.561: INFO: namespace deployment-2213 deletion completed in 6.519910753s

â€¢ [SLOW TEST:14.716 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:56:39.563: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9772
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-9772
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9772 to expose endpoints map[]
May 14 03:56:39.837: INFO: successfully validated that service multi-endpoint-test in namespace services-9772 exposes endpoints map[] (22.796606ms elapsed)
STEP: Creating pod pod1 in namespace services-9772
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9772 to expose endpoints map[pod1:[100]]
May 14 03:56:42.952: INFO: successfully validated that service multi-endpoint-test in namespace services-9772 exposes endpoints map[pod1:[100]] (3.097530084s elapsed)
STEP: Creating pod pod2 in namespace services-9772
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9772 to expose endpoints map[pod1:[100] pod2:[101]]
May 14 03:56:45.043: INFO: successfully validated that service multi-endpoint-test in namespace services-9772 exposes endpoints map[pod1:[100] pod2:[101]] (2.081844815s elapsed)
STEP: Deleting pod pod1 in namespace services-9772
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9772 to expose endpoints map[pod2:[101]]
May 14 03:56:45.076: INFO: successfully validated that service multi-endpoint-test in namespace services-9772 exposes endpoints map[pod2:[101]] (9.817017ms elapsed)
STEP: Deleting pod pod2 in namespace services-9772
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9772 to expose endpoints map[]
May 14 03:56:45.113: INFO: successfully validated that service multi-endpoint-test in namespace services-9772 exposes endpoints map[] (23.490356ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:56:45.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9772" for this suite.
May 14 03:56:57.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:56:57.859: INFO: namespace services-9772 deletion completed in 12.692538094s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:18.296 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:56:57.861: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6194
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 03:56:58.094: INFO: Creating deployment "webserver-deployment"
May 14 03:56:58.105: INFO: Waiting for observed generation 1
May 14 03:57:00.135: INFO: Waiting for all required pods to come up
May 14 03:57:00.144: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
May 14 03:57:12.186: INFO: Waiting for deployment "webserver-deployment" to complete
May 14 03:57:12.209: INFO: Updating deployment "webserver-deployment" with a non-existent image
May 14 03:57:12.225: INFO: Updating deployment webserver-deployment
May 14 03:57:12.226: INFO: Waiting for observed generation 2
May 14 03:57:14.311: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 14 03:57:14.325: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 14 03:57:14.329: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 14 03:57:14.340: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 14 03:57:14.341: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 14 03:57:14.350: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 14 03:57:14.359: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May 14 03:57:14.359: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May 14 03:57:14.370: INFO: Updating deployment webserver-deployment
May 14 03:57:14.371: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May 14 03:57:14.400: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 14 03:57:14.470: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May 14 03:57:14.620: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6194 /apis/apps/v1/namespaces/deployment-6194/deployments/webserver-deployment 7653070b-4a7c-4707-8e03-4aa62f196cfd 211785 3 2020-05-14 03:56:58 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001586048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-05-14 03:57:12 +0000 UTC,LastTransitionTime:2020-05-14 03:56:58 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-05-14 03:57:14 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May 14 03:57:14.834: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-6194 /apis/apps/v1/namespaces/deployment-6194/replicasets/webserver-deployment-c7997dcc8 a44f0fa4-3886-48a9-bfaf-4a58ae4adef0 211767 3 2020-05-14 03:57:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 7653070b-4a7c-4707-8e03-4aa62f196cfd 0xc001586537 0xc001586538}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0015865a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 14 03:57:14.834: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May 14 03:57:14.834: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-6194 /apis/apps/v1/namespaces/deployment-6194/replicasets/webserver-deployment-595b5b9587 f9b930be-971c-4f0b-9c38-d2677cf2fbd2 211760 3 2020-05-14 03:56:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 7653070b-4a7c-4707-8e03-4aa62f196cfd 0xc001586477 0xc001586478}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0015864d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May 14 03:57:14.911: INFO: Pod "webserver-deployment-595b5b9587-2trwr" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2trwr webserver-deployment-595b5b9587- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-595b5b9587-2trwr fcfe2c0f-cf66-4058-9de1-3f3b6610f613 211796 0 2020-05-14 03:57:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f9b930be-971c-4f0b-9c38-d2677cf2fbd2 0xc0026eeb27 0xc0026eeb28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.912: INFO: Pod "webserver-deployment-595b5b9587-4r6cj" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4r6cj webserver-deployment-595b5b9587- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-595b5b9587-4r6cj 0312b21a-8dd7-4a45-a6e4-a3a750890507 211681 0 2020-05-14 03:56:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.100.136.115/32 cni.projectcalico.org/podIPs:10.100.136.115/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f9b930be-971c-4f0b-9c38-d2677cf2fbd2 0xc0026eec20 0xc0026eec21}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:56:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:56:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.13,PodIP:10.100.136.115,StartTime:2020-05-14 03:56:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-14 03:57:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://3787bea6fa85eccfc354559477122740569e348981c9df53570b61b6fd30ee49,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.136.115,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.912: INFO: Pod "webserver-deployment-595b5b9587-69hqh" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-69hqh webserver-deployment-595b5b9587- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-595b5b9587-69hqh 272495f9-b4fd-4998-bcdb-b0fd19ccd60a 211818 0 2020-05-14 03:57:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f9b930be-971c-4f0b-9c38-d2677cf2fbd2 0xc0026eed70 0xc0026eed71}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.913: INFO: Pod "webserver-deployment-595b5b9587-7jxhn" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7jxhn webserver-deployment-595b5b9587- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-595b5b9587-7jxhn 0a998d9c-be77-49d2-96bb-a8b327899f2e 211780 0 2020-05-14 03:57:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f9b930be-971c-4f0b-9c38-d2677cf2fbd2 0xc0026eee60 0xc0026eee61}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.914: INFO: Pod "webserver-deployment-595b5b9587-9h5t6" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-9h5t6 webserver-deployment-595b5b9587- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-595b5b9587-9h5t6 54d06437-651b-4b19-bd4d-32608b58bd22 211684 0 2020-05-14 03:56:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.100.136.114/32 cni.projectcalico.org/podIPs:10.100.136.114/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f9b930be-971c-4f0b-9c38-d2677cf2fbd2 0xc0026eef50 0xc0026eef51}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:56:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:56:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.13,PodIP:10.100.136.114,StartTime:2020-05-14 03:56:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-14 03:57:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://e09f7e9f6c50795a56a991b73600004858643275c158d2ccfa752567c91d0016,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.136.114,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.914: INFO: Pod "webserver-deployment-595b5b9587-cqlkt" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-cqlkt webserver-deployment-595b5b9587- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-595b5b9587-cqlkt 1cdcffb3-3bf3-4b49-ad05-e9a727be5a76 211670 0 2020-05-14 03:56:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.100.136.118/32 cni.projectcalico.org/podIPs:10.100.136.118/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f9b930be-971c-4f0b-9c38-d2677cf2fbd2 0xc0026ef0a0 0xc0026ef0a1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:56:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:56:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.13,PodIP:10.100.136.118,StartTime:2020-05-14 03:56:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-14 03:57:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://9fef0bb8c69f9a6d581672a20a5f328561654a20ca8ae36cefda0f421ee22b19,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.136.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.915: INFO: Pod "webserver-deployment-595b5b9587-ddffn" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-ddffn webserver-deployment-595b5b9587- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-595b5b9587-ddffn 5639eeac-a84a-4f38-b0bb-658f5240b065 211819 0 2020-05-14 03:57:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f9b930be-971c-4f0b-9c38-d2677cf2fbd2 0xc0026ef200 0xc0026ef201}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.915: INFO: Pod "webserver-deployment-595b5b9587-dgr9m" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-dgr9m webserver-deployment-595b5b9587- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-595b5b9587-dgr9m 206d92de-8020-42bf-a376-1a8bea019631 211810 0 2020-05-14 03:57:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f9b930be-971c-4f0b-9c38-d2677cf2fbd2 0xc0026ef2f0 0xc0026ef2f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.917: INFO: Pod "webserver-deployment-595b5b9587-g2gpk" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-g2gpk webserver-deployment-595b5b9587- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-595b5b9587-g2gpk 704e638a-b27d-4d23-bd9f-209b19140c61 211674 0 2020-05-14 03:56:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.100.136.122/32 cni.projectcalico.org/podIPs:10.100.136.122/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f9b930be-971c-4f0b-9c38-d2677cf2fbd2 0xc0026ef3e0 0xc0026ef3e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:56:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:56:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.13,PodIP:10.100.136.122,StartTime:2020-05-14 03:56:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-14 03:57:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://5a84440bb7c7d14806a30778ad1bd8e254485413b4d042bdcff35505c82d62f7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.136.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.917: INFO: Pod "webserver-deployment-595b5b9587-gqfz4" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-gqfz4 webserver-deployment-595b5b9587- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-595b5b9587-gqfz4 80534d0e-2d03-41c5-af3e-61d9a2d65107 211784 0 2020-05-14 03:57:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f9b930be-971c-4f0b-9c38-d2677cf2fbd2 0xc0026ef530 0xc0026ef531}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.918: INFO: Pod "webserver-deployment-595b5b9587-h9mp8" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-h9mp8 webserver-deployment-595b5b9587- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-595b5b9587-h9mp8 abd0f3ce-69ad-4211-a27c-93a9eb8d03b6 211659 0 2020-05-14 03:56:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.100.136.119/32 cni.projectcalico.org/podIPs:10.100.136.119/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f9b930be-971c-4f0b-9c38-d2677cf2fbd2 0xc0026ef620 0xc0026ef621}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:56:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:56:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.13,PodIP:10.100.136.119,StartTime:2020-05-14 03:56:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-14 03:57:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://30a3b14b7af915f34386d236e0be13c3a42e1d8aa70821086f8d54bb041639ca,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.136.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.918: INFO: Pod "webserver-deployment-595b5b9587-rvrwf" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rvrwf webserver-deployment-595b5b9587- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-595b5b9587-rvrwf c58a3953-1567-4fc1-a85f-6ded33cb937d 211811 0 2020-05-14 03:57:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f9b930be-971c-4f0b-9c38-d2677cf2fbd2 0xc0026ef770 0xc0026ef771}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.919: INFO: Pod "webserver-deployment-595b5b9587-sd2dx" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-sd2dx webserver-deployment-595b5b9587- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-595b5b9587-sd2dx 01331706-7f42-4fe0-9057-cb545aa8576e 211791 0 2020-05-14 03:57:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f9b930be-971c-4f0b-9c38-d2677cf2fbd2 0xc0026ef860 0xc0026ef861}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.11,PodIP:,StartTime:2020-05-14 03:57:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.920: INFO: Pod "webserver-deployment-595b5b9587-sjfdf" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-sjfdf webserver-deployment-595b5b9587- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-595b5b9587-sjfdf 2f322aba-26a3-41a8-b8b7-b20e2272d467 211812 0 2020-05-14 03:57:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f9b930be-971c-4f0b-9c38-d2677cf2fbd2 0xc0026ef990 0xc0026ef991}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.920: INFO: Pod "webserver-deployment-595b5b9587-sqvt2" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-sqvt2 webserver-deployment-595b5b9587- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-595b5b9587-sqvt2 1a29ec29-9397-4284-aeb8-04c550bfb8bd 211667 0 2020-05-14 03:56:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.100.136.117/32 cni.projectcalico.org/podIPs:10.100.136.117/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f9b930be-971c-4f0b-9c38-d2677cf2fbd2 0xc0026efa80 0xc0026efa81}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:56:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:56:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.13,PodIP:10.100.136.117,StartTime:2020-05-14 03:56:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-14 03:57:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://cd5bb9f4fbdd7a92328e92229a4cfc2616e970501920d7c5cccc920858cedc9a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.136.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.921: INFO: Pod "webserver-deployment-595b5b9587-t8xs4" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-t8xs4 webserver-deployment-595b5b9587- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-595b5b9587-t8xs4 5444b35d-3e9f-424a-8454-81c4b5fa32e2 211800 0 2020-05-14 03:57:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f9b930be-971c-4f0b-9c38-d2677cf2fbd2 0xc0026efbd0 0xc0026efbd1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.13,PodIP:,StartTime:2020-05-14 03:57:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.921: INFO: Pod "webserver-deployment-595b5b9587-tjsp8" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-tjsp8 webserver-deployment-595b5b9587- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-595b5b9587-tjsp8 15889b66-5d31-4502-90ec-b2483fc3af53 211825 0 2020-05-14 03:57:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f9b930be-971c-4f0b-9c38-d2677cf2fbd2 0xc0026efd00 0xc0026efd01}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.11,PodIP:,StartTime:2020-05-14 03:57:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.923: INFO: Pod "webserver-deployment-595b5b9587-v755x" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-v755x webserver-deployment-595b5b9587- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-595b5b9587-v755x 8ef96f88-7e20-4cba-a412-d12e9c43b5f9 211803 0 2020-05-14 03:57:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f9b930be-971c-4f0b-9c38-d2677cf2fbd2 0xc0026efe30 0xc0026efe31}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.923: INFO: Pod "webserver-deployment-595b5b9587-x7v5t" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-x7v5t webserver-deployment-595b5b9587- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-595b5b9587-x7v5t 422d3908-1832-4079-9730-5cb92806b275 211593 0 2020-05-14 03:56:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.100.230.61/32 cni.projectcalico.org/podIPs:10.100.230.61/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f9b930be-971c-4f0b-9c38-d2677cf2fbd2 0xc0026eff40 0xc0026eff41}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:56:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:56:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.11,PodIP:10.100.230.61,StartTime:2020-05-14 03:56:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-14 03:57:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a295e0b0ec609c062fc1a727ad02d3b7aa71b7550d3cd885a17bc17fc585b6f7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.230.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.924: INFO: Pod "webserver-deployment-595b5b9587-x9vgn" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-x9vgn webserver-deployment-595b5b9587- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-595b5b9587-x9vgn b94ac60f-9fdc-4f16-9979-64bb27739ad5 211596 0 2020-05-14 03:56:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.100.230.63/32 cni.projectcalico.org/podIPs:10.100.230.63/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f9b930be-971c-4f0b-9c38-d2677cf2fbd2 0xc0027ba0b0 0xc0027ba0b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:56:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:56:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.11,PodIP:10.100.230.63,StartTime:2020-05-14 03:56:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-14 03:57:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://4f2e953dced729d1192255d0587e4a2c66b884b2651d2e0296eb816665b241b7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.230.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.925: INFO: Pod "webserver-deployment-c7997dcc8-2j4l4" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-2j4l4 webserver-deployment-c7997dcc8- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-c7997dcc8-2j4l4 d9cad844-8311-4ca3-ba90-8397a2a955ba 211813 0 2020-05-14 03:57:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a44f0fa4-3886-48a9-bfaf-4a58ae4adef0 0xc0027ba200 0xc0027ba201}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.925: INFO: Pod "webserver-deployment-c7997dcc8-5fgh2" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-5fgh2 webserver-deployment-c7997dcc8- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-c7997dcc8-5fgh2 435f4fdd-e4d4-4175-97a3-a61f2e3ebe89 211826 0 2020-05-14 03:57:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a44f0fa4-3886-48a9-bfaf-4a58ae4adef0 0xc0027ba300 0xc0027ba301}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.926: INFO: Pod "webserver-deployment-c7997dcc8-5gz9z" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-5gz9z webserver-deployment-c7997dcc8- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-c7997dcc8-5gz9z 3b2fbb30-d19a-42cc-9389-d8bac06cb140 211748 0 2020-05-14 03:57:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a44f0fa4-3886-48a9-bfaf-4a58ae4adef0 0xc0027ba400 0xc0027ba401}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.13,PodIP:,StartTime:2020-05-14 03:57:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.927: INFO: Pod "webserver-deployment-c7997dcc8-fr9zm" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-fr9zm webserver-deployment-c7997dcc8- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-c7997dcc8-fr9zm 2aad033b-0ad6-42b6-8aa0-02ed854eb0f6 211732 0 2020-05-14 03:57:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a44f0fa4-3886-48a9-bfaf-4a58ae4adef0 0xc0027ba550 0xc0027ba551}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.13,PodIP:,StartTime:2020-05-14 03:57:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.929: INFO: Pod "webserver-deployment-c7997dcc8-jqmwp" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-jqmwp webserver-deployment-c7997dcc8- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-c7997dcc8-jqmwp 64c13699-f07b-4b45-ab6b-ad0e443ce165 211750 0 2020-05-14 03:57:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a44f0fa4-3886-48a9-bfaf-4a58ae4adef0 0xc0027ba6a0 0xc0027ba6a1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.13,PodIP:,StartTime:2020-05-14 03:57:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.929: INFO: Pod "webserver-deployment-c7997dcc8-kc7ff" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-kc7ff webserver-deployment-c7997dcc8- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-c7997dcc8-kc7ff bb3e386e-2e3a-4d6f-8550-68f2cc11633c 211788 0 2020-05-14 03:57:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a44f0fa4-3886-48a9-bfaf-4a58ae4adef0 0xc0027ba800 0xc0027ba801}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.930: INFO: Pod "webserver-deployment-c7997dcc8-kf246" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-kf246 webserver-deployment-c7997dcc8- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-c7997dcc8-kf246 e81c8299-f003-49ad-86fb-412b09dc6e3d 211816 0 2020-05-14 03:57:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a44f0fa4-3886-48a9-bfaf-4a58ae4adef0 0xc0027ba900 0xc0027ba901}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.931: INFO: Pod "webserver-deployment-c7997dcc8-qpslq" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qpslq webserver-deployment-c7997dcc8- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-c7997dcc8-qpslq 8dec23f1-b536-4402-8daf-99e62680271d 211751 0 2020-05-14 03:57:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a44f0fa4-3886-48a9-bfaf-4a58ae4adef0 0xc0027baa00 0xc0027baa01}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.13,PodIP:,StartTime:2020-05-14 03:57:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.932: INFO: Pod "webserver-deployment-c7997dcc8-skbhz" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-skbhz webserver-deployment-c7997dcc8- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-c7997dcc8-skbhz 40c07286-8d48-4998-ba8b-a8db13d1ad6b 211722 0 2020-05-14 03:57:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a44f0fa4-3886-48a9-bfaf-4a58ae4adef0 0xc0027bab50 0xc0027bab51}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.13,PodIP:,StartTime:2020-05-14 03:57:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.932: INFO: Pod "webserver-deployment-c7997dcc8-stgmk" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-stgmk webserver-deployment-c7997dcc8- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-c7997dcc8-stgmk aff7e40c-990b-40f0-bad1-70ed5fed40af 211778 0 2020-05-14 03:57:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a44f0fa4-3886-48a9-bfaf-4a58ae4adef0 0xc0027baca0 0xc0027baca1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.933: INFO: Pod "webserver-deployment-c7997dcc8-vjtcs" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-vjtcs webserver-deployment-c7997dcc8- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-c7997dcc8-vjtcs c84326a4-0f8a-45e7-a99a-4f5e480d0e19 211820 0 2020-05-14 03:57:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a44f0fa4-3886-48a9-bfaf-4a58ae4adef0 0xc0027bada0 0xc0027bada1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.933: INFO: Pod "webserver-deployment-c7997dcc8-vstr8" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-vstr8 webserver-deployment-c7997dcc8- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-c7997dcc8-vstr8 bb3b4035-6e39-4c48-aee3-3aa146b0429f 211822 0 2020-05-14 03:57:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a44f0fa4-3886-48a9-bfaf-4a58ae4adef0 0xc0027baea0 0xc0027baea1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 14 03:57:14.934: INFO: Pod "webserver-deployment-c7997dcc8-wqm4s" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-wqm4s webserver-deployment-c7997dcc8- deployment-6194 /api/v1/namespaces/deployment-6194/pods/webserver-deployment-c7997dcc8-wqm4s 06228091-34ca-447f-a8e1-d5c0fcce96a6 211799 0 2020-05-14 03:57:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a44f0fa4-3886-48a9-bfaf-4a58ae4adef0 0xc0027bafa0 0xc0027bafa1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ndlqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ndlqp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ndlqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 03:57:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:57:14.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6194" for this suite.
May 14 03:57:23.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:57:23.604: INFO: namespace deployment-6194 deletion completed in 8.64250885s

â€¢ [SLOW TEST:25.744 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:57:23.607: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9505
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-f72fb0d3-8c41-45e0-86cb-df3c481f7924
STEP: Creating a pod to test consume configMaps
May 14 03:57:23.912: INFO: Waiting up to 5m0s for pod "pod-configmaps-02336802-b529-407e-98cf-17079ed0fa90" in namespace "configmap-9505" to be "success or failure"
May 14 03:57:25.094: INFO: Pod "pod-configmaps-02336802-b529-407e-98cf-17079ed0fa90": Phase="Pending", Reason="", readiness=false. Elapsed: 1.182715125s
May 14 03:57:27.102: INFO: Pod "pod-configmaps-02336802-b529-407e-98cf-17079ed0fa90": Phase="Pending", Reason="", readiness=false. Elapsed: 3.190650043s
May 14 03:57:29.120: INFO: Pod "pod-configmaps-02336802-b529-407e-98cf-17079ed0fa90": Phase="Pending", Reason="", readiness=false. Elapsed: 5.208632141s
May 14 03:57:31.159: INFO: Pod "pod-configmaps-02336802-b529-407e-98cf-17079ed0fa90": Phase="Pending", Reason="", readiness=false. Elapsed: 7.247712931s
May 14 03:57:33.166: INFO: Pod "pod-configmaps-02336802-b529-407e-98cf-17079ed0fa90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.253776155s
STEP: Saw pod success
May 14 03:57:33.166: INFO: Pod "pod-configmaps-02336802-b529-407e-98cf-17079ed0fa90" satisfied condition "success or failure"
May 14 03:57:33.170: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-configmaps-02336802-b529-407e-98cf-17079ed0fa90 container configmap-volume-test: <nil>
STEP: delete the pod
May 14 03:57:33.210: INFO: Waiting for pod pod-configmaps-02336802-b529-407e-98cf-17079ed0fa90 to disappear
May 14 03:57:33.217: INFO: Pod pod-configmaps-02336802-b529-407e-98cf-17079ed0fa90 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:57:33.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9505" for this suite.
May 14 03:57:41.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:57:41.669: INFO: namespace configmap-9505 deletion completed in 8.438555404s

â€¢ [SLOW TEST:18.063 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:57:41.679: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6624
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6624
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-6624
I0514 03:57:42.005451      20 runners.go:184] Created replication controller with name: externalname-service, namespace: services-6624, replica count: 2
I0514 03:57:45.070395      20 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0514 03:57:48.071016      20 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 14 03:57:48.072: INFO: Creating new exec pod
May 14 03:57:55.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=services-6624 execpodnhmnf -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 14 03:57:55.878: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 14 03:57:55.878: INFO: stdout: ""
May 14 03:57:55.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=services-6624 execpodnhmnf -- /bin/sh -x -c nc -zv -t -w 2 10.254.14.232 80'
May 14 03:57:56.506: INFO: stderr: "+ nc -zv -t -w 2 10.254.14.232 80\nConnection to 10.254.14.232 80 port [tcp/http] succeeded!\n"
May 14 03:57:56.506: INFO: stdout: ""
May 14 03:57:56.506: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:57:56.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6624" for this suite.
May 14 03:58:02.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:58:03.100: INFO: namespace services-6624 deletion completed in 6.539572789s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:21.431 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:58:03.111: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2542
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:58:09.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2542" for this suite.
May 14 03:58:25.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:58:25.910: INFO: namespace containers-2542 deletion completed in 16.370485591s

â€¢ [SLOW TEST:22.800 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:58:25.913: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2857
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 14 03:58:26.179: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:58:26.184: INFO: Number of nodes with available pods: 0
May 14 03:58:26.184: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:58:27.193: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:58:27.198: INFO: Number of nodes with available pods: 0
May 14 03:58:27.198: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:58:29.448: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:58:29.508: INFO: Number of nodes with available pods: 0
May 14 03:58:29.508: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:58:30.191: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:58:30.198: INFO: Number of nodes with available pods: 0
May 14 03:58:30.198: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:58:31.192: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:58:31.331: INFO: Number of nodes with available pods: 0
May 14 03:58:31.331: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-0 is running more than one daemon pod
May 14 03:58:32.190: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:58:32.195: INFO: Number of nodes with available pods: 2
May 14 03:58:32.195: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 14 03:58:32.229: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:58:32.242: INFO: Number of nodes with available pods: 1
May 14 03:58:32.242: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-1 is running more than one daemon pod
May 14 03:58:33.250: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:58:33.254: INFO: Number of nodes with available pods: 1
May 14 03:58:33.255: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-1 is running more than one daemon pod
May 14 03:58:34.257: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:58:34.273: INFO: Number of nodes with available pods: 1
May 14 03:58:34.273: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-1 is running more than one daemon pod
May 14 03:58:35.253: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:58:35.258: INFO: Number of nodes with available pods: 1
May 14 03:58:35.258: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-1 is running more than one daemon pod
May 14 03:58:36.426: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:58:36.511: INFO: Number of nodes with available pods: 1
May 14 03:58:36.511: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-1 is running more than one daemon pod
May 14 03:58:37.253: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:58:37.260: INFO: Number of nodes with available pods: 1
May 14 03:58:37.261: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-1 is running more than one daemon pod
May 14 03:58:38.250: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:58:38.256: INFO: Number of nodes with available pods: 1
May 14 03:58:38.256: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-1 is running more than one daemon pod
May 14 03:58:39.248: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:58:39.256: INFO: Number of nodes with available pods: 1
May 14 03:58:39.256: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-1 is running more than one daemon pod
May 14 03:58:40.254: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:58:40.265: INFO: Number of nodes with available pods: 1
May 14 03:58:40.265: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-1 is running more than one daemon pod
May 14 03:58:41.251: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:58:41.257: INFO: Number of nodes with available pods: 1
May 14 03:58:41.257: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-1 is running more than one daemon pod
May 14 03:58:42.249: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:58:42.254: INFO: Number of nodes with available pods: 1
May 14 03:58:42.254: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-1 is running more than one daemon pod
May 14 03:58:43.257: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:58:43.262: INFO: Number of nodes with available pods: 1
May 14 03:58:43.262: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-1 is running more than one daemon pod
May 14 03:58:44.249: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:58:44.254: INFO: Number of nodes with available pods: 1
May 14 03:58:44.254: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-1 is running more than one daemon pod
May 14 03:58:45.255: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:58:45.261: INFO: Number of nodes with available pods: 1
May 14 03:58:45.261: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-1 is running more than one daemon pod
May 14 03:58:46.248: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:58:46.253: INFO: Number of nodes with available pods: 1
May 14 03:58:46.253: INFO: Node k8s-fcos-flwang-pyqjxt4oox23-node-1 is running more than one daemon pod
May 14 03:58:47.253: INFO: DaemonSet pods can't tolerate node k8s-fcos-flwang-pyqjxt4oox23-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 14 03:58:47.267: INFO: Number of nodes with available pods: 2
May 14 03:58:47.267: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2857, will wait for the garbage collector to delete the pods
May 14 03:58:47.352: INFO: Deleting DaemonSet.extensions daemon-set took: 14.193291ms
May 14 03:58:48.352: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.000405068s
May 14 03:58:59.164: INFO: Number of nodes with available pods: 0
May 14 03:58:59.164: INFO: Number of running nodes: 0, number of available pods: 0
May 14 03:58:59.170: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2857/daemonsets","resourceVersion":"212629"},"items":null}

May 14 03:58:59.180: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2857/pods","resourceVersion":"212629"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:58:59.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2857" for this suite.
May 14 03:59:07.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:59:07.733: INFO: namespace daemonsets-2857 deletion completed in 8.5093954s

â€¢ [SLOW TEST:41.820 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:59:07.759: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3541
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-cd787ca4-50d6-487a-af43-824e15d1bace
STEP: Creating a pod to test consume secrets
May 14 03:59:07.989: INFO: Waiting up to 5m0s for pod "pod-secrets-e8df14f1-78bf-43cc-b52a-17e0d4ed2519" in namespace "secrets-3541" to be "success or failure"
May 14 03:59:08.000: INFO: Pod "pod-secrets-e8df14f1-78bf-43cc-b52a-17e0d4ed2519": Phase="Pending", Reason="", readiness=false. Elapsed: 10.472995ms
May 14 03:59:10.045: INFO: Pod "pod-secrets-e8df14f1-78bf-43cc-b52a-17e0d4ed2519": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055189412s
May 14 03:59:12.508: INFO: Pod "pod-secrets-e8df14f1-78bf-43cc-b52a-17e0d4ed2519": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.518803923s
STEP: Saw pod success
May 14 03:59:12.508: INFO: Pod "pod-secrets-e8df14f1-78bf-43cc-b52a-17e0d4ed2519" satisfied condition "success or failure"
May 14 03:59:12.517: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-secrets-e8df14f1-78bf-43cc-b52a-17e0d4ed2519 container secret-volume-test: <nil>
STEP: delete the pod
May 14 03:59:14.593: INFO: Waiting for pod pod-secrets-e8df14f1-78bf-43cc-b52a-17e0d4ed2519 to disappear
May 14 03:59:14.628: INFO: Pod pod-secrets-e8df14f1-78bf-43cc-b52a-17e0d4ed2519 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:59:14.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3541" for this suite.
May 14 03:59:20.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:59:21.080: INFO: namespace secrets-3541 deletion completed in 6.441535295s

â€¢ [SLOW TEST:13.322 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:59:21.084: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-949
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 03:59:21.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 create -f - --namespace=kubectl-949'
May 14 03:59:22.577: INFO: stderr: ""
May 14 03:59:22.577: INFO: stdout: "replicationcontroller/redis-master created\n"
May 14 03:59:22.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 create -f - --namespace=kubectl-949'
May 14 03:59:23.440: INFO: stderr: ""
May 14 03:59:23.440: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
May 14 03:59:24.454: INFO: Selector matched 1 pods for map[app:redis]
May 14 03:59:24.454: INFO: Found 0 / 1
May 14 03:59:25.453: INFO: Selector matched 1 pods for map[app:redis]
May 14 03:59:25.453: INFO: Found 0 / 1
May 14 03:59:26.445: INFO: Selector matched 1 pods for map[app:redis]
May 14 03:59:26.446: INFO: Found 0 / 1
May 14 03:59:27.447: INFO: Selector matched 1 pods for map[app:redis]
May 14 03:59:27.447: INFO: Found 0 / 1
May 14 03:59:28.447: INFO: Selector matched 1 pods for map[app:redis]
May 14 03:59:28.447: INFO: Found 1 / 1
May 14 03:59:28.447: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 14 03:59:28.452: INFO: Selector matched 1 pods for map[app:redis]
May 14 03:59:28.453: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 14 03:59:28.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 describe pod redis-master-fv8sz --namespace=kubectl-949'
May 14 03:59:28.873: INFO: stderr: ""
May 14 03:59:28.873: INFO: stdout: "Name:         redis-master-fv8sz\nNamespace:    kubectl-949\nNode:         k8s-fcos-flwang-pyqjxt4oox23-node-1/10.0.0.13\nStart Time:   Thu, 14 May 2020 03:59:22 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 10.100.136.79/32\n              cni.projectcalico.org/podIPs: 10.100.136.79/32\nStatus:       Running\nIP:           10.100.136.79\nIPs:\n  IP:           10.100.136.79\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://f3cd557d6f4906b35934c258c90d96574088ac057f7a2978ce0b4d0e623b52d1\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 14 May 2020 03:59:26 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-f5zqg (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-f5zqg:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-f5zqg\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                                          Message\n  ----    ------     ----       ----                                          -------\n  Normal  Scheduled  <unknown>  default-scheduler                             Successfully assigned kubectl-949/redis-master-fv8sz to k8s-fcos-flwang-pyqjxt4oox23-node-1\n  Normal  Pulled     4s         kubelet, k8s-fcos-flwang-pyqjxt4oox23-node-1  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    3s         kubelet, k8s-fcos-flwang-pyqjxt4oox23-node-1  Created container redis-master\n  Normal  Started    1s         kubelet, k8s-fcos-flwang-pyqjxt4oox23-node-1  Started container redis-master\n"
May 14 03:59:28.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 describe rc redis-master --namespace=kubectl-949'
May 14 03:59:29.355: INFO: stderr: ""
May 14 03:59:29.356: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-949\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  7s    replication-controller  Created pod: redis-master-fv8sz\n"
May 14 03:59:29.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 describe service redis-master --namespace=kubectl-949'
May 14 03:59:29.709: INFO: stderr: ""
May 14 03:59:29.709: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-949\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.254.250.247\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.100.136.79:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 14 03:59:29.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 describe node k8s-fcos-flwang-pyqjxt4oox23-master-0'
May 14 03:59:30.159: INFO: stderr: ""
May 14 03:59:30.159: INFO: stdout: "Name:               k8s-fcos-flwang-pyqjxt4oox23-master-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=5ff0b09b-684c-4212-8edc-826f26f9ab78\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=nz_wlg_2\n                    failure-domain.beta.kubernetes.io/zone=NZ-WLG-2\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-fcos-flwang-pyqjxt4oox23-master-0\n                    kubernetes.io/os=linux\n                    magnum.openstack.org/nodegroup=default-master\n                    magnum.openstack.org/role=master\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=5ff0b09b-684c-4212-8edc-826f26f9ab78\n                    topology.kubernetes.io/region=nz_wlg_2\n                    topology.kubernetes.io/zone=NZ-WLG-2\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.0.10/24\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 13 May 2020 10:11:17 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 13 May 2020 10:13:18 +0000   Wed, 13 May 2020 10:13:18 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 14 May 2020 03:58:43 +0000   Wed, 13 May 2020 10:11:10 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 14 May 2020 03:58:43 +0000   Wed, 13 May 2020 10:11:10 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 14 May 2020 03:58:43 +0000   Wed, 13 May 2020 10:11:10 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 14 May 2020 03:58:43 +0000   Wed, 13 May 2020 10:12:27 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.0.0.10\n  ExternalIP:  103.254.157.169\n  Hostname:    k8s-fcos-flwang-pyqjxt4oox23-master-0\nCapacity:\n cpu:                2\n ephemeral-storage:  9950188Ki\n hugepages-2Mi:      0\n memory:             4024304Ki\n pods:               110\nAllocatable:\n cpu:                2\n ephemeral-storage:  9170093246\n hugepages-2Mi:      0\n memory:             3921904Ki\n pods:               110\nSystem Info:\n Machine ID:                 eaaa14bcabad4e33959ede645617b44e\n System UUID:                eaaa14bc-abad-4e33-959e-de645617b44e\n Boot ID:                    fafbe101-6022-4b0b-b1ec-edf1162a4d73\n Kernel Version:             5.5.15-200.fc31.x86_64\n OS Image:                   Fedora CoreOS 31.20200407.3.0\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.8\n Kubelet Version:            v1.16.9\n Kube-Proxy Version:         v1.16.9\nPodCIDR:                     10.100.0.0/24\nPodCIDRs:                    10.100.0.0/24\nProviderID:                  openstack:///eaaa14bc-abad-4e33-959e-de645617b44e\nNon-terminated Pods:         (13 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                calico-kube-controllers-6747bb98fb-l6hbh                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         17h\n  kube-system                calico-node-kzxm7                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         17h\n  kube-system                coredns-66997f79d9-2tt6d                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     17h\n  kube-system                coredns-66997f79d9-llbzc                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     17h\n  kube-system                dashboard-metrics-scraper-65bc5949fb-ll785                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         17h\n  kube-system                k8s-keystone-auth-qthl9                                    200m (10%)    0 (0%)      0 (0%)           0 (0%)         17h\n  kube-system                kubernetes-dashboard-6f4597c5c6-6g5f5                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         17h\n  kube-system                magnum-auto-healer-kb2rf                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         17h\n  kube-system                octavia-ingress-controller-0                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         17h\n  kube-system                openstack-cloud-controller-manager-cbbsg                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         17h\n  kube-system                prometheus-operator-prometheus-node-exporter-5xtzw         20m (1%)      20m (1%)    20M (0%)         20M (0%)       17h\n  magnum-tiller              tiller-deploy-56878b4756-qkrdx                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         17h\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-2dde1b3f033a4765-rp4lg    0 (0%)        0 (0%)      0 (0%)           0 (0%)         136m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                670m (33%)      20m (1%)\n  memory             166800640 (4%)  376515840 (9%)\n  ephemeral-storage  0 (0%)          0 (0%)\nEvents:              <none>\n"
May 14 03:59:30.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 describe namespace kubectl-949'
May 14 03:59:30.439: INFO: stderr: ""
May 14 03:59:30.439: INFO: stdout: "Name:         kubectl-949\nLabels:       e2e-framework=kubectl\n              e2e-run=5c036a25-d9b5-46f9-8351-3b0a5a4e4a43\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:59:30.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-949" for this suite.
May 14 03:59:42.488: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 03:59:42.863: INFO: namespace kubectl-949 deletion completed in 12.409072313s

â€¢ [SLOW TEST:21.780 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 03:59:42.867: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2766
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 03:59:59.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2766" for this suite.
May 14 04:00:05.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:00:05.798: INFO: namespace resourcequota-2766 deletion completed in 6.344600365s

â€¢ [SLOW TEST:22.932 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:00:05.805: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-388
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 14 04:00:23.295: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-388 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 04:00:23.295: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
May 14 04:00:23.807: INFO: Exec stderr: ""
May 14 04:00:23.807: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-388 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 04:00:23.807: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
May 14 04:00:24.239: INFO: Exec stderr: ""
May 14 04:00:24.239: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-388 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 04:00:24.239: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
May 14 04:00:24.673: INFO: Exec stderr: ""
May 14 04:00:24.695: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-388 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 04:00:24.695: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
May 14 04:00:25.157: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 14 04:00:25.157: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-388 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 04:00:25.157: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
May 14 04:00:25.831: INFO: Exec stderr: ""
May 14 04:00:25.832: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-388 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 04:00:25.832: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
May 14 04:00:26.410: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 14 04:00:26.410: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-388 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 04:00:26.411: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
May 14 04:00:27.187: INFO: Exec stderr: ""
May 14 04:00:27.187: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-388 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 04:00:27.187: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
May 14 04:00:27.996: INFO: Exec stderr: ""
May 14 04:00:27.997: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-388 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 04:00:27.997: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
May 14 04:00:28.438: INFO: Exec stderr: ""
May 14 04:00:28.439: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-388 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 04:00:28.439: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
May 14 04:00:28.899: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:00:28.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-388" for this suite.
May 14 04:01:26.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:01:27.536: INFO: namespace e2e-kubelet-etc-hosts-388 deletion completed in 58.609497419s

â€¢ [SLOW TEST:81.732 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:01:27.552: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2399
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 14 04:01:27.742: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c7f9e363-ca67-4757-8815-656192703112" in namespace "projected-2399" to be "success or failure"
May 14 04:01:27.764: INFO: Pod "downwardapi-volume-c7f9e363-ca67-4757-8815-656192703112": Phase="Pending", Reason="", readiness=false. Elapsed: 21.232962ms
May 14 04:01:29.769: INFO: Pod "downwardapi-volume-c7f9e363-ca67-4757-8815-656192703112": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026314723s
May 14 04:01:31.779: INFO: Pod "downwardapi-volume-c7f9e363-ca67-4757-8815-656192703112": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036220934s
STEP: Saw pod success
May 14 04:01:31.779: INFO: Pod "downwardapi-volume-c7f9e363-ca67-4757-8815-656192703112" satisfied condition "success or failure"
May 14 04:01:31.796: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod downwardapi-volume-c7f9e363-ca67-4757-8815-656192703112 container client-container: <nil>
STEP: delete the pod
May 14 04:01:31.955: INFO: Waiting for pod downwardapi-volume-c7f9e363-ca67-4757-8815-656192703112 to disappear
May 14 04:01:31.968: INFO: Pod downwardapi-volume-c7f9e363-ca67-4757-8815-656192703112 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:01:31.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2399" for this suite.
May 14 04:01:38.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:01:38.387: INFO: namespace projected-2399 deletion completed in 6.406956855s

â€¢ [SLOW TEST:10.837 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:01:38.398: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5750
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 04:01:38.603: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:01:45.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5750" for this suite.
May 14 04:01:51.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:01:52.334: INFO: namespace custom-resource-definition-5750 deletion completed in 6.408761338s

â€¢ [SLOW TEST:13.938 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:01:52.337: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6625
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-d893cbd3-4a00-4822-a766-4144cbaef3b5
STEP: Creating a pod to test consume configMaps
May 14 04:01:52.598: INFO: Waiting up to 5m0s for pod "pod-configmaps-0c8874b9-1124-4f1b-800d-cdd84525ae83" in namespace "configmap-6625" to be "success or failure"
May 14 04:01:52.614: INFO: Pod "pod-configmaps-0c8874b9-1124-4f1b-800d-cdd84525ae83": Phase="Pending", Reason="", readiness=false. Elapsed: 15.159197ms
May 14 04:01:54.621: INFO: Pod "pod-configmaps-0c8874b9-1124-4f1b-800d-cdd84525ae83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02197748s
May 14 04:01:56.632: INFO: Pod "pod-configmaps-0c8874b9-1124-4f1b-800d-cdd84525ae83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032670334s
STEP: Saw pod success
May 14 04:01:56.632: INFO: Pod "pod-configmaps-0c8874b9-1124-4f1b-800d-cdd84525ae83" satisfied condition "success or failure"
May 14 04:01:56.638: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-configmaps-0c8874b9-1124-4f1b-800d-cdd84525ae83 container configmap-volume-test: <nil>
STEP: delete the pod
May 14 04:01:56.699: INFO: Waiting for pod pod-configmaps-0c8874b9-1124-4f1b-800d-cdd84525ae83 to disappear
May 14 04:01:56.714: INFO: Pod pod-configmaps-0c8874b9-1124-4f1b-800d-cdd84525ae83 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:01:56.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6625" for this suite.
May 14 04:02:04.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:02:09.133: INFO: namespace configmap-6625 deletion completed in 12.403931875s

â€¢ [SLOW TEST:16.797 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:02:09.138: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6684
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 04:02:09.408: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-d28aacc9-7791-4b09-b138-5e9bc3d49f3e" in namespace "security-context-test-6684" to be "success or failure"
May 14 04:02:09.970: INFO: Pod "busybox-privileged-false-d28aacc9-7791-4b09-b138-5e9bc3d49f3e": Phase="Pending", Reason="", readiness=false. Elapsed: 561.852311ms
May 14 04:02:12.639: INFO: Pod "busybox-privileged-false-d28aacc9-7791-4b09-b138-5e9bc3d49f3e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.231665176s
May 14 04:02:14.647: INFO: Pod "busybox-privileged-false-d28aacc9-7791-4b09-b138-5e9bc3d49f3e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.23966513s
May 14 04:02:18.567: INFO: Pod "busybox-privileged-false-d28aacc9-7791-4b09-b138-5e9bc3d49f3e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.1590695s
May 14 04:02:20.809: INFO: Pod "busybox-privileged-false-d28aacc9-7791-4b09-b138-5e9bc3d49f3e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.401450678s
May 14 04:02:33.300: INFO: Pod "busybox-privileged-false-d28aacc9-7791-4b09-b138-5e9bc3d49f3e": Phase="Pending", Reason="", readiness=false. Elapsed: 23.891846732s
May 14 04:02:40.524: INFO: Pod "busybox-privileged-false-d28aacc9-7791-4b09-b138-5e9bc3d49f3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 31.116563384s
May 14 04:02:40.525: INFO: Pod "busybox-privileged-false-d28aacc9-7791-4b09-b138-5e9bc3d49f3e" satisfied condition "success or failure"
May 14 04:02:40.707: INFO: Got logs for pod "busybox-privileged-false-d28aacc9-7791-4b09-b138-5e9bc3d49f3e": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:02:40.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6684" for this suite.
May 14 04:03:38.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:03:39.373: INFO: namespace security-context-test-6684 deletion completed in 58.628546948s

â€¢ [SLOW TEST:90.236 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:03:39.378: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4835
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-b9da8aa1-94b4-47b6-ae8a-d1fa7f086a80
STEP: Creating secret with name s-test-opt-upd-abc66223-cfe9-401b-8522-870fac7a2a13
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-b9da8aa1-94b4-47b6-ae8a-d1fa7f086a80
STEP: Updating secret s-test-opt-upd-abc66223-cfe9-401b-8522-870fac7a2a13
STEP: Creating secret with name s-test-opt-create-86f03777-80d0-4e46-890c-9d1ddfeb0b18
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:03:47.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4835" for this suite.
May 14 04:04:18.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:04:18.449: INFO: namespace projected-4835 deletion completed in 30.485603479s

â€¢ [SLOW TEST:39.071 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:04:18.456: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3666
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 14 04:04:19.895: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 14 04:04:21.942: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725025859, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725025859, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725025859, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725025859, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 14 04:04:24.994: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
May 14 04:04:29.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 attach --namespace=webhook-3666 to-be-attached-pod -i -c=container1'
May 14 04:04:31.856: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:04:31.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3666" for this suite.
May 14 04:04:46.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:04:46.582: INFO: namespace webhook-3666 deletion completed in 14.703782711s
STEP: Destroying namespace "webhook-3666-markers" for this suite.
May 14 04:04:52.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:04:52.927: INFO: namespace webhook-3666-markers deletion completed in 6.345091244s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:34.496 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:04:52.960: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7977
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:05:02.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7977" for this suite.
May 14 04:05:54.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:05:55.339: INFO: namespace kubelet-test-7977 deletion completed in 52.392227775s

â€¢ [SLOW TEST:62.379 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:05:55.344: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1231
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 14 04:06:05.689: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 14 04:06:05.708: INFO: Pod pod-with-prestop-http-hook still exists
May 14 04:06:07.718: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 14 04:06:07.727: INFO: Pod pod-with-prestop-http-hook still exists
May 14 04:06:09.708: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 14 04:06:10.512: INFO: Pod pod-with-prestop-http-hook still exists
May 14 04:06:11.709: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 14 04:06:11.861: INFO: Pod pod-with-prestop-http-hook still exists
May 14 04:06:13.708: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 14 04:06:15.050: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:06:15.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1231" for this suite.
May 14 04:06:43.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:06:43.490: INFO: namespace container-lifecycle-hook-1231 deletion completed in 28.386947589s

â€¢ [SLOW TEST:48.147 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:06:43.491: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4588
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 14 04:06:43.710: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fac95e75-8d40-4464-8899-5edfe2cad39d" in namespace "downward-api-4588" to be "success or failure"
May 14 04:06:43.722: INFO: Pod "downwardapi-volume-fac95e75-8d40-4464-8899-5edfe2cad39d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.983402ms
May 14 04:06:45.736: INFO: Pod "downwardapi-volume-fac95e75-8d40-4464-8899-5edfe2cad39d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026227069s
May 14 04:06:47.743: INFO: Pod "downwardapi-volume-fac95e75-8d40-4464-8899-5edfe2cad39d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033313035s
STEP: Saw pod success
May 14 04:06:47.746: INFO: Pod "downwardapi-volume-fac95e75-8d40-4464-8899-5edfe2cad39d" satisfied condition "success or failure"
May 14 04:06:47.749: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod downwardapi-volume-fac95e75-8d40-4464-8899-5edfe2cad39d container client-container: <nil>
STEP: delete the pod
May 14 04:06:47.783: INFO: Waiting for pod downwardapi-volume-fac95e75-8d40-4464-8899-5edfe2cad39d to disappear
May 14 04:06:47.792: INFO: Pod downwardapi-volume-fac95e75-8d40-4464-8899-5edfe2cad39d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:06:47.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4588" for this suite.
May 14 04:06:53.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:06:54.193: INFO: namespace downward-api-4588 deletion completed in 6.393164014s

â€¢ [SLOW TEST:10.702 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:06:54.204: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9466
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
May 14 04:06:54.444: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

May 14 04:06:54.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 create -f - --namespace=kubectl-9466'
May 14 04:06:55.173: INFO: stderr: ""
May 14 04:06:55.173: INFO: stdout: "service/redis-slave created\n"
May 14 04:06:55.173: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

May 14 04:06:55.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 create -f - --namespace=kubectl-9466'
May 14 04:06:55.899: INFO: stderr: ""
May 14 04:06:55.899: INFO: stdout: "service/redis-master created\n"
May 14 04:06:55.899: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 14 04:06:55.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 create -f - --namespace=kubectl-9466'
May 14 04:06:56.437: INFO: stderr: ""
May 14 04:06:56.437: INFO: stdout: "service/frontend created\n"
May 14 04:06:56.438: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

May 14 04:06:56.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 create -f - --namespace=kubectl-9466'
May 14 04:06:57.121: INFO: stderr: ""
May 14 04:06:57.121: INFO: stdout: "deployment.apps/frontend created\n"
May 14 04:06:57.121: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 14 04:06:57.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 create -f - --namespace=kubectl-9466'
May 14 04:06:57.690: INFO: stderr: ""
May 14 04:06:57.690: INFO: stdout: "deployment.apps/redis-master created\n"
May 14 04:06:57.690: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

May 14 04:06:57.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 create -f - --namespace=kubectl-9466'
May 14 04:06:59.300: INFO: stderr: ""
May 14 04:06:59.300: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
May 14 04:06:59.300: INFO: Waiting for all frontend pods to be Running.
May 14 04:07:14.434: INFO: Waiting for frontend to serve content.
May 14 04:07:16.568: INFO: Trying to add a new entry to the guestbook.
May 14 04:07:17.138: INFO: Verifying that added entry can be retrieved.
May 14 04:07:17.159: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
May 14 04:07:22.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 delete --grace-period=0 --force -f - --namespace=kubectl-9466'
May 14 04:07:22.501: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 14 04:07:22.501: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
May 14 04:07:22.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 delete --grace-period=0 --force -f - --namespace=kubectl-9466'
May 14 04:07:22.851: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 14 04:07:22.852: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 14 04:07:22.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 delete --grace-period=0 --force -f - --namespace=kubectl-9466'
May 14 04:07:23.167: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 14 04:07:23.167: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 14 04:07:23.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 delete --grace-period=0 --force -f - --namespace=kubectl-9466'
May 14 04:07:23.480: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 14 04:07:23.480: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 14 04:07:23.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 delete --grace-period=0 --force -f - --namespace=kubectl-9466'
May 14 04:07:23.748: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 14 04:07:23.748: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 14 04:07:23.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 delete --grace-period=0 --force -f - --namespace=kubectl-9466'
May 14 04:07:24.144: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 14 04:07:24.144: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:07:24.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9466" for this suite.
May 14 04:07:36.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:07:36.673: INFO: namespace kubectl-9466 deletion completed in 12.518482418s

â€¢ [SLOW TEST:42.470 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:07:36.675: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5626
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-65fb0fca-2988-48d9-b1ee-c2f007584f6f
STEP: Creating a pod to test consume secrets
May 14 04:07:36.931: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-210a0494-34f6-457d-b182-89c1f20bf16f" in namespace "projected-5626" to be "success or failure"
May 14 04:07:36.943: INFO: Pod "pod-projected-secrets-210a0494-34f6-457d-b182-89c1f20bf16f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.734516ms
May 14 04:07:39.010: INFO: Pod "pod-projected-secrets-210a0494-34f6-457d-b182-89c1f20bf16f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.07822014s
May 14 04:07:41.023: INFO: Pod "pod-projected-secrets-210a0494-34f6-457d-b182-89c1f20bf16f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.092082929s
STEP: Saw pod success
May 14 04:07:41.025: INFO: Pod "pod-projected-secrets-210a0494-34f6-457d-b182-89c1f20bf16f" satisfied condition "success or failure"
May 14 04:07:41.037: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-projected-secrets-210a0494-34f6-457d-b182-89c1f20bf16f container secret-volume-test: <nil>
STEP: delete the pod
May 14 04:07:41.084: INFO: Waiting for pod pod-projected-secrets-210a0494-34f6-457d-b182-89c1f20bf16f to disappear
May 14 04:07:41.090: INFO: Pod pod-projected-secrets-210a0494-34f6-457d-b182-89c1f20bf16f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:07:41.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5626" for this suite.
May 14 04:07:47.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:07:47.838: INFO: namespace projected-5626 deletion completed in 6.739603082s

â€¢ [SLOW TEST:11.164 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:07:47.844: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7645
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 14 04:07:48.071: INFO: Waiting up to 5m0s for pod "pod-dea09870-c469-4d46-9cfb-d3a5de113595" in namespace "emptydir-7645" to be "success or failure"
May 14 04:07:48.105: INFO: Pod "pod-dea09870-c469-4d46-9cfb-d3a5de113595": Phase="Pending", Reason="", readiness=false. Elapsed: 33.858446ms
May 14 04:07:50.976: INFO: Pod "pod-dea09870-c469-4d46-9cfb-d3a5de113595": Phase="Pending", Reason="", readiness=false. Elapsed: 2.90410926s
May 14 04:07:53.000: INFO: Pod "pod-dea09870-c469-4d46-9cfb-d3a5de113595": Phase="Pending", Reason="", readiness=false. Elapsed: 4.928219167s
May 14 04:07:55.004: INFO: Pod "pod-dea09870-c469-4d46-9cfb-d3a5de113595": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.932637548s
STEP: Saw pod success
May 14 04:07:55.004: INFO: Pod "pod-dea09870-c469-4d46-9cfb-d3a5de113595" satisfied condition "success or failure"
May 14 04:07:55.016: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-dea09870-c469-4d46-9cfb-d3a5de113595 container test-container: <nil>
STEP: delete the pod
May 14 04:07:55.048: INFO: Waiting for pod pod-dea09870-c469-4d46-9cfb-d3a5de113595 to disappear
May 14 04:07:55.056: INFO: Pod pod-dea09870-c469-4d46-9cfb-d3a5de113595 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:07:55.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7645" for this suite.
May 14 04:08:01.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:08:01.448: INFO: namespace emptydir-7645 deletion completed in 6.381645909s

â€¢ [SLOW TEST:13.604 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:08:01.453: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1895
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-0c5fd1bb-f2fa-45cd-87ea-7a1026482a00
STEP: Creating a pod to test consume configMaps
May 14 04:08:01.653: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4730ed3b-642e-4e26-a7ac-a5d1dbc939b7" in namespace "projected-1895" to be "success or failure"
May 14 04:08:01.671: INFO: Pod "pod-projected-configmaps-4730ed3b-642e-4e26-a7ac-a5d1dbc939b7": Phase="Pending", Reason="", readiness=false. Elapsed: 17.424913ms
May 14 04:08:03.698: INFO: Pod "pod-projected-configmaps-4730ed3b-642e-4e26-a7ac-a5d1dbc939b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044163399s
May 14 04:08:05.709: INFO: Pod "pod-projected-configmaps-4730ed3b-642e-4e26-a7ac-a5d1dbc939b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055281938s
May 14 04:08:07.718: INFO: Pod "pod-projected-configmaps-4730ed3b-642e-4e26-a7ac-a5d1dbc939b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064298052s
STEP: Saw pod success
May 14 04:08:07.719: INFO: Pod "pod-projected-configmaps-4730ed3b-642e-4e26-a7ac-a5d1dbc939b7" satisfied condition "success or failure"
May 14 04:08:07.725: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-projected-configmaps-4730ed3b-642e-4e26-a7ac-a5d1dbc939b7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 14 04:08:07.762: INFO: Waiting for pod pod-projected-configmaps-4730ed3b-642e-4e26-a7ac-a5d1dbc939b7 to disappear
May 14 04:08:07.787: INFO: Pod pod-projected-configmaps-4730ed3b-642e-4e26-a7ac-a5d1dbc939b7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:08:07.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1895" for this suite.
May 14 04:08:13.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:08:14.283: INFO: namespace projected-1895 deletion completed in 6.469558998s

â€¢ [SLOW TEST:12.831 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:08:14.301: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6104
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6104
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6104
STEP: creating replication controller externalsvc in namespace services-6104
I0514 04:08:14.628447      20 runners.go:184] Created replication controller with name: externalsvc, namespace: services-6104, replica count: 2
I0514 04:08:17.685066      20 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0514 04:08:20.685634      20 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0514 04:08:23.686085      20 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
May 14 04:08:23.719: INFO: Creating new exec pod
May 14 04:08:29.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=services-6104 execpod49zt4 -- /bin/sh -x -c nslookup clusterip-service'
May 14 04:08:30.502: INFO: stderr: "+ nslookup clusterip-service\n"
May 14 04:08:30.502: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nclusterip-service.services-6104.svc.cluster.local\tcanonical name = externalsvc.services-6104.svc.cluster.local.\nName:\texternalsvc.services-6104.svc.cluster.local\nAddress: 10.254.138.114\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6104, will wait for the garbage collector to delete the pods
May 14 04:08:30.572: INFO: Deleting ReplicationController externalsvc took: 11.887607ms
May 14 04:08:31.575: INFO: Terminating ReplicationController externalsvc pods took: 1.00303628s
May 14 04:08:42.848: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:08:42.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6104" for this suite.
May 14 04:08:49.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:08:49.711: INFO: namespace services-6104 deletion completed in 6.695932792s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:35.410 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:08:49.712: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6174
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 14 04:08:49.922: INFO: Waiting up to 5m0s for pod "downwardapi-volume-da92232d-a0a6-4285-a5ec-05e793737fdb" in namespace "projected-6174" to be "success or failure"
May 14 04:08:49.962: INFO: Pod "downwardapi-volume-da92232d-a0a6-4285-a5ec-05e793737fdb": Phase="Pending", Reason="", readiness=false. Elapsed: 40.689266ms
May 14 04:08:51.971: INFO: Pod "downwardapi-volume-da92232d-a0a6-4285-a5ec-05e793737fdb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049730426s
May 14 04:08:53.996: INFO: Pod "downwardapi-volume-da92232d-a0a6-4285-a5ec-05e793737fdb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.074083141s
STEP: Saw pod success
May 14 04:08:53.996: INFO: Pod "downwardapi-volume-da92232d-a0a6-4285-a5ec-05e793737fdb" satisfied condition "success or failure"
May 14 04:08:54.002: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod downwardapi-volume-da92232d-a0a6-4285-a5ec-05e793737fdb container client-container: <nil>
STEP: delete the pod
May 14 04:08:54.095: INFO: Waiting for pod downwardapi-volume-da92232d-a0a6-4285-a5ec-05e793737fdb to disappear
May 14 04:08:54.107: INFO: Pod downwardapi-volume-da92232d-a0a6-4285-a5ec-05e793737fdb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:08:54.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6174" for this suite.
May 14 04:09:00.162: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:09:00.458: INFO: namespace projected-6174 deletion completed in 6.339008657s

â€¢ [SLOW TEST:10.746 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:09:00.459: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-653
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
May 14 04:09:01.228: INFO: created pod pod-service-account-defaultsa
May 14 04:09:01.229: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 14 04:09:01.239: INFO: created pod pod-service-account-mountsa
May 14 04:09:01.239: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 14 04:09:01.252: INFO: created pod pod-service-account-nomountsa
May 14 04:09:01.252: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 14 04:09:01.266: INFO: created pod pod-service-account-defaultsa-mountspec
May 14 04:09:01.266: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 14 04:09:01.286: INFO: created pod pod-service-account-mountsa-mountspec
May 14 04:09:01.286: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 14 04:09:01.293: INFO: created pod pod-service-account-nomountsa-mountspec
May 14 04:09:01.293: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 14 04:09:01.313: INFO: created pod pod-service-account-defaultsa-nomountspec
May 14 04:09:01.313: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 14 04:09:01.322: INFO: created pod pod-service-account-mountsa-nomountspec
May 14 04:09:01.322: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 14 04:09:01.340: INFO: created pod pod-service-account-nomountsa-nomountspec
May 14 04:09:01.340: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:09:01.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-653" for this suite.
May 14 04:09:29.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:09:29.810: INFO: namespace svcaccounts-653 deletion completed in 28.452197324s

â€¢ [SLOW TEST:29.351 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:09:29.822: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4903
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-ffc66c0c-8e59-4cd5-bd17-389ca0401797
STEP: Creating a pod to test consume configMaps
May 14 04:09:30.134: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3a108fdd-d7b4-40cd-a85b-7030ed6fdafb" in namespace "projected-4903" to be "success or failure"
May 14 04:09:30.144: INFO: Pod "pod-projected-configmaps-3a108fdd-d7b4-40cd-a85b-7030ed6fdafb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.146207ms
May 14 04:09:32.153: INFO: Pod "pod-projected-configmaps-3a108fdd-d7b4-40cd-a85b-7030ed6fdafb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018651836s
May 14 04:09:34.159: INFO: Pod "pod-projected-configmaps-3a108fdd-d7b4-40cd-a85b-7030ed6fdafb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025168333s
May 14 04:09:36.164: INFO: Pod "pod-projected-configmaps-3a108fdd-d7b4-40cd-a85b-7030ed6fdafb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030495944s
STEP: Saw pod success
May 14 04:09:36.165: INFO: Pod "pod-projected-configmaps-3a108fdd-d7b4-40cd-a85b-7030ed6fdafb" satisfied condition "success or failure"
May 14 04:09:36.167: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-projected-configmaps-3a108fdd-d7b4-40cd-a85b-7030ed6fdafb container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 14 04:09:36.214: INFO: Waiting for pod pod-projected-configmaps-3a108fdd-d7b4-40cd-a85b-7030ed6fdafb to disappear
May 14 04:09:36.222: INFO: Pod pod-projected-configmaps-3a108fdd-d7b4-40cd-a85b-7030ed6fdafb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:09:36.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4903" for this suite.
May 14 04:09:42.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:09:42.533: INFO: namespace projected-4903 deletion completed in 6.300959586s

â€¢ [SLOW TEST:12.712 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:09:42.539: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-373
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 14 04:09:42.835: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:10:02.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-373" for this suite.
May 14 04:10:08.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:10:09.435: INFO: namespace pods-373 deletion completed in 6.493903212s

â€¢ [SLOW TEST:26.897 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:10:09.440: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9810
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9810
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-9810
I0514 04:10:09.723286      20 runners.go:184] Created replication controller with name: externalname-service, namespace: services-9810, replica count: 2
I0514 04:10:12.776611      20 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0514 04:10:15.777281      20 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 14 04:10:15.777: INFO: Creating new exec pod
May 14 04:10:20.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=services-9810 execpodr2v8f -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 14 04:10:21.453: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 14 04:10:21.453: INFO: stdout: ""
May 14 04:10:21.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=services-9810 execpodr2v8f -- /bin/sh -x -c nc -zv -t -w 2 10.254.104.54 80'
May 14 04:10:22.064: INFO: stderr: "+ nc -zv -t -w 2 10.254.104.54 80\nConnection to 10.254.104.54 80 port [tcp/http] succeeded!\n"
May 14 04:10:22.064: INFO: stdout: ""
May 14 04:10:22.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=services-9810 execpodr2v8f -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.11 31510'
May 14 04:10:22.766: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.11 31510\nConnection to 10.0.0.11 31510 port [tcp/31510] succeeded!\n"
May 14 04:10:22.766: INFO: stdout: ""
May 14 04:10:22.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=services-9810 execpodr2v8f -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.13 31510'
May 14 04:10:23.473: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.13 31510\nConnection to 10.0.0.13 31510 port [tcp/31510] succeeded!\n"
May 14 04:10:23.473: INFO: stdout: ""
May 14 04:10:23.473: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:10:23.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9810" for this suite.
May 14 04:10:31.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:10:31.914: INFO: namespace services-9810 deletion completed in 8.3526632s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:22.475 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:10:31.916: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5154
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
May 14 04:10:33.039: INFO: Waiting up to 5m0s for pod "pod-0413a0af-160a-4b5c-a0e8-17622ef4761f" in namespace "emptydir-5154" to be "success or failure"
May 14 04:10:33.077: INFO: Pod "pod-0413a0af-160a-4b5c-a0e8-17622ef4761f": Phase="Pending", Reason="", readiness=false. Elapsed: 38.121118ms
May 14 04:10:35.105: INFO: Pod "pod-0413a0af-160a-4b5c-a0e8-17622ef4761f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066044861s
May 14 04:10:37.155: INFO: Pod "pod-0413a0af-160a-4b5c-a0e8-17622ef4761f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.115475902s
May 14 04:10:39.168: INFO: Pod "pod-0413a0af-160a-4b5c-a0e8-17622ef4761f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.128285294s
STEP: Saw pod success
May 14 04:10:39.168: INFO: Pod "pod-0413a0af-160a-4b5c-a0e8-17622ef4761f" satisfied condition "success or failure"
May 14 04:10:39.172: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-0413a0af-160a-4b5c-a0e8-17622ef4761f container test-container: <nil>
STEP: delete the pod
May 14 04:10:39.227: INFO: Waiting for pod pod-0413a0af-160a-4b5c-a0e8-17622ef4761f to disappear
May 14 04:10:39.234: INFO: Pod pod-0413a0af-160a-4b5c-a0e8-17622ef4761f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:10:39.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5154" for this suite.
May 14 04:10:45.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:10:45.718: INFO: namespace emptydir-5154 deletion completed in 6.476376903s

â€¢ [SLOW TEST:13.803 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:10:45.722: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-381
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 14 04:10:45.961: INFO: Waiting up to 5m0s for pod "downwardapi-volume-193f5e79-64e2-4a40-87d2-c6bbc5140497" in namespace "projected-381" to be "success or failure"
May 14 04:10:45.983: INFO: Pod "downwardapi-volume-193f5e79-64e2-4a40-87d2-c6bbc5140497": Phase="Pending", Reason="", readiness=false. Elapsed: 21.644483ms
May 14 04:10:47.991: INFO: Pod "downwardapi-volume-193f5e79-64e2-4a40-87d2-c6bbc5140497": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029475554s
May 14 04:10:49.998: INFO: Pod "downwardapi-volume-193f5e79-64e2-4a40-87d2-c6bbc5140497": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036091235s
May 14 04:10:52.003: INFO: Pod "downwardapi-volume-193f5e79-64e2-4a40-87d2-c6bbc5140497": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041255956s
STEP: Saw pod success
May 14 04:10:52.003: INFO: Pod "downwardapi-volume-193f5e79-64e2-4a40-87d2-c6bbc5140497" satisfied condition "success or failure"
May 14 04:10:52.007: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod downwardapi-volume-193f5e79-64e2-4a40-87d2-c6bbc5140497 container client-container: <nil>
STEP: delete the pod
May 14 04:10:52.053: INFO: Waiting for pod downwardapi-volume-193f5e79-64e2-4a40-87d2-c6bbc5140497 to disappear
May 14 04:10:52.058: INFO: Pod downwardapi-volume-193f5e79-64e2-4a40-87d2-c6bbc5140497 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:10:52.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-381" for this suite.
May 14 04:10:58.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:10:58.418: INFO: namespace projected-381 deletion completed in 6.352476067s

â€¢ [SLOW TEST:12.697 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:10:58.420: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1855
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 14 04:10:58.604: INFO: Waiting up to 5m0s for pod "pod-82e4ca6c-54b9-45ab-87df-6dae2e8b1af5" in namespace "emptydir-1855" to be "success or failure"
May 14 04:10:58.623: INFO: Pod "pod-82e4ca6c-54b9-45ab-87df-6dae2e8b1af5": Phase="Pending", Reason="", readiness=false. Elapsed: 19.240466ms
May 14 04:11:00.647: INFO: Pod "pod-82e4ca6c-54b9-45ab-87df-6dae2e8b1af5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043179805s
May 14 04:11:02.654: INFO: Pod "pod-82e4ca6c-54b9-45ab-87df-6dae2e8b1af5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049432512s
STEP: Saw pod success
May 14 04:11:02.654: INFO: Pod "pod-82e4ca6c-54b9-45ab-87df-6dae2e8b1af5" satisfied condition "success or failure"
May 14 04:11:02.658: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-82e4ca6c-54b9-45ab-87df-6dae2e8b1af5 container test-container: <nil>
STEP: delete the pod
May 14 04:11:02.690: INFO: Waiting for pod pod-82e4ca6c-54b9-45ab-87df-6dae2e8b1af5 to disappear
May 14 04:11:02.699: INFO: Pod pod-82e4ca6c-54b9-45ab-87df-6dae2e8b1af5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:11:02.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1855" for this suite.
May 14 04:11:08.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:11:09.137: INFO: namespace emptydir-1855 deletion completed in 6.431836017s

â€¢ [SLOW TEST:10.719 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:11:09.156: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6054
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-fcc5b2a5-66b8-44e6-a8f5-edf5ffd8cb43
STEP: Creating a pod to test consume secrets
May 14 04:11:09.599: INFO: Waiting up to 5m0s for pod "pod-secrets-9c1f8de6-4b66-4dc5-8ac6-88b9704ae45c" in namespace "secrets-6054" to be "success or failure"
May 14 04:11:09.618: INFO: Pod "pod-secrets-9c1f8de6-4b66-4dc5-8ac6-88b9704ae45c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.928809ms
May 14 04:11:11.623: INFO: Pod "pod-secrets-9c1f8de6-4b66-4dc5-8ac6-88b9704ae45c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023862052s
May 14 04:11:13.629: INFO: Pod "pod-secrets-9c1f8de6-4b66-4dc5-8ac6-88b9704ae45c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030271644s
STEP: Saw pod success
May 14 04:11:13.630: INFO: Pod "pod-secrets-9c1f8de6-4b66-4dc5-8ac6-88b9704ae45c" satisfied condition "success or failure"
May 14 04:11:13.633: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-secrets-9c1f8de6-4b66-4dc5-8ac6-88b9704ae45c container secret-volume-test: <nil>
STEP: delete the pod
May 14 04:11:13.680: INFO: Waiting for pod pod-secrets-9c1f8de6-4b66-4dc5-8ac6-88b9704ae45c to disappear
May 14 04:11:13.687: INFO: Pod pod-secrets-9c1f8de6-4b66-4dc5-8ac6-88b9704ae45c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:11:13.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6054" for this suite.
May 14 04:11:25.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:11:26.426: INFO: namespace secrets-6054 deletion completed in 12.727107743s

â€¢ [SLOW TEST:17.272 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:11:26.430: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6980
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 14 04:11:26.645: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d52aa888-d59b-4be2-af17-2893c3d79877" in namespace "downward-api-6980" to be "success or failure"
May 14 04:11:26.659: INFO: Pod "downwardapi-volume-d52aa888-d59b-4be2-af17-2893c3d79877": Phase="Pending", Reason="", readiness=false. Elapsed: 13.411507ms
May 14 04:11:28.795: INFO: Pod "downwardapi-volume-d52aa888-d59b-4be2-af17-2893c3d79877": Phase="Pending", Reason="", readiness=false. Elapsed: 2.149330658s
May 14 04:11:30.824: INFO: Pod "downwardapi-volume-d52aa888-d59b-4be2-af17-2893c3d79877": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.178757331s
STEP: Saw pod success
May 14 04:11:30.825: INFO: Pod "downwardapi-volume-d52aa888-d59b-4be2-af17-2893c3d79877" satisfied condition "success or failure"
May 14 04:11:30.842: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod downwardapi-volume-d52aa888-d59b-4be2-af17-2893c3d79877 container client-container: <nil>
STEP: delete the pod
May 14 04:11:30.879: INFO: Waiting for pod downwardapi-volume-d52aa888-d59b-4be2-af17-2893c3d79877 to disappear
May 14 04:11:30.889: INFO: Pod downwardapi-volume-d52aa888-d59b-4be2-af17-2893c3d79877 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:11:30.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6980" for this suite.
May 14 04:11:36.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:11:37.477: INFO: namespace downward-api-6980 deletion completed in 6.576362104s

â€¢ [SLOW TEST:11.048 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:11:37.492: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6937
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:11:54.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6937" for this suite.
May 14 04:12:00.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:12:00.624: INFO: namespace resourcequota-6937 deletion completed in 6.445447864s

â€¢ [SLOW TEST:23.134 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:12:00.633: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1573
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 14 04:12:05.408: INFO: Successfully updated pod "pod-update-831a05c6-e788-4b45-81ff-0e24eee6e707"
STEP: verifying the updated pod is in kubernetes
May 14 04:12:05.428: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:12:05.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1573" for this suite.
May 14 04:12:33.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:12:33.822: INFO: namespace pods-1573 deletion completed in 28.37856132s

â€¢ [SLOW TEST:33.189 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:12:33.834: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-3096
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-3096
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3096
STEP: Deleting pre-stop pod
May 14 04:12:47.687: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:12:47.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3096" for this suite.
May 14 04:13:31.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:13:32.363: INFO: namespace prestop-3096 deletion completed in 44.563402127s

â€¢ [SLOW TEST:58.531 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:13:32.372: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4344
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 14 04:13:32.717: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fddd422f-8261-46a0-a245-fcee0aae5d1a" in namespace "projected-4344" to be "success or failure"
May 14 04:13:32.748: INFO: Pod "downwardapi-volume-fddd422f-8261-46a0-a245-fcee0aae5d1a": Phase="Pending", Reason="", readiness=false. Elapsed: 29.64648ms
May 14 04:13:34.767: INFO: Pod "downwardapi-volume-fddd422f-8261-46a0-a245-fcee0aae5d1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048797348s
May 14 04:13:36.786: INFO: Pod "downwardapi-volume-fddd422f-8261-46a0-a245-fcee0aae5d1a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068512689s
May 14 04:13:38.815: INFO: Pod "downwardapi-volume-fddd422f-8261-46a0-a245-fcee0aae5d1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.096951162s
STEP: Saw pod success
May 14 04:13:38.815: INFO: Pod "downwardapi-volume-fddd422f-8261-46a0-a245-fcee0aae5d1a" satisfied condition "success or failure"
May 14 04:13:38.820: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod downwardapi-volume-fddd422f-8261-46a0-a245-fcee0aae5d1a container client-container: <nil>
STEP: delete the pod
May 14 04:13:38.987: INFO: Waiting for pod downwardapi-volume-fddd422f-8261-46a0-a245-fcee0aae5d1a to disappear
May 14 04:13:39.009: INFO: Pod downwardapi-volume-fddd422f-8261-46a0-a245-fcee0aae5d1a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:13:39.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4344" for this suite.
May 14 04:13:45.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:13:45.322: INFO: namespace projected-4344 deletion completed in 6.302811918s

â€¢ [SLOW TEST:12.951 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:13:45.323: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6873
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May 14 04:13:45.577: INFO: Waiting up to 5m0s for pod "downward-api-6b8b8e2e-c9a1-4ab9-9148-9940e6329352" in namespace "downward-api-6873" to be "success or failure"
May 14 04:13:45.591: INFO: Pod "downward-api-6b8b8e2e-c9a1-4ab9-9148-9940e6329352": Phase="Pending", Reason="", readiness=false. Elapsed: 14.182726ms
May 14 04:13:47.824: INFO: Pod "downward-api-6b8b8e2e-c9a1-4ab9-9148-9940e6329352": Phase="Pending", Reason="", readiness=false. Elapsed: 2.246902143s
May 14 04:13:49.830: INFO: Pod "downward-api-6b8b8e2e-c9a1-4ab9-9148-9940e6329352": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.252569647s
STEP: Saw pod success
May 14 04:13:49.830: INFO: Pod "downward-api-6b8b8e2e-c9a1-4ab9-9148-9940e6329352" satisfied condition "success or failure"
May 14 04:13:49.833: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod downward-api-6b8b8e2e-c9a1-4ab9-9148-9940e6329352 container dapi-container: <nil>
STEP: delete the pod
May 14 04:13:49.883: INFO: Waiting for pod downward-api-6b8b8e2e-c9a1-4ab9-9148-9940e6329352 to disappear
May 14 04:13:49.892: INFO: Pod downward-api-6b8b8e2e-c9a1-4ab9-9148-9940e6329352 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:13:49.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6873" for this suite.
May 14 04:13:55.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:13:56.229: INFO: namespace downward-api-6873 deletion completed in 6.326747039s

â€¢ [SLOW TEST:10.907 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:13:56.231: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4472
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
May 14 04:13:56.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 create -f - --namespace=kubectl-4472'
May 14 04:13:58.740: INFO: stderr: ""
May 14 04:13:58.741: INFO: stdout: "pod/pause created\n"
May 14 04:13:58.741: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 14 04:13:58.741: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4472" to be "running and ready"
May 14 04:13:58.754: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.788858ms
May 14 04:14:00.758: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017335019s
May 14 04:14:02.766: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024799086s
May 14 04:14:04.770: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 6.029576071s
May 14 04:14:04.770: INFO: Pod "pause" satisfied condition "running and ready"
May 14 04:14:04.770: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
May 14 04:14:04.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 label pods pause testing-label=testing-label-value --namespace=kubectl-4472'
May 14 04:14:05.132: INFO: stderr: ""
May 14 04:14:05.132: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 14 04:14:05.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pod pause -L testing-label --namespace=kubectl-4472'
May 14 04:14:05.420: INFO: stderr: ""
May 14 04:14:05.420: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          7s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 14 04:14:05.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 label pods pause testing-label- --namespace=kubectl-4472'
May 14 04:14:05.665: INFO: stderr: ""
May 14 04:14:05.665: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 14 04:14:05.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pod pause -L testing-label --namespace=kubectl-4472'
May 14 04:14:06.002: INFO: stderr: ""
May 14 04:14:06.002: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          7s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
May 14 04:14:06.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 delete --grace-period=0 --force -f - --namespace=kubectl-4472'
May 14 04:14:06.419: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 14 04:14:06.419: INFO: stdout: "pod \"pause\" force deleted\n"
May 14 04:14:06.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get rc,svc -l name=pause --no-headers --namespace=kubectl-4472'
May 14 04:14:07.239: INFO: stderr: "No resources found in kubectl-4472 namespace.\n"
May 14 04:14:07.239: INFO: stdout: ""
May 14 04:14:07.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 get pods -l name=pause --namespace=kubectl-4472 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 14 04:14:07.734: INFO: stderr: ""
May 14 04:14:07.734: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:14:07.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4472" for this suite.
May 14 04:14:13.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:14:14.220: INFO: namespace kubectl-4472 deletion completed in 6.476942682s

â€¢ [SLOW TEST:17.989 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:14:14.228: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3055
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
May 14 04:14:18.520: INFO: Pod pod-hostip-f9d3cec6-c09d-489d-93af-e2bcddc9fb38 has hostIP: 10.0.0.13
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:14:18.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3055" for this suite.
May 14 04:14:46.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:14:47.007: INFO: namespace pods-3055 deletion completed in 28.47454234s

â€¢ [SLOW TEST:32.780 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:14:47.023: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6059
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 04:14:48.130: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-7150507d-4d1d-4705-b23a-f59fe9d37501" in namespace "security-context-test-6059" to be "success or failure"
May 14 04:14:48.145: INFO: Pod "alpine-nnp-false-7150507d-4d1d-4705-b23a-f59fe9d37501": Phase="Pending", Reason="", readiness=false. Elapsed: 15.020408ms
May 14 04:14:50.150: INFO: Pod "alpine-nnp-false-7150507d-4d1d-4705-b23a-f59fe9d37501": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020020784s
May 14 04:14:52.174: INFO: Pod "alpine-nnp-false-7150507d-4d1d-4705-b23a-f59fe9d37501": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044119618s
May 14 04:14:52.175: INFO: Pod "alpine-nnp-false-7150507d-4d1d-4705-b23a-f59fe9d37501" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:14:52.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6059" for this suite.
May 14 04:14:58.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:14:58.970: INFO: namespace security-context-test-6059 deletion completed in 6.739814869s

â€¢ [SLOW TEST:11.948 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:14:58.971: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6450
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-8qrn
STEP: Creating a pod to test atomic-volume-subpath
May 14 04:14:59.219: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-8qrn" in namespace "subpath-6450" to be "success or failure"
May 14 04:14:59.227: INFO: Pod "pod-subpath-test-downwardapi-8qrn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.433799ms
May 14 04:15:01.239: INFO: Pod "pod-subpath-test-downwardapi-8qrn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020313397s
May 14 04:15:03.443: INFO: Pod "pod-subpath-test-downwardapi-8qrn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.224075562s
May 14 04:15:05.449: INFO: Pod "pod-subpath-test-downwardapi-8qrn": Phase="Running", Reason="", readiness=true. Elapsed: 6.230618851s
May 14 04:15:07.605: INFO: Pod "pod-subpath-test-downwardapi-8qrn": Phase="Running", Reason="", readiness=true. Elapsed: 8.386041412s
May 14 04:15:09.611: INFO: Pod "pod-subpath-test-downwardapi-8qrn": Phase="Running", Reason="", readiness=true. Elapsed: 10.392536676s
May 14 04:15:11.618: INFO: Pod "pod-subpath-test-downwardapi-8qrn": Phase="Running", Reason="", readiness=true. Elapsed: 12.399173634s
May 14 04:15:13.931: INFO: Pod "pod-subpath-test-downwardapi-8qrn": Phase="Running", Reason="", readiness=true. Elapsed: 14.711876397s
May 14 04:15:16.086: INFO: Pod "pod-subpath-test-downwardapi-8qrn": Phase="Running", Reason="", readiness=true. Elapsed: 16.867028555s
May 14 04:15:18.093: INFO: Pod "pod-subpath-test-downwardapi-8qrn": Phase="Running", Reason="", readiness=true. Elapsed: 18.874102622s
May 14 04:15:20.110: INFO: Pod "pod-subpath-test-downwardapi-8qrn": Phase="Running", Reason="", readiness=true. Elapsed: 20.890648751s
May 14 04:15:22.117: INFO: Pod "pod-subpath-test-downwardapi-8qrn": Phase="Running", Reason="", readiness=true. Elapsed: 22.897937142s
May 14 04:15:24.125: INFO: Pod "pod-subpath-test-downwardapi-8qrn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.905953762s
STEP: Saw pod success
May 14 04:15:24.125: INFO: Pod "pod-subpath-test-downwardapi-8qrn" satisfied condition "success or failure"
May 14 04:15:24.130: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-subpath-test-downwardapi-8qrn container test-container-subpath-downwardapi-8qrn: <nil>
STEP: delete the pod
May 14 04:15:24.860: INFO: Waiting for pod pod-subpath-test-downwardapi-8qrn to disappear
May 14 04:15:24.892: INFO: Pod pod-subpath-test-downwardapi-8qrn no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-8qrn
May 14 04:15:24.893: INFO: Deleting pod "pod-subpath-test-downwardapi-8qrn" in namespace "subpath-6450"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:15:24.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6450" for this suite.
May 14 04:15:30.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:15:31.364: INFO: namespace subpath-6450 deletion completed in 6.426311582s

â€¢ [SLOW TEST:32.394 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:15:31.371: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8728
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
May 14 04:15:31.612: INFO: Waiting up to 5m0s for pod "pod-19e5ae99-d15f-4d72-9a4a-0068bf302d09" in namespace "emptydir-8728" to be "success or failure"
May 14 04:15:31.808: INFO: Pod "pod-19e5ae99-d15f-4d72-9a4a-0068bf302d09": Phase="Pending", Reason="", readiness=false. Elapsed: 195.300634ms
May 14 04:15:34.233: INFO: Pod "pod-19e5ae99-d15f-4d72-9a4a-0068bf302d09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.620731583s
May 14 04:15:36.241: INFO: Pod "pod-19e5ae99-d15f-4d72-9a4a-0068bf302d09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.628170774s
May 14 04:15:38.844: INFO: Pod "pod-19e5ae99-d15f-4d72-9a4a-0068bf302d09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.231167658s
STEP: Saw pod success
May 14 04:15:38.844: INFO: Pod "pod-19e5ae99-d15f-4d72-9a4a-0068bf302d09" satisfied condition "success or failure"
May 14 04:15:38.854: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-19e5ae99-d15f-4d72-9a4a-0068bf302d09 container test-container: <nil>
STEP: delete the pod
May 14 04:15:38.899: INFO: Waiting for pod pod-19e5ae99-d15f-4d72-9a4a-0068bf302d09 to disappear
May 14 04:15:38.908: INFO: Pod pod-19e5ae99-d15f-4d72-9a4a-0068bf302d09 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:15:38.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8728" for this suite.
May 14 04:15:51.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:15:52.838: INFO: namespace emptydir-8728 deletion completed in 13.921052756s

â€¢ [SLOW TEST:21.468 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:15:52.839: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5009
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
May 14 04:15:53.473: INFO: namespace kubectl-5009
May 14 04:15:53.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 create -f - --namespace=kubectl-5009'
May 14 04:16:01.294: INFO: stderr: ""
May 14 04:16:01.294: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 14 04:16:02.473: INFO: Selector matched 1 pods for map[app:redis]
May 14 04:16:02.473: INFO: Found 0 / 1
May 14 04:16:03.300: INFO: Selector matched 1 pods for map[app:redis]
May 14 04:16:03.300: INFO: Found 0 / 1
May 14 04:16:04.827: INFO: Selector matched 1 pods for map[app:redis]
May 14 04:16:04.827: INFO: Found 0 / 1
May 14 04:16:05.307: INFO: Selector matched 1 pods for map[app:redis]
May 14 04:16:05.307: INFO: Found 0 / 1
May 14 04:16:06.300: INFO: Selector matched 1 pods for map[app:redis]
May 14 04:16:06.301: INFO: Found 1 / 1
May 14 04:16:06.301: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 14 04:16:06.304: INFO: Selector matched 1 pods for map[app:redis]
May 14 04:16:06.305: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 14 04:16:06.305: INFO: wait on redis-master startup in kubectl-5009 
May 14 04:16:06.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 logs redis-master-j94pq redis-master --namespace=kubectl-5009'
May 14 04:16:06.639: INFO: stderr: ""
May 14 04:16:06.639: INFO: stdout: "1:C 14 May 2020 04:16:05.385 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 14 May 2020 04:16:05.385 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 14 May 2020 04:16:05.385 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 14 May 2020 04:16:05.391 # You requested maxclients of 10000 requiring at least 10032 max file descriptors.\n1:M 14 May 2020 04:16:05.391 # Server can't set maximum open files to 10032 because of OS error: Operation not permitted.\n1:M 14 May 2020 04:16:05.391 # Current maximum open files is 1024. maxclients has been reduced to 992 to compensate for low ulimit. If you need higher maxclients increase 'ulimit -n'.\n1:M 14 May 2020 04:16:05.394 * Running mode=standalone, port=6379.\n1:M 14 May 2020 04:16:05.394 # Server initialized\n1:M 14 May 2020 04:16:05.394 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 May 2020 04:16:05.394 * Ready to accept connections\n"
STEP: exposing RC
May 14 04:16:06.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-5009'
May 14 04:16:07.109: INFO: stderr: ""
May 14 04:16:07.109: INFO: stdout: "service/rm2 exposed\n"
May 14 04:16:07.146: INFO: Service rm2 in namespace kubectl-5009 found.
STEP: exposing service
May 14 04:16:09.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-5009'
May 14 04:16:09.573: INFO: stderr: ""
May 14 04:16:09.573: INFO: stdout: "service/rm3 exposed\n"
May 14 04:16:09.580: INFO: Service rm3 in namespace kubectl-5009 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:16:11.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5009" for this suite.
May 14 04:16:40.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:16:41.874: INFO: namespace kubectl-5009 deletion completed in 30.267278042s

â€¢ [SLOW TEST:49.035 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:16:41.895: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-255
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-d14a043d-3df7-42ed-adba-2f252a3bdb7c
STEP: Creating a pod to test consume configMaps
May 14 04:16:42.163: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c7c357fb-88d5-442c-a278-9f10cb3b36c7" in namespace "projected-255" to be "success or failure"
May 14 04:16:42.194: INFO: Pod "pod-projected-configmaps-c7c357fb-88d5-442c-a278-9f10cb3b36c7": Phase="Pending", Reason="", readiness=false. Elapsed: 30.576585ms
May 14 04:16:44.269: INFO: Pod "pod-projected-configmaps-c7c357fb-88d5-442c-a278-9f10cb3b36c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.105341784s
May 14 04:16:46.289: INFO: Pod "pod-projected-configmaps-c7c357fb-88d5-442c-a278-9f10cb3b36c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.125262599s
May 14 04:16:48.295: INFO: Pod "pod-projected-configmaps-c7c357fb-88d5-442c-a278-9f10cb3b36c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.131270307s
STEP: Saw pod success
May 14 04:16:48.295: INFO: Pod "pod-projected-configmaps-c7c357fb-88d5-442c-a278-9f10cb3b36c7" satisfied condition "success or failure"
May 14 04:16:48.299: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-projected-configmaps-c7c357fb-88d5-442c-a278-9f10cb3b36c7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 14 04:16:48.352: INFO: Waiting for pod pod-projected-configmaps-c7c357fb-88d5-442c-a278-9f10cb3b36c7 to disappear
May 14 04:16:48.359: INFO: Pod pod-projected-configmaps-c7c357fb-88d5-442c-a278-9f10cb3b36c7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:16:48.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-255" for this suite.
May 14 04:16:58.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:16:59.749: INFO: namespace projected-255 deletion completed in 11.380217823s

â€¢ [SLOW TEST:17.855 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:16:59.753: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1972
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-9914b76c-9b66-4120-b065-2d91168a1154
STEP: Creating a pod to test consume secrets
May 14 04:16:59.999: INFO: Waiting up to 5m0s for pod "pod-secrets-00a49126-c70a-41e1-8dc2-45c3962e2ece" in namespace "secrets-1972" to be "success or failure"
May 14 04:17:00.027: INFO: Pod "pod-secrets-00a49126-c70a-41e1-8dc2-45c3962e2ece": Phase="Pending", Reason="", readiness=false. Elapsed: 27.59475ms
May 14 04:17:02.036: INFO: Pod "pod-secrets-00a49126-c70a-41e1-8dc2-45c3962e2ece": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036678251s
May 14 04:17:04.061: INFO: Pod "pod-secrets-00a49126-c70a-41e1-8dc2-45c3962e2ece": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061342385s
May 14 04:17:06.079: INFO: Pod "pod-secrets-00a49126-c70a-41e1-8dc2-45c3962e2ece": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.079275671s
STEP: Saw pod success
May 14 04:17:06.079: INFO: Pod "pod-secrets-00a49126-c70a-41e1-8dc2-45c3962e2ece" satisfied condition "success or failure"
May 14 04:17:06.084: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-secrets-00a49126-c70a-41e1-8dc2-45c3962e2ece container secret-volume-test: <nil>
STEP: delete the pod
May 14 04:17:06.116: INFO: Waiting for pod pod-secrets-00a49126-c70a-41e1-8dc2-45c3962e2ece to disappear
May 14 04:17:06.137: INFO: Pod pod-secrets-00a49126-c70a-41e1-8dc2-45c3962e2ece no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:17:06.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1972" for this suite.
May 14 04:17:12.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:17:12.531: INFO: namespace secrets-1972 deletion completed in 6.385754439s

â€¢ [SLOW TEST:12.778 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:17:12.534: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1168
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
May 14 04:17:12.799: INFO: Waiting up to 5m0s for pod "pod-eb0a0e57-648b-4f09-94aa-bca00c5c170e" in namespace "emptydir-1168" to be "success or failure"
May 14 04:17:12.822: INFO: Pod "pod-eb0a0e57-648b-4f09-94aa-bca00c5c170e": Phase="Pending", Reason="", readiness=false. Elapsed: 22.727983ms
May 14 04:17:14.842: INFO: Pod "pod-eb0a0e57-648b-4f09-94aa-bca00c5c170e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037039528s
May 14 04:17:16.847: INFO: Pod "pod-eb0a0e57-648b-4f09-94aa-bca00c5c170e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047923016s
STEP: Saw pod success
May 14 04:17:16.847: INFO: Pod "pod-eb0a0e57-648b-4f09-94aa-bca00c5c170e" satisfied condition "success or failure"
May 14 04:17:16.851: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-eb0a0e57-648b-4f09-94aa-bca00c5c170e container test-container: <nil>
STEP: delete the pod
May 14 04:17:16.886: INFO: Waiting for pod pod-eb0a0e57-648b-4f09-94aa-bca00c5c170e to disappear
May 14 04:17:16.899: INFO: Pod pod-eb0a0e57-648b-4f09-94aa-bca00c5c170e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:17:16.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1168" for this suite.
May 14 04:17:22.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:17:23.242: INFO: namespace emptydir-1168 deletion completed in 6.334436478s

â€¢ [SLOW TEST:10.708 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:17:23.247: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4777
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 14 04:17:25.099: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 14 04:17:27.123: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026645, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026645, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026645, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026645, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 04:17:29.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026645, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026645, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026645, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026645, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 14 04:17:32.160: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:17:32.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4777" for this suite.
May 14 04:17:38.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:17:39.274: INFO: namespace webhook-4777 deletion completed in 6.758246573s
STEP: Destroying namespace "webhook-4777-markers" for this suite.
May 14 04:17:45.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:17:46.204: INFO: namespace webhook-4777-markers deletion completed in 6.928911861s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:22.986 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:17:46.241: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9937
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9937
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9937
STEP: creating replication controller externalsvc in namespace services-9937
I0514 04:17:46.509296      20 runners.go:184] Created replication controller with name: externalsvc, namespace: services-9937, replica count: 2
I0514 04:17:49.565737      20 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0514 04:17:52.566550      20 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0514 04:17:55.567052      20 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
May 14 04:17:55.624: INFO: Creating new exec pod
May 14 04:18:03.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-817715921 exec --namespace=services-9937 execpodgzqrz -- /bin/sh -x -c nslookup nodeport-service'
May 14 04:18:04.354: INFO: stderr: "+ nslookup nodeport-service\n"
May 14 04:18:04.354: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nnodeport-service.services-9937.svc.cluster.local\tcanonical name = externalsvc.services-9937.svc.cluster.local.\nName:\texternalsvc.services-9937.svc.cluster.local\nAddress: 10.254.18.96\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9937, will wait for the garbage collector to delete the pods
May 14 04:18:04.438: INFO: Deleting ReplicationController externalsvc took: 16.58408ms
May 14 04:18:05.463: INFO: Terminating ReplicationController externalsvc pods took: 1.024961597s
May 14 04:18:22.908: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:18:22.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9937" for this suite.
May 14 04:18:30.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:18:31.454: INFO: namespace services-9937 deletion completed in 8.508986627s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:45.214 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:18:31.460: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-2325
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
May 14 04:18:41.221: INFO: Successfully updated pod "adopt-release-hl8tc"
STEP: Checking that the Job readopts the Pod
May 14 04:18:41.222: INFO: Waiting up to 15m0s for pod "adopt-release-hl8tc" in namespace "job-2325" to be "adopted"
May 14 04:18:41.229: INFO: Pod "adopt-release-hl8tc": Phase="Running", Reason="", readiness=true. Elapsed: 6.930559ms
May 14 04:18:43.586: INFO: Pod "adopt-release-hl8tc": Phase="Running", Reason="", readiness=true. Elapsed: 2.363943949s
May 14 04:18:43.586: INFO: Pod "adopt-release-hl8tc" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
May 14 04:18:44.097: INFO: Successfully updated pod "adopt-release-hl8tc"
STEP: Checking that the Job releases the Pod
May 14 04:18:44.098: INFO: Waiting up to 15m0s for pod "adopt-release-hl8tc" in namespace "job-2325" to be "released"
May 14 04:18:44.107: INFO: Pod "adopt-release-hl8tc": Phase="Running", Reason="", readiness=true. Elapsed: 8.825956ms
May 14 04:18:46.113: INFO: Pod "adopt-release-hl8tc": Phase="Running", Reason="", readiness=true. Elapsed: 2.015101362s
May 14 04:18:46.113: INFO: Pod "adopt-release-hl8tc" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:18:46.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2325" for this suite.
May 14 04:19:34.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:19:36.005: INFO: namespace job-2325 deletion completed in 49.886916027s

â€¢ [SLOW TEST:64.546 seconds]
[sig-apps] Job
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:19:36.009: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3558
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0514 04:20:17.949864      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 14 04:20:17.950: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:20:17.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3558" for this suite.
May 14 04:20:24.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:20:24.557: INFO: namespace gc-3558 deletion completed in 6.599267374s

â€¢ [SLOW TEST:48.549 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:20:24.561: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6860
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 04:20:25.018: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 14 04:20:30.024: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 14 04:20:34.058: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 14 04:20:36.070: INFO: Creating deployment "test-rollover-deployment"
May 14 04:20:36.098: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 14 04:20:38.119: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 14 04:20:38.135: INFO: Ensure that both replica sets have 1 created replica
May 14 04:20:38.151: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 14 04:20:38.166: INFO: Updating deployment test-rollover-deployment
May 14 04:20:38.166: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 14 04:20:40.241: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 14 04:20:40.255: INFO: Make sure deployment "test-rollover-deployment" is complete
May 14 04:20:40.271: INFO: all replica sets need to contain the pod-template-hash label
May 14 04:20:40.272: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026836, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026836, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026838, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026836, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 04:20:42.288: INFO: all replica sets need to contain the pod-template-hash label
May 14 04:20:42.289: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026836, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026836, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026838, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026836, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 04:20:44.309: INFO: all replica sets need to contain the pod-template-hash label
May 14 04:20:44.309: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026836, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026836, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026844, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026836, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 04:20:46.284: INFO: all replica sets need to contain the pod-template-hash label
May 14 04:20:46.285: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026836, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026836, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026844, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026836, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 04:20:48.282: INFO: all replica sets need to contain the pod-template-hash label
May 14 04:20:48.282: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026836, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026836, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026844, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026836, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 04:20:50.289: INFO: all replica sets need to contain the pod-template-hash label
May 14 04:20:50.289: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026836, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026836, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026844, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026836, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 04:20:52.281: INFO: all replica sets need to contain the pod-template-hash label
May 14 04:20:52.282: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026836, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026836, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026844, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725026836, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 04:20:54.301: INFO: 
May 14 04:20:54.301: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May 14 04:20:54.321: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6860 /apis/apps/v1/namespaces/deployment-6860/deployments/test-rollover-deployment 50c8aeb8-3a5a-4e36-91aa-d8c9a5e56eca 218696 2 2020-05-14 04:20:36 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001561878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-05-14 04:20:36 +0000 UTC,LastTransitionTime:2020-05-14 04:20:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-05-14 04:20:54 +0000 UTC,LastTransitionTime:2020-05-14 04:20:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 14 04:20:54.326: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-6860 /apis/apps/v1/namespaces/deployment-6860/replicasets/test-rollover-deployment-7d7dc6548c 03142b39-99b1-415f-b7c0-4f0ccccb887c 218686 2 2020-05-14 04:20:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 50c8aeb8-3a5a-4e36-91aa-d8c9a5e56eca 0xc001efead7 0xc001efead8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001efeb38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 14 04:20:54.326: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 14 04:20:54.326: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6860 /apis/apps/v1/namespaces/deployment-6860/replicasets/test-rollover-controller f771ce2a-3f7c-4f5d-8380-b69de3cb2537 218695 2 2020-05-14 04:20:25 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 50c8aeb8-3a5a-4e36-91aa-d8c9a5e56eca 0xc001efea07 0xc001efea08}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001efea68 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 14 04:20:54.327: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-6860 /apis/apps/v1/namespaces/deployment-6860/replicasets/test-rollover-deployment-f6c94f66c 345e0299-c604-4f79-8125-4432f1ccb274 218623 2 2020-05-14 04:20:36 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 50c8aeb8-3a5a-4e36-91aa-d8c9a5e56eca 0xc001efeba0 0xc001efeba1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001efec18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 14 04:20:54.333: INFO: Pod "test-rollover-deployment-7d7dc6548c-4kd5l" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-4kd5l test-rollover-deployment-7d7dc6548c- deployment-6860 /api/v1/namespaces/deployment-6860/pods/test-rollover-deployment-7d7dc6548c-4kd5l 6bdc0724-dd57-4186-a856-84808fd93a98 218654 0 2020-05-14 04:20:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[cni.projectcalico.org/podIP:10.100.136.87/32 cni.projectcalico.org/podIPs:10.100.136.87/32] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c 03142b39-99b1-415f-b7c0-4f0ccccb887c 0xc001561f87 0xc001561f88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zf845,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zf845,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zf845,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-fcos-flwang-pyqjxt4oox23-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 04:20:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 04:20:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 04:20:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-14 04:20:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.13,PodIP:10.100.136.87,StartTime:2020-05-14 04:20:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-14 04:20:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://21cf81c5f16fb5752269b6857816b290dfdfd5dfe9eede309f0237b04761d15b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.136.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:20:54.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6860" for this suite.
May 14 04:21:00.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:21:00.718: INFO: namespace deployment-6860 deletion completed in 6.37505081s

â€¢ [SLOW TEST:36.157 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:21:00.720: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9314
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 14 04:21:00.983: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:21:01.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9314" for this suite.
May 14 04:21:07.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:21:07.971: INFO: namespace custom-resource-definition-9314 deletion completed in 6.396177506s

â€¢ [SLOW TEST:7.252 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:21:07.972: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8520
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-tc7g
STEP: Creating a pod to test atomic-volume-subpath
May 14 04:21:08.283: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tc7g" in namespace "subpath-8520" to be "success or failure"
May 14 04:21:08.292: INFO: Pod "pod-subpath-test-configmap-tc7g": Phase="Pending", Reason="", readiness=false. Elapsed: 8.933208ms
May 14 04:21:11.778: INFO: Pod "pod-subpath-test-configmap-tc7g": Phase="Pending", Reason="", readiness=false. Elapsed: 3.495615878s
May 14 04:21:13.786: INFO: Pod "pod-subpath-test-configmap-tc7g": Phase="Running", Reason="", readiness=true. Elapsed: 5.503435108s
May 14 04:21:15.974: INFO: Pod "pod-subpath-test-configmap-tc7g": Phase="Running", Reason="", readiness=true. Elapsed: 7.690827231s
May 14 04:21:17.981: INFO: Pod "pod-subpath-test-configmap-tc7g": Phase="Running", Reason="", readiness=true. Elapsed: 9.69800854s
May 14 04:21:19.988: INFO: Pod "pod-subpath-test-configmap-tc7g": Phase="Running", Reason="", readiness=true. Elapsed: 11.705191212s
May 14 04:21:22.010: INFO: Pod "pod-subpath-test-configmap-tc7g": Phase="Running", Reason="", readiness=true. Elapsed: 13.72709763s
May 14 04:21:24.033: INFO: Pod "pod-subpath-test-configmap-tc7g": Phase="Running", Reason="", readiness=true. Elapsed: 15.750347473s
May 14 04:21:26.038: INFO: Pod "pod-subpath-test-configmap-tc7g": Phase="Running", Reason="", readiness=true. Elapsed: 17.75535877s
May 14 04:21:28.045: INFO: Pod "pod-subpath-test-configmap-tc7g": Phase="Running", Reason="", readiness=true. Elapsed: 19.761716924s
May 14 04:21:30.059: INFO: Pod "pod-subpath-test-configmap-tc7g": Phase="Running", Reason="", readiness=true. Elapsed: 21.776433028s
May 14 04:21:32.066: INFO: Pod "pod-subpath-test-configmap-tc7g": Phase="Running", Reason="", readiness=true. Elapsed: 23.783348207s
May 14 04:21:34.308: INFO: Pod "pod-subpath-test-configmap-tc7g": Phase="Running", Reason="", readiness=true. Elapsed: 26.024923545s
May 14 04:21:36.315: INFO: Pod "pod-subpath-test-configmap-tc7g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.031710394s
STEP: Saw pod success
May 14 04:21:36.315: INFO: Pod "pod-subpath-test-configmap-tc7g" satisfied condition "success or failure"
May 14 04:21:36.320: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-subpath-test-configmap-tc7g container test-container-subpath-configmap-tc7g: <nil>
STEP: delete the pod
May 14 04:21:36.472: INFO: Waiting for pod pod-subpath-test-configmap-tc7g to disappear
May 14 04:21:36.477: INFO: Pod pod-subpath-test-configmap-tc7g no longer exists
STEP: Deleting pod pod-subpath-test-configmap-tc7g
May 14 04:21:36.478: INFO: Deleting pod "pod-subpath-test-configmap-tc7g" in namespace "subpath-8520"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:21:36.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8520" for this suite.
May 14 04:21:42.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:21:42.941: INFO: namespace subpath-8520 deletion completed in 6.44626332s

â€¢ [SLOW TEST:34.969 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:21:42.946: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4295
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
May 14 04:21:43.197: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:21:49.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4295" for this suite.
May 14 04:22:03.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:22:03.925: INFO: namespace init-container-4295 deletion completed in 14.420541013s

â€¢ [SLOW TEST:20.980 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:22:03.930: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5678
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 14 04:22:12.246: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 14 04:22:12.253: INFO: Pod pod-with-poststart-exec-hook still exists
May 14 04:22:14.279: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 14 04:22:14.308: INFO: Pod pod-with-poststart-exec-hook still exists
May 14 04:22:16.254: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 14 04:22:16.262: INFO: Pod pod-with-poststart-exec-hook still exists
May 14 04:22:18.253: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 14 04:22:18.259: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:22:18.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5678" for this suite.
May 14 04:22:46.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:22:46.718: INFO: namespace container-lifecycle-hook-5678 deletion completed in 28.445752407s

â€¢ [SLOW TEST:42.789 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:22:46.729: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-638
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
May 14 04:22:47.262: INFO: Waiting up to 5m0s for pod "pod-0d8fefb1-fb25-4944-b7cf-f5a557c78fef" in namespace "emptydir-638" to be "success or failure"
May 14 04:22:47.369: INFO: Pod "pod-0d8fefb1-fb25-4944-b7cf-f5a557c78fef": Phase="Pending", Reason="", readiness=false. Elapsed: 107.017623ms
May 14 04:22:49.612: INFO: Pod "pod-0d8fefb1-fb25-4944-b7cf-f5a557c78fef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.350792919s
May 14 04:22:51.619: INFO: Pod "pod-0d8fefb1-fb25-4944-b7cf-f5a557c78fef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.357733548s
STEP: Saw pod success
May 14 04:22:51.619: INFO: Pod "pod-0d8fefb1-fb25-4944-b7cf-f5a557c78fef" satisfied condition "success or failure"
May 14 04:22:51.634: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod pod-0d8fefb1-fb25-4944-b7cf-f5a557c78fef container test-container: <nil>
STEP: delete the pod
May 14 04:22:51.719: INFO: Waiting for pod pod-0d8fefb1-fb25-4944-b7cf-f5a557c78fef to disappear
May 14 04:22:51.728: INFO: Pod pod-0d8fefb1-fb25-4944-b7cf-f5a557c78fef no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:22:51.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-638" for this suite.
May 14 04:22:57.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:22:58.093: INFO: namespace emptydir-638 deletion completed in 6.358941294s

â€¢ [SLOW TEST:11.365 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 14 04:22:58.110: INFO: >>> kubeConfig: /tmp/kubeconfig-817715921
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5988
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
May 14 04:22:58.343: INFO: Waiting up to 5m0s for pod "client-containers-eeecfc9f-cb3c-4a64-bd4f-7b3f0696cd9a" in namespace "containers-5988" to be "success or failure"
May 14 04:22:59.134: INFO: Pod "client-containers-eeecfc9f-cb3c-4a64-bd4f-7b3f0696cd9a": Phase="Pending", Reason="", readiness=false. Elapsed: 790.438937ms
May 14 04:23:01.141: INFO: Pod "client-containers-eeecfc9f-cb3c-4a64-bd4f-7b3f0696cd9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.798145862s
May 14 04:23:03.147: INFO: Pod "client-containers-eeecfc9f-cb3c-4a64-bd4f-7b3f0696cd9a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.803239094s
May 14 04:23:05.152: INFO: Pod "client-containers-eeecfc9f-cb3c-4a64-bd4f-7b3f0696cd9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.808824609s
STEP: Saw pod success
May 14 04:23:05.152: INFO: Pod "client-containers-eeecfc9f-cb3c-4a64-bd4f-7b3f0696cd9a" satisfied condition "success or failure"
May 14 04:23:05.161: INFO: Trying to get logs from node k8s-fcos-flwang-pyqjxt4oox23-node-1 pod client-containers-eeecfc9f-cb3c-4a64-bd4f-7b3f0696cd9a container test-container: <nil>
STEP: delete the pod
May 14 04:23:05.200: INFO: Waiting for pod client-containers-eeecfc9f-cb3c-4a64-bd4f-7b3f0696cd9a to disappear
May 14 04:23:05.217: INFO: Pod client-containers-eeecfc9f-cb3c-4a64-bd4f-7b3f0696cd9a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 14 04:23:05.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5988" for this suite.
May 14 04:23:11.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 04:23:11.633: INFO: namespace containers-5988 deletion completed in 6.404817199s

â€¢ [SLOW TEST:13.523 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
May 14 04:23:11.633: INFO: Running AfterSuite actions on all nodes
May 14 04:23:11.634: INFO: Running AfterSuite actions on node 1
May 14 04:23:11.634: INFO: Skipping dumping logs from cluster

Ran 274 of 4732 Specs in 9544.654 seconds
SUCCESS! -- 274 Passed | 0 Failed | 0 Pending | 4458 Skipped
PASS

Ginkgo ran 1 suite in 2h39m8.570025362s
Test Suite Passed
