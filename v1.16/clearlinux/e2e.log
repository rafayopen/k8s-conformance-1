I1213 19:13:25.748070      24 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-728461388
I1213 19:13:25.748792      24 e2e.go:92] Starting e2e run "d8a556ce-920b-4c76-b590-1b04889e3a79" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1576264403 - Will randomize all specs
Will run 276 of 4731 specs

Dec 13 19:13:25.768: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 19:13:25.771: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec 13 19:13:25.798: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec 13 19:13:25.876: INFO: 22 / 22 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 13 19:13:25.876: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Dec 13 19:13:25.876: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 13 19:13:25.887: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Dec 13 19:13:25.887: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'fluentd-es-v2.5.2' (0 seconds elapsed)
Dec 13 19:13:25.887: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kata-deploy' (0 seconds elapsed)
Dec 13 19:13:25.887: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec 13 19:13:25.887: INFO: e2e test version: v1.16.4
Dec 13 19:13:25.895: INFO: kube-apiserver version: v1.16.4
Dec 13 19:13:25.895: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 19:13:25.901: INFO: Cluster IP family: ipv4
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:13:25.901: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename webhook
Dec 13 19:13:26.000: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Dec 13 19:13:26.031: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7180
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 13 19:13:27.301: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 13 19:13:29.329: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 19:13:31.335: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 19:13:33.332: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 19:13:35.332: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 19:13:37.333: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 19:13:39.656: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 19:13:41.332: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 19:13:43.338: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 19:13:45.332: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 19:13:47.331: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 19:13:49.332: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 19:13:51.334: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 19:13:53.345: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 19:13:55.333: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 19:13:57.342: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 19:13:59.332: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 19:14:01.332: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 19:14:03.332: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711861207, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 13 19:14:06.345: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 19:14:06.348: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3493-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:14:07.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7180" for this suite.
Dec 13 19:14:13.556: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:14:13.638: INFO: namespace webhook-7180 deletion completed in 6.09225765s
STEP: Destroying namespace "webhook-7180-markers" for this suite.
Dec 13 19:14:19.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:14:19.754: INFO: namespace webhook-7180-markers deletion completed in 6.115781953s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:53.868 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:14:19.770: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3487
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec 13 19:14:19.940: INFO: Pod name pod-release: Found 0 pods out of 1
Dec 13 19:14:24.943: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:14:24.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3487" for this suite.
Dec 13 19:14:31.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:14:31.156: INFO: namespace replication-controller-3487 deletion completed in 6.166033191s

• [SLOW TEST:11.386 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:14:31.156: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9312
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec 13 19:14:31.314: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:14:44.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9312" for this suite.
Dec 13 19:14:50.391: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:14:50.469: INFO: namespace init-container-9312 deletion completed in 6.096238185s

• [SLOW TEST:19.312 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:14:50.470: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6621
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 13 19:14:51.638: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:14:51.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6621" for this suite.
Dec 13 19:14:57.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:14:57.755: INFO: namespace container-runtime-6621 deletion completed in 6.091299733s

• [SLOW TEST:7.285 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:14:57.755: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2571
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-57a325ec-e3b7-4727-a515-c3ec67d7e67f
STEP: Creating a pod to test consume configMaps
Dec 13 19:14:57.908: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c89be1be-872c-4bd2-a1b2-18e76c7ff817" in namespace "projected-2571" to be "success or failure"
Dec 13 19:14:57.928: INFO: Pod "pod-projected-configmaps-c89be1be-872c-4bd2-a1b2-18e76c7ff817": Phase="Pending", Reason="", readiness=false. Elapsed: 20.13693ms
Dec 13 19:14:59.930: INFO: Pod "pod-projected-configmaps-c89be1be-872c-4bd2-a1b2-18e76c7ff817": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022157965s
Dec 13 19:15:01.932: INFO: Pod "pod-projected-configmaps-c89be1be-872c-4bd2-a1b2-18e76c7ff817": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024372699s
STEP: Saw pod success
Dec 13 19:15:01.932: INFO: Pod "pod-projected-configmaps-c89be1be-872c-4bd2-a1b2-18e76c7ff817" satisfied condition "success or failure"
Dec 13 19:15:01.934: INFO: Trying to get logs from node master-0 pod pod-projected-configmaps-c89be1be-872c-4bd2-a1b2-18e76c7ff817 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 19:15:01.957: INFO: Waiting for pod pod-projected-configmaps-c89be1be-872c-4bd2-a1b2-18e76c7ff817 to disappear
Dec 13 19:15:01.962: INFO: Pod pod-projected-configmaps-c89be1be-872c-4bd2-a1b2-18e76c7ff817 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:15:01.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2571" for this suite.
Dec 13 19:15:07.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:15:08.053: INFO: namespace projected-2571 deletion completed in 6.088399044s

• [SLOW TEST:10.298 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:15:08.054: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3570
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-7cf38df5-9218-4fd9-b8d0-9623c3db3ecb
STEP: Creating a pod to test consume configMaps
Dec 13 19:15:08.206: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fb4297d0-305f-4c6f-8a7a-3cb613ea5e3f" in namespace "projected-3570" to be "success or failure"
Dec 13 19:15:08.215: INFO: Pod "pod-projected-configmaps-fb4297d0-305f-4c6f-8a7a-3cb613ea5e3f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.485922ms
Dec 13 19:15:10.217: INFO: Pod "pod-projected-configmaps-fb4297d0-305f-4c6f-8a7a-3cb613ea5e3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011469504s
STEP: Saw pod success
Dec 13 19:15:10.217: INFO: Pod "pod-projected-configmaps-fb4297d0-305f-4c6f-8a7a-3cb613ea5e3f" satisfied condition "success or failure"
Dec 13 19:15:10.219: INFO: Trying to get logs from node master-0 pod pod-projected-configmaps-fb4297d0-305f-4c6f-8a7a-3cb613ea5e3f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 19:15:10.242: INFO: Waiting for pod pod-projected-configmaps-fb4297d0-305f-4c6f-8a7a-3cb613ea5e3f to disappear
Dec 13 19:15:10.251: INFO: Pod pod-projected-configmaps-fb4297d0-305f-4c6f-8a7a-3cb613ea5e3f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:15:10.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3570" for this suite.
Dec 13 19:15:16.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:15:16.371: INFO: namespace projected-3570 deletion completed in 6.117901372s

• [SLOW TEST:8.318 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:15:16.372: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7090
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec 13 19:15:16.541: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7090 /api/v1/namespaces/watch-7090/configmaps/e2e-watch-test-configmap-a bcc4c31b-b87b-4d6f-afe5-cc65662dd5bb 6416 0 2019-12-13 19:15:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 13 19:15:16.541: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7090 /api/v1/namespaces/watch-7090/configmaps/e2e-watch-test-configmap-a bcc4c31b-b87b-4d6f-afe5-cc65662dd5bb 6416 0 2019-12-13 19:15:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec 13 19:15:26.547: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7090 /api/v1/namespaces/watch-7090/configmaps/e2e-watch-test-configmap-a bcc4c31b-b87b-4d6f-afe5-cc65662dd5bb 6453 0 2019-12-13 19:15:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 13 19:15:26.547: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7090 /api/v1/namespaces/watch-7090/configmaps/e2e-watch-test-configmap-a bcc4c31b-b87b-4d6f-afe5-cc65662dd5bb 6453 0 2019-12-13 19:15:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec 13 19:15:36.552: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7090 /api/v1/namespaces/watch-7090/configmaps/e2e-watch-test-configmap-a bcc4c31b-b87b-4d6f-afe5-cc65662dd5bb 6489 0 2019-12-13 19:15:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 13 19:15:36.552: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7090 /api/v1/namespaces/watch-7090/configmaps/e2e-watch-test-configmap-a bcc4c31b-b87b-4d6f-afe5-cc65662dd5bb 6489 0 2019-12-13 19:15:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec 13 19:15:46.556: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7090 /api/v1/namespaces/watch-7090/configmaps/e2e-watch-test-configmap-a bcc4c31b-b87b-4d6f-afe5-cc65662dd5bb 6526 0 2019-12-13 19:15:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 13 19:15:46.556: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7090 /api/v1/namespaces/watch-7090/configmaps/e2e-watch-test-configmap-a bcc4c31b-b87b-4d6f-afe5-cc65662dd5bb 6526 0 2019-12-13 19:15:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec 13 19:15:56.563: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7090 /api/v1/namespaces/watch-7090/configmaps/e2e-watch-test-configmap-b 38267f28-70a2-45e2-9d0b-33c4f87b431b 6562 0 2019-12-13 19:15:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 13 19:15:56.563: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7090 /api/v1/namespaces/watch-7090/configmaps/e2e-watch-test-configmap-b 38267f28-70a2-45e2-9d0b-33c4f87b431b 6562 0 2019-12-13 19:15:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec 13 19:16:06.567: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7090 /api/v1/namespaces/watch-7090/configmaps/e2e-watch-test-configmap-b 38267f28-70a2-45e2-9d0b-33c4f87b431b 6601 0 2019-12-13 19:15:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 13 19:16:06.567: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7090 /api/v1/namespaces/watch-7090/configmaps/e2e-watch-test-configmap-b 38267f28-70a2-45e2-9d0b-33c4f87b431b 6601 0 2019-12-13 19:15:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:16:16.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7090" for this suite.
Dec 13 19:16:22.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:16:22.666: INFO: namespace watch-7090 deletion completed in 6.096632611s

• [SLOW TEST:66.295 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:16:22.667: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8909
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8909
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-8909
I1213 19:16:22.831992      24 runners.go:184] Created replication controller with name: externalname-service, namespace: services-8909, replica count: 2
I1213 19:16:25.883244      24 runners.go:184] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1213 19:16:28.883478      24 runners.go:184] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1213 19:16:31.883692      24 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 13 19:16:31.883: INFO: Creating new exec pod
Dec 13 19:16:34.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=services-8909 execpod95nz9 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 13 19:16:35.387: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 13 19:16:35.387: INFO: stdout: ""
Dec 13 19:16:35.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=services-8909 execpod95nz9 -- /bin/sh -x -c nc -zv -t -w 2 10.97.234.214 80'
Dec 13 19:16:35.556: INFO: stderr: "+ nc -zv -t -w 2 10.97.234.214 80\nConnection to 10.97.234.214 80 port [tcp/http] succeeded!\n"
Dec 13 19:16:35.556: INFO: stdout: ""
Dec 13 19:16:35.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=services-8909 execpod95nz9 -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.2 32557'
Dec 13 19:16:35.733: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.2 32557\nConnection to 192.168.122.2 32557 port [tcp/32557] succeeded!\n"
Dec 13 19:16:35.733: INFO: stdout: ""
Dec 13 19:16:35.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=services-8909 execpod95nz9 -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.3 32557'
Dec 13 19:16:35.885: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.3 32557\nConnection to 192.168.122.3 32557 port [tcp/32557] succeeded!\n"
Dec 13 19:16:35.885: INFO: stdout: ""
Dec 13 19:16:35.885: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:16:35.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8909" for this suite.
Dec 13 19:16:41.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:16:42.023: INFO: namespace services-8909 deletion completed in 6.091366403s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:19.356 seconds]
[sig-network] Services
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:16:42.023: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-8777
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-8777, will wait for the garbage collector to delete the pods
Dec 13 19:16:44.219: INFO: Deleting Job.batch foo took: 3.855387ms
Dec 13 19:16:44.820: INFO: Terminating Job.batch foo pods took: 600.229287ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:17:17.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8777" for this suite.
Dec 13 19:17:23.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:17:24.009: INFO: namespace job-8777 deletion completed in 6.081639522s

• [SLOW TEST:41.986 seconds]
[sig-apps] Job
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:17:24.010: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2291
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-cc516b81-3f98-4c28-ac78-7d45f9bfa3f7 in namespace container-probe-2291
Dec 13 19:17:26.156: INFO: Started pod liveness-cc516b81-3f98-4c28-ac78-7d45f9bfa3f7 in namespace container-probe-2291
STEP: checking the pod's current state and verifying that restartCount is present
Dec 13 19:17:26.159: INFO: Initial restart count of pod liveness-cc516b81-3f98-4c28-ac78-7d45f9bfa3f7 is 0
Dec 13 19:17:44.180: INFO: Restart count of pod container-probe-2291/liveness-cc516b81-3f98-4c28-ac78-7d45f9bfa3f7 is now 1 (18.020912533s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:17:44.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2291" for this suite.
Dec 13 19:17:50.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:17:50.305: INFO: namespace container-probe-2291 deletion completed in 6.096216922s

• [SLOW TEST:26.295 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:17:50.305: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-818
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Dec 13 19:17:50.446: INFO: Waiting up to 5m0s for pod "var-expansion-2e373f26-7765-41a9-a8fb-e3535eecc34a" in namespace "var-expansion-818" to be "success or failure"
Dec 13 19:17:50.448: INFO: Pod "var-expansion-2e373f26-7765-41a9-a8fb-e3535eecc34a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.698252ms
Dec 13 19:17:52.450: INFO: Pod "var-expansion-2e373f26-7765-41a9-a8fb-e3535eecc34a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004724118s
STEP: Saw pod success
Dec 13 19:17:52.450: INFO: Pod "var-expansion-2e373f26-7765-41a9-a8fb-e3535eecc34a" satisfied condition "success or failure"
Dec 13 19:17:52.452: INFO: Trying to get logs from node master-0 pod var-expansion-2e373f26-7765-41a9-a8fb-e3535eecc34a container dapi-container: <nil>
STEP: delete the pod
Dec 13 19:17:52.479: INFO: Waiting for pod var-expansion-2e373f26-7765-41a9-a8fb-e3535eecc34a to disappear
Dec 13 19:17:52.483: INFO: Pod var-expansion-2e373f26-7765-41a9-a8fb-e3535eecc34a no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:17:52.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-818" for this suite.
Dec 13 19:17:58.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:17:58.580: INFO: namespace var-expansion-818 deletion completed in 6.094933417s

• [SLOW TEST:8.276 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:17:58.581: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-278
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Dec 13 19:17:58.737: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:18:17.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-278" for this suite.
Dec 13 19:18:23.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:18:23.409: INFO: namespace crd-publish-openapi-278 deletion completed in 6.083792755s

• [SLOW TEST:24.829 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:18:23.410: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3201
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3201.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3201.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 13 19:18:45.575: INFO: DNS probes using dns-3201/dns-test-a4d3bac2-63eb-4f0f-8ae4-153932f1c03e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:18:45.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3201" for this suite.
Dec 13 19:18:51.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:18:51.715: INFO: namespace dns-3201 deletion completed in 6.111870949s

• [SLOW TEST:28.305 seconds]
[sig-network] DNS
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:18:51.715: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4637
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 13 19:18:51.853: INFO: Waiting up to 5m0s for pod "downwardapi-volume-19cc8308-fe0d-4b13-bca4-8255ad006c8c" in namespace "projected-4637" to be "success or failure"
Dec 13 19:18:51.859: INFO: Pod "downwardapi-volume-19cc8308-fe0d-4b13-bca4-8255ad006c8c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053466ms
Dec 13 19:18:53.861: INFO: Pod "downwardapi-volume-19cc8308-fe0d-4b13-bca4-8255ad006c8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008453601s
STEP: Saw pod success
Dec 13 19:18:53.862: INFO: Pod "downwardapi-volume-19cc8308-fe0d-4b13-bca4-8255ad006c8c" satisfied condition "success or failure"
Dec 13 19:18:53.863: INFO: Trying to get logs from node master-0 pod downwardapi-volume-19cc8308-fe0d-4b13-bca4-8255ad006c8c container client-container: <nil>
STEP: delete the pod
Dec 13 19:18:53.889: INFO: Waiting for pod downwardapi-volume-19cc8308-fe0d-4b13-bca4-8255ad006c8c to disappear
Dec 13 19:18:53.894: INFO: Pod downwardapi-volume-19cc8308-fe0d-4b13-bca4-8255ad006c8c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:18:53.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4637" for this suite.
Dec 13 19:18:59.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:18:59.995: INFO: namespace projected-4637 deletion completed in 6.097693002s

• [SLOW TEST:8.280 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:18:59.996: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8600
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8600.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8600.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8600.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8600.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 62.133.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.133.62_udp@PTR;check="$$(dig +tcp +noall +answer +search 62.133.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.133.62_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8600.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8600.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8600.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8600.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 62.133.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.133.62_udp@PTR;check="$$(dig +tcp +noall +answer +search 62.133.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.133.62_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 13 19:19:02.177: INFO: Unable to read wheezy_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:02.179: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:02.182: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:02.184: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:02.201: INFO: Unable to read jessie_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:02.203: INFO: Unable to read jessie_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:02.205: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:02.208: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:02.221: INFO: Lookups using dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1 failed for: [wheezy_udp@dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_udp@dns-test-service.dns-8600.svc.cluster.local jessie_tcp@dns-test-service.dns-8600.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local]

Dec 13 19:19:07.224: INFO: Unable to read wheezy_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:07.226: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:07.228: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:07.230: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:07.244: INFO: Unable to read jessie_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:07.247: INFO: Unable to read jessie_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:07.249: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:07.251: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:07.263: INFO: Lookups using dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1 failed for: [wheezy_udp@dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_udp@dns-test-service.dns-8600.svc.cluster.local jessie_tcp@dns-test-service.dns-8600.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local]

Dec 13 19:19:12.224: INFO: Unable to read wheezy_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:12.226: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:12.229: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:12.231: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:12.248: INFO: Unable to read jessie_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:12.251: INFO: Unable to read jessie_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:12.254: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:12.258: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:12.276: INFO: Lookups using dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1 failed for: [wheezy_udp@dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_udp@dns-test-service.dns-8600.svc.cluster.local jessie_tcp@dns-test-service.dns-8600.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local]

Dec 13 19:19:17.224: INFO: Unable to read wheezy_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:17.226: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:17.228: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:17.230: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:17.251: INFO: Unable to read jessie_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:17.256: INFO: Unable to read jessie_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:17.258: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:17.260: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:17.275: INFO: Lookups using dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1 failed for: [wheezy_udp@dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_udp@dns-test-service.dns-8600.svc.cluster.local jessie_tcp@dns-test-service.dns-8600.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local]

Dec 13 19:19:22.224: INFO: Unable to read wheezy_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:22.226: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:22.229: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:22.231: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:22.247: INFO: Unable to read jessie_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:22.249: INFO: Unable to read jessie_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:22.252: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:22.254: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:22.268: INFO: Lookups using dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1 failed for: [wheezy_udp@dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_udp@dns-test-service.dns-8600.svc.cluster.local jessie_tcp@dns-test-service.dns-8600.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local]

Dec 13 19:19:27.224: INFO: Unable to read wheezy_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:27.227: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:27.230: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:27.232: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:27.250: INFO: Unable to read jessie_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:27.253: INFO: Unable to read jessie_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:27.255: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:27.257: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1: the server could not find the requested resource (get pods dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1)
Dec 13 19:19:27.271: INFO: Lookups using dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1 failed for: [wheezy_udp@dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_udp@dns-test-service.dns-8600.svc.cluster.local jessie_tcp@dns-test-service.dns-8600.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local]

Dec 13 19:19:32.268: INFO: DNS probes using dns-8600/dns-test-c9aac5cb-f636-459c-97e3-39525d7a41b1 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:19:32.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8600" for this suite.
Dec 13 19:19:38.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:19:38.463: INFO: namespace dns-8600 deletion completed in 6.084955354s

• [SLOW TEST:38.467 seconds]
[sig-network] DNS
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:19:38.463: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9258
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-b1e3a8b3-9b72-439d-bf24-56ce4bb3027b
STEP: Creating a pod to test consume configMaps
Dec 13 19:19:38.608: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7fba6f55-42d3-41e5-9c27-080973e05ba2" in namespace "projected-9258" to be "success or failure"
Dec 13 19:19:38.612: INFO: Pod "pod-projected-configmaps-7fba6f55-42d3-41e5-9c27-080973e05ba2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048043ms
Dec 13 19:19:40.614: INFO: Pod "pod-projected-configmaps-7fba6f55-42d3-41e5-9c27-080973e05ba2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006029108s
STEP: Saw pod success
Dec 13 19:19:40.614: INFO: Pod "pod-projected-configmaps-7fba6f55-42d3-41e5-9c27-080973e05ba2" satisfied condition "success or failure"
Dec 13 19:19:40.616: INFO: Trying to get logs from node master-0 pod pod-projected-configmaps-7fba6f55-42d3-41e5-9c27-080973e05ba2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 19:19:40.638: INFO: Waiting for pod pod-projected-configmaps-7fba6f55-42d3-41e5-9c27-080973e05ba2 to disappear
Dec 13 19:19:40.643: INFO: Pod pod-projected-configmaps-7fba6f55-42d3-41e5-9c27-080973e05ba2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:19:40.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9258" for this suite.
Dec 13 19:19:46.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:19:46.775: INFO: namespace projected-9258 deletion completed in 6.129695358s

• [SLOW TEST:8.312 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:19:46.775: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7597
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-41f43773-158b-472e-b5a7-8ba003f5db2b
STEP: Creating a pod to test consume configMaps
Dec 13 19:19:46.939: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9085a3a7-201f-4d16-90a1-2326f9add0ef" in namespace "projected-7597" to be "success or failure"
Dec 13 19:19:46.951: INFO: Pod "pod-projected-configmaps-9085a3a7-201f-4d16-90a1-2326f9add0ef": Phase="Pending", Reason="", readiness=false. Elapsed: 11.525525ms
Dec 13 19:19:48.953: INFO: Pod "pod-projected-configmaps-9085a3a7-201f-4d16-90a1-2326f9add0ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013595785s
STEP: Saw pod success
Dec 13 19:19:48.953: INFO: Pod "pod-projected-configmaps-9085a3a7-201f-4d16-90a1-2326f9add0ef" satisfied condition "success or failure"
Dec 13 19:19:48.954: INFO: Trying to get logs from node master-0 pod pod-projected-configmaps-9085a3a7-201f-4d16-90a1-2326f9add0ef container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 19:19:48.972: INFO: Waiting for pod pod-projected-configmaps-9085a3a7-201f-4d16-90a1-2326f9add0ef to disappear
Dec 13 19:19:48.976: INFO: Pod pod-projected-configmaps-9085a3a7-201f-4d16-90a1-2326f9add0ef no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:19:48.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7597" for this suite.
Dec 13 19:19:54.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:19:55.064: INFO: namespace projected-7597 deletion completed in 6.084863794s

• [SLOW TEST:8.288 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:19:55.064: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2765
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-18a53564-dff6-44ff-b1d7-c026f4b4479e
STEP: Creating secret with name secret-projected-all-test-volume-57f5e550-8850-4acf-ac2f-43390eb6c358
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec 13 19:19:55.210: INFO: Waiting up to 5m0s for pod "projected-volume-c20d2ac4-f01f-4881-ba21-825e6d47a35d" in namespace "projected-2765" to be "success or failure"
Dec 13 19:19:55.226: INFO: Pod "projected-volume-c20d2ac4-f01f-4881-ba21-825e6d47a35d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.455938ms
Dec 13 19:19:57.228: INFO: Pod "projected-volume-c20d2ac4-f01f-4881-ba21-825e6d47a35d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018032267s
STEP: Saw pod success
Dec 13 19:19:57.228: INFO: Pod "projected-volume-c20d2ac4-f01f-4881-ba21-825e6d47a35d" satisfied condition "success or failure"
Dec 13 19:19:57.230: INFO: Trying to get logs from node master-0 pod projected-volume-c20d2ac4-f01f-4881-ba21-825e6d47a35d container projected-all-volume-test: <nil>
STEP: delete the pod
Dec 13 19:19:57.257: INFO: Waiting for pod projected-volume-c20d2ac4-f01f-4881-ba21-825e6d47a35d to disappear
Dec 13 19:19:57.259: INFO: Pod projected-volume-c20d2ac4-f01f-4881-ba21-825e6d47a35d no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:19:57.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2765" for this suite.
Dec 13 19:20:03.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:20:03.359: INFO: namespace projected-2765 deletion completed in 6.096705672s

• [SLOW TEST:8.295 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:20:03.359: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4615
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 13 19:20:03.495: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0cbaba39-1979-4113-845e-f04df6e0d94c" in namespace "downward-api-4615" to be "success or failure"
Dec 13 19:20:03.497: INFO: Pod "downwardapi-volume-0cbaba39-1979-4113-845e-f04df6e0d94c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.933027ms
Dec 13 19:20:05.498: INFO: Pod "downwardapi-volume-0cbaba39-1979-4113-845e-f04df6e0d94c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003726206s
STEP: Saw pod success
Dec 13 19:20:05.499: INFO: Pod "downwardapi-volume-0cbaba39-1979-4113-845e-f04df6e0d94c" satisfied condition "success or failure"
Dec 13 19:20:05.500: INFO: Trying to get logs from node master-0 pod downwardapi-volume-0cbaba39-1979-4113-845e-f04df6e0d94c container client-container: <nil>
STEP: delete the pod
Dec 13 19:20:05.517: INFO: Waiting for pod downwardapi-volume-0cbaba39-1979-4113-845e-f04df6e0d94c to disappear
Dec 13 19:20:05.520: INFO: Pod downwardapi-volume-0cbaba39-1979-4113-845e-f04df6e0d94c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:20:05.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4615" for this suite.
Dec 13 19:20:11.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:20:11.608: INFO: namespace downward-api-4615 deletion completed in 6.085309096s

• [SLOW TEST:8.249 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:20:11.608: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6390
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6390
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-6390
I1213 19:20:11.770646      24 runners.go:184] Created replication controller with name: externalname-service, namespace: services-6390, replica count: 2
I1213 19:20:14.821082      24 runners.go:184] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1213 19:20:17.821423      24 runners.go:184] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 13 19:20:20.821: INFO: Creating new exec pod
I1213 19:20:20.821607      24 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 13 19:20:23.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=services-6390 execpodvlvf8 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 13 19:20:23.995: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 13 19:20:23.995: INFO: stdout: ""
Dec 13 19:20:23.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=services-6390 execpodvlvf8 -- /bin/sh -x -c nc -zv -t -w 2 10.100.23.113 80'
Dec 13 19:20:24.172: INFO: stderr: "+ nc -zv -t -w 2 10.100.23.113 80\nConnection to 10.100.23.113 80 port [tcp/http] succeeded!\n"
Dec 13 19:20:24.172: INFO: stdout: ""
Dec 13 19:20:24.172: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:20:24.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6390" for this suite.
Dec 13 19:20:30.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:20:30.319: INFO: namespace services-6390 deletion completed in 6.106415906s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:18.712 seconds]
[sig-network] Services
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:20:30.319: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8785
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Dec 13 19:20:30.458: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Dec 13 19:20:30.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 create -f - --namespace=kubectl-8785'
Dec 13 19:20:30.768: INFO: stderr: ""
Dec 13 19:20:30.768: INFO: stdout: "service/redis-slave created\n"
Dec 13 19:20:30.768: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Dec 13 19:20:30.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 create -f - --namespace=kubectl-8785'
Dec 13 19:20:31.186: INFO: stderr: ""
Dec 13 19:20:31.186: INFO: stdout: "service/redis-master created\n"
Dec 13 19:20:31.186: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec 13 19:20:31.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 create -f - --namespace=kubectl-8785'
Dec 13 19:20:31.482: INFO: stderr: ""
Dec 13 19:20:31.482: INFO: stdout: "service/frontend created\n"
Dec 13 19:20:31.482: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Dec 13 19:20:31.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 create -f - --namespace=kubectl-8785'
Dec 13 19:20:31.784: INFO: stderr: ""
Dec 13 19:20:31.784: INFO: stdout: "deployment.apps/frontend created\n"
Dec 13 19:20:31.785: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 13 19:20:31.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 create -f - --namespace=kubectl-8785'
Dec 13 19:20:32.270: INFO: stderr: ""
Dec 13 19:20:32.270: INFO: stdout: "deployment.apps/redis-master created\n"
Dec 13 19:20:32.270: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Dec 13 19:20:32.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 create -f - --namespace=kubectl-8785'
Dec 13 19:20:32.852: INFO: stderr: ""
Dec 13 19:20:32.852: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Dec 13 19:20:32.852: INFO: Waiting for all frontend pods to be Running.
Dec 13 19:21:37.915: INFO: Waiting for frontend to serve content.
Dec 13 19:21:38.023: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'No route to host [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('No route to hos...', 113)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\StreamCo in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Dec 13 19:21:43.040: INFO: Trying to add a new entry to the guestbook.
Dec 13 19:21:43.050: INFO: Verifying that added entry can be retrieved.
Dec 13 19:21:43.061: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Dec 13 19:21:48.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 delete --grace-period=0 --force -f - --namespace=kubectl-8785'
Dec 13 19:21:48.178: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 13 19:21:48.178: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Dec 13 19:21:48.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 delete --grace-period=0 --force -f - --namespace=kubectl-8785'
Dec 13 19:21:48.315: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 13 19:21:48.315: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 13 19:21:48.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 delete --grace-period=0 --force -f - --namespace=kubectl-8785'
Dec 13 19:21:48.430: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 13 19:21:48.430: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 13 19:21:48.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 delete --grace-period=0 --force -f - --namespace=kubectl-8785'
Dec 13 19:21:48.524: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 13 19:21:48.524: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 13 19:21:48.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 delete --grace-period=0 --force -f - --namespace=kubectl-8785'
Dec 13 19:21:48.589: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 13 19:21:48.589: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 13 19:21:48.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 delete --grace-period=0 --force -f - --namespace=kubectl-8785'
Dec 13 19:21:48.649: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 13 19:21:48.649: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:21:48.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8785" for this suite.
Dec 13 19:22:00.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:22:00.735: INFO: namespace kubectl-8785 deletion completed in 12.083366702s

• [SLOW TEST:90.416 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:22:00.735: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3855
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 13 19:22:01.575: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 13 19:22:04.591: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:22:04.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3855" for this suite.
Dec 13 19:22:10.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:22:10.747: INFO: namespace webhook-3855 deletion completed in 6.090943616s
STEP: Destroying namespace "webhook-3855-markers" for this suite.
Dec 13 19:22:16.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:22:16.830: INFO: namespace webhook-3855-markers deletion completed in 6.082794041s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:16.103 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:22:16.838: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3782
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 13 19:22:17.505: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 13 19:22:20.544: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:22:20.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3782" for this suite.
Dec 13 19:22:28.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:22:28.732: INFO: namespace webhook-3782 deletion completed in 8.083791117s
STEP: Destroying namespace "webhook-3782-markers" for this suite.
Dec 13 19:22:34.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:22:34.817: INFO: namespace webhook-3782-markers deletion completed in 6.084454055s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.986 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:22:34.825: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9938
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:22:50.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9938" for this suite.
Dec 13 19:22:56.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:22:57.072: INFO: namespace resourcequota-9938 deletion completed in 6.084518191s

• [SLOW TEST:22.247 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:22:57.072: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3984
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Dec 13 19:22:57.726: INFO: created pod pod-service-account-defaultsa
Dec 13 19:22:57.726: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec 13 19:22:57.735: INFO: created pod pod-service-account-mountsa
Dec 13 19:22:57.735: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec 13 19:22:57.746: INFO: created pod pod-service-account-nomountsa
Dec 13 19:22:57.746: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec 13 19:22:57.764: INFO: created pod pod-service-account-defaultsa-mountspec
Dec 13 19:22:57.764: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec 13 19:22:57.775: INFO: created pod pod-service-account-mountsa-mountspec
Dec 13 19:22:57.775: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec 13 19:22:57.797: INFO: created pod pod-service-account-nomountsa-mountspec
Dec 13 19:22:57.797: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec 13 19:22:57.808: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec 13 19:22:57.808: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec 13 19:22:57.823: INFO: created pod pod-service-account-mountsa-nomountspec
Dec 13 19:22:57.823: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec 13 19:22:57.856: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec 13 19:22:57.856: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:22:57.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3984" for this suite.
Dec 13 19:23:09.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:23:09.999: INFO: namespace svcaccounts-3984 deletion completed in 12.124818881s

• [SLOW TEST:12.927 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:23:09.999: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6435
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Dec 13 19:23:20.153: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-728461388 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec 13 19:23:30.235: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:23:30.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6435" for this suite.
Dec 13 19:23:36.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:23:36.322: INFO: namespace pods-6435 deletion completed in 6.082693788s

• [SLOW TEST:26.323 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:23:36.322: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1884
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 13 19:23:36.466: INFO: Waiting up to 5m0s for pod "pod-2f3e378e-a13c-42da-91fd-58c8ee404d76" in namespace "emptydir-1884" to be "success or failure"
Dec 13 19:23:36.470: INFO: Pod "pod-2f3e378e-a13c-42da-91fd-58c8ee404d76": Phase="Pending", Reason="", readiness=false. Elapsed: 3.685748ms
Dec 13 19:23:38.474: INFO: Pod "pod-2f3e378e-a13c-42da-91fd-58c8ee404d76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007973398s
STEP: Saw pod success
Dec 13 19:23:38.474: INFO: Pod "pod-2f3e378e-a13c-42da-91fd-58c8ee404d76" satisfied condition "success or failure"
Dec 13 19:23:38.477: INFO: Trying to get logs from node master-0 pod pod-2f3e378e-a13c-42da-91fd-58c8ee404d76 container test-container: <nil>
STEP: delete the pod
Dec 13 19:23:38.504: INFO: Waiting for pod pod-2f3e378e-a13c-42da-91fd-58c8ee404d76 to disappear
Dec 13 19:23:38.506: INFO: Pod pod-2f3e378e-a13c-42da-91fd-58c8ee404d76 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:23:38.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1884" for this suite.
Dec 13 19:23:44.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:23:44.616: INFO: namespace emptydir-1884 deletion completed in 6.10593096s

• [SLOW TEST:8.294 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:23:44.616: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4612
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Dec 13 19:23:44.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 --namespace=kubectl-4612 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Dec 13 19:23:46.034: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Dec 13 19:23:46.034: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:23:48.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4612" for this suite.
Dec 13 19:23:54.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:23:54.121: INFO: namespace kubectl-4612 deletion completed in 6.080899351s

• [SLOW TEST:9.505 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:23:54.121: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-947
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 13 19:23:54.267: INFO: Waiting up to 5m0s for pod "downwardapi-volume-89e6f766-5803-4b3b-912a-bf5ec3725889" in namespace "projected-947" to be "success or failure"
Dec 13 19:23:54.271: INFO: Pod "downwardapi-volume-89e6f766-5803-4b3b-912a-bf5ec3725889": Phase="Pending", Reason="", readiness=false. Elapsed: 4.36665ms
Dec 13 19:23:56.282: INFO: Pod "downwardapi-volume-89e6f766-5803-4b3b-912a-bf5ec3725889": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015224211s
STEP: Saw pod success
Dec 13 19:23:56.282: INFO: Pod "downwardapi-volume-89e6f766-5803-4b3b-912a-bf5ec3725889" satisfied condition "success or failure"
Dec 13 19:23:56.285: INFO: Trying to get logs from node master-0 pod downwardapi-volume-89e6f766-5803-4b3b-912a-bf5ec3725889 container client-container: <nil>
STEP: delete the pod
Dec 13 19:23:56.382: INFO: Waiting for pod downwardapi-volume-89e6f766-5803-4b3b-912a-bf5ec3725889 to disappear
Dec 13 19:23:56.421: INFO: Pod downwardapi-volume-89e6f766-5803-4b3b-912a-bf5ec3725889 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:23:56.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-947" for this suite.
Dec 13 19:24:02.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:24:02.549: INFO: namespace projected-947 deletion completed in 6.112541757s

• [SLOW TEST:8.428 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:24:02.549: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1669
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 13 19:24:06.720: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 13 19:24:06.725: INFO: Pod pod-with-prestop-http-hook still exists
Dec 13 19:24:08.725: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 13 19:24:08.727: INFO: Pod pod-with-prestop-http-hook still exists
Dec 13 19:24:10.725: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 13 19:24:10.728: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:24:10.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1669" for this suite.
Dec 13 19:24:22.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:24:22.818: INFO: namespace container-lifecycle-hook-1669 deletion completed in 12.084448496s

• [SLOW TEST:20.269 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:24:22.819: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4217
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-4217
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4217 to expose endpoints map[]
Dec 13 19:24:22.986: INFO: Get endpoints failed (7.235207ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Dec 13 19:24:23.988: INFO: successfully validated that service endpoint-test2 in namespace services-4217 exposes endpoints map[] (1.009037073s elapsed)
STEP: Creating pod pod1 in namespace services-4217
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4217 to expose endpoints map[pod1:[80]]
Dec 13 19:24:26.011: INFO: successfully validated that service endpoint-test2 in namespace services-4217 exposes endpoints map[pod1:[80]] (2.018182212s elapsed)
STEP: Creating pod pod2 in namespace services-4217
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4217 to expose endpoints map[pod1:[80] pod2:[80]]
Dec 13 19:24:28.054: INFO: successfully validated that service endpoint-test2 in namespace services-4217 exposes endpoints map[pod1:[80] pod2:[80]] (2.036413239s elapsed)
STEP: Deleting pod pod1 in namespace services-4217
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4217 to expose endpoints map[pod2:[80]]
Dec 13 19:24:28.076: INFO: successfully validated that service endpoint-test2 in namespace services-4217 exposes endpoints map[pod2:[80]] (15.633319ms elapsed)
STEP: Deleting pod pod2 in namespace services-4217
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4217 to expose endpoints map[]
Dec 13 19:24:29.094: INFO: successfully validated that service endpoint-test2 in namespace services-4217 exposes endpoints map[] (1.008310338s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:24:29.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4217" for this suite.
Dec 13 19:24:35.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:24:35.249: INFO: namespace services-4217 deletion completed in 6.102818053s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:12.430 seconds]
[sig-network] Services
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:24:35.249: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5776
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-5776/secret-test-7efbfc98-dbab-426d-98f5-efbf15f1372a
STEP: Creating a pod to test consume secrets
Dec 13 19:24:35.397: INFO: Waiting up to 5m0s for pod "pod-configmaps-05c682c1-63e1-4b4e-b3e7-473658d4c60e" in namespace "secrets-5776" to be "success or failure"
Dec 13 19:24:35.404: INFO: Pod "pod-configmaps-05c682c1-63e1-4b4e-b3e7-473658d4c60e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.667101ms
Dec 13 19:24:37.406: INFO: Pod "pod-configmaps-05c682c1-63e1-4b4e-b3e7-473658d4c60e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008744212s
STEP: Saw pod success
Dec 13 19:24:37.406: INFO: Pod "pod-configmaps-05c682c1-63e1-4b4e-b3e7-473658d4c60e" satisfied condition "success or failure"
Dec 13 19:24:37.407: INFO: Trying to get logs from node master-0 pod pod-configmaps-05c682c1-63e1-4b4e-b3e7-473658d4c60e container env-test: <nil>
STEP: delete the pod
Dec 13 19:24:37.421: INFO: Waiting for pod pod-configmaps-05c682c1-63e1-4b4e-b3e7-473658d4c60e to disappear
Dec 13 19:24:37.426: INFO: Pod pod-configmaps-05c682c1-63e1-4b4e-b3e7-473658d4c60e no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:24:37.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5776" for this suite.
Dec 13 19:24:43.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:24:43.513: INFO: namespace secrets-5776 deletion completed in 6.084986047s

• [SLOW TEST:8.264 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:24:43.513: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-3878
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Dec 13 19:24:43.669: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-3878" to be "success or failure"
Dec 13 19:24:43.674: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.767392ms
Dec 13 19:24:45.676: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006841919s
STEP: Saw pod success
Dec 13 19:24:45.676: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Dec 13 19:24:45.677: INFO: Trying to get logs from node master-0 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Dec 13 19:24:45.693: INFO: Waiting for pod pod-host-path-test to disappear
Dec 13 19:24:45.697: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:24:45.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-3878" for this suite.
Dec 13 19:24:51.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:24:51.805: INFO: namespace hostpath-3878 deletion completed in 6.101618801s

• [SLOW TEST:8.292 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:24:51.805: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-404
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 13 19:24:51.953: INFO: Waiting up to 5m0s for pod "pod-1674f3b9-1a5c-4805-8af1-b5429fb4fd6c" in namespace "emptydir-404" to be "success or failure"
Dec 13 19:24:51.962: INFO: Pod "pod-1674f3b9-1a5c-4805-8af1-b5429fb4fd6c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.466125ms
Dec 13 19:24:53.964: INFO: Pod "pod-1674f3b9-1a5c-4805-8af1-b5429fb4fd6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011628476s
STEP: Saw pod success
Dec 13 19:24:53.964: INFO: Pod "pod-1674f3b9-1a5c-4805-8af1-b5429fb4fd6c" satisfied condition "success or failure"
Dec 13 19:24:53.966: INFO: Trying to get logs from node master-0 pod pod-1674f3b9-1a5c-4805-8af1-b5429fb4fd6c container test-container: <nil>
STEP: delete the pod
Dec 13 19:24:53.984: INFO: Waiting for pod pod-1674f3b9-1a5c-4805-8af1-b5429fb4fd6c to disappear
Dec 13 19:24:53.990: INFO: Pod pod-1674f3b9-1a5c-4805-8af1-b5429fb4fd6c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:24:53.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-404" for this suite.
Dec 13 19:25:00.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:25:00.083: INFO: namespace emptydir-404 deletion completed in 6.09010038s

• [SLOW TEST:8.278 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:25:00.083: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7881
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:25:00.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7881" for this suite.
Dec 13 19:25:28.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:25:28.317: INFO: namespace pods-7881 deletion completed in 28.088871082s

• [SLOW TEST:28.233 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:25:28.317: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1345
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 13 19:25:30.979: INFO: Successfully updated pod "pod-update-activedeadlineseconds-66cf0543-3650-4222-9a98-9e82d7c26cd0"
Dec 13 19:25:30.979: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-66cf0543-3650-4222-9a98-9e82d7c26cd0" in namespace "pods-1345" to be "terminated due to deadline exceeded"
Dec 13 19:25:30.988: INFO: Pod "pod-update-activedeadlineseconds-66cf0543-3650-4222-9a98-9e82d7c26cd0": Phase="Running", Reason="", readiness=true. Elapsed: 9.71439ms
Dec 13 19:25:32.991: INFO: Pod "pod-update-activedeadlineseconds-66cf0543-3650-4222-9a98-9e82d7c26cd0": Phase="Running", Reason="", readiness=true. Elapsed: 2.011947591s
Dec 13 19:25:34.993: INFO: Pod "pod-update-activedeadlineseconds-66cf0543-3650-4222-9a98-9e82d7c26cd0": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.013987286s
Dec 13 19:25:34.993: INFO: Pod "pod-update-activedeadlineseconds-66cf0543-3650-4222-9a98-9e82d7c26cd0" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:25:34.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1345" for this suite.
Dec 13 19:25:41.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:25:41.103: INFO: namespace pods-1345 deletion completed in 6.107357083s

• [SLOW TEST:12.785 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:25:41.103: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-7783
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:25:49.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7783" for this suite.
Dec 13 19:25:55.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:25:55.368: INFO: namespace job-7783 deletion completed in 6.106924913s

• [SLOW TEST:14.265 seconds]
[sig-apps] Job
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:25:55.368: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5812
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-4fed6f82-53e3-4d23-8b7a-61c4262339fb
STEP: Creating a pod to test consume configMaps
Dec 13 19:25:55.521: INFO: Waiting up to 5m0s for pod "pod-configmaps-3b8a4216-9b2a-4328-be76-d1285e3f4a1e" in namespace "configmap-5812" to be "success or failure"
Dec 13 19:25:55.527: INFO: Pod "pod-configmaps-3b8a4216-9b2a-4328-be76-d1285e3f4a1e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.969132ms
Dec 13 19:25:57.532: INFO: Pod "pod-configmaps-3b8a4216-9b2a-4328-be76-d1285e3f4a1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011043621s
STEP: Saw pod success
Dec 13 19:25:57.532: INFO: Pod "pod-configmaps-3b8a4216-9b2a-4328-be76-d1285e3f4a1e" satisfied condition "success or failure"
Dec 13 19:25:57.539: INFO: Trying to get logs from node master-0 pod pod-configmaps-3b8a4216-9b2a-4328-be76-d1285e3f4a1e container configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 19:25:57.565: INFO: Waiting for pod pod-configmaps-3b8a4216-9b2a-4328-be76-d1285e3f4a1e to disappear
Dec 13 19:25:57.568: INFO: Pod pod-configmaps-3b8a4216-9b2a-4328-be76-d1285e3f4a1e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:25:57.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5812" for this suite.
Dec 13 19:26:03.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:26:03.649: INFO: namespace configmap-5812 deletion completed in 6.079064907s

• [SLOW TEST:8.281 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:26:03.649: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-695
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec 13 19:26:06.320: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-695 pod-service-account-b3f46a65-984e-480f-98b8-01e89eda1e72 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec 13 19:26:06.468: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-695 pod-service-account-b3f46a65-984e-480f-98b8-01e89eda1e72 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec 13 19:26:06.615: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-695 pod-service-account-b3f46a65-984e-480f-98b8-01e89eda1e72 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:26:06.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-695" for this suite.
Dec 13 19:26:12.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:26:12.822: INFO: namespace svcaccounts-695 deletion completed in 6.083583872s

• [SLOW TEST:9.173 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:26:12.823: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2971
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Dec 13 19:26:12.960: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-728461388 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:26:13.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2971" for this suite.
Dec 13 19:26:19.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:26:19.105: INFO: namespace kubectl-2971 deletion completed in 6.091841709s

• [SLOW TEST:6.283 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:26:19.106: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8300
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-8300
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8300 to expose endpoints map[]
Dec 13 19:26:19.252: INFO: Get endpoints failed (6.196158ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Dec 13 19:26:20.254: INFO: successfully validated that service multi-endpoint-test in namespace services-8300 exposes endpoints map[] (1.008117073s elapsed)
STEP: Creating pod pod1 in namespace services-8300
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8300 to expose endpoints map[pod1:[100]]
Dec 13 19:26:22.279: INFO: successfully validated that service multi-endpoint-test in namespace services-8300 exposes endpoints map[pod1:[100]] (2.017518288s elapsed)
STEP: Creating pod pod2 in namespace services-8300
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8300 to expose endpoints map[pod1:[100] pod2:[101]]
Dec 13 19:26:24.312: INFO: successfully validated that service multi-endpoint-test in namespace services-8300 exposes endpoints map[pod1:[100] pod2:[101]] (2.029475626s elapsed)
STEP: Deleting pod pod1 in namespace services-8300
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8300 to expose endpoints map[pod2:[101]]
Dec 13 19:26:25.405: INFO: successfully validated that service multi-endpoint-test in namespace services-8300 exposes endpoints map[pod2:[101]] (1.08824163s elapsed)
STEP: Deleting pod pod2 in namespace services-8300
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8300 to expose endpoints map[]
Dec 13 19:26:25.423: INFO: successfully validated that service multi-endpoint-test in namespace services-8300 exposes endpoints map[] (14.112228ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:26:25.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8300" for this suite.
Dec 13 19:26:37.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:26:37.559: INFO: namespace services-8300 deletion completed in 12.091612732s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:18.453 seconds]
[sig-network] Services
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:26:37.559: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8676
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 13 19:26:37.694: INFO: Waiting up to 5m0s for pod "pod-f37a72a3-be6a-44f5-bf0e-e18f61ec1bcc" in namespace "emptydir-8676" to be "success or failure"
Dec 13 19:26:37.697: INFO: Pod "pod-f37a72a3-be6a-44f5-bf0e-e18f61ec1bcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.790459ms
Dec 13 19:26:39.700: INFO: Pod "pod-f37a72a3-be6a-44f5-bf0e-e18f61ec1bcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005159722s
Dec 13 19:26:41.702: INFO: Pod "pod-f37a72a3-be6a-44f5-bf0e-e18f61ec1bcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007365867s
STEP: Saw pod success
Dec 13 19:26:41.702: INFO: Pod "pod-f37a72a3-be6a-44f5-bf0e-e18f61ec1bcc" satisfied condition "success or failure"
Dec 13 19:26:41.708: INFO: Trying to get logs from node master-0 pod pod-f37a72a3-be6a-44f5-bf0e-e18f61ec1bcc container test-container: <nil>
STEP: delete the pod
Dec 13 19:26:41.730: INFO: Waiting for pod pod-f37a72a3-be6a-44f5-bf0e-e18f61ec1bcc to disappear
Dec 13 19:26:41.734: INFO: Pod pod-f37a72a3-be6a-44f5-bf0e-e18f61ec1bcc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:26:41.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8676" for this suite.
Dec 13 19:26:47.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:26:47.827: INFO: namespace emptydir-8676 deletion completed in 6.089386039s

• [SLOW TEST:10.268 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:26:47.827: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1750
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1750
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1750
STEP: creating replication controller externalsvc in namespace services-1750
I1213 19:26:48.010026      24 runners.go:184] Created replication controller with name: externalsvc, namespace: services-1750, replica count: 2
I1213 19:26:51.068065      24 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Dec 13 19:26:51.084: INFO: Creating new exec pod
Dec 13 19:26:53.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=services-1750 execpodx45qt -- /bin/sh -x -c nslookup nodeport-service'
Dec 13 19:26:53.532: INFO: stderr: "+ nslookup nodeport-service\n"
Dec 13 19:26:53.532: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-1750.svc.cluster.local\tcanonical name = externalsvc.services-1750.svc.cluster.local.\nName:\texternalsvc.services-1750.svc.cluster.local\nAddress: 10.98.103.135\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1750, will wait for the garbage collector to delete the pods
Dec 13 19:26:53.587: INFO: Deleting ReplicationController externalsvc took: 3.528484ms
Dec 13 19:26:54.188: INFO: Terminating ReplicationController externalsvc pods took: 600.474112ms
Dec 13 19:27:06.928: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:27:06.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1750" for this suite.
Dec 13 19:27:12.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:27:13.088: INFO: namespace services-1750 deletion completed in 6.120464124s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:25.261 seconds]
[sig-network] Services
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:27:13.088: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-756
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 19:27:13.241: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Creating first CR 
Dec 13 19:27:13.783: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-13T19:27:13Z generation:1 name:name1 resourceVersion:10809 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:5c043410-997f-44a0-9aed-a9cc67595e01] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Dec 13 19:27:23.786: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-13T19:27:23Z generation:1 name:name2 resourceVersion:10845 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:6892db02-91e6-4e58-832f-726f04725e28] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Dec 13 19:27:33.790: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-13T19:27:13Z generation:2 name:name1 resourceVersion:10882 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:5c043410-997f-44a0-9aed-a9cc67595e01] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Dec 13 19:27:43.793: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-13T19:27:23Z generation:2 name:name2 resourceVersion:10918 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:6892db02-91e6-4e58-832f-726f04725e28] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Dec 13 19:27:53.798: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-13T19:27:13Z generation:2 name:name1 resourceVersion:10954 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:5c043410-997f-44a0-9aed-a9cc67595e01] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Dec 13 19:28:03.802: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-13T19:27:23Z generation:2 name:name2 resourceVersion:10993 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:6892db02-91e6-4e58-832f-726f04725e28] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:28:14.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-756" for this suite.
Dec 13 19:28:20.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:28:20.410: INFO: namespace crd-watch-756 deletion completed in 6.100005204s

• [SLOW TEST:67.322 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:28:20.410: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9234
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-c996
STEP: Creating a pod to test atomic-volume-subpath
Dec 13 19:28:20.573: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-c996" in namespace "subpath-9234" to be "success or failure"
Dec 13 19:28:20.579: INFO: Pod "pod-subpath-test-configmap-c996": Phase="Pending", Reason="", readiness=false. Elapsed: 6.093876ms
Dec 13 19:28:22.581: INFO: Pod "pod-subpath-test-configmap-c996": Phase="Running", Reason="", readiness=true. Elapsed: 2.008486792s
Dec 13 19:28:24.584: INFO: Pod "pod-subpath-test-configmap-c996": Phase="Running", Reason="", readiness=true. Elapsed: 4.010777592s
Dec 13 19:28:26.590: INFO: Pod "pod-subpath-test-configmap-c996": Phase="Running", Reason="", readiness=true. Elapsed: 6.017520958s
Dec 13 19:28:28.592: INFO: Pod "pod-subpath-test-configmap-c996": Phase="Running", Reason="", readiness=true. Elapsed: 8.01962744s
Dec 13 19:28:30.595: INFO: Pod "pod-subpath-test-configmap-c996": Phase="Running", Reason="", readiness=true. Elapsed: 10.022042217s
Dec 13 19:28:32.597: INFO: Pod "pod-subpath-test-configmap-c996": Phase="Running", Reason="", readiness=true. Elapsed: 12.024708266s
Dec 13 19:28:34.599: INFO: Pod "pod-subpath-test-configmap-c996": Phase="Running", Reason="", readiness=true. Elapsed: 14.026524203s
Dec 13 19:28:36.602: INFO: Pod "pod-subpath-test-configmap-c996": Phase="Running", Reason="", readiness=true. Elapsed: 16.02920741s
Dec 13 19:28:38.604: INFO: Pod "pod-subpath-test-configmap-c996": Phase="Running", Reason="", readiness=true. Elapsed: 18.031298687s
Dec 13 19:28:40.606: INFO: Pod "pod-subpath-test-configmap-c996": Phase="Running", Reason="", readiness=true. Elapsed: 20.033292253s
Dec 13 19:28:42.608: INFO: Pod "pod-subpath-test-configmap-c996": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.035394481s
STEP: Saw pod success
Dec 13 19:28:42.608: INFO: Pod "pod-subpath-test-configmap-c996" satisfied condition "success or failure"
Dec 13 19:28:42.610: INFO: Trying to get logs from node master-0 pod pod-subpath-test-configmap-c996 container test-container-subpath-configmap-c996: <nil>
STEP: delete the pod
Dec 13 19:28:42.634: INFO: Waiting for pod pod-subpath-test-configmap-c996 to disappear
Dec 13 19:28:42.639: INFO: Pod pod-subpath-test-configmap-c996 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-c996
Dec 13 19:28:42.639: INFO: Deleting pod "pod-subpath-test-configmap-c996" in namespace "subpath-9234"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:28:42.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9234" for this suite.
Dec 13 19:28:48.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:28:48.730: INFO: namespace subpath-9234 deletion completed in 6.087440479s

• [SLOW TEST:28.320 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:28:48.731: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1492
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-761cd798-449b-475a-b068-634236176c6e
STEP: Creating a pod to test consume configMaps
Dec 13 19:28:48.877: INFO: Waiting up to 5m0s for pod "pod-configmaps-27f9f5c2-3bd3-4d9d-8a85-e85dc65b2edb" in namespace "configmap-1492" to be "success or failure"
Dec 13 19:28:48.887: INFO: Pod "pod-configmaps-27f9f5c2-3bd3-4d9d-8a85-e85dc65b2edb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.067982ms
Dec 13 19:28:50.889: INFO: Pod "pod-configmaps-27f9f5c2-3bd3-4d9d-8a85-e85dc65b2edb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012673539s
STEP: Saw pod success
Dec 13 19:28:50.889: INFO: Pod "pod-configmaps-27f9f5c2-3bd3-4d9d-8a85-e85dc65b2edb" satisfied condition "success or failure"
Dec 13 19:28:50.891: INFO: Trying to get logs from node master-0 pod pod-configmaps-27f9f5c2-3bd3-4d9d-8a85-e85dc65b2edb container configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 19:28:50.905: INFO: Waiting for pod pod-configmaps-27f9f5c2-3bd3-4d9d-8a85-e85dc65b2edb to disappear
Dec 13 19:28:50.916: INFO: Pod pod-configmaps-27f9f5c2-3bd3-4d9d-8a85-e85dc65b2edb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:28:50.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1492" for this suite.
Dec 13 19:28:56.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:28:57.003: INFO: namespace configmap-1492 deletion completed in 6.084102611s

• [SLOW TEST:8.272 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:28:57.003: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6427
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 13 19:28:57.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-6427'
Dec 13 19:28:57.210: INFO: stderr: ""
Dec 13 19:28:57.211: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Dec 13 19:29:02.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pod e2e-test-httpd-pod --namespace=kubectl-6427 -o json'
Dec 13 19:29:02.321: INFO: stderr: ""
Dec 13 19:29:02.321: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.244.0.84/32\"\n        },\n        \"creationTimestamp\": \"2019-12-13T19:28:57Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6427\",\n        \"resourceVersion\": \"11274\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6427/pods/e2e-test-httpd-pod\",\n        \"uid\": \"918efc5b-6c80-4785-b4ec-8bdf5234af3d\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-pnvx9\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"master-0\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-pnvx9\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-pnvx9\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-13T19:28:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-13T19:28:59Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-13T19:28:59Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-13T19:28:57Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://36023ae501b0d26a74cf0d1c9158eea582602c62e02b82539ef01738b9a01db6\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-12-13T19:28:58Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.122.2\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.0.84\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.0.84\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-12-13T19:28:57Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec 13 19:29:02.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 replace -f - --namespace=kubectl-6427'
Dec 13 19:29:02.541: INFO: stderr: ""
Dec 13 19:29:02.541: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Dec 13 19:29:02.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 delete pods e2e-test-httpd-pod --namespace=kubectl-6427'
Dec 13 19:29:04.268: INFO: stderr: ""
Dec 13 19:29:04.268: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:29:04.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6427" for this suite.
Dec 13 19:29:10.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:29:10.370: INFO: namespace kubectl-6427 deletion completed in 6.095818815s

• [SLOW TEST:13.367 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:29:10.371: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-774
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 13 19:29:10.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-774'
Dec 13 19:29:10.578: INFO: stderr: ""
Dec 13 19:29:10.578: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Dec 13 19:29:10.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 delete pods e2e-test-httpd-pod --namespace=kubectl-774'
Dec 13 19:29:25.710: INFO: stderr: ""
Dec 13 19:29:25.710: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:29:25.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-774" for this suite.
Dec 13 19:29:31.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:29:31.799: INFO: namespace kubectl-774 deletion completed in 6.08585426s

• [SLOW TEST:21.428 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:29:31.799: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5176
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 13 19:29:32.317: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 13 19:29:35.332: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:29:35.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5176" for this suite.
Dec 13 19:29:47.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:29:47.456: INFO: namespace webhook-5176 deletion completed in 12.085374775s
STEP: Destroying namespace "webhook-5176-markers" for this suite.
Dec 13 19:29:53.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:29:53.550: INFO: namespace webhook-5176-markers deletion completed in 6.093842264s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.759 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:29:53.558: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4174
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 19:29:53.687: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec 13 19:29:53.694: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 13 19:29:58.696: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 13 19:29:58.696: INFO: Creating deployment "test-rolling-update-deployment"
Dec 13 19:29:58.699: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec 13 19:29:58.703: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec 13 19:30:00.707: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec 13 19:30:00.709: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 13 19:30:00.716: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4174 /apis/apps/v1/namespaces/deployment-4174/deployments/test-rolling-update-deployment 87dd5541-1413-4d63-b392-3882ba5e05d7 11677 1 2019-12-13 19:29:58 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00398a3d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-12-13 19:29:58 +0000 UTC,LastTransitionTime:2019-12-13 19:29:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2019-12-13 19:30:00 +0000 UTC,LastTransitionTime:2019-12-13 19:29:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 13 19:30:00.718: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-4174 /apis/apps/v1/namespaces/deployment-4174/replicasets/test-rolling-update-deployment-55d946486 546bcbb7-70b0-454f-9eb9-37e51b9bd20a 11666 1 2019-12-13 19:29:58 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 87dd5541-1413-4d63-b392-3882ba5e05d7 0xc00398a8c0 0xc00398a8c1}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00398a938 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 13 19:30:00.718: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec 13 19:30:00.718: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4174 /apis/apps/v1/namespaces/deployment-4174/replicasets/test-rolling-update-controller 7bdd7ef4-5e11-4219-97d9-8781f7703fda 11675 2 2019-12-13 19:29:53 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 87dd5541-1413-4d63-b392-3882ba5e05d7 0xc00398a7f7 0xc00398a7f8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00398a858 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 13 19:30:00.720: INFO: Pod "test-rolling-update-deployment-55d946486-7w7kb" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-7w7kb test-rolling-update-deployment-55d946486- deployment-4174 /api/v1/namespaces/deployment-4174/pods/test-rolling-update-deployment-55d946486-7w7kb c1407f8e-826b-4a34-af31-67fd295854bf 11665 0 2019-12-13 19:29:58 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[cni.projectcalico.org/podIP:10.244.0.89/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 546bcbb7-70b0-454f-9eb9-37e51b9bd20a 0xc00398add0 0xc00398add1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lkf65,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lkf65,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lkf65,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 19:29:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 19:30:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 19:30:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 19:29:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.2,PodIP:10.244.0.89,StartTime:2019-12-13 19:29:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-13 19:29:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://032a4146f39bc91a17fadde50ec8cb39c01ccce9e6e4c1d9e50ba584a457865b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:30:00.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4174" for this suite.
Dec 13 19:30:06.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:30:06.799: INFO: namespace deployment-4174 deletion completed in 6.076033416s

• [SLOW TEST:13.241 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:30:06.799: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6319
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 13 19:30:06.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-6319'
Dec 13 19:30:07.027: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 13 19:30:07.027: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Dec 13 19:30:07.044: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-6bj7k]
Dec 13 19:30:07.044: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-6bj7k" in namespace "kubectl-6319" to be "running and ready"
Dec 13 19:30:07.060: INFO: Pod "e2e-test-httpd-rc-6bj7k": Phase="Pending", Reason="", readiness=false. Elapsed: 16.642773ms
Dec 13 19:30:09.062: INFO: Pod "e2e-test-httpd-rc-6bj7k": Phase="Running", Reason="", readiness=true. Elapsed: 2.018479256s
Dec 13 19:30:09.062: INFO: Pod "e2e-test-httpd-rc-6bj7k" satisfied condition "running and ready"
Dec 13 19:30:09.062: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-6bj7k]
Dec 13 19:30:09.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 logs rc/e2e-test-httpd-rc --namespace=kubectl-6319'
Dec 13 19:30:09.138: INFO: stderr: ""
Dec 13 19:30:09.138: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.244.0.90. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.244.0.90. Set the 'ServerName' directive globally to suppress this message\n[Fri Dec 13 19:30:07.723873 2019] [mpm_event:notice] [pid 1:tid 140240019389288] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Fri Dec 13 19:30:07.723914 2019] [core:notice] [pid 1:tid 140240019389288] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Dec 13 19:30:09.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 delete rc e2e-test-httpd-rc --namespace=kubectl-6319'
Dec 13 19:30:09.206: INFO: stderr: ""
Dec 13 19:30:09.206: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:30:09.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6319" for this suite.
Dec 13 19:30:15.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:30:15.299: INFO: namespace kubectl-6319 deletion completed in 6.087864661s

• [SLOW TEST:8.500 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:30:15.299: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6889
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec 13 19:30:15.436: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:30:17.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6889" for this suite.
Dec 13 19:30:23.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:30:23.535: INFO: namespace init-container-6889 deletion completed in 6.121262469s

• [SLOW TEST:8.236 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:30:23.535: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9223
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 19:30:23.697: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Dec 13 19:30:26.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 --namespace=crd-publish-openapi-9223 create -f -'
Dec 13 19:30:27.455: INFO: stderr: ""
Dec 13 19:30:27.455: INFO: stdout: "e2e-test-crd-publish-openapi-5719-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 13 19:30:27.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 --namespace=crd-publish-openapi-9223 delete e2e-test-crd-publish-openapi-5719-crds test-foo'
Dec 13 19:30:27.517: INFO: stderr: ""
Dec 13 19:30:27.517: INFO: stdout: "e2e-test-crd-publish-openapi-5719-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Dec 13 19:30:27.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 --namespace=crd-publish-openapi-9223 apply -f -'
Dec 13 19:30:27.664: INFO: stderr: ""
Dec 13 19:30:27.664: INFO: stdout: "e2e-test-crd-publish-openapi-5719-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 13 19:30:27.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 --namespace=crd-publish-openapi-9223 delete e2e-test-crd-publish-openapi-5719-crds test-foo'
Dec 13 19:30:27.745: INFO: stderr: ""
Dec 13 19:30:27.745: INFO: stdout: "e2e-test-crd-publish-openapi-5719-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Dec 13 19:30:27.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 --namespace=crd-publish-openapi-9223 create -f -'
Dec 13 19:30:27.904: INFO: rc: 1
Dec 13 19:30:27.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 --namespace=crd-publish-openapi-9223 apply -f -'
Dec 13 19:30:28.084: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Dec 13 19:30:28.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 --namespace=crd-publish-openapi-9223 create -f -'
Dec 13 19:30:28.209: INFO: rc: 1
Dec 13 19:30:28.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 --namespace=crd-publish-openapi-9223 apply -f -'
Dec 13 19:30:28.338: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Dec 13 19:30:28.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 explain e2e-test-crd-publish-openapi-5719-crds'
Dec 13 19:30:28.495: INFO: stderr: ""
Dec 13 19:30:28.495: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5719-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Dec 13 19:30:28.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 explain e2e-test-crd-publish-openapi-5719-crds.metadata'
Dec 13 19:30:28.659: INFO: stderr: ""
Dec 13 19:30:28.659: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5719-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Dec 13 19:30:28.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 explain e2e-test-crd-publish-openapi-5719-crds.spec'
Dec 13 19:30:28.864: INFO: stderr: ""
Dec 13 19:30:28.864: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5719-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Dec 13 19:30:28.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 explain e2e-test-crd-publish-openapi-5719-crds.spec.bars'
Dec 13 19:30:29.029: INFO: stderr: ""
Dec 13 19:30:29.029: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5719-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Dec 13 19:30:29.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 explain e2e-test-crd-publish-openapi-5719-crds.spec.bars2'
Dec 13 19:30:29.185: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:30:32.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9223" for this suite.
Dec 13 19:30:38.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:30:38.790: INFO: namespace crd-publish-openapi-9223 deletion completed in 6.083676483s

• [SLOW TEST:15.255 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:30:38.790: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9247
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-866c09f9-308b-47a0-bd55-fc3dcb936b2d
STEP: Creating a pod to test consume secrets
Dec 13 19:30:38.931: INFO: Waiting up to 5m0s for pod "pod-secrets-95acc6a8-5385-475d-b7b3-8ab993d6aea1" in namespace "secrets-9247" to be "success or failure"
Dec 13 19:30:38.941: INFO: Pod "pod-secrets-95acc6a8-5385-475d-b7b3-8ab993d6aea1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.557745ms
Dec 13 19:30:40.944: INFO: Pod "pod-secrets-95acc6a8-5385-475d-b7b3-8ab993d6aea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012655186s
STEP: Saw pod success
Dec 13 19:30:40.944: INFO: Pod "pod-secrets-95acc6a8-5385-475d-b7b3-8ab993d6aea1" satisfied condition "success or failure"
Dec 13 19:30:40.945: INFO: Trying to get logs from node master-0 pod pod-secrets-95acc6a8-5385-475d-b7b3-8ab993d6aea1 container secret-volume-test: <nil>
STEP: delete the pod
Dec 13 19:30:40.964: INFO: Waiting for pod pod-secrets-95acc6a8-5385-475d-b7b3-8ab993d6aea1 to disappear
Dec 13 19:30:40.966: INFO: Pod pod-secrets-95acc6a8-5385-475d-b7b3-8ab993d6aea1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:30:40.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9247" for this suite.
Dec 13 19:30:46.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:30:47.053: INFO: namespace secrets-9247 deletion completed in 6.084893733s

• [SLOW TEST:8.263 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:30:47.054: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9382
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:31:04.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9382" for this suite.
Dec 13 19:31:10.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:31:10.308: INFO: namespace resourcequota-9382 deletion completed in 6.087667931s

• [SLOW TEST:23.255 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:31:10.309: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5482
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-20032dc4-3fd2-4fde-b612-2bd654172dc7 in namespace container-probe-5482
Dec 13 19:31:12.453: INFO: Started pod test-webserver-20032dc4-3fd2-4fde-b612-2bd654172dc7 in namespace container-probe-5482
STEP: checking the pod's current state and verifying that restartCount is present
Dec 13 19:31:12.454: INFO: Initial restart count of pod test-webserver-20032dc4-3fd2-4fde-b612-2bd654172dc7 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:35:12.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5482" for this suite.
Dec 13 19:35:18.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:35:18.910: INFO: namespace container-probe-5482 deletion completed in 6.089663801s

• [SLOW TEST:248.601 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:35:18.911: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6497
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-6497
STEP: creating replication controller nodeport-test in namespace services-6497
I1213 19:35:19.063923      24 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-6497, replica count: 2
I1213 19:35:22.122145      24 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 13 19:35:22.122: INFO: Creating new exec pod
Dec 13 19:35:25.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=services-6497 execpod6dxsm -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Dec 13 19:35:25.277: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec 13 19:35:25.277: INFO: stdout: ""
Dec 13 19:35:25.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=services-6497 execpod6dxsm -- /bin/sh -x -c nc -zv -t -w 2 10.100.175.227 80'
Dec 13 19:35:25.409: INFO: stderr: "+ nc -zv -t -w 2 10.100.175.227 80\nConnection to 10.100.175.227 80 port [tcp/http] succeeded!\n"
Dec 13 19:35:25.409: INFO: stdout: ""
Dec 13 19:35:25.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=services-6497 execpod6dxsm -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.2 31898'
Dec 13 19:35:25.540: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.2 31898\nConnection to 192.168.122.2 31898 port [tcp/31898] succeeded!\n"
Dec 13 19:35:25.540: INFO: stdout: ""
Dec 13 19:35:25.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=services-6497 execpod6dxsm -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.3 31898'
Dec 13 19:35:25.659: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.3 31898\nConnection to 192.168.122.3 31898 port [tcp/31898] succeeded!\n"
Dec 13 19:35:25.659: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:35:25.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6497" for this suite.
Dec 13 19:35:31.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:35:31.780: INFO: namespace services-6497 deletion completed in 6.118363283s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:12.869 seconds]
[sig-network] Services
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:35:31.780: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8289
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1213 19:36:01.989302      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 13 19:36:01.989: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:36:01.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8289" for this suite.
Dec 13 19:36:07.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:36:08.066: INFO: namespace gc-8289 deletion completed in 6.074371425s

• [SLOW TEST:36.285 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:36:08.066: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3170
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 13 19:36:08.209: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f449f47c-4e77-4754-a429-210a921bb5c2" in namespace "downward-api-3170" to be "success or failure"
Dec 13 19:36:08.215: INFO: Pod "downwardapi-volume-f449f47c-4e77-4754-a429-210a921bb5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.079688ms
Dec 13 19:36:10.217: INFO: Pod "downwardapi-volume-f449f47c-4e77-4754-a429-210a921bb5c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008484582s
STEP: Saw pod success
Dec 13 19:36:10.217: INFO: Pod "downwardapi-volume-f449f47c-4e77-4754-a429-210a921bb5c2" satisfied condition "success or failure"
Dec 13 19:36:10.219: INFO: Trying to get logs from node master-0 pod downwardapi-volume-f449f47c-4e77-4754-a429-210a921bb5c2 container client-container: <nil>
STEP: delete the pod
Dec 13 19:36:10.243: INFO: Waiting for pod downwardapi-volume-f449f47c-4e77-4754-a429-210a921bb5c2 to disappear
Dec 13 19:36:10.252: INFO: Pod downwardapi-volume-f449f47c-4e77-4754-a429-210a921bb5c2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:36:10.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3170" for this suite.
Dec 13 19:36:16.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:36:16.346: INFO: namespace downward-api-3170 deletion completed in 6.091879739s

• [SLOW TEST:8.280 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:36:16.346: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6909
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:36:23.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6909" for this suite.
Dec 13 19:36:29.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:36:29.584: INFO: namespace resourcequota-6909 deletion completed in 6.092287234s

• [SLOW TEST:13.238 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:36:29.585: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2446
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2446.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2446.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2446.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2446.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2446.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2446.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2446.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2446.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2446.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2446.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 13 19:36:31.755: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:31.758: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:31.761: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:31.763: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:31.770: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:31.772: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:31.774: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:31.776: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:31.781: INFO: Lookups using dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2446.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2446.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local jessie_udp@dns-test-service-2.dns-2446.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2446.svc.cluster.local]

Dec 13 19:36:36.784: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:36.787: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:36.789: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:36.791: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:36.798: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:36.801: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:36.803: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:36.805: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:36.810: INFO: Lookups using dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2446.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2446.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local jessie_udp@dns-test-service-2.dns-2446.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2446.svc.cluster.local]

Dec 13 19:36:41.784: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:41.787: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:41.789: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:41.791: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:41.797: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:41.799: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:41.802: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:41.804: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:41.808: INFO: Lookups using dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2446.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2446.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local jessie_udp@dns-test-service-2.dns-2446.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2446.svc.cluster.local]

Dec 13 19:36:46.785: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:46.789: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:46.793: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:46.796: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:46.806: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:46.808: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:46.810: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:46.812: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:46.816: INFO: Lookups using dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2446.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2446.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local jessie_udp@dns-test-service-2.dns-2446.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2446.svc.cluster.local]

Dec 13 19:36:51.784: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:51.786: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:51.788: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:51.791: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:51.797: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:51.800: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:51.802: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:51.805: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:51.809: INFO: Lookups using dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2446.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2446.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local jessie_udp@dns-test-service-2.dns-2446.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2446.svc.cluster.local]

Dec 13 19:36:56.789: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:56.793: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:56.796: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:56.800: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:56.810: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:56.815: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:56.818: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:56.824: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2446.svc.cluster.local from pod dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960: the server could not find the requested resource (get pods dns-test-d931b425-ad83-4fad-af01-807477c7c960)
Dec 13 19:36:56.832: INFO: Lookups using dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2446.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2446.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2446.svc.cluster.local jessie_udp@dns-test-service-2.dns-2446.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2446.svc.cluster.local]

Dec 13 19:37:01.810: INFO: DNS probes using dns-2446/dns-test-d931b425-ad83-4fad-af01-807477c7c960 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:37:01.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2446" for this suite.
Dec 13 19:37:07.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:37:07.991: INFO: namespace dns-2446 deletion completed in 6.121122503s

• [SLOW TEST:38.406 seconds]
[sig-network] DNS
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:37:07.991: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-218
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec 13 19:37:11.161: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:37:12.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-218" for this suite.
Dec 13 19:37:40.186: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:37:40.263: INFO: namespace replicaset-218 deletion completed in 28.087917567s

• [SLOW TEST:32.272 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:37:40.263: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7998
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:37:42.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7998" for this suite.
Dec 13 19:38:26.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:38:26.642: INFO: namespace kubelet-test-7998 deletion completed in 44.183219866s

• [SLOW TEST:46.378 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:38:26.642: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-530
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Dec 13 19:38:28.844: INFO: Pod pod-hostip-060bdc33-8e01-46a5-bae8-1874702a1763 has hostIP: 192.168.122.2
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:38:28.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-530" for this suite.
Dec 13 19:38:40.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:38:40.935: INFO: namespace pods-530 deletion completed in 12.088144917s

• [SLOW TEST:14.293 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:38:40.935: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6771
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 13 19:38:41.432: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 13 19:38:44.449: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:38:54.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6771" for this suite.
Dec 13 19:39:00.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:39:00.749: INFO: namespace webhook-6771 deletion completed in 6.109590407s
STEP: Destroying namespace "webhook-6771-markers" for this suite.
Dec 13 19:39:06.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:39:06.825: INFO: namespace webhook-6771-markers deletion completed in 6.075540598s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:25.897 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:39:06.832: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8622
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-7eee2a74-8dc5-4278-a91b-a49c865abaaa
STEP: Creating a pod to test consume secrets
Dec 13 19:39:06.973: INFO: Waiting up to 5m0s for pod "pod-secrets-80fae582-ace9-4db1-ad5a-9976e53cd209" in namespace "secrets-8622" to be "success or failure"
Dec 13 19:39:06.980: INFO: Pod "pod-secrets-80fae582-ace9-4db1-ad5a-9976e53cd209": Phase="Pending", Reason="", readiness=false. Elapsed: 7.812298ms
Dec 13 19:39:08.983: INFO: Pod "pod-secrets-80fae582-ace9-4db1-ad5a-9976e53cd209": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010161938s
STEP: Saw pod success
Dec 13 19:39:08.983: INFO: Pod "pod-secrets-80fae582-ace9-4db1-ad5a-9976e53cd209" satisfied condition "success or failure"
Dec 13 19:39:08.985: INFO: Trying to get logs from node master-0 pod pod-secrets-80fae582-ace9-4db1-ad5a-9976e53cd209 container secret-volume-test: <nil>
STEP: delete the pod
Dec 13 19:39:09.002: INFO: Waiting for pod pod-secrets-80fae582-ace9-4db1-ad5a-9976e53cd209 to disappear
Dec 13 19:39:09.006: INFO: Pod pod-secrets-80fae582-ace9-4db1-ad5a-9976e53cd209 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:39:09.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8622" for this suite.
Dec 13 19:39:15.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:39:15.100: INFO: namespace secrets-8622 deletion completed in 6.090877258s

• [SLOW TEST:8.269 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:39:15.101: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-8186
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 13 19:39:15.529: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 13 19:39:18.548: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 19:39:18.550: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:39:19.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8186" for this suite.
Dec 13 19:39:25.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:39:25.810: INFO: namespace crd-webhook-8186 deletion completed in 6.102499872s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:10.717 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:39:25.818: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8820
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-mzxg
STEP: Creating a pod to test atomic-volume-subpath
Dec 13 19:39:25.963: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mzxg" in namespace "subpath-8820" to be "success or failure"
Dec 13 19:39:25.971: INFO: Pod "pod-subpath-test-configmap-mzxg": Phase="Pending", Reason="", readiness=false. Elapsed: 7.813017ms
Dec 13 19:39:27.973: INFO: Pod "pod-subpath-test-configmap-mzxg": Phase="Running", Reason="", readiness=true. Elapsed: 2.009885608s
Dec 13 19:39:29.975: INFO: Pod "pod-subpath-test-configmap-mzxg": Phase="Running", Reason="", readiness=true. Elapsed: 4.012000306s
Dec 13 19:39:31.977: INFO: Pod "pod-subpath-test-configmap-mzxg": Phase="Running", Reason="", readiness=true. Elapsed: 6.014255705s
Dec 13 19:39:33.979: INFO: Pod "pod-subpath-test-configmap-mzxg": Phase="Running", Reason="", readiness=true. Elapsed: 8.016175904s
Dec 13 19:39:35.982: INFO: Pod "pod-subpath-test-configmap-mzxg": Phase="Running", Reason="", readiness=true. Elapsed: 10.018981873s
Dec 13 19:39:37.984: INFO: Pod "pod-subpath-test-configmap-mzxg": Phase="Running", Reason="", readiness=true. Elapsed: 12.021319431s
Dec 13 19:39:39.986: INFO: Pod "pod-subpath-test-configmap-mzxg": Phase="Running", Reason="", readiness=true. Elapsed: 14.023097376s
Dec 13 19:39:41.989: INFO: Pod "pod-subpath-test-configmap-mzxg": Phase="Running", Reason="", readiness=true. Elapsed: 16.025558549s
Dec 13 19:39:43.991: INFO: Pod "pod-subpath-test-configmap-mzxg": Phase="Running", Reason="", readiness=true. Elapsed: 18.02754634s
Dec 13 19:39:45.993: INFO: Pod "pod-subpath-test-configmap-mzxg": Phase="Running", Reason="", readiness=true. Elapsed: 20.02968232s
Dec 13 19:39:47.995: INFO: Pod "pod-subpath-test-configmap-mzxg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.03188733s
STEP: Saw pod success
Dec 13 19:39:47.995: INFO: Pod "pod-subpath-test-configmap-mzxg" satisfied condition "success or failure"
Dec 13 19:39:47.997: INFO: Trying to get logs from node master-0 pod pod-subpath-test-configmap-mzxg container test-container-subpath-configmap-mzxg: <nil>
STEP: delete the pod
Dec 13 19:39:48.017: INFO: Waiting for pod pod-subpath-test-configmap-mzxg to disappear
Dec 13 19:39:48.020: INFO: Pod pod-subpath-test-configmap-mzxg no longer exists
STEP: Deleting pod pod-subpath-test-configmap-mzxg
Dec 13 19:39:48.020: INFO: Deleting pod "pod-subpath-test-configmap-mzxg" in namespace "subpath-8820"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:39:48.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8820" for this suite.
Dec 13 19:39:54.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:39:54.110: INFO: namespace subpath-8820 deletion completed in 6.08434306s

• [SLOW TEST:28.292 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:39:54.110: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3992
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-324cc0e8-4b8b-42ff-9abd-d2071c5f6231
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-324cc0e8-4b8b-42ff-9abd-d2071c5f6231
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:39:58.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3992" for this suite.
Dec 13 19:40:22.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:40:22.509: INFO: namespace configmap-3992 deletion completed in 24.086498819s

• [SLOW TEST:28.399 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:40:22.509: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-7110
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Dec 13 19:40:22.640: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 13 19:41:22.672: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 19:41:22.674: INFO: Starting informer...
STEP: Starting pod...
Dec 13 19:41:22.885: INFO: Pod is running on master-0. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Dec 13 19:41:22.906: INFO: Pod wasn't evicted. Proceeding
Dec 13 19:41:22.906: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Dec 13 19:42:38.132: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:42:38.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-7110" for this suite.
Dec 13 19:43:06.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:43:06.431: INFO: namespace taint-single-pod-7110 deletion completed in 28.181326675s

• [SLOW TEST:163.921 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:43:06.431: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6593
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 19:43:06.677: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:43:13.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6593" for this suite.
Dec 13 19:43:19.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:43:19.508: INFO: namespace custom-resource-definition-6593 deletion completed in 6.096215486s

• [SLOW TEST:13.076 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:43:19.508: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4078
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 19:43:19.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 create -f - --namespace=kubectl-4078'
Dec 13 19:43:20.079: INFO: stderr: ""
Dec 13 19:43:20.079: INFO: stdout: "replicationcontroller/redis-master created\n"
Dec 13 19:43:20.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 create -f - --namespace=kubectl-4078'
Dec 13 19:43:20.263: INFO: stderr: ""
Dec 13 19:43:20.263: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 13 19:43:21.266: INFO: Selector matched 1 pods for map[app:redis]
Dec 13 19:43:21.266: INFO: Found 1 / 1
Dec 13 19:43:21.266: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 13 19:43:21.268: INFO: Selector matched 1 pods for map[app:redis]
Dec 13 19:43:21.268: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 13 19:43:21.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 describe pod redis-master-p86kg --namespace=kubectl-4078'
Dec 13 19:43:21.339: INFO: stderr: ""
Dec 13 19:43:21.339: INFO: stdout: "Name:         redis-master-p86kg\nNamespace:    kubectl-4078\nPriority:     0\nNode:         master-0/192.168.122.2\nStart Time:   Fri, 13 Dec 2019 19:43:20 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 10.244.0.124/32\nStatus:       Running\nIP:           10.244.0.124\nIPs:\n  IP:           10.244.0.124\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   cri-o://7240c25e2e23a95f2624597768b467bab1bf21b1c54c4dc1de1aed0d3838313c\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 13 Dec 2019 19:43:20 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-wvq6d (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-wvq6d:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-wvq6d\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-4078/redis-master-p86kg to master-0\n  Normal  Pulled     1s    kubelet, master-0  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    1s    kubelet, master-0  Created container redis-master\n  Normal  Started    1s    kubelet, master-0  Started container redis-master\n"
Dec 13 19:43:21.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 describe rc redis-master --namespace=kubectl-4078'
Dec 13 19:43:21.413: INFO: stderr: ""
Dec 13 19:43:21.413: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-4078\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: redis-master-p86kg\n"
Dec 13 19:43:21.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 describe service redis-master --namespace=kubectl-4078'
Dec 13 19:43:21.476: INFO: stderr: ""
Dec 13 19:43:21.476: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-4078\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.108.59.238\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.244.0.124:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec 13 19:43:21.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 describe node master-0'
Dec 13 19:43:21.578: INFO: stderr: ""
Dec 13 19:43:21.578: INFO: stdout: "Name:               master-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    katacontainers.io/kata-runtime=true\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master-0\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"rook-ceph.cephfs.csi.ceph.com\":\"master-0\",\"rook-ceph.rbd.csi.ceph.com\":\"master-0\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"c2:25:2e:cd:17:df\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.122.2\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 13 Dec 2019 18:54:10 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Fri, 13 Dec 2019 19:42:25 +0000   Fri, 13 Dec 2019 18:54:07 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 13 Dec 2019 19:42:25 +0000   Fri, 13 Dec 2019 18:54:07 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 13 Dec 2019 19:42:25 +0000   Fri, 13 Dec 2019 18:54:07 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 13 Dec 2019 19:42:25 +0000   Fri, 13 Dec 2019 18:55:51 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.122.2\n  Hostname:    master-0\nCapacity:\n cpu:                8\n ephemeral-storage:  28840928Ki\n hugepages-2Mi:      0\n memory:             16396928Ki\n pods:               110\nAllocatable:\n cpu:                7\n ephemeral-storage:  26579799201\n hugepages-2Mi:      0\n memory:             15794528Ki\n pods:               110\nSystem Info:\n Machine ID:                 0fcc750bdc5740a98e2bc7f6c65822fd\n System UUID:                0fcc750b-dc57-40a9-8e2b-c7f6c65822fd\n Boot ID:                    ad1a78da-0201-4407-aa44-e154006bb192\n Kernel Version:             5.3.14-408.kvm\n OS Image:                   Clear Linux OS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  cri-o://1.16.1\n Kubelet Version:            v1.16.3\n Kube-Proxy Version:         v1.16.3\nPodCIDR:                     10.244.0.0/24\nPodCIDRs:                    10.244.0.0/24\nNon-terminated Pods:         (27 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                canal-xbp9c                                                250m (3%)     0 (0%)      0 (0%)           0 (0%)         48m\n  kube-system                elasticsearch-logging-0                                    100m (1%)     1 (14%)     0 (0%)           0 (0%)         66s\n  kube-system                etcd-master-0                                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         48m\n  kube-system                fluentd-es-v2.5.2-v86nv                                    100m (1%)     0 (0%)      200Mi (1%)       500Mi (3%)     85s\n  kube-system                kata-deploy-6dgtq                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         78s\n  kube-system                kibana-logging-7f6b4b96b4-q6qjw                            100m (1%)     1 (14%)     0 (0%)           0 (0%)         118s\n  kube-system                kube-apiserver-master-0                                    250m (3%)     0 (0%)      0 (0%)           0 (0%)         47m\n  kube-system                kube-controller-manager-master-0                           200m (2%)     0 (0%)      0 (0%)           0 (0%)         48m\n  kube-system                kube-proxy-dfvlw                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         48m\n  kube-system                kube-scheduler-master-0                                    100m (1%)     0 (0%)      0 (0%)           0 (0%)         47m\n  kubectl-4078               redis-master-p86kg                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         1s\n  kubernetes-dashboard       dashboard-metrics-scraper-6ccf7f6cc8-8lv27                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         118s\n  monitoring                 alertmanager-main-1                                        100m (1%)     100m (1%)   225Mi (1%)       25Mi (0%)      93s\n  monitoring                 kube-state-metrics-79d4b9b497-xf569                        120m (1%)     140m (2%)   190Mi (1%)       230Mi (1%)     118s\n  monitoring                 node-exporter-tk6xn                                        112m (1%)     270m (3%)   200Mi (1%)       240Mi (1%)     38m\n  monitoring                 prometheus-k8s-0                                           200m (2%)     200m (2%)   450Mi (2%)       50Mi (0%)      84s\n  monitoring                 prometheus-operator-7559d67ff-wdtkm                        100m (1%)     200m (2%)   100Mi (0%)       200Mi (1%)     118s\n  rook-ceph                  csi-cephfsplugin-kpth2                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         65s\n  rook-ceph                  csi-cephfsplugin-provisioner-75c965db4f-lg6nr              0 (0%)        0 (0%)      0 (0%)           0 (0%)         118s\n  rook-ceph                  csi-rbdplugin-dnzpm                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         96s\n  rook-ceph                  csi-rbdplugin-provisioner-56cbc4d585-np67m                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         118s\n  rook-ceph                  rook-ceph-mgr-a-5775778c4d-x4pkz                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         118s\n  rook-ceph                  rook-ceph-mon-b-5b988bf97b-8z66h                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         118s\n  rook-ceph                  rook-ceph-osd-2-58969bddc7-87hzv                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         118s\n  rook-ceph                  rook-discover-4vb5s                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         94s\n  sonobuoy                   sonobuoy-e2e-job-8f6bded5e69344cd                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-9616c2f784dc4b45-rxl5l    0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1732m (24%)  2910m (41%)\n  memory             1365Mi (8%)  1245Mi (8%)\n  ephemeral-storage  0 (0%)       0 (0%)\nEvents:\n  Type    Reason                   Age                From                  Message\n  ----    ------                   ----               ----                  -------\n  Normal  Starting                 49m                kubelet, master-0     Starting kubelet.\n  Normal  NodeAllocatableEnforced  49m                kubelet, master-0     Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  49m (x8 over 49m)  kubelet, master-0     Node master-0 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    49m (x8 over 49m)  kubelet, master-0     Node master-0 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     49m (x7 over 49m)  kubelet, master-0     Node master-0 status is now: NodeHasSufficientPID\n  Normal  Starting                 48m                kube-proxy, master-0  Starting kube-proxy.\n"
Dec 13 19:43:21.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 describe namespace kubectl-4078'
Dec 13 19:43:21.655: INFO: stderr: ""
Dec 13 19:43:21.655: INFO: stdout: "Name:         kubectl-4078\nLabels:       e2e-framework=kubectl\n              e2e-run=d8a556ce-920b-4c76-b590-1b04889e3a79\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:43:21.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4078" for this suite.
Dec 13 19:43:49.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:43:49.758: INFO: namespace kubectl-4078 deletion completed in 28.100284766s

• [SLOW TEST:30.250 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:43:49.758: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9403
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-84c3ad94-6ac3-45ab-9b22-2f82dbdc3a69
STEP: Creating configMap with name cm-test-opt-upd-25b76dd8-fb19-4c0a-ae96-c8e3a16f41df
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-84c3ad94-6ac3-45ab-9b22-2f82dbdc3a69
STEP: Updating configmap cm-test-opt-upd-25b76dd8-fb19-4c0a-ae96-c8e3a16f41df
STEP: Creating configMap with name cm-test-opt-create-ab7deaea-6c8e-444b-8625-7d78a6a29bad
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:43:53.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9403" for this suite.
Dec 13 19:44:07.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:44:08.042: INFO: namespace configmap-9403 deletion completed in 14.075126537s

• [SLOW TEST:18.284 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:44:08.042: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8544
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-826a67a1-3f0b-4279-b346-ca4c1ff67583
STEP: Creating a pod to test consume secrets
Dec 13 19:44:08.213: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0cfb5a6d-6f99-46e0-8023-c0690086c153" in namespace "projected-8544" to be "success or failure"
Dec 13 19:44:08.224: INFO: Pod "pod-projected-secrets-0cfb5a6d-6f99-46e0-8023-c0690086c153": Phase="Pending", Reason="", readiness=false. Elapsed: 10.842121ms
Dec 13 19:44:10.226: INFO: Pod "pod-projected-secrets-0cfb5a6d-6f99-46e0-8023-c0690086c153": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012997104s
Dec 13 19:44:12.229: INFO: Pod "pod-projected-secrets-0cfb5a6d-6f99-46e0-8023-c0690086c153": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015571649s
STEP: Saw pod success
Dec 13 19:44:12.229: INFO: Pod "pod-projected-secrets-0cfb5a6d-6f99-46e0-8023-c0690086c153" satisfied condition "success or failure"
Dec 13 19:44:12.231: INFO: Trying to get logs from node master-0 pod pod-projected-secrets-0cfb5a6d-6f99-46e0-8023-c0690086c153 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 13 19:44:12.257: INFO: Waiting for pod pod-projected-secrets-0cfb5a6d-6f99-46e0-8023-c0690086c153 to disappear
Dec 13 19:44:12.261: INFO: Pod pod-projected-secrets-0cfb5a6d-6f99-46e0-8023-c0690086c153 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:44:12.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8544" for this suite.
Dec 13 19:44:18.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:44:18.373: INFO: namespace projected-8544 deletion completed in 6.108150854s

• [SLOW TEST:10.331 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:44:18.373: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1323
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1997
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8659
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:44:24.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1323" for this suite.
Dec 13 19:44:30.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:44:30.910: INFO: namespace namespaces-1323 deletion completed in 6.10258889s
STEP: Destroying namespace "nsdeletetest-1997" for this suite.
Dec 13 19:44:30.912: INFO: Namespace nsdeletetest-1997 was already deleted
STEP: Destroying namespace "nsdeletetest-8659" for this suite.
Dec 13 19:44:36.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:44:37.001: INFO: namespace nsdeletetest-8659 deletion completed in 6.089097337s

• [SLOW TEST:18.628 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:44:37.001: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1080
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-0933cbf8-950a-4e86-9f63-ca6aa478a540
STEP: Creating a pod to test consume secrets
Dec 13 19:44:37.189: INFO: Waiting up to 5m0s for pod "pod-secrets-168acc62-866a-44d1-920b-27c74ca935e1" in namespace "secrets-1080" to be "success or failure"
Dec 13 19:44:37.200: INFO: Pod "pod-secrets-168acc62-866a-44d1-920b-27c74ca935e1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.201872ms
Dec 13 19:44:39.202: INFO: Pod "pod-secrets-168acc62-866a-44d1-920b-27c74ca935e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013434953s
STEP: Saw pod success
Dec 13 19:44:39.202: INFO: Pod "pod-secrets-168acc62-866a-44d1-920b-27c74ca935e1" satisfied condition "success or failure"
Dec 13 19:44:39.204: INFO: Trying to get logs from node master-0 pod pod-secrets-168acc62-866a-44d1-920b-27c74ca935e1 container secret-volume-test: <nil>
STEP: delete the pod
Dec 13 19:44:39.225: INFO: Waiting for pod pod-secrets-168acc62-866a-44d1-920b-27c74ca935e1 to disappear
Dec 13 19:44:39.227: INFO: Pod pod-secrets-168acc62-866a-44d1-920b-27c74ca935e1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:44:39.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1080" for this suite.
Dec 13 19:44:45.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:44:45.318: INFO: namespace secrets-1080 deletion completed in 6.089094667s

• [SLOW TEST:8.317 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:44:45.318: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9690
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Dec 13 19:44:45.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 create -f - --namespace=kubectl-9690'
Dec 13 19:44:45.641: INFO: stderr: ""
Dec 13 19:44:45.641: INFO: stdout: "pod/pause created\n"
Dec 13 19:44:45.641: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec 13 19:44:45.642: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9690" to be "running and ready"
Dec 13 19:44:45.644: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.400127ms
Dec 13 19:44:47.646: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.004618247s
Dec 13 19:44:47.646: INFO: Pod "pause" satisfied condition "running and ready"
Dec 13 19:44:47.646: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Dec 13 19:44:47.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 label pods pause testing-label=testing-label-value --namespace=kubectl-9690'
Dec 13 19:44:47.759: INFO: stderr: ""
Dec 13 19:44:47.759: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec 13 19:44:47.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pod pause -L testing-label --namespace=kubectl-9690'
Dec 13 19:44:47.837: INFO: stderr: ""
Dec 13 19:44:47.837: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec 13 19:44:47.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 label pods pause testing-label- --namespace=kubectl-9690'
Dec 13 19:44:47.910: INFO: stderr: ""
Dec 13 19:44:47.910: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec 13 19:44:47.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pod pause -L testing-label --namespace=kubectl-9690'
Dec 13 19:44:47.993: INFO: stderr: ""
Dec 13 19:44:47.993: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Dec 13 19:44:47.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 delete --grace-period=0 --force -f - --namespace=kubectl-9690'
Dec 13 19:44:48.069: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 13 19:44:48.069: INFO: stdout: "pod \"pause\" force deleted\n"
Dec 13 19:44:48.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get rc,svc -l name=pause --no-headers --namespace=kubectl-9690'
Dec 13 19:44:48.161: INFO: stderr: "No resources found in kubectl-9690 namespace.\n"
Dec 13 19:44:48.161: INFO: stdout: ""
Dec 13 19:44:48.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods -l name=pause --namespace=kubectl-9690 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 13 19:44:48.239: INFO: stderr: ""
Dec 13 19:44:48.239: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:44:48.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9690" for this suite.
Dec 13 19:44:54.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:44:54.352: INFO: namespace kubectl-9690 deletion completed in 6.109740977s

• [SLOW TEST:9.034 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:44:54.352: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5864
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 13 19:44:54.506: INFO: Waiting up to 5m0s for pod "downward-api-47100f23-67a6-47ea-bd8a-1d64331f20a3" in namespace "downward-api-5864" to be "success or failure"
Dec 13 19:44:54.510: INFO: Pod "downward-api-47100f23-67a6-47ea-bd8a-1d64331f20a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.377297ms
Dec 13 19:44:56.512: INFO: Pod "downward-api-47100f23-67a6-47ea-bd8a-1d64331f20a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006592779s
STEP: Saw pod success
Dec 13 19:44:56.512: INFO: Pod "downward-api-47100f23-67a6-47ea-bd8a-1d64331f20a3" satisfied condition "success or failure"
Dec 13 19:44:56.514: INFO: Trying to get logs from node master-0 pod downward-api-47100f23-67a6-47ea-bd8a-1d64331f20a3 container dapi-container: <nil>
STEP: delete the pod
Dec 13 19:44:56.542: INFO: Waiting for pod downward-api-47100f23-67a6-47ea-bd8a-1d64331f20a3 to disappear
Dec 13 19:44:56.551: INFO: Pod downward-api-47100f23-67a6-47ea-bd8a-1d64331f20a3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:44:56.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5864" for this suite.
Dec 13 19:45:02.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:45:02.654: INFO: namespace downward-api-5864 deletion completed in 6.098861889s

• [SLOW TEST:8.302 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:45:02.654: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9145
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 13 19:45:03.062: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 13 19:45:06.076: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:45:06.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9145" for this suite.
Dec 13 19:45:12.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:45:12.196: INFO: namespace webhook-9145 deletion completed in 6.096980409s
STEP: Destroying namespace "webhook-9145-markers" for this suite.
Dec 13 19:45:18.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:45:18.279: INFO: namespace webhook-9145-markers deletion completed in 6.083688688s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:15.635 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:45:18.289: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5708
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 13 19:45:18.427: INFO: Waiting up to 5m0s for pod "downwardapi-volume-43902297-da41-4ecd-bf54-8992c408f8b9" in namespace "downward-api-5708" to be "success or failure"
Dec 13 19:45:18.439: INFO: Pod "downwardapi-volume-43902297-da41-4ecd-bf54-8992c408f8b9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.265178ms
Dec 13 19:45:20.441: INFO: Pod "downwardapi-volume-43902297-da41-4ecd-bf54-8992c408f8b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014721896s
STEP: Saw pod success
Dec 13 19:45:20.441: INFO: Pod "downwardapi-volume-43902297-da41-4ecd-bf54-8992c408f8b9" satisfied condition "success or failure"
Dec 13 19:45:20.443: INFO: Trying to get logs from node master-0 pod downwardapi-volume-43902297-da41-4ecd-bf54-8992c408f8b9 container client-container: <nil>
STEP: delete the pod
Dec 13 19:45:20.460: INFO: Waiting for pod downwardapi-volume-43902297-da41-4ecd-bf54-8992c408f8b9 to disappear
Dec 13 19:45:20.464: INFO: Pod downwardapi-volume-43902297-da41-4ecd-bf54-8992c408f8b9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:45:20.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5708" for this suite.
Dec 13 19:45:26.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:45:26.589: INFO: namespace downward-api-5708 deletion completed in 6.122345718s

• [SLOW TEST:8.300 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:45:26.589: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9231
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 19:45:26.753: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec 13 19:45:31.755: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 13 19:45:31.755: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec 13 19:45:33.757: INFO: Creating deployment "test-rollover-deployment"
Dec 13 19:45:33.763: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec 13 19:45:35.767: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec 13 19:45:35.771: INFO: Ensure that both replica sets have 1 created replica
Dec 13 19:45:35.774: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec 13 19:45:35.779: INFO: Updating deployment test-rollover-deployment
Dec 13 19:45:35.779: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec 13 19:45:37.788: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec 13 19:45:37.793: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec 13 19:45:37.797: INFO: all replica sets need to contain the pod-template-hash label
Dec 13 19:45:37.797: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863133, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863133, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863136, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863133, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 19:45:39.801: INFO: all replica sets need to contain the pod-template-hash label
Dec 13 19:45:39.801: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863133, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863133, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863136, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863133, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 19:45:41.802: INFO: all replica sets need to contain the pod-template-hash label
Dec 13 19:45:41.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863133, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863133, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863136, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863133, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 19:45:43.801: INFO: all replica sets need to contain the pod-template-hash label
Dec 13 19:45:43.801: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863133, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863133, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863136, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863133, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 19:45:45.802: INFO: all replica sets need to contain the pod-template-hash label
Dec 13 19:45:45.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863133, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863133, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863136, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863133, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 19:45:47.802: INFO: 
Dec 13 19:45:47.802: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 13 19:45:47.807: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9231 /apis/apps/v1/namespaces/deployment-9231/deployments/test-rollover-deployment 0553a14b-74a1-40ad-92ee-c8ab34456793 16990 2 2019-12-13 19:45:33 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003274108 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-12-13 19:45:33 +0000 UTC,LastTransitionTime:2019-12-13 19:45:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2019-12-13 19:45:46 +0000 UTC,LastTransitionTime:2019-12-13 19:45:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 13 19:45:47.809: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-9231 /apis/apps/v1/namespaces/deployment-9231/replicasets/test-rollover-deployment-7d7dc6548c a76ca9a0-7b5b-48d1-b183-b1fe1fb81785 16979 2 2019-12-13 19:45:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 0553a14b-74a1-40ad-92ee-c8ab34456793 0xc005ca4107 0xc005ca4108}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005ca4168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 13 19:45:47.809: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec 13 19:45:47.809: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9231 /apis/apps/v1/namespaces/deployment-9231/replicasets/test-rollover-controller 4fcceb1c-4536-44a7-8510-2d72457c3888 16988 2 2019-12-13 19:45:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 0553a14b-74a1-40ad-92ee-c8ab34456793 0xc005ca4037 0xc005ca4038}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005ca4098 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 13 19:45:47.809: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-9231 /apis/apps/v1/namespaces/deployment-9231/replicasets/test-rollover-deployment-f6c94f66c 1ef2cc4b-3e68-455d-862f-7e990f6a4552 16927 2 2019-12-13 19:45:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 0553a14b-74a1-40ad-92ee-c8ab34456793 0xc005ca41d0 0xc005ca41d1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005ca4248 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 13 19:45:47.811: INFO: Pod "test-rollover-deployment-7d7dc6548c-hjzgf" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-hjzgf test-rollover-deployment-7d7dc6548c- deployment-9231 /api/v1/namespaces/deployment-9231/pods/test-rollover-deployment-7d7dc6548c-hjzgf 200436df-5fa8-48ed-a1a6-913d85b52f11 16941 0 2019-12-13 19:45:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[cni.projectcalico.org/podIP:10.244.0.134/32] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c a76ca9a0-7b5b-48d1-b183-b1fe1fb81785 0xc005ca47b7 0xc005ca47b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6m9hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6m9hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6m9hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 19:45:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 19:45:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 19:45:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 19:45:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.2,PodIP:10.244.0.134,StartTime:2019-12-13 19:45:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-13 19:45:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://045b86cf426ef98a4aff3dfc5f833b89eb5cdabecf09467d0162d11a9f3bc041,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.134,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:45:47.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9231" for this suite.
Dec 13 19:45:53.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:45:53.927: INFO: namespace deployment-9231 deletion completed in 6.113254585s

• [SLOW TEST:27.338 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:45:53.927: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7417
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 13 19:45:54.084: INFO: Waiting up to 5m0s for pod "downwardapi-volume-093712bf-83ea-4852-b4ef-20481a9cefb7" in namespace "downward-api-7417" to be "success or failure"
Dec 13 19:45:54.091: INFO: Pod "downwardapi-volume-093712bf-83ea-4852-b4ef-20481a9cefb7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.145943ms
Dec 13 19:45:56.100: INFO: Pod "downwardapi-volume-093712bf-83ea-4852-b4ef-20481a9cefb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015935591s
STEP: Saw pod success
Dec 13 19:45:56.100: INFO: Pod "downwardapi-volume-093712bf-83ea-4852-b4ef-20481a9cefb7" satisfied condition "success or failure"
Dec 13 19:45:56.119: INFO: Trying to get logs from node master-0 pod downwardapi-volume-093712bf-83ea-4852-b4ef-20481a9cefb7 container client-container: <nil>
STEP: delete the pod
Dec 13 19:45:56.166: INFO: Waiting for pod downwardapi-volume-093712bf-83ea-4852-b4ef-20481a9cefb7 to disappear
Dec 13 19:45:56.175: INFO: Pod downwardapi-volume-093712bf-83ea-4852-b4ef-20481a9cefb7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:45:56.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7417" for this suite.
Dec 13 19:46:02.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:46:02.276: INFO: namespace downward-api-7417 deletion completed in 6.094149389s

• [SLOW TEST:8.349 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:46:02.276: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7897
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-7897
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 13 19:46:02.412: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 13 19:46:20.503: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.25:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7897 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 19:46:20.504: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 19:46:20.591: INFO: Found all expected endpoints: [netserver-0]
Dec 13 19:46:20.594: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.136:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7897 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 19:46:20.594: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 19:46:20.680: INFO: Found all expected endpoints: [netserver-1]
Dec 13 19:46:20.682: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.19:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7897 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 19:46:20.682: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 19:46:20.743: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:46:20.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7897" for this suite.
Dec 13 19:46:32.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:46:32.829: INFO: namespace pod-network-test-7897 deletion completed in 12.083278142s

• [SLOW TEST:30.553 seconds]
[sig-network] Networking
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:46:32.829: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4696
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Dec 13 19:46:32.962: INFO: Waiting up to 5m0s for pod "var-expansion-60bfbd3d-6770-45b2-9140-80489ff7cb50" in namespace "var-expansion-4696" to be "success or failure"
Dec 13 19:46:32.966: INFO: Pod "var-expansion-60bfbd3d-6770-45b2-9140-80489ff7cb50": Phase="Pending", Reason="", readiness=false. Elapsed: 3.654848ms
Dec 13 19:46:34.969: INFO: Pod "var-expansion-60bfbd3d-6770-45b2-9140-80489ff7cb50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006550554s
STEP: Saw pod success
Dec 13 19:46:34.969: INFO: Pod "var-expansion-60bfbd3d-6770-45b2-9140-80489ff7cb50" satisfied condition "success or failure"
Dec 13 19:46:34.971: INFO: Trying to get logs from node master-0 pod var-expansion-60bfbd3d-6770-45b2-9140-80489ff7cb50 container dapi-container: <nil>
STEP: delete the pod
Dec 13 19:46:34.990: INFO: Waiting for pod var-expansion-60bfbd3d-6770-45b2-9140-80489ff7cb50 to disappear
Dec 13 19:46:35.005: INFO: Pod var-expansion-60bfbd3d-6770-45b2-9140-80489ff7cb50 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:46:35.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4696" for this suite.
Dec 13 19:46:41.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:46:41.097: INFO: namespace var-expansion-4696 deletion completed in 6.089700178s

• [SLOW TEST:8.268 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:46:41.097: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6617
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:46:43.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6617" for this suite.
Dec 13 19:47:27.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:47:27.348: INFO: namespace kubelet-test-6617 deletion completed in 44.091031001s

• [SLOW TEST:46.251 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:47:27.349: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1861
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 13 19:47:27.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-1861'
Dec 13 19:47:27.557: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 13 19:47:27.557: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Dec 13 19:47:29.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 delete deployment e2e-test-httpd-deployment --namespace=kubectl-1861'
Dec 13 19:47:29.641: INFO: stderr: ""
Dec 13 19:47:29.641: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:47:29.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1861" for this suite.
Dec 13 19:47:35.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:47:35.744: INFO: namespace kubectl-1861 deletion completed in 6.100297296s

• [SLOW TEST:8.395 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:47:35.744: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-5003
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:47:35.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-5003" for this suite.
Dec 13 19:47:41.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:47:41.981: INFO: namespace tables-5003 deletion completed in 6.089703369s

• [SLOW TEST:6.237 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:47:41.981: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2435
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec 13 19:47:42.112: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:47:45.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2435" for this suite.
Dec 13 19:47:57.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:47:57.515: INFO: namespace init-container-2435 deletion completed in 12.088143044s

• [SLOW TEST:15.534 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:47:57.516: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1119
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 13 19:47:57.659: INFO: Waiting up to 5m0s for pod "pod-cb34e3f6-d116-4890-ad28-68103c4dd70b" in namespace "emptydir-1119" to be "success or failure"
Dec 13 19:47:57.662: INFO: Pod "pod-cb34e3f6-d116-4890-ad28-68103c4dd70b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.070581ms
Dec 13 19:47:59.664: INFO: Pod "pod-cb34e3f6-d116-4890-ad28-68103c4dd70b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005167192s
STEP: Saw pod success
Dec 13 19:47:59.664: INFO: Pod "pod-cb34e3f6-d116-4890-ad28-68103c4dd70b" satisfied condition "success or failure"
Dec 13 19:47:59.666: INFO: Trying to get logs from node master-0 pod pod-cb34e3f6-d116-4890-ad28-68103c4dd70b container test-container: <nil>
STEP: delete the pod
Dec 13 19:47:59.685: INFO: Waiting for pod pod-cb34e3f6-d116-4890-ad28-68103c4dd70b to disappear
Dec 13 19:47:59.694: INFO: Pod pod-cb34e3f6-d116-4890-ad28-68103c4dd70b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:47:59.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1119" for this suite.
Dec 13 19:48:05.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:48:05.785: INFO: namespace emptydir-1119 deletion completed in 6.088043929s

• [SLOW TEST:8.270 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:48:05.786: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8950
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8950.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8950.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8950.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8950.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8950.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8950.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 13 19:48:07.951: INFO: DNS probes using dns-8950/dns-test-c2520d48-7432-46ea-9ad9-f20f8702bd29 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:48:07.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8950" for this suite.
Dec 13 19:48:14.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:48:14.091: INFO: namespace dns-8950 deletion completed in 6.100544899s

• [SLOW TEST:8.305 seconds]
[sig-network] DNS
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:48:14.091: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8343
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 19:48:14.227: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 13 19:48:17.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 --namespace=crd-publish-openapi-8343 create -f -'
Dec 13 19:48:18.565: INFO: stderr: ""
Dec 13 19:48:18.565: INFO: stdout: "e2e-test-crd-publish-openapi-6071-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 13 19:48:18.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 --namespace=crd-publish-openapi-8343 delete e2e-test-crd-publish-openapi-6071-crds test-cr'
Dec 13 19:48:18.650: INFO: stderr: ""
Dec 13 19:48:18.650: INFO: stdout: "e2e-test-crd-publish-openapi-6071-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Dec 13 19:48:18.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 --namespace=crd-publish-openapi-8343 apply -f -'
Dec 13 19:48:18.797: INFO: stderr: ""
Dec 13 19:48:18.797: INFO: stdout: "e2e-test-crd-publish-openapi-6071-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 13 19:48:18.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 --namespace=crd-publish-openapi-8343 delete e2e-test-crd-publish-openapi-6071-crds test-cr'
Dec 13 19:48:18.862: INFO: stderr: ""
Dec 13 19:48:18.862: INFO: stdout: "e2e-test-crd-publish-openapi-6071-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 13 19:48:18.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 explain e2e-test-crd-publish-openapi-6071-crds'
Dec 13 19:48:19.003: INFO: stderr: ""
Dec 13 19:48:19.003: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6071-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:48:22.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8343" for this suite.
Dec 13 19:48:28.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:48:28.559: INFO: namespace crd-publish-openapi-8343 deletion completed in 6.08260725s

• [SLOW TEST:14.468 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:48:28.559: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5843
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:48:39.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5843" for this suite.
Dec 13 19:48:45.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:48:45.815: INFO: namespace resourcequota-5843 deletion completed in 6.090365969s

• [SLOW TEST:17.256 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:48:45.815: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6456
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Dec 13 19:48:48.460: INFO: Successfully updated pod "adopt-release-4v8dw"
STEP: Checking that the Job readopts the Pod
Dec 13 19:48:48.460: INFO: Waiting up to 15m0s for pod "adopt-release-4v8dw" in namespace "job-6456" to be "adopted"
Dec 13 19:48:48.476: INFO: Pod "adopt-release-4v8dw": Phase="Running", Reason="", readiness=true. Elapsed: 15.319036ms
Dec 13 19:48:50.478: INFO: Pod "adopt-release-4v8dw": Phase="Running", Reason="", readiness=true. Elapsed: 2.017649719s
Dec 13 19:48:50.478: INFO: Pod "adopt-release-4v8dw" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Dec 13 19:48:50.990: INFO: Successfully updated pod "adopt-release-4v8dw"
STEP: Checking that the Job releases the Pod
Dec 13 19:48:50.990: INFO: Waiting up to 15m0s for pod "adopt-release-4v8dw" in namespace "job-6456" to be "released"
Dec 13 19:48:51.006: INFO: Pod "adopt-release-4v8dw": Phase="Running", Reason="", readiness=true. Elapsed: 16.549355ms
Dec 13 19:48:53.009: INFO: Pod "adopt-release-4v8dw": Phase="Running", Reason="", readiness=true. Elapsed: 2.019156601s
Dec 13 19:48:53.009: INFO: Pod "adopt-release-4v8dw" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:48:53.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6456" for this suite.
Dec 13 19:49:35.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:49:35.097: INFO: namespace job-6456 deletion completed in 42.084878158s

• [SLOW TEST:49.282 seconds]
[sig-apps] Job
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:49:35.097: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7365
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 13 19:49:35.236: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9d41d0d6-61e9-4872-9f10-0a255e80a99a" in namespace "projected-7365" to be "success or failure"
Dec 13 19:49:35.240: INFO: Pod "downwardapi-volume-9d41d0d6-61e9-4872-9f10-0a255e80a99a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.115081ms
Dec 13 19:49:37.243: INFO: Pod "downwardapi-volume-9d41d0d6-61e9-4872-9f10-0a255e80a99a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006352134s
STEP: Saw pod success
Dec 13 19:49:37.243: INFO: Pod "downwardapi-volume-9d41d0d6-61e9-4872-9f10-0a255e80a99a" satisfied condition "success or failure"
Dec 13 19:49:37.245: INFO: Trying to get logs from node master-0 pod downwardapi-volume-9d41d0d6-61e9-4872-9f10-0a255e80a99a container client-container: <nil>
STEP: delete the pod
Dec 13 19:49:37.288: INFO: Waiting for pod downwardapi-volume-9d41d0d6-61e9-4872-9f10-0a255e80a99a to disappear
Dec 13 19:49:37.293: INFO: Pod downwardapi-volume-9d41d0d6-61e9-4872-9f10-0a255e80a99a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:49:37.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7365" for this suite.
Dec 13 19:49:43.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:49:43.393: INFO: namespace projected-7365 deletion completed in 6.097227028s

• [SLOW TEST:8.296 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:49:43.393: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8211
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:49:47.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8211" for this suite.
Dec 13 19:49:53.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:49:53.640: INFO: namespace kubelet-test-8211 deletion completed in 6.091412163s

• [SLOW TEST:10.247 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:49:53.640: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3424
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-2cd36421-f393-4ab4-904d-0f20d826bee8
STEP: Creating a pod to test consume secrets
Dec 13 19:49:53.803: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2d22cf62-935a-4f2a-bd0a-233600cd5bde" in namespace "projected-3424" to be "success or failure"
Dec 13 19:49:53.821: INFO: Pod "pod-projected-secrets-2d22cf62-935a-4f2a-bd0a-233600cd5bde": Phase="Pending", Reason="", readiness=false. Elapsed: 17.632627ms
Dec 13 19:49:55.823: INFO: Pod "pod-projected-secrets-2d22cf62-935a-4f2a-bd0a-233600cd5bde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019747852s
STEP: Saw pod success
Dec 13 19:49:55.823: INFO: Pod "pod-projected-secrets-2d22cf62-935a-4f2a-bd0a-233600cd5bde" satisfied condition "success or failure"
Dec 13 19:49:55.825: INFO: Trying to get logs from node master-0 pod pod-projected-secrets-2d22cf62-935a-4f2a-bd0a-233600cd5bde container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 13 19:49:55.847: INFO: Waiting for pod pod-projected-secrets-2d22cf62-935a-4f2a-bd0a-233600cd5bde to disappear
Dec 13 19:49:55.852: INFO: Pod pod-projected-secrets-2d22cf62-935a-4f2a-bd0a-233600cd5bde no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:49:55.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3424" for this suite.
Dec 13 19:50:01.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:50:01.945: INFO: namespace projected-3424 deletion completed in 6.08887328s

• [SLOW TEST:8.305 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:50:01.945: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3467
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:50:25.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3467" for this suite.
Dec 13 19:50:31.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:50:31.381: INFO: namespace container-runtime-3467 deletion completed in 6.100887353s

• [SLOW TEST:29.435 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:50:31.381: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4836
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 19:50:31.527: INFO: (0) /api/v1/nodes/master-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 4.239487ms)
Dec 13 19:50:31.529: INFO: (1) /api/v1/nodes/master-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.427234ms)
Dec 13 19:50:31.531: INFO: (2) /api/v1/nodes/master-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.443622ms)
Dec 13 19:50:31.534: INFO: (3) /api/v1/nodes/master-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.253518ms)
Dec 13 19:50:31.536: INFO: (4) /api/v1/nodes/master-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.324714ms)
Dec 13 19:50:31.538: INFO: (5) /api/v1/nodes/master-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.340927ms)
Dec 13 19:50:31.541: INFO: (6) /api/v1/nodes/master-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.378789ms)
Dec 13 19:50:31.543: INFO: (7) /api/v1/nodes/master-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.347905ms)
Dec 13 19:50:31.545: INFO: (8) /api/v1/nodes/master-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.188475ms)
Dec 13 19:50:31.548: INFO: (9) /api/v1/nodes/master-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.32406ms)
Dec 13 19:50:31.550: INFO: (10) /api/v1/nodes/master-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.307441ms)
Dec 13 19:50:31.553: INFO: (11) /api/v1/nodes/master-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.852688ms)
Dec 13 19:50:31.555: INFO: (12) /api/v1/nodes/master-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.171693ms)
Dec 13 19:50:31.557: INFO: (13) /api/v1/nodes/master-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.199108ms)
Dec 13 19:50:31.560: INFO: (14) /api/v1/nodes/master-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.11989ms)
Dec 13 19:50:31.562: INFO: (15) /api/v1/nodes/master-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.428ms)
Dec 13 19:50:31.564: INFO: (16) /api/v1/nodes/master-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.430775ms)
Dec 13 19:50:31.567: INFO: (17) /api/v1/nodes/master-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.300733ms)
Dec 13 19:50:31.569: INFO: (18) /api/v1/nodes/master-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.246835ms)
Dec 13 19:50:31.571: INFO: (19) /api/v1/nodes/master-0:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.285094ms)
[AfterEach] version v1
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:50:31.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4836" for this suite.
Dec 13 19:50:37.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:50:37.662: INFO: namespace proxy-4836 deletion completed in 6.087738429s

• [SLOW TEST:6.281 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:50:37.662: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3350
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 19:50:37.804: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 13 19:50:40.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 --namespace=crd-publish-openapi-3350 create -f -'
Dec 13 19:50:41.210: INFO: stderr: ""
Dec 13 19:50:41.210: INFO: stdout: "e2e-test-crd-publish-openapi-6674-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 13 19:50:41.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 --namespace=crd-publish-openapi-3350 delete e2e-test-crd-publish-openapi-6674-crds test-cr'
Dec 13 19:50:41.271: INFO: stderr: ""
Dec 13 19:50:41.271: INFO: stdout: "e2e-test-crd-publish-openapi-6674-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Dec 13 19:50:41.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 --namespace=crd-publish-openapi-3350 apply -f -'
Dec 13 19:50:41.430: INFO: stderr: ""
Dec 13 19:50:41.430: INFO: stdout: "e2e-test-crd-publish-openapi-6674-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 13 19:50:41.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 --namespace=crd-publish-openapi-3350 delete e2e-test-crd-publish-openapi-6674-crds test-cr'
Dec 13 19:50:41.491: INFO: stderr: ""
Dec 13 19:50:41.491: INFO: stdout: "e2e-test-crd-publish-openapi-6674-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 13 19:50:41.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 explain e2e-test-crd-publish-openapi-6674-crds'
Dec 13 19:50:41.624: INFO: stderr: ""
Dec 13 19:50:41.624: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6674-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:50:45.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3350" for this suite.
Dec 13 19:50:51.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:50:51.115: INFO: namespace crd-publish-openapi-3350 deletion completed in 6.092582351s

• [SLOW TEST:13.453 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:50:51.115: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7835
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 19:50:51.252: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 13 19:50:54.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 --namespace=crd-publish-openapi-7835 create -f -'
Dec 13 19:50:55.242: INFO: stderr: ""
Dec 13 19:50:55.242: INFO: stdout: "e2e-test-crd-publish-openapi-9470-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 13 19:50:55.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 --namespace=crd-publish-openapi-7835 delete e2e-test-crd-publish-openapi-9470-crds test-cr'
Dec 13 19:50:55.311: INFO: stderr: ""
Dec 13 19:50:55.311: INFO: stdout: "e2e-test-crd-publish-openapi-9470-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Dec 13 19:50:55.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 --namespace=crd-publish-openapi-7835 apply -f -'
Dec 13 19:50:55.462: INFO: stderr: ""
Dec 13 19:50:55.462: INFO: stdout: "e2e-test-crd-publish-openapi-9470-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 13 19:50:55.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 --namespace=crd-publish-openapi-7835 delete e2e-test-crd-publish-openapi-9470-crds test-cr'
Dec 13 19:50:55.531: INFO: stderr: ""
Dec 13 19:50:55.531: INFO: stdout: "e2e-test-crd-publish-openapi-9470-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Dec 13 19:50:55.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 explain e2e-test-crd-publish-openapi-9470-crds'
Dec 13 19:50:55.659: INFO: stderr: ""
Dec 13 19:50:55.659: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9470-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:50:59.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7835" for this suite.
Dec 13 19:51:05.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:51:06.051: INFO: namespace crd-publish-openapi-7835 deletion completed in 6.078163743s

• [SLOW TEST:14.936 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:51:06.051: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-703
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec 13 19:51:12.318: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:51:12.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1213 19:51:12.318794      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-703" for this suite.
Dec 13 19:51:18.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:51:18.461: INFO: namespace gc-703 deletion completed in 6.124597148s

• [SLOW TEST:12.410 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:51:18.462: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5709
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 13 19:51:19.317: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 13 19:51:21.325: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863479, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863479, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863479, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711863479, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 13 19:51:24.336: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:51:24.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5709" for this suite.
Dec 13 19:51:30.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:51:30.437: INFO: namespace webhook-5709 deletion completed in 6.093348456s
STEP: Destroying namespace "webhook-5709-markers" for this suite.
Dec 13 19:51:36.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:51:36.519: INFO: namespace webhook-5709-markers deletion completed in 6.081672427s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.071 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:51:36.533: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5860
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 13 19:51:36.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-5860'
Dec 13 19:51:36.740: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 13 19:51:36.740: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Dec 13 19:51:36.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 delete jobs e2e-test-httpd-job --namespace=kubectl-5860'
Dec 13 19:51:36.825: INFO: stderr: ""
Dec 13 19:51:36.825: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:51:36.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5860" for this suite.
Dec 13 19:51:48.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:51:48.937: INFO: namespace kubectl-5860 deletion completed in 12.101661921s

• [SLOW TEST:12.405 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:51:48.937: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3611
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 19:51:49.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 version'
Dec 13 19:51:49.126: INFO: stderr: ""
Dec 13 19:51:49.126: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.4\", GitCommit:\"224be7bdce5a9dd0c2fd0d46b83865648e2fe0ba\", GitTreeState:\"clean\", BuildDate:\"2019-12-11T12:47:40Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.4\", GitCommit:\"224be7bdce5a9dd0c2fd0d46b83865648e2fe0ba\", GitTreeState:\"clean\", BuildDate:\"2019-12-11T12:37:43Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:51:49.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3611" for this suite.
Dec 13 19:51:55.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:51:55.243: INFO: namespace kubectl-3611 deletion completed in 6.114452718s

• [SLOW TEST:6.306 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:51:55.243: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1026
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 13 19:51:56.161: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 13 19:51:59.176: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 19:51:59.178: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:52:00.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1026" for this suite.
Dec 13 19:52:06.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:52:06.369: INFO: namespace webhook-1026 deletion completed in 6.099860693s
STEP: Destroying namespace "webhook-1026-markers" for this suite.
Dec 13 19:52:12.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:52:12.460: INFO: namespace webhook-1026-markers deletion completed in 6.091118419s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.225 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:52:12.468: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-869
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-869
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-869
STEP: Deleting pre-stop pod
Dec 13 19:52:21.629: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:52:21.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-869" for this suite.
Dec 13 19:53:05.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:53:05.765: INFO: namespace prestop-869 deletion completed in 44.112186102s

• [SLOW TEST:53.296 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:53:05.765: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1913
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 13 19:53:07.922: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:53:07.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1913" for this suite.
Dec 13 19:53:13.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:53:14.037: INFO: namespace container-runtime-1913 deletion completed in 6.090547142s

• [SLOW TEST:8.272 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:53:14.037: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7727
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 19:53:16.222: INFO: Waiting up to 5m0s for pod "client-envvars-6eca8922-87ea-4513-9dc0-8ddda8de641d" in namespace "pods-7727" to be "success or failure"
Dec 13 19:53:16.238: INFO: Pod "client-envvars-6eca8922-87ea-4513-9dc0-8ddda8de641d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.13238ms
Dec 13 19:53:18.241: INFO: Pod "client-envvars-6eca8922-87ea-4513-9dc0-8ddda8de641d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019235173s
STEP: Saw pod success
Dec 13 19:53:18.241: INFO: Pod "client-envvars-6eca8922-87ea-4513-9dc0-8ddda8de641d" satisfied condition "success or failure"
Dec 13 19:53:18.243: INFO: Trying to get logs from node master-0 pod client-envvars-6eca8922-87ea-4513-9dc0-8ddda8de641d container env3cont: <nil>
STEP: delete the pod
Dec 13 19:53:18.270: INFO: Waiting for pod client-envvars-6eca8922-87ea-4513-9dc0-8ddda8de641d to disappear
Dec 13 19:53:18.274: INFO: Pod client-envvars-6eca8922-87ea-4513-9dc0-8ddda8de641d no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:53:18.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7727" for this suite.
Dec 13 19:53:46.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:53:46.365: INFO: namespace pods-7727 deletion completed in 28.088082831s

• [SLOW TEST:32.328 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:53:46.365: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8790
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-6b6d908e-ba9b-4fd2-b4bf-fb4da6d794c9
STEP: Creating a pod to test consume secrets
Dec 13 19:53:46.571: INFO: Waiting up to 5m0s for pod "pod-secrets-6426ca30-5953-4aee-bc15-9bec100708fb" in namespace "secrets-8790" to be "success or failure"
Dec 13 19:53:46.615: INFO: Pod "pod-secrets-6426ca30-5953-4aee-bc15-9bec100708fb": Phase="Pending", Reason="", readiness=false. Elapsed: 44.172653ms
Dec 13 19:53:48.618: INFO: Pod "pod-secrets-6426ca30-5953-4aee-bc15-9bec100708fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046436053s
STEP: Saw pod success
Dec 13 19:53:48.618: INFO: Pod "pod-secrets-6426ca30-5953-4aee-bc15-9bec100708fb" satisfied condition "success or failure"
Dec 13 19:53:48.620: INFO: Trying to get logs from node master-0 pod pod-secrets-6426ca30-5953-4aee-bc15-9bec100708fb container secret-volume-test: <nil>
STEP: delete the pod
Dec 13 19:53:48.638: INFO: Waiting for pod pod-secrets-6426ca30-5953-4aee-bc15-9bec100708fb to disappear
Dec 13 19:53:48.643: INFO: Pod pod-secrets-6426ca30-5953-4aee-bc15-9bec100708fb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:53:48.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8790" for this suite.
Dec 13 19:53:54.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:53:54.729: INFO: namespace secrets-8790 deletion completed in 6.083630968s

• [SLOW TEST:8.365 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:53:54.730: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4451
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-e269fd73-16c9-483b-a610-d14db62e357c
STEP: Creating a pod to test consume secrets
Dec 13 19:53:54.896: INFO: Waiting up to 5m0s for pod "pod-secrets-01b68f2e-54cf-4403-adca-a9753c4816ea" in namespace "secrets-4451" to be "success or failure"
Dec 13 19:53:54.909: INFO: Pod "pod-secrets-01b68f2e-54cf-4403-adca-a9753c4816ea": Phase="Pending", Reason="", readiness=false. Elapsed: 12.929193ms
Dec 13 19:53:56.913: INFO: Pod "pod-secrets-01b68f2e-54cf-4403-adca-a9753c4816ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016474449s
STEP: Saw pod success
Dec 13 19:53:56.913: INFO: Pod "pod-secrets-01b68f2e-54cf-4403-adca-a9753c4816ea" satisfied condition "success or failure"
Dec 13 19:53:56.915: INFO: Trying to get logs from node master-0 pod pod-secrets-01b68f2e-54cf-4403-adca-a9753c4816ea container secret-volume-test: <nil>
STEP: delete the pod
Dec 13 19:53:56.935: INFO: Waiting for pod pod-secrets-01b68f2e-54cf-4403-adca-a9753c4816ea to disappear
Dec 13 19:53:56.950: INFO: Pod pod-secrets-01b68f2e-54cf-4403-adca-a9753c4816ea no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:53:56.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4451" for this suite.
Dec 13 19:54:02.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:54:03.058: INFO: namespace secrets-4451 deletion completed in 6.100304584s

• [SLOW TEST:8.328 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:54:03.058: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1688
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-1688
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 13 19:54:03.210: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 13 19:54:23.307: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.168:8080/dial?request=hostName&protocol=http&host=10.244.2.23&port=8080&tries=1'] Namespace:pod-network-test-1688 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 19:54:23.307: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 19:54:23.378: INFO: Waiting for endpoints: map[]
Dec 13 19:54:23.380: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.168:8080/dial?request=hostName&protocol=http&host=10.244.0.167&port=8080&tries=1'] Namespace:pod-network-test-1688 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 19:54:23.380: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 19:54:23.448: INFO: Waiting for endpoints: map[]
Dec 13 19:54:23.450: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.168:8080/dial?request=hostName&protocol=http&host=10.244.1.29&port=8080&tries=1'] Namespace:pod-network-test-1688 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 19:54:23.450: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 19:54:23.520: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:54:23.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1688" for this suite.
Dec 13 19:54:35.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:54:35.634: INFO: namespace pod-network-test-1688 deletion completed in 12.11034313s

• [SLOW TEST:32.576 seconds]
[sig-network] Networking
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:54:35.634: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1710
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-d0484c3a-27c6-4272-9951-0246afde6cfe
STEP: Creating configMap with name cm-test-opt-upd-0d977f5e-d977-42c4-9106-59b74fe8a309
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-d0484c3a-27c6-4272-9951-0246afde6cfe
STEP: Updating configmap cm-test-opt-upd-0d977f5e-d977-42c4-9106-59b74fe8a309
STEP: Creating configMap with name cm-test-opt-create-8d68c50e-97d2-4bb4-a67e-bd1426934958
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:54:39.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1710" for this suite.
Dec 13 19:54:57.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:54:57.985: INFO: namespace projected-1710 deletion completed in 18.146695239s

• [SLOW TEST:22.351 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:54:57.985: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1204
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 19:54:58.128: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:54:59.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1204" for this suite.
Dec 13 19:55:05.162: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:55:05.239: INFO: namespace custom-resource-definition-1204 deletion completed in 6.090472857s

• [SLOW TEST:7.254 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:55:05.239: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-476
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 19:55:05.377: INFO: Waiting up to 5m0s for pod "busybox-user-65534-99d4f703-eed8-4de0-bfe8-8ac6df9df538" in namespace "security-context-test-476" to be "success or failure"
Dec 13 19:55:05.379: INFO: Pod "busybox-user-65534-99d4f703-eed8-4de0-bfe8-8ac6df9df538": Phase="Pending", Reason="", readiness=false. Elapsed: 2.293658ms
Dec 13 19:55:07.382: INFO: Pod "busybox-user-65534-99d4f703-eed8-4de0-bfe8-8ac6df9df538": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004690861s
Dec 13 19:55:07.382: INFO: Pod "busybox-user-65534-99d4f703-eed8-4de0-bfe8-8ac6df9df538" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:55:07.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-476" for this suite.
Dec 13 19:55:13.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:55:13.473: INFO: namespace security-context-test-476 deletion completed in 6.087882955s

• [SLOW TEST:8.233 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:55:13.473: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8689
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 13 19:55:13.610: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9f1413ff-dac1-4abd-bcaa-556e134aaf33" in namespace "projected-8689" to be "success or failure"
Dec 13 19:55:13.616: INFO: Pod "downwardapi-volume-9f1413ff-dac1-4abd-bcaa-556e134aaf33": Phase="Pending", Reason="", readiness=false. Elapsed: 5.506635ms
Dec 13 19:55:15.618: INFO: Pod "downwardapi-volume-9f1413ff-dac1-4abd-bcaa-556e134aaf33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007768997s
STEP: Saw pod success
Dec 13 19:55:15.618: INFO: Pod "downwardapi-volume-9f1413ff-dac1-4abd-bcaa-556e134aaf33" satisfied condition "success or failure"
Dec 13 19:55:15.620: INFO: Trying to get logs from node master-0 pod downwardapi-volume-9f1413ff-dac1-4abd-bcaa-556e134aaf33 container client-container: <nil>
STEP: delete the pod
Dec 13 19:55:15.639: INFO: Waiting for pod downwardapi-volume-9f1413ff-dac1-4abd-bcaa-556e134aaf33 to disappear
Dec 13 19:55:15.645: INFO: Pod downwardapi-volume-9f1413ff-dac1-4abd-bcaa-556e134aaf33 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:55:15.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8689" for this suite.
Dec 13 19:55:21.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:55:21.760: INFO: namespace projected-8689 deletion completed in 6.112512574s

• [SLOW TEST:8.286 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:55:21.760: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9171
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Dec 13 19:55:21.902: INFO: Waiting up to 5m0s for pod "client-containers-b99a9f84-67ab-461a-a912-c325980511fd" in namespace "containers-9171" to be "success or failure"
Dec 13 19:55:21.905: INFO: Pod "client-containers-b99a9f84-67ab-461a-a912-c325980511fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.906262ms
Dec 13 19:55:23.908: INFO: Pod "client-containers-b99a9f84-67ab-461a-a912-c325980511fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005553506s
STEP: Saw pod success
Dec 13 19:55:23.908: INFO: Pod "client-containers-b99a9f84-67ab-461a-a912-c325980511fd" satisfied condition "success or failure"
Dec 13 19:55:23.910: INFO: Trying to get logs from node master-0 pod client-containers-b99a9f84-67ab-461a-a912-c325980511fd container test-container: <nil>
STEP: delete the pod
Dec 13 19:55:23.934: INFO: Waiting for pod client-containers-b99a9f84-67ab-461a-a912-c325980511fd to disappear
Dec 13 19:55:23.939: INFO: Pod client-containers-b99a9f84-67ab-461a-a912-c325980511fd no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:55:23.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9171" for this suite.
Dec 13 19:55:29.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:55:30.056: INFO: namespace containers-9171 deletion completed in 6.111226587s

• [SLOW TEST:8.296 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:55:30.056: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1854
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 13 19:55:30.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-1854'
Dec 13 19:55:30.307: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 13 19:55:30.307: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Dec 13 19:55:32.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 delete deployment e2e-test-httpd-deployment --namespace=kubectl-1854'
Dec 13 19:55:32.388: INFO: stderr: ""
Dec 13 19:55:32.388: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:55:32.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1854" for this suite.
Dec 13 19:55:38.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:55:38.488: INFO: namespace kubectl-1854 deletion completed in 6.096306132s

• [SLOW TEST:8.431 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:55:38.488: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-297
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:55:41.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-297" for this suite.
Dec 13 19:56:09.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:56:09.736: INFO: namespace replication-controller-297 deletion completed in 28.086157871s

• [SLOW TEST:31.248 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:56:09.736: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2382
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Dec 13 19:56:09.870: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:56:28.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2382" for this suite.
Dec 13 19:56:34.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:56:34.939: INFO: namespace crd-publish-openapi-2382 deletion completed in 6.096844731s

• [SLOW TEST:25.203 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:56:34.940: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8769
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:56:35.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8769" for this suite.
Dec 13 19:56:41.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:56:41.168: INFO: namespace resourcequota-8769 deletion completed in 6.083253634s

• [SLOW TEST:6.228 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:56:41.168: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3529
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-3529
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Dec 13 19:56:41.309: INFO: Found 0 stateful pods, waiting for 3
Dec 13 19:56:51.311: INFO: Found 2 stateful pods, waiting for 3
Dec 13 19:57:01.311: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 19:57:01.311: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 19:57:01.311: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec 13 19:57:11.311: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 19:57:11.311: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 19:57:11.311: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec 13 19:57:11.331: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec 13 19:57:21.357: INFO: Updating stateful set ss2
Dec 13 19:57:21.362: INFO: Waiting for Pod statefulset-3529/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 13 19:57:31.367: INFO: Waiting for Pod statefulset-3529/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Dec 13 19:57:41.519: INFO: Found 2 stateful pods, waiting for 3
Dec 13 19:57:51.521: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 19:57:51.521: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 19:57:51.521: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec 13 19:57:51.541: INFO: Updating stateful set ss2
Dec 13 19:57:51.561: INFO: Waiting for Pod statefulset-3529/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 13 19:58:01.582: INFO: Updating stateful set ss2
Dec 13 19:58:01.597: INFO: Waiting for StatefulSet statefulset-3529/ss2 to complete update
Dec 13 19:58:01.597: INFO: Waiting for Pod statefulset-3529/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 13 19:58:11.601: INFO: Waiting for StatefulSet statefulset-3529/ss2 to complete update
Dec 13 19:58:21.606: INFO: Waiting for StatefulSet statefulset-3529/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 13 19:58:31.601: INFO: Deleting all statefulset in ns statefulset-3529
Dec 13 19:58:31.603: INFO: Scaling statefulset ss2 to 0
Dec 13 19:58:51.612: INFO: Waiting for statefulset status.replicas updated to 0
Dec 13 19:58:51.614: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:58:51.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3529" for this suite.
Dec 13 19:58:57.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:58:57.726: INFO: namespace statefulset-3529 deletion completed in 6.097604513s

• [SLOW TEST:136.558 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 19:58:57.726: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-637
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-637
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-637
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-637
Dec 13 19:58:57.879: INFO: Found 0 stateful pods, waiting for 1
Dec 13 19:59:07.881: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec 13 19:59:07.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=statefulset-637 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 13 19:59:08.041: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 13 19:59:08.041: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 13 19:59:08.041: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 13 19:59:08.043: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 13 19:59:18.047: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 13 19:59:18.047: INFO: Waiting for statefulset status.replicas updated to 0
Dec 13 19:59:18.061: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Dec 13 19:59:18.061: INFO: ss-0  master-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:58:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:58:57 +0000 UTC  }]
Dec 13 19:59:18.061: INFO: ss-1            Pending         []
Dec 13 19:59:18.061: INFO: 
Dec 13 19:59:18.061: INFO: StatefulSet ss has not reached scale 3, at 2
Dec 13 19:59:19.064: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996132991s
Dec 13 19:59:20.066: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993462985s
Dec 13 19:59:21.069: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991076254s
Dec 13 19:59:22.071: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.988021996s
Dec 13 19:59:23.074: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.985700146s
Dec 13 19:59:24.076: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.983122909s
Dec 13 19:59:25.079: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.980769251s
Dec 13 19:59:26.088: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.97378389s
Dec 13 19:59:27.091: INFO: Verifying statefulset ss doesn't scale past 3 for another 968.694017ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-637
Dec 13 19:59:28.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=statefulset-637 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 13 19:59:28.261: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 13 19:59:28.261: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 13 19:59:28.261: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 13 19:59:28.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=statefulset-637 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 13 19:59:28.427: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 13 19:59:28.427: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 13 19:59:28.427: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 13 19:59:28.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=statefulset-637 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 13 19:59:28.579: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 13 19:59:28.580: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 13 19:59:28.580: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 13 19:59:28.582: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Dec 13 19:59:38.585: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 19:59:38.585: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 19:59:38.585: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec 13 19:59:38.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=statefulset-637 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 13 19:59:38.732: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 13 19:59:38.732: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 13 19:59:38.732: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 13 19:59:38.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=statefulset-637 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 13 19:59:38.864: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 13 19:59:38.864: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 13 19:59:38.864: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 13 19:59:38.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=statefulset-637 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 13 19:59:38.989: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 13 19:59:38.989: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 13 19:59:38.989: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 13 19:59:38.989: INFO: Waiting for statefulset status.replicas updated to 0
Dec 13 19:59:38.991: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec 13 19:59:48.996: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 13 19:59:48.996: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 13 19:59:48.996: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 13 19:59:49.005: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Dec 13 19:59:49.005: INFO: ss-0  master-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:58:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:58:57 +0000 UTC  }]
Dec 13 19:59:49.005: INFO: ss-1  worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:18 +0000 UTC  }]
Dec 13 19:59:49.006: INFO: ss-2  worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:18 +0000 UTC  }]
Dec 13 19:59:49.006: INFO: 
Dec 13 19:59:49.006: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 13 19:59:50.009: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Dec 13 19:59:50.009: INFO: ss-0  master-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:58:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:58:57 +0000 UTC  }]
Dec 13 19:59:50.009: INFO: ss-1  worker-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:18 +0000 UTC  }]
Dec 13 19:59:50.009: INFO: ss-2  worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:18 +0000 UTC  }]
Dec 13 19:59:50.009: INFO: 
Dec 13 19:59:50.009: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 13 19:59:51.012: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Dec 13 19:59:51.012: INFO: ss-0  master-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:58:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:58:57 +0000 UTC  }]
Dec 13 19:59:51.012: INFO: ss-1  worker-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:18 +0000 UTC  }]
Dec 13 19:59:51.012: INFO: ss-2  worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:18 +0000 UTC  }]
Dec 13 19:59:51.012: INFO: 
Dec 13 19:59:51.012: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 13 19:59:52.015: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Dec 13 19:59:52.015: INFO: ss-0  master-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:58:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:58:57 +0000 UTC  }]
Dec 13 19:59:52.015: INFO: ss-2  worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:18 +0000 UTC  }]
Dec 13 19:59:52.015: INFO: 
Dec 13 19:59:52.015: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 13 19:59:53.017: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Dec 13 19:59:53.017: INFO: ss-0  master-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:58:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:58:57 +0000 UTC  }]
Dec 13 19:59:53.017: INFO: ss-2  worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:18 +0000 UTC  }]
Dec 13 19:59:53.017: INFO: 
Dec 13 19:59:53.017: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 13 19:59:54.020: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Dec 13 19:59:54.020: INFO: ss-0  master-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:58:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:58:57 +0000 UTC  }]
Dec 13 19:59:54.020: INFO: ss-2  worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:18 +0000 UTC  }]
Dec 13 19:59:54.020: INFO: 
Dec 13 19:59:54.020: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 13 19:59:55.023: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Dec 13 19:59:55.023: INFO: ss-0  master-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:58:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:58:57 +0000 UTC  }]
Dec 13 19:59:55.023: INFO: ss-2  worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:18 +0000 UTC  }]
Dec 13 19:59:55.023: INFO: 
Dec 13 19:59:55.023: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 13 19:59:56.028: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Dec 13 19:59:56.028: INFO: ss-2  worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-13 19:59:18 +0000 UTC  }]
Dec 13 19:59:56.028: INFO: 
Dec 13 19:59:56.028: INFO: StatefulSet ss has not reached scale 0, at 1
Dec 13 19:59:57.030: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.971963859s
Dec 13 19:59:58.033: INFO: Verifying statefulset ss doesn't scale past 0 for another 969.478631ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-637
Dec 13 19:59:59.035: INFO: Scaling statefulset ss to 0
Dec 13 19:59:59.041: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 13 19:59:59.043: INFO: Deleting all statefulset in ns statefulset-637
Dec 13 19:59:59.045: INFO: Scaling statefulset ss to 0
Dec 13 19:59:59.051: INFO: Waiting for statefulset status.replicas updated to 0
Dec 13 19:59:59.052: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 19:59:59.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-637" for this suite.
Dec 13 20:00:05.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:00:05.157: INFO: namespace statefulset-637 deletion completed in 6.093746246s

• [SLOW TEST:67.431 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:00:05.157: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5614
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec 13 20:00:05.300: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:00:15.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5614" for this suite.
Dec 13 20:00:21.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:00:21.812: INFO: namespace pods-5614 deletion completed in 6.095884683s

• [SLOW TEST:16.655 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:00:21.813: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3662
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-3662
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-3662
STEP: Creating statefulset with conflicting port in namespace statefulset-3662
STEP: Waiting until pod test-pod will start running in namespace statefulset-3662
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3662
Dec 13 20:00:25.989: INFO: Observed stateful pod in namespace: statefulset-3662, name: ss-0, uid: a3706df9-4cc9-4dc4-9029-0a7f2a264f2b, status phase: Pending. Waiting for statefulset controller to delete.
Dec 13 20:00:26.577: INFO: Observed stateful pod in namespace: statefulset-3662, name: ss-0, uid: a3706df9-4cc9-4dc4-9029-0a7f2a264f2b, status phase: Failed. Waiting for statefulset controller to delete.
Dec 13 20:00:26.612: INFO: Observed stateful pod in namespace: statefulset-3662, name: ss-0, uid: a3706df9-4cc9-4dc4-9029-0a7f2a264f2b, status phase: Failed. Waiting for statefulset controller to delete.
Dec 13 20:00:26.630: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3662
STEP: Removing pod with conflicting port in namespace statefulset-3662
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3662 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 13 20:00:30.738: INFO: Deleting all statefulset in ns statefulset-3662
Dec 13 20:00:30.740: INFO: Scaling statefulset ss to 0
Dec 13 20:00:40.750: INFO: Waiting for statefulset status.replicas updated to 0
Dec 13 20:00:40.752: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:00:40.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3662" for this suite.
Dec 13 20:00:46.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:00:46.911: INFO: namespace statefulset-3662 deletion completed in 6.14555948s

• [SLOW TEST:25.098 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:00:46.911: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6220
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 13 20:00:47.066: INFO: Waiting up to 5m0s for pod "downwardapi-volume-def5671f-5dcb-4652-90a7-7e559c56c912" in namespace "projected-6220" to be "success or failure"
Dec 13 20:00:47.078: INFO: Pod "downwardapi-volume-def5671f-5dcb-4652-90a7-7e559c56c912": Phase="Pending", Reason="", readiness=false. Elapsed: 11.14849ms
Dec 13 20:00:49.080: INFO: Pod "downwardapi-volume-def5671f-5dcb-4652-90a7-7e559c56c912": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013443011s
STEP: Saw pod success
Dec 13 20:00:49.080: INFO: Pod "downwardapi-volume-def5671f-5dcb-4652-90a7-7e559c56c912" satisfied condition "success or failure"
Dec 13 20:00:49.082: INFO: Trying to get logs from node master-0 pod downwardapi-volume-def5671f-5dcb-4652-90a7-7e559c56c912 container client-container: <nil>
STEP: delete the pod
Dec 13 20:00:49.113: INFO: Waiting for pod downwardapi-volume-def5671f-5dcb-4652-90a7-7e559c56c912 to disappear
Dec 13 20:00:49.123: INFO: Pod downwardapi-volume-def5671f-5dcb-4652-90a7-7e559c56c912 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:00:49.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6220" for this suite.
Dec 13 20:00:55.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:00:55.225: INFO: namespace projected-6220 deletion completed in 6.096858248s

• [SLOW TEST:8.314 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:00:55.225: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6753
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 13 20:00:55.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-6753'
Dec 13 20:00:55.661: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 13 20:00:55.661: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Dec 13 20:00:55.671: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Dec 13 20:00:55.674: INFO: scanned /root for discovery docs: <nil>
Dec 13 20:00:55.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-6753'
Dec 13 20:01:11.562: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 13 20:01:11.562: INFO: stdout: "Created e2e-test-httpd-rc-b37e0e029f8a10b254f92649b0512bd6\nScaling up e2e-test-httpd-rc-b37e0e029f8a10b254f92649b0512bd6 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-b37e0e029f8a10b254f92649b0512bd6 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-b37e0e029f8a10b254f92649b0512bd6 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Dec 13 20:01:11.562: INFO: stdout: "Created e2e-test-httpd-rc-b37e0e029f8a10b254f92649b0512bd6\nScaling up e2e-test-httpd-rc-b37e0e029f8a10b254f92649b0512bd6 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-b37e0e029f8a10b254f92649b0512bd6 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-b37e0e029f8a10b254f92649b0512bd6 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Dec 13 20:01:11.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-6753'
Dec 13 20:01:11.638: INFO: stderr: ""
Dec 13 20:01:11.638: INFO: stdout: "e2e-test-httpd-rc-b37e0e029f8a10b254f92649b0512bd6-8wjv4 "
Dec 13 20:01:11.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods e2e-test-httpd-rc-b37e0e029f8a10b254f92649b0512bd6-8wjv4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6753'
Dec 13 20:01:11.706: INFO: stderr: ""
Dec 13 20:01:11.706: INFO: stdout: "true"
Dec 13 20:01:11.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods e2e-test-httpd-rc-b37e0e029f8a10b254f92649b0512bd6-8wjv4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6753'
Dec 13 20:01:11.767: INFO: stderr: ""
Dec 13 20:01:11.767: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Dec 13 20:01:11.767: INFO: e2e-test-httpd-rc-b37e0e029f8a10b254f92649b0512bd6-8wjv4 is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Dec 13 20:01:11.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 delete rc e2e-test-httpd-rc --namespace=kubectl-6753'
Dec 13 20:01:11.831: INFO: stderr: ""
Dec 13 20:01:11.831: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:01:11.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6753" for this suite.
Dec 13 20:01:17.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:01:17.944: INFO: namespace kubectl-6753 deletion completed in 6.100443898s

• [SLOW TEST:22.719 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:01:17.944: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7783
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-d7838915-3aec-4d4a-9139-b6913e664923
STEP: Creating a pod to test consume configMaps
Dec 13 20:01:18.093: INFO: Waiting up to 5m0s for pod "pod-configmaps-3b83a23a-fb13-4155-9645-a31b7892abcc" in namespace "configmap-7783" to be "success or failure"
Dec 13 20:01:18.098: INFO: Pod "pod-configmaps-3b83a23a-fb13-4155-9645-a31b7892abcc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.665095ms
Dec 13 20:01:20.101: INFO: Pod "pod-configmaps-3b83a23a-fb13-4155-9645-a31b7892abcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008181171s
STEP: Saw pod success
Dec 13 20:01:20.101: INFO: Pod "pod-configmaps-3b83a23a-fb13-4155-9645-a31b7892abcc" satisfied condition "success or failure"
Dec 13 20:01:20.103: INFO: Trying to get logs from node master-0 pod pod-configmaps-3b83a23a-fb13-4155-9645-a31b7892abcc container configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 20:01:20.133: INFO: Waiting for pod pod-configmaps-3b83a23a-fb13-4155-9645-a31b7892abcc to disappear
Dec 13 20:01:20.137: INFO: Pod pod-configmaps-3b83a23a-fb13-4155-9645-a31b7892abcc no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:01:20.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7783" for this suite.
Dec 13 20:01:26.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:01:26.411: INFO: namespace configmap-7783 deletion completed in 6.268164549s

• [SLOW TEST:8.467 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:01:26.412: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9580
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 13 20:01:27.390: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 13 20:01:30.412: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:01:30.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9580" for this suite.
Dec 13 20:01:36.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:01:36.548: INFO: namespace webhook-9580 deletion completed in 6.088390193s
STEP: Destroying namespace "webhook-9580-markers" for this suite.
Dec 13 20:01:42.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:01:42.634: INFO: namespace webhook-9580-markers deletion completed in 6.085808488s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:16.230 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:01:42.642: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7253
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Dec 13 20:01:42.778: INFO: namespace kubectl-7253
Dec 13 20:01:42.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 create -f - --namespace=kubectl-7253'
Dec 13 20:01:42.994: INFO: stderr: ""
Dec 13 20:01:42.994: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 13 20:01:43.997: INFO: Selector matched 1 pods for map[app:redis]
Dec 13 20:01:43.997: INFO: Found 0 / 1
Dec 13 20:01:44.997: INFO: Selector matched 1 pods for map[app:redis]
Dec 13 20:01:44.997: INFO: Found 1 / 1
Dec 13 20:01:44.997: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 13 20:01:44.999: INFO: Selector matched 1 pods for map[app:redis]
Dec 13 20:01:44.999: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 13 20:01:44.999: INFO: wait on redis-master startup in kubectl-7253 
Dec 13 20:01:44.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 logs redis-master-shs59 redis-master --namespace=kubectl-7253'
Dec 13 20:01:45.133: INFO: stderr: ""
Dec 13 20:01:45.133: INFO: stdout: "1:C 13 Dec 2019 20:01:43.827 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 13 Dec 2019 20:01:43.827 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 13 Dec 2019 20:01:43.827 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 13 Dec 2019 20:01:43.828 * Running mode=standalone, port=6379.\n1:M 13 Dec 2019 20:01:43.828 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 Dec 2019 20:01:43.828 # Server initialized\n1:M 13 Dec 2019 20:01:43.828 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 Dec 2019 20:01:43.828 * Ready to accept connections\n"
STEP: exposing RC
Dec 13 20:01:45.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-7253'
Dec 13 20:01:45.218: INFO: stderr: ""
Dec 13 20:01:45.218: INFO: stdout: "service/rm2 exposed\n"
Dec 13 20:01:45.225: INFO: Service rm2 in namespace kubectl-7253 found.
STEP: exposing service
Dec 13 20:01:47.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-7253'
Dec 13 20:01:47.310: INFO: stderr: ""
Dec 13 20:01:47.310: INFO: stdout: "service/rm3 exposed\n"
Dec 13 20:01:47.315: INFO: Service rm3 in namespace kubectl-7253 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:01:49.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7253" for this suite.
Dec 13 20:02:17.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:02:17.411: INFO: namespace kubectl-7253 deletion completed in 28.08930887s

• [SLOW TEST:34.769 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:02:17.411: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9811
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 13 20:02:17.548: INFO: Waiting up to 5m0s for pod "downward-api-84cc379d-814a-467b-8c39-2e33e22d51f0" in namespace "downward-api-9811" to be "success or failure"
Dec 13 20:02:17.553: INFO: Pod "downward-api-84cc379d-814a-467b-8c39-2e33e22d51f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.763069ms
Dec 13 20:02:19.555: INFO: Pod "downward-api-84cc379d-814a-467b-8c39-2e33e22d51f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006982005s
STEP: Saw pod success
Dec 13 20:02:19.555: INFO: Pod "downward-api-84cc379d-814a-467b-8c39-2e33e22d51f0" satisfied condition "success or failure"
Dec 13 20:02:19.557: INFO: Trying to get logs from node master-0 pod downward-api-84cc379d-814a-467b-8c39-2e33e22d51f0 container dapi-container: <nil>
STEP: delete the pod
Dec 13 20:02:19.573: INFO: Waiting for pod downward-api-84cc379d-814a-467b-8c39-2e33e22d51f0 to disappear
Dec 13 20:02:19.581: INFO: Pod downward-api-84cc379d-814a-467b-8c39-2e33e22d51f0 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:02:19.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9811" for this suite.
Dec 13 20:02:25.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:02:25.677: INFO: namespace downward-api-9811 deletion completed in 6.092435259s

• [SLOW TEST:8.266 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:02:25.678: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-869
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:02:31.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-869" for this suite.
Dec 13 20:02:37.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:02:37.895: INFO: namespace watch-869 deletion completed in 6.18412505s

• [SLOW TEST:12.218 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:02:37.896: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1132
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Dec 13 20:02:38.056: INFO: Waiting up to 5m0s for pod "client-containers-a2bfc72b-2f06-4761-b268-0627eff776bb" in namespace "containers-1132" to be "success or failure"
Dec 13 20:02:38.059: INFO: Pod "client-containers-a2bfc72b-2f06-4761-b268-0627eff776bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.864421ms
Dec 13 20:02:40.061: INFO: Pod "client-containers-a2bfc72b-2f06-4761-b268-0627eff776bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005389057s
STEP: Saw pod success
Dec 13 20:02:40.062: INFO: Pod "client-containers-a2bfc72b-2f06-4761-b268-0627eff776bb" satisfied condition "success or failure"
Dec 13 20:02:40.064: INFO: Trying to get logs from node master-0 pod client-containers-a2bfc72b-2f06-4761-b268-0627eff776bb container test-container: <nil>
STEP: delete the pod
Dec 13 20:02:40.081: INFO: Waiting for pod client-containers-a2bfc72b-2f06-4761-b268-0627eff776bb to disappear
Dec 13 20:02:40.086: INFO: Pod client-containers-a2bfc72b-2f06-4761-b268-0627eff776bb no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:02:40.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1132" for this suite.
Dec 13 20:02:46.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:02:46.172: INFO: namespace containers-1132 deletion completed in 6.082494584s

• [SLOW TEST:8.276 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:02:46.172: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8085
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Dec 13 20:02:46.318: INFO: Waiting up to 5m0s for pod "pod-c244e181-2601-4886-8644-ae924718f3d7" in namespace "emptydir-8085" to be "success or failure"
Dec 13 20:02:46.325: INFO: Pod "pod-c244e181-2601-4886-8644-ae924718f3d7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.768772ms
Dec 13 20:02:48.327: INFO: Pod "pod-c244e181-2601-4886-8644-ae924718f3d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009029272s
STEP: Saw pod success
Dec 13 20:02:48.327: INFO: Pod "pod-c244e181-2601-4886-8644-ae924718f3d7" satisfied condition "success or failure"
Dec 13 20:02:48.329: INFO: Trying to get logs from node master-0 pod pod-c244e181-2601-4886-8644-ae924718f3d7 container test-container: <nil>
STEP: delete the pod
Dec 13 20:02:48.353: INFO: Waiting for pod pod-c244e181-2601-4886-8644-ae924718f3d7 to disappear
Dec 13 20:02:48.355: INFO: Pod pod-c244e181-2601-4886-8644-ae924718f3d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:02:48.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8085" for this suite.
Dec 13 20:02:54.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:02:54.462: INFO: namespace emptydir-8085 deletion completed in 6.105038523s

• [SLOW TEST:8.290 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:02:54.463: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2645
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-346063da-c209-429d-8d57-28d19e7fba58
STEP: Creating a pod to test consume configMaps
Dec 13 20:02:54.669: INFO: Waiting up to 5m0s for pod "pod-configmaps-acf76131-38d3-4164-934a-9a764eac2d46" in namespace "configmap-2645" to be "success or failure"
Dec 13 20:02:54.678: INFO: Pod "pod-configmaps-acf76131-38d3-4164-934a-9a764eac2d46": Phase="Pending", Reason="", readiness=false. Elapsed: 9.729415ms
Dec 13 20:02:56.682: INFO: Pod "pod-configmaps-acf76131-38d3-4164-934a-9a764eac2d46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013530325s
STEP: Saw pod success
Dec 13 20:02:56.682: INFO: Pod "pod-configmaps-acf76131-38d3-4164-934a-9a764eac2d46" satisfied condition "success or failure"
Dec 13 20:02:56.685: INFO: Trying to get logs from node master-0 pod pod-configmaps-acf76131-38d3-4164-934a-9a764eac2d46 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 20:02:56.722: INFO: Waiting for pod pod-configmaps-acf76131-38d3-4164-934a-9a764eac2d46 to disappear
Dec 13 20:02:56.731: INFO: Pod pod-configmaps-acf76131-38d3-4164-934a-9a764eac2d46 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:02:56.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2645" for this suite.
Dec 13 20:03:02.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:03:02.847: INFO: namespace configmap-2645 deletion completed in 6.110085162s

• [SLOW TEST:8.384 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:03:02.848: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4183
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-7182c361-6b16-4c37-9645-5038930cf579
Dec 13 20:03:02.999: INFO: Pod name my-hostname-basic-7182c361-6b16-4c37-9645-5038930cf579: Found 0 pods out of 1
Dec 13 20:03:08.002: INFO: Pod name my-hostname-basic-7182c361-6b16-4c37-9645-5038930cf579: Found 1 pods out of 1
Dec 13 20:03:08.002: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-7182c361-6b16-4c37-9645-5038930cf579" are running
Dec 13 20:03:08.003: INFO: Pod "my-hostname-basic-7182c361-6b16-4c37-9645-5038930cf579-lpk48" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-13 20:03:03 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-13 20:03:04 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-13 20:03:04 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-13 20:03:03 +0000 UTC Reason: Message:}])
Dec 13 20:03:08.003: INFO: Trying to dial the pod
Dec 13 20:03:13.009: INFO: Controller my-hostname-basic-7182c361-6b16-4c37-9645-5038930cf579: Got expected result from replica 1 [my-hostname-basic-7182c361-6b16-4c37-9645-5038930cf579-lpk48]: "my-hostname-basic-7182c361-6b16-4c37-9645-5038930cf579-lpk48", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:03:13.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4183" for this suite.
Dec 13 20:03:19.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:03:19.179: INFO: namespace replication-controller-4183 deletion completed in 6.16729814s

• [SLOW TEST:16.332 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:03:19.179: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3545
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-n6z5x in namespace proxy-3545
I1213 20:03:19.330379      24 runners.go:184] Created replication controller with name: proxy-service-n6z5x, namespace: proxy-3545, replica count: 1
I1213 20:03:20.386873      24 runners.go:184] proxy-service-n6z5x Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1213 20:03:21.387105      24 runners.go:184] proxy-service-n6z5x Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1213 20:03:22.387276      24 runners.go:184] proxy-service-n6z5x Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1213 20:03:23.387434      24 runners.go:184] proxy-service-n6z5x Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1213 20:03:24.387666      24 runners.go:184] proxy-service-n6z5x Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1213 20:03:25.387862      24 runners.go:184] proxy-service-n6z5x Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1213 20:03:26.391275      24 runners.go:184] proxy-service-n6z5x Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1213 20:03:27.391447      24 runners.go:184] proxy-service-n6z5x Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1213 20:03:28.391608      24 runners.go:184] proxy-service-n6z5x Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1213 20:03:29.392311      24 runners.go:184] proxy-service-n6z5x Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 13 20:03:29.395: INFO: setup took 10.086237196s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec 13 20:03:29.406: INFO: (0) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">... (200; 10.240339ms)
Dec 13 20:03:29.406: INFO: (0) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 10.596673ms)
Dec 13 20:03:29.406: INFO: (0) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 10.828194ms)
Dec 13 20:03:29.407: INFO: (0) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/rewriteme">test</a> (200; 10.206738ms)
Dec 13 20:03:29.407: INFO: (0) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 11.051033ms)
Dec 13 20:03:29.407: INFO: (0) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 10.979563ms)
Dec 13 20:03:29.407: INFO: (0) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname1/proxy/: foo (200; 11.254118ms)
Dec 13 20:03:29.407: INFO: (0) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">test<... (200; 10.216606ms)
Dec 13 20:03:29.410: INFO: (0) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:460/proxy/: tls baz (200; 14.202479ms)
Dec 13 20:03:29.410: INFO: (0) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:462/proxy/: tls qux (200; 13.931146ms)
Dec 13 20:03:29.410: INFO: (0) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/tlsrewritem... (200; 15.023844ms)
Dec 13 20:03:29.414: INFO: (0) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname2/proxy/: tls qux (200; 17.849643ms)
Dec 13 20:03:29.414: INFO: (0) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname2/proxy/: bar (200; 18.266749ms)
Dec 13 20:03:29.414: INFO: (0) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname1/proxy/: foo (200; 18.011463ms)
Dec 13 20:03:29.414: INFO: (0) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname2/proxy/: bar (200; 18.240542ms)
Dec 13 20:03:29.414: INFO: (0) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname1/proxy/: tls baz (200; 18.646713ms)
Dec 13 20:03:29.419: INFO: (1) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:460/proxy/: tls baz (200; 4.49607ms)
Dec 13 20:03:29.419: INFO: (1) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 4.612845ms)
Dec 13 20:03:29.419: INFO: (1) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">... (200; 5.198541ms)
Dec 13 20:03:29.419: INFO: (1) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 4.712223ms)
Dec 13 20:03:29.419: INFO: (1) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 4.907721ms)
Dec 13 20:03:29.419: INFO: (1) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 4.989055ms)
Dec 13 20:03:29.419: INFO: (1) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/rewriteme">test</a> (200; 5.191164ms)
Dec 13 20:03:29.419: INFO: (1) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">test<... (200; 5.147086ms)
Dec 13 20:03:29.422: INFO: (1) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname2/proxy/: tls qux (200; 7.13434ms)
Dec 13 20:03:29.422: INFO: (1) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname1/proxy/: foo (200; 7.534642ms)
Dec 13 20:03:29.422: INFO: (1) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/tlsrewritem... (200; 7.721746ms)
Dec 13 20:03:29.422: INFO: (1) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname1/proxy/: foo (200; 7.239685ms)
Dec 13 20:03:29.422: INFO: (1) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:462/proxy/: tls qux (200; 7.372199ms)
Dec 13 20:03:29.422: INFO: (1) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname1/proxy/: tls baz (200; 7.525803ms)
Dec 13 20:03:29.422: INFO: (1) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname2/proxy/: bar (200; 7.326669ms)
Dec 13 20:03:29.422: INFO: (1) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname2/proxy/: bar (200; 7.295983ms)
Dec 13 20:03:29.427: INFO: (2) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 5.013525ms)
Dec 13 20:03:29.427: INFO: (2) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:460/proxy/: tls baz (200; 4.915515ms)
Dec 13 20:03:29.427: INFO: (2) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/tlsrewritem... (200; 5.165773ms)
Dec 13 20:03:29.427: INFO: (2) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/rewriteme">test</a> (200; 5.162448ms)
Dec 13 20:03:29.427: INFO: (2) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">... (200; 4.934155ms)
Dec 13 20:03:29.427: INFO: (2) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">test<... (200; 5.271015ms)
Dec 13 20:03:29.427: INFO: (2) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 5.139604ms)
Dec 13 20:03:29.427: INFO: (2) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 5.091396ms)
Dec 13 20:03:29.428: INFO: (2) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:462/proxy/: tls qux (200; 5.081274ms)
Dec 13 20:03:29.428: INFO: (2) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 5.240506ms)
Dec 13 20:03:29.429: INFO: (2) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname1/proxy/: foo (200; 7.032542ms)
Dec 13 20:03:29.429: INFO: (2) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname1/proxy/: foo (200; 7.027332ms)
Dec 13 20:03:29.429: INFO: (2) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname2/proxy/: tls qux (200; 6.726703ms)
Dec 13 20:03:29.429: INFO: (2) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname2/proxy/: bar (200; 6.823519ms)
Dec 13 20:03:29.429: INFO: (2) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname1/proxy/: tls baz (200; 6.947947ms)
Dec 13 20:03:29.429: INFO: (2) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname2/proxy/: bar (200; 6.864696ms)
Dec 13 20:03:29.438: INFO: (3) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:462/proxy/: tls qux (200; 8.205762ms)
Dec 13 20:03:29.438: INFO: (3) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">test<... (200; 8.262959ms)
Dec 13 20:03:29.438: INFO: (3) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 8.312805ms)
Dec 13 20:03:29.438: INFO: (3) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">... (200; 8.36864ms)
Dec 13 20:03:29.438: INFO: (3) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 8.348799ms)
Dec 13 20:03:29.438: INFO: (3) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/tlsrewritem... (200; 8.49519ms)
Dec 13 20:03:29.438: INFO: (3) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 8.478032ms)
Dec 13 20:03:29.438: INFO: (3) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/rewriteme">test</a> (200; 8.287396ms)
Dec 13 20:03:29.438: INFO: (3) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:460/proxy/: tls baz (200; 8.376857ms)
Dec 13 20:03:29.438: INFO: (3) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 8.415683ms)
Dec 13 20:03:29.439: INFO: (3) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname2/proxy/: bar (200; 9.165377ms)
Dec 13 20:03:29.439: INFO: (3) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname1/proxy/: foo (200; 9.22628ms)
Dec 13 20:03:29.439: INFO: (3) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname2/proxy/: tls qux (200; 9.21722ms)
Dec 13 20:03:29.439: INFO: (3) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname1/proxy/: foo (200; 9.335367ms)
Dec 13 20:03:29.439: INFO: (3) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname1/proxy/: tls baz (200; 9.337226ms)
Dec 13 20:03:29.439: INFO: (3) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname2/proxy/: bar (200; 9.260426ms)
Dec 13 20:03:29.444: INFO: (4) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:460/proxy/: tls baz (200; 5.408821ms)
Dec 13 20:03:29.444: INFO: (4) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">... (200; 5.449622ms)
Dec 13 20:03:29.444: INFO: (4) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 5.494682ms)
Dec 13 20:03:29.444: INFO: (4) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/tlsrewritem... (200; 5.547289ms)
Dec 13 20:03:29.444: INFO: (4) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 5.393938ms)
Dec 13 20:03:29.444: INFO: (4) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">test<... (200; 5.381857ms)
Dec 13 20:03:29.444: INFO: (4) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 5.432868ms)
Dec 13 20:03:29.444: INFO: (4) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:462/proxy/: tls qux (200; 5.383574ms)
Dec 13 20:03:29.444: INFO: (4) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 5.405559ms)
Dec 13 20:03:29.444: INFO: (4) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/rewriteme">test</a> (200; 5.37795ms)
Dec 13 20:03:29.446: INFO: (4) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname1/proxy/: foo (200; 7.272823ms)
Dec 13 20:03:29.446: INFO: (4) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname2/proxy/: tls qux (200; 7.27183ms)
Dec 13 20:03:29.446: INFO: (4) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname2/proxy/: bar (200; 7.195852ms)
Dec 13 20:03:29.446: INFO: (4) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname2/proxy/: bar (200; 7.231283ms)
Dec 13 20:03:29.446: INFO: (4) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname1/proxy/: foo (200; 7.373779ms)
Dec 13 20:03:29.446: INFO: (4) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname1/proxy/: tls baz (200; 7.364651ms)
Dec 13 20:03:29.452: INFO: (5) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 5.245798ms)
Dec 13 20:03:29.452: INFO: (5) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/tlsrewritem... (200; 5.200446ms)
Dec 13 20:03:29.452: INFO: (5) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:460/proxy/: tls baz (200; 5.446199ms)
Dec 13 20:03:29.452: INFO: (5) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 5.307699ms)
Dec 13 20:03:29.452: INFO: (5) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:462/proxy/: tls qux (200; 5.424484ms)
Dec 13 20:03:29.452: INFO: (5) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 5.294564ms)
Dec 13 20:03:29.452: INFO: (5) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/rewriteme">test</a> (200; 5.463649ms)
Dec 13 20:03:29.452: INFO: (5) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">test<... (200; 5.332328ms)
Dec 13 20:03:29.452: INFO: (5) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">... (200; 5.432203ms)
Dec 13 20:03:29.452: INFO: (5) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 5.394779ms)
Dec 13 20:03:29.453: INFO: (5) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname2/proxy/: bar (200; 6.39374ms)
Dec 13 20:03:29.453: INFO: (5) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname2/proxy/: bar (200; 6.428841ms)
Dec 13 20:03:29.453: INFO: (5) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname2/proxy/: tls qux (200; 6.501697ms)
Dec 13 20:03:29.453: INFO: (5) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname1/proxy/: tls baz (200; 6.530379ms)
Dec 13 20:03:29.453: INFO: (5) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname1/proxy/: foo (200; 6.586168ms)
Dec 13 20:03:29.453: INFO: (5) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname1/proxy/: foo (200; 6.639642ms)
Dec 13 20:03:29.457: INFO: (6) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">test<... (200; 3.352255ms)
Dec 13 20:03:29.457: INFO: (6) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/tlsrewritem... (200; 3.535338ms)
Dec 13 20:03:29.457: INFO: (6) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/rewriteme">test</a> (200; 3.486698ms)
Dec 13 20:03:29.460: INFO: (6) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname1/proxy/: foo (200; 5.640809ms)
Dec 13 20:03:29.460: INFO: (6) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 6.285685ms)
Dec 13 20:03:29.460: INFO: (6) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname2/proxy/: bar (200; 5.982428ms)
Dec 13 20:03:29.461: INFO: (6) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">... (200; 6.959983ms)
Dec 13 20:03:29.461: INFO: (6) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:460/proxy/: tls baz (200; 7.193204ms)
Dec 13 20:03:29.461: INFO: (6) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname1/proxy/: tls baz (200; 7.307379ms)
Dec 13 20:03:29.461: INFO: (6) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 7.557376ms)
Dec 13 20:03:29.461: INFO: (6) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname2/proxy/: tls qux (200; 7.873125ms)
Dec 13 20:03:29.461: INFO: (6) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:462/proxy/: tls qux (200; 6.950701ms)
Dec 13 20:03:29.461: INFO: (6) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 7.293644ms)
Dec 13 20:03:29.461: INFO: (6) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 7.478385ms)
Dec 13 20:03:29.461: INFO: (6) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname2/proxy/: bar (200; 7.435353ms)
Dec 13 20:03:29.461: INFO: (6) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname1/proxy/: foo (200; 7.756105ms)
Dec 13 20:03:29.468: INFO: (7) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname2/proxy/: tls qux (200; 6.921634ms)
Dec 13 20:03:29.468: INFO: (7) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/tlsrewritem... (200; 6.809418ms)
Dec 13 20:03:29.468: INFO: (7) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">... (200; 6.953949ms)
Dec 13 20:03:29.468: INFO: (7) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:460/proxy/: tls baz (200; 6.919763ms)
Dec 13 20:03:29.469: INFO: (7) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 6.953403ms)
Dec 13 20:03:29.469: INFO: (7) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:462/proxy/: tls qux (200; 7.23282ms)
Dec 13 20:03:29.469: INFO: (7) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 6.831828ms)
Dec 13 20:03:29.469: INFO: (7) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/rewriteme">test</a> (200; 6.985074ms)
Dec 13 20:03:29.469: INFO: (7) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">test<... (200; 6.976346ms)
Dec 13 20:03:29.469: INFO: (7) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 7.170728ms)
Dec 13 20:03:29.469: INFO: (7) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 7.460462ms)
Dec 13 20:03:29.469: INFO: (7) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname1/proxy/: tls baz (200; 7.116269ms)
Dec 13 20:03:29.469: INFO: (7) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname1/proxy/: foo (200; 7.303447ms)
Dec 13 20:03:29.469: INFO: (7) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname1/proxy/: foo (200; 7.328383ms)
Dec 13 20:03:29.469: INFO: (7) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname2/proxy/: bar (200; 7.13267ms)
Dec 13 20:03:29.469: INFO: (7) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname2/proxy/: bar (200; 7.128062ms)
Dec 13 20:03:29.475: INFO: (8) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:462/proxy/: tls qux (200; 6.121717ms)
Dec 13 20:03:29.475: INFO: (8) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:460/proxy/: tls baz (200; 6.185276ms)
Dec 13 20:03:29.475: INFO: (8) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">... (200; 6.202592ms)
Dec 13 20:03:29.475: INFO: (8) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 6.258007ms)
Dec 13 20:03:29.475: INFO: (8) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 6.30665ms)
Dec 13 20:03:29.475: INFO: (8) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/tlsrewritem... (200; 6.466718ms)
Dec 13 20:03:29.475: INFO: (8) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/rewriteme">test</a> (200; 6.236441ms)
Dec 13 20:03:29.475: INFO: (8) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 6.386211ms)
Dec 13 20:03:29.475: INFO: (8) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 6.453189ms)
Dec 13 20:03:29.475: INFO: (8) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">test<... (200; 6.321944ms)
Dec 13 20:03:29.476: INFO: (8) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname2/proxy/: bar (200; 6.900731ms)
Dec 13 20:03:29.476: INFO: (8) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname2/proxy/: bar (200; 7.235832ms)
Dec 13 20:03:29.476: INFO: (8) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname1/proxy/: tls baz (200; 7.249085ms)
Dec 13 20:03:29.476: INFO: (8) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname2/proxy/: tls qux (200; 7.158483ms)
Dec 13 20:03:29.476: INFO: (8) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname1/proxy/: foo (200; 7.213651ms)
Dec 13 20:03:29.476: INFO: (8) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname1/proxy/: foo (200; 7.388684ms)
Dec 13 20:03:29.481: INFO: (9) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/rewriteme">test</a> (200; 4.900787ms)
Dec 13 20:03:29.482: INFO: (9) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 5.025629ms)
Dec 13 20:03:29.482: INFO: (9) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 5.091442ms)
Dec 13 20:03:29.482: INFO: (9) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:460/proxy/: tls baz (200; 5.058005ms)
Dec 13 20:03:29.482: INFO: (9) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">test<... (200; 4.975422ms)
Dec 13 20:03:29.482: INFO: (9) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 5.083593ms)
Dec 13 20:03:29.482: INFO: (9) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">... (200; 5.079798ms)
Dec 13 20:03:29.482: INFO: (9) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 5.184624ms)
Dec 13 20:03:29.482: INFO: (9) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/tlsrewritem... (200; 5.073316ms)
Dec 13 20:03:29.482: INFO: (9) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:462/proxy/: tls qux (200; 5.124373ms)
Dec 13 20:03:29.483: INFO: (9) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname1/proxy/: foo (200; 6.781658ms)
Dec 13 20:03:29.483: INFO: (9) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname1/proxy/: foo (200; 6.920724ms)
Dec 13 20:03:29.483: INFO: (9) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname2/proxy/: tls qux (200; 6.731876ms)
Dec 13 20:03:29.483: INFO: (9) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname1/proxy/: tls baz (200; 6.946485ms)
Dec 13 20:03:29.483: INFO: (9) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname2/proxy/: bar (200; 6.892058ms)
Dec 13 20:03:29.483: INFO: (9) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname2/proxy/: bar (200; 7.050573ms)
Dec 13 20:03:29.489: INFO: (10) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">test<... (200; 5.667917ms)
Dec 13 20:03:29.489: INFO: (10) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 5.76136ms)
Dec 13 20:03:29.489: INFO: (10) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 5.887795ms)
Dec 13 20:03:29.489: INFO: (10) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:460/proxy/: tls baz (200; 5.821467ms)
Dec 13 20:03:29.489: INFO: (10) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 5.756952ms)
Dec 13 20:03:29.490: INFO: (10) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:462/proxy/: tls qux (200; 5.905381ms)
Dec 13 20:03:29.490: INFO: (10) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/rewriteme">test</a> (200; 5.807148ms)
Dec 13 20:03:29.490: INFO: (10) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/tlsrewritem... (200; 5.844277ms)
Dec 13 20:03:29.490: INFO: (10) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 5.906738ms)
Dec 13 20:03:29.490: INFO: (10) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">... (200; 6.032986ms)
Dec 13 20:03:29.490: INFO: (10) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname1/proxy/: foo (200; 6.715793ms)
Dec 13 20:03:29.490: INFO: (10) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname2/proxy/: bar (200; 6.667368ms)
Dec 13 20:03:29.490: INFO: (10) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname2/proxy/: bar (200; 6.813575ms)
Dec 13 20:03:29.490: INFO: (10) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname1/proxy/: foo (200; 6.850505ms)
Dec 13 20:03:29.490: INFO: (10) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname1/proxy/: tls baz (200; 6.79116ms)
Dec 13 20:03:29.491: INFO: (10) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname2/proxy/: tls qux (200; 7.250751ms)
Dec 13 20:03:29.505: INFO: (11) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/tlsrewritem... (200; 13.866829ms)
Dec 13 20:03:29.505: INFO: (11) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 13.989038ms)
Dec 13 20:03:29.505: INFO: (11) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">... (200; 13.83399ms)
Dec 13 20:03:29.505: INFO: (11) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:462/proxy/: tls qux (200; 13.828ms)
Dec 13 20:03:29.505: INFO: (11) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname1/proxy/: foo (200; 13.93458ms)
Dec 13 20:03:29.505: INFO: (11) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 14.068821ms)
Dec 13 20:03:29.505: INFO: (11) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 13.891084ms)
Dec 13 20:03:29.505: INFO: (11) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:460/proxy/: tls baz (200; 14.011755ms)
Dec 13 20:03:29.505: INFO: (11) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/rewriteme">test</a> (200; 14.068809ms)
Dec 13 20:03:29.505: INFO: (11) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname2/proxy/: bar (200; 13.913801ms)
Dec 13 20:03:29.505: INFO: (11) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname2/proxy/: tls qux (200; 14.021772ms)
Dec 13 20:03:29.505: INFO: (11) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">test<... (200; 14.149278ms)
Dec 13 20:03:29.505: INFO: (11) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname1/proxy/: tls baz (200; 14.082192ms)
Dec 13 20:03:29.505: INFO: (11) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 14.065344ms)
Dec 13 20:03:29.508: INFO: (11) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname1/proxy/: foo (200; 16.910193ms)
Dec 13 20:03:29.508: INFO: (11) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname2/proxy/: bar (200; 16.925681ms)
Dec 13 20:03:29.513: INFO: (12) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 4.576411ms)
Dec 13 20:03:29.513: INFO: (12) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 4.74277ms)
Dec 13 20:03:29.513: INFO: (12) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/rewriteme">test</a> (200; 4.743118ms)
Dec 13 20:03:29.513: INFO: (12) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 4.652501ms)
Dec 13 20:03:29.513: INFO: (12) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:462/proxy/: tls qux (200; 4.822333ms)
Dec 13 20:03:29.515: INFO: (12) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">test<... (200; 6.111594ms)
Dec 13 20:03:29.515: INFO: (12) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 6.103758ms)
Dec 13 20:03:29.515: INFO: (12) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:460/proxy/: tls baz (200; 6.28326ms)
Dec 13 20:03:29.515: INFO: (12) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">... (200; 6.296164ms)
Dec 13 20:03:29.515: INFO: (12) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/tlsrewritem... (200; 6.892638ms)
Dec 13 20:03:29.516: INFO: (12) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname2/proxy/: bar (200; 7.931948ms)
Dec 13 20:03:29.516: INFO: (12) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname2/proxy/: tls qux (200; 7.938234ms)
Dec 13 20:03:29.516: INFO: (12) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname1/proxy/: tls baz (200; 8.087558ms)
Dec 13 20:03:29.516: INFO: (12) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname1/proxy/: foo (200; 7.993989ms)
Dec 13 20:03:29.516: INFO: (12) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname2/proxy/: bar (200; 7.939703ms)
Dec 13 20:03:29.516: INFO: (12) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname1/proxy/: foo (200; 8.186055ms)
Dec 13 20:03:29.523: INFO: (13) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">... (200; 6.454686ms)
Dec 13 20:03:29.523: INFO: (13) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:462/proxy/: tls qux (200; 6.588657ms)
Dec 13 20:03:29.523: INFO: (13) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 6.732056ms)
Dec 13 20:03:29.523: INFO: (13) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">test<... (200; 6.471517ms)
Dec 13 20:03:29.523: INFO: (13) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 6.750209ms)
Dec 13 20:03:29.523: INFO: (13) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname1/proxy/: tls baz (200; 6.859284ms)
Dec 13 20:03:29.523: INFO: (13) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/tlsrewritem... (200; 6.590939ms)
Dec 13 20:03:29.523: INFO: (13) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 6.760623ms)
Dec 13 20:03:29.523: INFO: (13) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname1/proxy/: foo (200; 6.727515ms)
Dec 13 20:03:29.523: INFO: (13) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 6.926586ms)
Dec 13 20:03:29.524: INFO: (13) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/rewriteme">test</a> (200; 6.681326ms)
Dec 13 20:03:29.525: INFO: (13) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:460/proxy/: tls baz (200; 8.146628ms)
Dec 13 20:03:29.525: INFO: (13) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname2/proxy/: tls qux (200; 7.841026ms)
Dec 13 20:03:29.525: INFO: (13) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname1/proxy/: foo (200; 8.109499ms)
Dec 13 20:03:29.525: INFO: (13) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname2/proxy/: bar (200; 7.985326ms)
Dec 13 20:03:29.525: INFO: (13) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname2/proxy/: bar (200; 7.973947ms)
Dec 13 20:03:29.531: INFO: (14) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">... (200; 5.999075ms)
Dec 13 20:03:29.531: INFO: (14) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/tlsrewritem... (200; 6.261297ms)
Dec 13 20:03:29.531: INFO: (14) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:460/proxy/: tls baz (200; 6.158224ms)
Dec 13 20:03:29.531: INFO: (14) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:462/proxy/: tls qux (200; 6.053386ms)
Dec 13 20:03:29.531: INFO: (14) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/rewriteme">test</a> (200; 6.048598ms)
Dec 13 20:03:29.531: INFO: (14) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 5.909758ms)
Dec 13 20:03:29.531: INFO: (14) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">test<... (200; 6.069235ms)
Dec 13 20:03:29.531: INFO: (14) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 6.004691ms)
Dec 13 20:03:29.531: INFO: (14) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 6.087261ms)
Dec 13 20:03:29.531: INFO: (14) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 6.340545ms)
Dec 13 20:03:29.531: INFO: (14) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname1/proxy/: foo (200; 6.184448ms)
Dec 13 20:03:29.531: INFO: (14) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname1/proxy/: foo (200; 6.377646ms)
Dec 13 20:03:29.531: INFO: (14) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname2/proxy/: tls qux (200; 6.367064ms)
Dec 13 20:03:29.531: INFO: (14) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname2/proxy/: bar (200; 6.204301ms)
Dec 13 20:03:29.531: INFO: (14) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname1/proxy/: tls baz (200; 6.31817ms)
Dec 13 20:03:29.532: INFO: (14) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname2/proxy/: bar (200; 6.890137ms)
Dec 13 20:03:29.537: INFO: (15) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/rewriteme">test</a> (200; 3.935643ms)
Dec 13 20:03:29.538: INFO: (15) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:460/proxy/: tls baz (200; 5.858973ms)
Dec 13 20:03:29.539: INFO: (15) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/tlsrewritem... (200; 6.521132ms)
Dec 13 20:03:29.539: INFO: (15) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">... (200; 5.693159ms)
Dec 13 20:03:29.539: INFO: (15) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 6.145042ms)
Dec 13 20:03:29.539: INFO: (15) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:462/proxy/: tls qux (200; 5.65715ms)
Dec 13 20:03:29.539: INFO: (15) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 6.420406ms)
Dec 13 20:03:29.539: INFO: (15) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 6.229702ms)
Dec 13 20:03:29.539: INFO: (15) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 6.009949ms)
Dec 13 20:03:29.539: INFO: (15) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">test<... (200; 5.575058ms)
Dec 13 20:03:29.539: INFO: (15) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname2/proxy/: tls qux (200; 6.610257ms)
Dec 13 20:03:29.539: INFO: (15) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname1/proxy/: tls baz (200; 7.126874ms)
Dec 13 20:03:29.539: INFO: (15) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname1/proxy/: foo (200; 6.720888ms)
Dec 13 20:03:29.539: INFO: (15) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname1/proxy/: foo (200; 7.215211ms)
Dec 13 20:03:29.539: INFO: (15) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname2/proxy/: bar (200; 6.981509ms)
Dec 13 20:03:29.539: INFO: (15) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname2/proxy/: bar (200; 6.928994ms)
Dec 13 20:03:29.545: INFO: (16) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 5.194909ms)
Dec 13 20:03:29.545: INFO: (16) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">test<... (200; 5.304775ms)
Dec 13 20:03:29.545: INFO: (16) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/rewriteme">test</a> (200; 5.376824ms)
Dec 13 20:03:29.545: INFO: (16) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/tlsrewritem... (200; 5.305513ms)
Dec 13 20:03:29.545: INFO: (16) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 5.407306ms)
Dec 13 20:03:29.545: INFO: (16) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">... (200; 5.30812ms)
Dec 13 20:03:29.545: INFO: (16) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 5.576197ms)
Dec 13 20:03:29.545: INFO: (16) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 5.32423ms)
Dec 13 20:03:29.545: INFO: (16) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:462/proxy/: tls qux (200; 5.533719ms)
Dec 13 20:03:29.545: INFO: (16) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:460/proxy/: tls baz (200; 5.424972ms)
Dec 13 20:03:29.546: INFO: (16) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname2/proxy/: bar (200; 6.409468ms)
Dec 13 20:03:29.546: INFO: (16) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname2/proxy/: tls qux (200; 6.31251ms)
Dec 13 20:03:29.546: INFO: (16) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname1/proxy/: foo (200; 6.454353ms)
Dec 13 20:03:29.546: INFO: (16) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname1/proxy/: foo (200; 6.38059ms)
Dec 13 20:03:29.546: INFO: (16) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname2/proxy/: bar (200; 6.47717ms)
Dec 13 20:03:29.547: INFO: (16) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname1/proxy/: tls baz (200; 7.020441ms)
Dec 13 20:03:29.551: INFO: (17) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:460/proxy/: tls baz (200; 3.967577ms)
Dec 13 20:03:29.552: INFO: (17) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">... (200; 4.877774ms)
Dec 13 20:03:29.552: INFO: (17) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/tlsrewritem... (200; 5.080717ms)
Dec 13 20:03:29.552: INFO: (17) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:462/proxy/: tls qux (200; 4.909067ms)
Dec 13 20:03:29.552: INFO: (17) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 5.2103ms)
Dec 13 20:03:29.552: INFO: (17) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 5.109941ms)
Dec 13 20:03:29.552: INFO: (17) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 5.105809ms)
Dec 13 20:03:29.552: INFO: (17) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/rewriteme">test</a> (200; 5.198739ms)
Dec 13 20:03:29.552: INFO: (17) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 5.004894ms)
Dec 13 20:03:29.553: INFO: (17) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">test<... (200; 5.534828ms)
Dec 13 20:03:29.554: INFO: (17) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname1/proxy/: foo (200; 6.526767ms)
Dec 13 20:03:29.554: INFO: (17) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname2/proxy/: bar (200; 6.630763ms)
Dec 13 20:03:29.554: INFO: (17) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname2/proxy/: bar (200; 6.781319ms)
Dec 13 20:03:29.554: INFO: (17) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname1/proxy/: tls baz (200; 7.033745ms)
Dec 13 20:03:29.554: INFO: (17) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname1/proxy/: foo (200; 6.937684ms)
Dec 13 20:03:29.554: INFO: (17) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname2/proxy/: tls qux (200; 6.934575ms)
Dec 13 20:03:29.558: INFO: (18) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:462/proxy/: tls qux (200; 4.089372ms)
Dec 13 20:03:29.558: INFO: (18) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/tlsrewritem... (200; 4.202833ms)
Dec 13 20:03:29.558: INFO: (18) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 4.01363ms)
Dec 13 20:03:29.558: INFO: (18) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 3.975461ms)
Dec 13 20:03:29.560: INFO: (18) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/rewriteme">test</a> (200; 5.699057ms)
Dec 13 20:03:29.560: INFO: (18) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">... (200; 5.761998ms)
Dec 13 20:03:29.560: INFO: (18) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">test<... (200; 5.715456ms)
Dec 13 20:03:29.560: INFO: (18) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 5.800692ms)
Dec 13 20:03:29.560: INFO: (18) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 5.669706ms)
Dec 13 20:03:29.560: INFO: (18) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:460/proxy/: tls baz (200; 5.765965ms)
Dec 13 20:03:29.561: INFO: (18) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname2/proxy/: bar (200; 6.489906ms)
Dec 13 20:03:29.561: INFO: (18) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname2/proxy/: tls qux (200; 6.541135ms)
Dec 13 20:03:29.561: INFO: (18) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname1/proxy/: tls baz (200; 6.480432ms)
Dec 13 20:03:29.561: INFO: (18) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname1/proxy/: foo (200; 6.571511ms)
Dec 13 20:03:29.561: INFO: (18) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname2/proxy/: bar (200; 6.481505ms)
Dec 13 20:03:29.561: INFO: (18) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname1/proxy/: foo (200; 6.524944ms)
Dec 13 20:03:29.563: INFO: (19) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 2.546364ms)
Dec 13 20:03:29.566: INFO: (19) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:462/proxy/: tls qux (200; 5.618696ms)
Dec 13 20:03:29.566: INFO: (19) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 5.244914ms)
Dec 13 20:03:29.566: INFO: (19) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq/proxy/rewriteme">test</a> (200; 5.389927ms)
Dec 13 20:03:29.566: INFO: (19) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:160/proxy/: foo (200; 5.355313ms)
Dec 13 20:03:29.567: INFO: (19) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname2/proxy/: bar (200; 5.886999ms)
Dec 13 20:03:29.567: INFO: (19) /api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">test<... (200; 5.641447ms)
Dec 13 20:03:29.567: INFO: (19) /api/v1/namespaces/proxy-3545/services/proxy-service-n6z5x:portname1/proxy/: foo (200; 5.677051ms)
Dec 13 20:03:29.567: INFO: (19) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:1080/proxy/rewriteme">... (200; 5.823943ms)
Dec 13 20:03:29.567: INFO: (19) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/: <a href="/api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:443/proxy/tlsrewritem... (200; 5.795887ms)
Dec 13 20:03:29.567: INFO: (19) /api/v1/namespaces/proxy-3545/pods/https:proxy-service-n6z5x-snhfq:460/proxy/: tls baz (200; 6.244123ms)
Dec 13 20:03:29.567: INFO: (19) /api/v1/namespaces/proxy-3545/pods/http:proxy-service-n6z5x-snhfq:162/proxy/: bar (200; 6.625113ms)
Dec 13 20:03:29.569: INFO: (19) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname2/proxy/: tls qux (200; 7.699218ms)
Dec 13 20:03:29.569: INFO: (19) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname2/proxy/: bar (200; 7.826449ms)
Dec 13 20:03:29.569: INFO: (19) /api/v1/namespaces/proxy-3545/services/http:proxy-service-n6z5x:portname1/proxy/: foo (200; 7.72115ms)
Dec 13 20:03:29.569: INFO: (19) /api/v1/namespaces/proxy-3545/services/https:proxy-service-n6z5x:tlsportname1/proxy/: tls baz (200; 7.684666ms)
STEP: deleting ReplicationController proxy-service-n6z5x in namespace proxy-3545, will wait for the garbage collector to delete the pods
Dec 13 20:03:29.624: INFO: Deleting ReplicationController proxy-service-n6z5x took: 3.676237ms
Dec 13 20:03:30.226: INFO: Terminating ReplicationController proxy-service-n6z5x pods took: 602.119055ms
[AfterEach] version v1
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:03:35.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3545" for this suite.
Dec 13 20:03:41.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:03:41.836: INFO: namespace proxy-3545 deletion completed in 6.105970131s

• [SLOW TEST:22.657 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:03:41.836: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7078
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7078
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7078
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7078
Dec 13 20:03:42.002: INFO: Found 0 stateful pods, waiting for 1
Dec 13 20:03:52.005: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec 13 20:03:52.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=statefulset-7078 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 13 20:03:52.145: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 13 20:03:52.145: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 13 20:03:52.145: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 13 20:03:52.149: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 13 20:04:02.151: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 13 20:04:02.151: INFO: Waiting for statefulset status.replicas updated to 0
Dec 13 20:04:02.160: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999691s
Dec 13 20:04:03.163: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997546551s
Dec 13 20:04:04.166: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.994862095s
Dec 13 20:04:05.168: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.992292095s
Dec 13 20:04:06.170: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.98966568s
Dec 13 20:04:07.173: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.987505844s
Dec 13 20:04:08.175: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.985101723s
Dec 13 20:04:09.177: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.983044985s
Dec 13 20:04:10.180: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.980204573s
Dec 13 20:04:11.184: INFO: Verifying statefulset ss doesn't scale past 1 for another 977.417872ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7078
Dec 13 20:04:12.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=statefulset-7078 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 13 20:04:12.321: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 13 20:04:12.321: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 13 20:04:12.321: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 13 20:04:12.323: INFO: Found 1 stateful pods, waiting for 3
Dec 13 20:04:22.326: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 20:04:22.326: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 20:04:22.326: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec 13 20:04:22.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=statefulset-7078 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 13 20:04:22.552: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 13 20:04:22.552: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 13 20:04:22.552: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 13 20:04:22.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=statefulset-7078 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 13 20:04:22.870: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 13 20:04:22.870: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 13 20:04:22.870: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 13 20:04:22.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=statefulset-7078 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 13 20:04:23.082: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 13 20:04:23.082: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 13 20:04:23.082: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 13 20:04:23.082: INFO: Waiting for statefulset status.replicas updated to 0
Dec 13 20:04:23.085: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec 13 20:04:33.092: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 13 20:04:33.092: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 13 20:04:33.092: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 13 20:04:33.104: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999496s
Dec 13 20:04:34.107: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995896115s
Dec 13 20:04:35.110: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99320007s
Dec 13 20:04:36.113: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990443716s
Dec 13 20:04:37.117: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.987171959s
Dec 13 20:04:38.119: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.983587769s
Dec 13 20:04:39.122: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.980863711s
Dec 13 20:04:40.124: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.978512115s
Dec 13 20:04:41.127: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.975815804s
Dec 13 20:04:42.130: INFO: Verifying statefulset ss doesn't scale past 3 for another 972.674585ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7078
Dec 13 20:04:43.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=statefulset-7078 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 13 20:04:43.264: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 13 20:04:43.264: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 13 20:04:43.264: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 13 20:04:43.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=statefulset-7078 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 13 20:04:43.386: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 13 20:04:43.387: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 13 20:04:43.387: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 13 20:04:43.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=statefulset-7078 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 13 20:04:43.517: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 13 20:04:43.517: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 13 20:04:43.517: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 13 20:04:43.517: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 13 20:05:03.526: INFO: Deleting all statefulset in ns statefulset-7078
Dec 13 20:05:03.528: INFO: Scaling statefulset ss to 0
Dec 13 20:05:03.534: INFO: Waiting for statefulset status.replicas updated to 0
Dec 13 20:05:03.536: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:05:03.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7078" for this suite.
Dec 13 20:05:09.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:05:09.631: INFO: namespace statefulset-7078 deletion completed in 6.083338334s

• [SLOW TEST:87.794 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:05:09.631: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9114
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 13 20:05:10.139: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 13 20:05:13.160: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 20:05:13.163: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1592-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:05:14.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9114" for this suite.
Dec 13 20:05:20.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:05:20.328: INFO: namespace webhook-9114 deletion completed in 6.096987967s
STEP: Destroying namespace "webhook-9114-markers" for this suite.
Dec 13 20:05:26.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:05:26.563: INFO: namespace webhook-9114-markers deletion completed in 6.234756433s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:16.941 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:05:26.572: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7909
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 13 20:05:26.738: INFO: Waiting up to 5m0s for pod "pod-1a9e4550-4c86-46a5-b29c-f7ba92624217" in namespace "emptydir-7909" to be "success or failure"
Dec 13 20:05:26.749: INFO: Pod "pod-1a9e4550-4c86-46a5-b29c-f7ba92624217": Phase="Pending", Reason="", readiness=false. Elapsed: 11.031051ms
Dec 13 20:05:28.752: INFO: Pod "pod-1a9e4550-4c86-46a5-b29c-f7ba92624217": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013284193s
STEP: Saw pod success
Dec 13 20:05:28.752: INFO: Pod "pod-1a9e4550-4c86-46a5-b29c-f7ba92624217" satisfied condition "success or failure"
Dec 13 20:05:28.753: INFO: Trying to get logs from node master-0 pod pod-1a9e4550-4c86-46a5-b29c-f7ba92624217 container test-container: <nil>
STEP: delete the pod
Dec 13 20:05:28.781: INFO: Waiting for pod pod-1a9e4550-4c86-46a5-b29c-f7ba92624217 to disappear
Dec 13 20:05:28.784: INFO: Pod pod-1a9e4550-4c86-46a5-b29c-f7ba92624217 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:05:28.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7909" for this suite.
Dec 13 20:05:34.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:05:34.876: INFO: namespace emptydir-7909 deletion completed in 6.087883935s

• [SLOW TEST:8.304 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:05:34.876: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6752
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:05:35.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6752" for this suite.
Dec 13 20:05:41.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:05:41.095: INFO: namespace services-6752 deletion completed in 6.08337047s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:6.219 seconds]
[sig-network] Services
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:05:41.096: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3298
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Dec 13 20:05:41.230: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-728461388 proxy --unix-socket=/tmp/kubectl-proxy-unix217906247/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:05:41.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3298" for this suite.
Dec 13 20:05:47.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:05:47.365: INFO: namespace kubectl-3298 deletion completed in 6.087350687s

• [SLOW TEST:6.270 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:05:47.365: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5042
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 13 20:05:51.532: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 13 20:05:51.541: INFO: Pod pod-with-poststart-http-hook still exists
Dec 13 20:05:53.541: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 13 20:05:53.543: INFO: Pod pod-with-poststart-http-hook still exists
Dec 13 20:05:55.541: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 13 20:05:55.543: INFO: Pod pod-with-poststart-http-hook still exists
Dec 13 20:05:57.541: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 13 20:05:57.543: INFO: Pod pod-with-poststart-http-hook still exists
Dec 13 20:05:59.541: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 13 20:05:59.543: INFO: Pod pod-with-poststart-http-hook still exists
Dec 13 20:06:01.541: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 13 20:06:01.543: INFO: Pod pod-with-poststart-http-hook still exists
Dec 13 20:06:03.541: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 13 20:06:03.543: INFO: Pod pod-with-poststart-http-hook still exists
Dec 13 20:06:05.541: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 13 20:06:05.543: INFO: Pod pod-with-poststart-http-hook still exists
Dec 13 20:06:07.541: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 13 20:06:07.543: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:06:07.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5042" for this suite.
Dec 13 20:06:35.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:06:35.636: INFO: namespace container-lifecycle-hook-5042 deletion completed in 28.091074239s

• [SLOW TEST:48.271 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:06:35.637: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-527
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:06:35.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-527" for this suite.
Dec 13 20:06:47.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:06:47.957: INFO: namespace kubelet-test-527 deletion completed in 12.130055232s

• [SLOW TEST:12.320 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:06:47.957: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3838
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-9344bbf5-2c8d-4053-960c-f2eeb00eee6b in namespace container-probe-3838
Dec 13 20:06:50.132: INFO: Started pod liveness-9344bbf5-2c8d-4053-960c-f2eeb00eee6b in namespace container-probe-3838
STEP: checking the pod's current state and verifying that restartCount is present
Dec 13 20:06:50.134: INFO: Initial restart count of pod liveness-9344bbf5-2c8d-4053-960c-f2eeb00eee6b is 0
Dec 13 20:07:10.160: INFO: Restart count of pod container-probe-3838/liveness-9344bbf5-2c8d-4053-960c-f2eeb00eee6b is now 1 (20.025856663s elapsed)
Dec 13 20:07:30.187: INFO: Restart count of pod container-probe-3838/liveness-9344bbf5-2c8d-4053-960c-f2eeb00eee6b is now 2 (40.053053581s elapsed)
Dec 13 20:07:50.210: INFO: Restart count of pod container-probe-3838/liveness-9344bbf5-2c8d-4053-960c-f2eeb00eee6b is now 3 (1m0.07515912s elapsed)
Dec 13 20:08:10.282: INFO: Restart count of pod container-probe-3838/liveness-9344bbf5-2c8d-4053-960c-f2eeb00eee6b is now 4 (1m20.14782485s elapsed)
Dec 13 20:09:10.477: INFO: Restart count of pod container-probe-3838/liveness-9344bbf5-2c8d-4053-960c-f2eeb00eee6b is now 5 (2m20.342115254s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:09:10.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3838" for this suite.
Dec 13 20:09:16.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:09:16.683: INFO: namespace container-probe-3838 deletion completed in 6.181786751s

• [SLOW TEST:148.726 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:09:16.683: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-937
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec 13 20:09:18.830: INFO: &Pod{ObjectMeta:{send-events-ba2b033c-f27e-4999-8e22-c4106fa1ebeb  events-937 /api/v1/namespaces/events-937/pods/send-events-ba2b033c-f27e-4999-8e22-c4106fa1ebeb b7bc139e-8f2b-4fe2-afcc-d9af37ac503a 25401 0 2019-12-13 20:09:16 +0000 UTC <nil> <nil> map[name:foo time:810859553] map[cni.projectcalico.org/podIP:10.244.0.201/32] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcnw7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcnw7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcnw7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:09:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:09:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:09:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:09:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.2,PodIP:10.244.0.201,StartTime:2019-12-13 20:09:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-13 20:09:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:cri-o://8d3583c56e71f23c791ba3cf35d51cd35665e87a07c0a1b08f0ed05d1fb681e7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.201,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Dec 13 20:09:20.832: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec 13 20:09:22.835: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:09:22.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-937" for this suite.
Dec 13 20:10:06.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:10:06.950: INFO: namespace events-937 deletion completed in 44.090672291s

• [SLOW TEST:50.267 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:10:06.950: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7150
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-06a17765-cb7e-4aa7-9155-30ae52e8d3d3
STEP: Creating a pod to test consume secrets
Dec 13 20:10:07.102: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-09c179f3-69c1-4a10-af9b-aa9f2deb1189" in namespace "projected-7150" to be "success or failure"
Dec 13 20:10:07.115: INFO: Pod "pod-projected-secrets-09c179f3-69c1-4a10-af9b-aa9f2deb1189": Phase="Pending", Reason="", readiness=false. Elapsed: 12.653641ms
Dec 13 20:10:09.117: INFO: Pod "pod-projected-secrets-09c179f3-69c1-4a10-af9b-aa9f2deb1189": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014925419s
STEP: Saw pod success
Dec 13 20:10:09.117: INFO: Pod "pod-projected-secrets-09c179f3-69c1-4a10-af9b-aa9f2deb1189" satisfied condition "success or failure"
Dec 13 20:10:09.119: INFO: Trying to get logs from node master-0 pod pod-projected-secrets-09c179f3-69c1-4a10-af9b-aa9f2deb1189 container secret-volume-test: <nil>
STEP: delete the pod
Dec 13 20:10:09.147: INFO: Waiting for pod pod-projected-secrets-09c179f3-69c1-4a10-af9b-aa9f2deb1189 to disappear
Dec 13 20:10:09.149: INFO: Pod pod-projected-secrets-09c179f3-69c1-4a10-af9b-aa9f2deb1189 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:10:09.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7150" for this suite.
Dec 13 20:10:15.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:10:15.239: INFO: namespace projected-7150 deletion completed in 6.087232291s

• [SLOW TEST:8.289 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:10:15.239: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-584
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 13 20:10:17.922: INFO: Successfully updated pod "pod-update-c1b4ee16-ec63-4b83-af50-943a12b7ee59"
STEP: verifying the updated pod is in kubernetes
Dec 13 20:10:17.953: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:10:17.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-584" for this suite.
Dec 13 20:10:45.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:10:46.052: INFO: namespace pods-584 deletion completed in 28.095393611s

• [SLOW TEST:30.813 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:10:46.052: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8352
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-8352
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Dec 13 20:10:46.194: INFO: Found 0 stateful pods, waiting for 3
Dec 13 20:10:56.284: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 20:10:56.284: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 20:10:56.284: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 20:10:56.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=statefulset-8352 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 13 20:10:57.261: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 13 20:10:57.261: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 13 20:10:57.261: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec 13 20:11:07.289: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec 13 20:11:17.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=statefulset-8352 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 13 20:11:17.432: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 13 20:11:17.432: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 13 20:11:17.432: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 13 20:11:27.444: INFO: Waiting for StatefulSet statefulset-8352/ss2 to complete update
Dec 13 20:11:27.444: INFO: Waiting for Pod statefulset-8352/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 13 20:11:27.444: INFO: Waiting for Pod statefulset-8352/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Dec 13 20:11:37.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=statefulset-8352 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 13 20:11:37.566: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 13 20:11:37.566: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 13 20:11:37.566: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 13 20:11:47.590: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec 13 20:11:57.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=statefulset-8352 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 13 20:11:57.741: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 13 20:11:57.741: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 13 20:11:57.741: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 13 20:12:27.756: INFO: Waiting for StatefulSet statefulset-8352/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 13 20:12:37.762: INFO: Deleting all statefulset in ns statefulset-8352
Dec 13 20:12:37.764: INFO: Scaling statefulset ss2 to 0
Dec 13 20:12:47.778: INFO: Waiting for statefulset status.replicas updated to 0
Dec 13 20:12:47.780: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:12:47.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8352" for this suite.
Dec 13 20:12:53.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:12:53.932: INFO: namespace statefulset-8352 deletion completed in 6.1008588s

• [SLOW TEST:127.879 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:12:53.932: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9058
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 13 20:12:54.072: INFO: Waiting up to 5m0s for pod "downward-api-9133d3d6-aeee-4e98-9b4f-0728086106cf" in namespace "downward-api-9058" to be "success or failure"
Dec 13 20:12:54.082: INFO: Pod "downward-api-9133d3d6-aeee-4e98-9b4f-0728086106cf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.929041ms
Dec 13 20:12:56.097: INFO: Pod "downward-api-9133d3d6-aeee-4e98-9b4f-0728086106cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025681887s
STEP: Saw pod success
Dec 13 20:12:56.098: INFO: Pod "downward-api-9133d3d6-aeee-4e98-9b4f-0728086106cf" satisfied condition "success or failure"
Dec 13 20:12:56.101: INFO: Trying to get logs from node master-0 pod downward-api-9133d3d6-aeee-4e98-9b4f-0728086106cf container dapi-container: <nil>
STEP: delete the pod
Dec 13 20:12:56.143: INFO: Waiting for pod downward-api-9133d3d6-aeee-4e98-9b4f-0728086106cf to disappear
Dec 13 20:12:56.152: INFO: Pod downward-api-9133d3d6-aeee-4e98-9b4f-0728086106cf no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:12:56.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9058" for this suite.
Dec 13 20:13:02.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:13:02.274: INFO: namespace downward-api-9058 deletion completed in 6.117336506s

• [SLOW TEST:8.342 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:13:02.274: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8872
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-33acb2d7-3387-42ba-95c2-696b443ddbe6
STEP: Creating a pod to test consume secrets
Dec 13 20:13:02.441: INFO: Waiting up to 5m0s for pod "pod-secrets-97a53a73-b26a-4d90-8b4b-f7661a43ad16" in namespace "secrets-8872" to be "success or failure"
Dec 13 20:13:02.452: INFO: Pod "pod-secrets-97a53a73-b26a-4d90-8b4b-f7661a43ad16": Phase="Pending", Reason="", readiness=false. Elapsed: 10.662411ms
Dec 13 20:13:04.455: INFO: Pod "pod-secrets-97a53a73-b26a-4d90-8b4b-f7661a43ad16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013133642s
STEP: Saw pod success
Dec 13 20:13:04.455: INFO: Pod "pod-secrets-97a53a73-b26a-4d90-8b4b-f7661a43ad16" satisfied condition "success or failure"
Dec 13 20:13:04.456: INFO: Trying to get logs from node master-0 pod pod-secrets-97a53a73-b26a-4d90-8b4b-f7661a43ad16 container secret-env-test: <nil>
STEP: delete the pod
Dec 13 20:13:04.477: INFO: Waiting for pod pod-secrets-97a53a73-b26a-4d90-8b4b-f7661a43ad16 to disappear
Dec 13 20:13:04.485: INFO: Pod pod-secrets-97a53a73-b26a-4d90-8b4b-f7661a43ad16 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:13:04.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8872" for this suite.
Dec 13 20:13:10.498: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:13:10.591: INFO: namespace secrets-8872 deletion completed in 6.101809286s

• [SLOW TEST:8.316 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:13:10.591: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6688
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 13 20:13:10.733: INFO: Waiting up to 5m0s for pod "downward-api-083e62be-acbe-4f49-9611-826439002a3d" in namespace "downward-api-6688" to be "success or failure"
Dec 13 20:13:10.736: INFO: Pod "downward-api-083e62be-acbe-4f49-9611-826439002a3d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.434405ms
Dec 13 20:13:12.741: INFO: Pod "downward-api-083e62be-acbe-4f49-9611-826439002a3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008175183s
STEP: Saw pod success
Dec 13 20:13:12.741: INFO: Pod "downward-api-083e62be-acbe-4f49-9611-826439002a3d" satisfied condition "success or failure"
Dec 13 20:13:12.746: INFO: Trying to get logs from node master-0 pod downward-api-083e62be-acbe-4f49-9611-826439002a3d container dapi-container: <nil>
STEP: delete the pod
Dec 13 20:13:12.785: INFO: Waiting for pod downward-api-083e62be-acbe-4f49-9611-826439002a3d to disappear
Dec 13 20:13:12.792: INFO: Pod downward-api-083e62be-acbe-4f49-9611-826439002a3d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:13:12.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6688" for this suite.
Dec 13 20:13:18.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:13:18.901: INFO: namespace downward-api-6688 deletion completed in 6.104579833s

• [SLOW TEST:8.310 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:13:18.901: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8720
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Dec 13 20:13:19.042: INFO: Waiting up to 5m0s for pod "client-containers-a786436e-624f-4c22-92fe-8a0c1c280b92" in namespace "containers-8720" to be "success or failure"
Dec 13 20:13:19.044: INFO: Pod "client-containers-a786436e-624f-4c22-92fe-8a0c1c280b92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.149591ms
Dec 13 20:13:21.047: INFO: Pod "client-containers-a786436e-624f-4c22-92fe-8a0c1c280b92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004556992s
STEP: Saw pod success
Dec 13 20:13:21.047: INFO: Pod "client-containers-a786436e-624f-4c22-92fe-8a0c1c280b92" satisfied condition "success or failure"
Dec 13 20:13:21.049: INFO: Trying to get logs from node master-0 pod client-containers-a786436e-624f-4c22-92fe-8a0c1c280b92 container test-container: <nil>
STEP: delete the pod
Dec 13 20:13:21.069: INFO: Waiting for pod client-containers-a786436e-624f-4c22-92fe-8a0c1c280b92 to disappear
Dec 13 20:13:21.075: INFO: Pod client-containers-a786436e-624f-4c22-92fe-8a0c1c280b92 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:13:21.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8720" for this suite.
Dec 13 20:13:27.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:13:27.167: INFO: namespace containers-8720 deletion completed in 6.088709207s

• [SLOW TEST:8.266 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:13:27.167: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-5392
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Dec 13 20:13:27.318: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 13 20:14:27.355: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 20:14:27.357: INFO: Starting informer...
STEP: Starting pods...
Dec 13 20:14:27.569: INFO: Pod1 is running on master-0. Tainting Node
Dec 13 20:14:29.782: INFO: Pod2 is running on master-0. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Dec 13 20:15:06.046: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
Dec 13 20:15:27.104: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:15:27.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-5392" for this suite.
Dec 13 20:15:35.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:15:35.620: INFO: namespace taint-multiple-pods-5392 deletion completed in 8.45929305s

• [SLOW TEST:128.453 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:15:35.620: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9503
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-34db499c-e399-4b4a-b94a-a72cee2f2a35
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-34db499c-e399-4b4a-b94a-a72cee2f2a35
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:17:16.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9503" for this suite.
Dec 13 20:17:28.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:17:28.464: INFO: namespace projected-9503 deletion completed in 12.096186376s

• [SLOW TEST:112.844 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:17:28.464: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2024
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Dec 13 20:17:28.606: INFO: Waiting up to 5m0s for pod "var-expansion-e40d6f54-fbea-4df9-a7f4-bf6829371604" in namespace "var-expansion-2024" to be "success or failure"
Dec 13 20:17:28.615: INFO: Pod "var-expansion-e40d6f54-fbea-4df9-a7f4-bf6829371604": Phase="Pending", Reason="", readiness=false. Elapsed: 9.505225ms
Dec 13 20:17:30.618: INFO: Pod "var-expansion-e40d6f54-fbea-4df9-a7f4-bf6829371604": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011864939s
STEP: Saw pod success
Dec 13 20:17:30.618: INFO: Pod "var-expansion-e40d6f54-fbea-4df9-a7f4-bf6829371604" satisfied condition "success or failure"
Dec 13 20:17:30.620: INFO: Trying to get logs from node master-0 pod var-expansion-e40d6f54-fbea-4df9-a7f4-bf6829371604 container dapi-container: <nil>
STEP: delete the pod
Dec 13 20:17:30.640: INFO: Waiting for pod var-expansion-e40d6f54-fbea-4df9-a7f4-bf6829371604 to disappear
Dec 13 20:17:30.644: INFO: Pod var-expansion-e40d6f54-fbea-4df9-a7f4-bf6829371604 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:17:30.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2024" for this suite.
Dec 13 20:17:36.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:17:36.755: INFO: namespace var-expansion-2024 deletion completed in 6.107824131s

• [SLOW TEST:8.291 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:17:36.755: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-912
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 13 20:17:37.229: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 13 20:17:40.250: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:17:40.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-912" for this suite.
Dec 13 20:17:46.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:17:46.400: INFO: namespace webhook-912 deletion completed in 6.106845782s
STEP: Destroying namespace "webhook-912-markers" for this suite.
Dec 13 20:17:52.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:17:52.509: INFO: namespace webhook-912-markers deletion completed in 6.108855289s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:15.767 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:17:52.522: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3724
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Dec 13 20:17:52.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 create -f - --namespace=kubectl-3724'
Dec 13 20:17:52.962: INFO: stderr: ""
Dec 13 20:17:52.962: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 13 20:17:52.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3724'
Dec 13 20:17:53.087: INFO: stderr: ""
Dec 13 20:17:53.087: INFO: stdout: "update-demo-nautilus-dtphf update-demo-nautilus-khgbf "
Dec 13 20:17:53.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-nautilus-dtphf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3724'
Dec 13 20:17:53.169: INFO: stderr: ""
Dec 13 20:17:53.169: INFO: stdout: ""
Dec 13 20:17:53.169: INFO: update-demo-nautilus-dtphf is created but not running
Dec 13 20:17:58.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3724'
Dec 13 20:17:58.234: INFO: stderr: ""
Dec 13 20:17:58.234: INFO: stdout: "update-demo-nautilus-dtphf update-demo-nautilus-khgbf "
Dec 13 20:17:58.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-nautilus-dtphf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3724'
Dec 13 20:17:58.295: INFO: stderr: ""
Dec 13 20:17:58.295: INFO: stdout: ""
Dec 13 20:17:58.295: INFO: update-demo-nautilus-dtphf is created but not running
Dec 13 20:18:03.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3724'
Dec 13 20:18:03.400: INFO: stderr: ""
Dec 13 20:18:03.400: INFO: stdout: "update-demo-nautilus-dtphf update-demo-nautilus-khgbf "
Dec 13 20:18:03.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-nautilus-dtphf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3724'
Dec 13 20:18:03.927: INFO: stderr: ""
Dec 13 20:18:03.927: INFO: stdout: "true"
Dec 13 20:18:03.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-nautilus-dtphf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3724'
Dec 13 20:18:04.041: INFO: stderr: ""
Dec 13 20:18:04.041: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 13 20:18:04.041: INFO: validating pod update-demo-nautilus-dtphf
Dec 13 20:18:04.049: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 13 20:18:04.049: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 13 20:18:04.049: INFO: update-demo-nautilus-dtphf is verified up and running
Dec 13 20:18:04.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-nautilus-khgbf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3724'
Dec 13 20:18:04.149: INFO: stderr: ""
Dec 13 20:18:04.149: INFO: stdout: "true"
Dec 13 20:18:04.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-nautilus-khgbf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3724'
Dec 13 20:18:04.251: INFO: stderr: ""
Dec 13 20:18:04.251: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 13 20:18:04.251: INFO: validating pod update-demo-nautilus-khgbf
Dec 13 20:18:04.254: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 13 20:18:04.254: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 13 20:18:04.254: INFO: update-demo-nautilus-khgbf is verified up and running
STEP: scaling down the replication controller
Dec 13 20:18:04.256: INFO: scanned /root for discovery docs: <nil>
Dec 13 20:18:04.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-3724'
Dec 13 20:18:05.401: INFO: stderr: ""
Dec 13 20:18:05.401: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 13 20:18:05.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3724'
Dec 13 20:18:05.465: INFO: stderr: ""
Dec 13 20:18:05.465: INFO: stdout: "update-demo-nautilus-dtphf update-demo-nautilus-khgbf "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 13 20:18:10.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3724'
Dec 13 20:18:10.529: INFO: stderr: ""
Dec 13 20:18:10.529: INFO: stdout: "update-demo-nautilus-dtphf update-demo-nautilus-khgbf "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 13 20:18:15.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3724'
Dec 13 20:18:15.612: INFO: stderr: ""
Dec 13 20:18:15.612: INFO: stdout: "update-demo-nautilus-dtphf update-demo-nautilus-khgbf "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 13 20:18:20.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3724'
Dec 13 20:18:20.673: INFO: stderr: ""
Dec 13 20:18:20.673: INFO: stdout: "update-demo-nautilus-khgbf "
Dec 13 20:18:20.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-nautilus-khgbf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3724'
Dec 13 20:18:20.736: INFO: stderr: ""
Dec 13 20:18:20.736: INFO: stdout: "true"
Dec 13 20:18:20.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-nautilus-khgbf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3724'
Dec 13 20:18:20.795: INFO: stderr: ""
Dec 13 20:18:20.795: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 13 20:18:20.795: INFO: validating pod update-demo-nautilus-khgbf
Dec 13 20:18:20.798: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 13 20:18:20.798: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 13 20:18:20.798: INFO: update-demo-nautilus-khgbf is verified up and running
STEP: scaling up the replication controller
Dec 13 20:18:20.800: INFO: scanned /root for discovery docs: <nil>
Dec 13 20:18:20.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-3724'
Dec 13 20:18:21.893: INFO: stderr: ""
Dec 13 20:18:21.893: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 13 20:18:21.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3724'
Dec 13 20:18:21.952: INFO: stderr: ""
Dec 13 20:18:21.952: INFO: stdout: "update-demo-nautilus-cpgch update-demo-nautilus-khgbf "
Dec 13 20:18:21.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-nautilus-cpgch -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3724'
Dec 13 20:18:22.010: INFO: stderr: ""
Dec 13 20:18:22.010: INFO: stdout: "true"
Dec 13 20:18:22.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-nautilus-cpgch -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3724'
Dec 13 20:18:22.074: INFO: stderr: ""
Dec 13 20:18:22.074: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 13 20:18:22.074: INFO: validating pod update-demo-nautilus-cpgch
Dec 13 20:18:22.076: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 13 20:18:22.076: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 13 20:18:22.076: INFO: update-demo-nautilus-cpgch is verified up and running
Dec 13 20:18:22.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-nautilus-khgbf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3724'
Dec 13 20:18:22.140: INFO: stderr: ""
Dec 13 20:18:22.140: INFO: stdout: "true"
Dec 13 20:18:22.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-nautilus-khgbf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3724'
Dec 13 20:18:22.199: INFO: stderr: ""
Dec 13 20:18:22.199: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 13 20:18:22.199: INFO: validating pod update-demo-nautilus-khgbf
Dec 13 20:18:22.201: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 13 20:18:22.201: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 13 20:18:22.201: INFO: update-demo-nautilus-khgbf is verified up and running
STEP: using delete to clean up resources
Dec 13 20:18:22.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 delete --grace-period=0 --force -f - --namespace=kubectl-3724'
Dec 13 20:18:22.263: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 13 20:18:22.263: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 13 20:18:22.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3724'
Dec 13 20:18:22.336: INFO: stderr: "No resources found in kubectl-3724 namespace.\n"
Dec 13 20:18:22.336: INFO: stdout: ""
Dec 13 20:18:22.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods -l name=update-demo --namespace=kubectl-3724 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 13 20:18:22.400: INFO: stderr: ""
Dec 13 20:18:22.400: INFO: stdout: "update-demo-nautilus-cpgch\nupdate-demo-nautilus-khgbf\n"
Dec 13 20:18:22.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3724'
Dec 13 20:18:22.990: INFO: stderr: "No resources found in kubectl-3724 namespace.\n"
Dec 13 20:18:22.990: INFO: stdout: ""
Dec 13 20:18:22.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods -l name=update-demo --namespace=kubectl-3724 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 13 20:18:23.093: INFO: stderr: ""
Dec 13 20:18:23.093: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:18:23.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3724" for this suite.
Dec 13 20:18:51.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:18:51.178: INFO: namespace kubectl-3724 deletion completed in 28.081194599s

• [SLOW TEST:58.656 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:18:51.178: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8160
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 13 20:18:51.310: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 13 20:18:51.320: INFO: Waiting for terminating namespaces to be deleted...
Dec 13 20:18:51.322: INFO: 
Logging pods the kubelet thinks is on node master-0 before test
Dec 13 20:18:51.331: INFO: csi-rbdplugin-8rpjc from rook-ceph started at 2019-12-13 20:15:27 +0000 UTC (3 container statuses recorded)
Dec 13 20:18:51.331: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 20:18:51.331: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:18:51.331: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:18:51.331: INFO: csi-cephfsplugin-vvg2s from rook-ceph started at 2019-12-13 20:15:27 +0000 UTC (3 container statuses recorded)
Dec 13 20:18:51.331: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 20:18:51.331: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:18:51.331: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:18:51.331: INFO: kube-apiserver-master-0 from kube-system started at 2019-12-13 18:54:06 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.331: INFO: 	Container kube-apiserver ready: true, restart count 0
Dec 13 20:18:51.331: INFO: sonobuoy-e2e-job-8f6bded5e69344cd from sonobuoy started at 2019-12-13 19:12:21 +0000 UTC (2 container statuses recorded)
Dec 13 20:18:51.331: INFO: 	Container e2e ready: true, restart count 0
Dec 13 20:18:51.331: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 13 20:18:51.331: INFO: sonobuoy-systemd-logs-daemon-set-9616c2f784dc4b45-rxl5l from sonobuoy started at 2019-12-13 19:12:22 +0000 UTC (2 container statuses recorded)
Dec 13 20:18:51.331: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 13 20:18:51.331: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 13 20:18:51.331: INFO: rook-discover-p77wp from rook-ceph started at 2019-12-13 20:15:31 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.331: INFO: 	Container rook-discover ready: true, restart count 0
Dec 13 20:18:51.331: INFO: kube-scheduler-master-0 from kube-system started at 2019-12-13 18:54:06 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.331: INFO: 	Container kube-scheduler ready: true, restart count 0
Dec 13 20:18:51.331: INFO: kube-proxy-dfvlw from kube-system started at 2019-12-13 18:54:29 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.331: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 13 20:18:51.331: INFO: fluentd-es-v2.5.2-mfcr4 from kube-system started at 2019-12-13 20:15:27 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.331: INFO: 	Container fluentd-es ready: true, restart count 0
Dec 13 20:18:51.331: INFO: rook-ceph-osd-2-58969bddc7-6zxpl from rook-ceph started at 2019-12-13 20:15:27 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.331: INFO: 	Container osd ready: true, restart count 0
Dec 13 20:18:51.331: INFO: rook-ceph-mon-b-5b988bf97b-4xkn2 from rook-ceph started at 2019-12-13 20:15:27 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.331: INFO: 	Container mon ready: true, restart count 0
Dec 13 20:18:51.331: INFO: kube-controller-manager-master-0 from kube-system started at 2019-12-13 18:54:06 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.331: INFO: 	Container kube-controller-manager ready: true, restart count 0
Dec 13 20:18:51.331: INFO: canal-xbp9c from kube-system started at 2019-12-13 18:55:18 +0000 UTC (2 container statuses recorded)
Dec 13 20:18:51.331: INFO: 	Container calico-node ready: true, restart count 0
Dec 13 20:18:51.331: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 13 20:18:51.331: INFO: kata-deploy-m4sjm from kube-system started at 2019-12-13 20:15:27 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.331: INFO: 	Container kube-kata ready: true, restart count 0
Dec 13 20:18:51.331: INFO: node-exporter-tk6xn from monitoring started at 2019-12-13 19:05:14 +0000 UTC (2 container statuses recorded)
Dec 13 20:18:51.331: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 13 20:18:51.331: INFO: 	Container node-exporter ready: true, restart count 0
Dec 13 20:18:51.331: INFO: etcd-master-0 from kube-system started at 2019-12-13 18:54:06 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.331: INFO: 	Container etcd ready: true, restart count 0
Dec 13 20:18:51.331: INFO: 
Logging pods the kubelet thinks is on node worker-0 before test
Dec 13 20:18:51.369: INFO: kubernetes-dashboard-6865b885fc-7tv2c from kubernetes-dashboard started at 2019-12-13 19:05:45 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec 13 20:18:51.370: INFO: alertmanager-main-2 from monitoring started at 2019-12-13 19:06:20 +0000 UTC (2 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container alertmanager ready: true, restart count 0
Dec 13 20:18:51.370: INFO: 	Container config-reloader ready: true, restart count 0
Dec 13 20:18:51.370: INFO: prometheus-k8s-1 from monitoring started at 2019-12-13 19:06:30 +0000 UTC (3 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container prometheus ready: true, restart count 1
Dec 13 20:18:51.370: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec 13 20:18:51.370: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec 13 20:18:51.370: INFO: coredns-5644d7b6d9-rngbs from kube-system started at 2019-12-13 18:55:47 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container coredns ready: true, restart count 0
Dec 13 20:18:51.370: INFO: coredns-5644d7b6d9-tdq6s from kube-system started at 2019-12-13 18:55:47 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container coredns ready: true, restart count 0
Dec 13 20:18:51.370: INFO: rook-discover-5t2vg from rook-ceph started at 2019-12-13 18:59:33 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container rook-discover ready: true, restart count 0
Dec 13 20:18:51.370: INFO: csi-rbdplugin-9vnm2 from rook-ceph started at 2019-12-13 18:59:33 +0000 UTC (3 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 20:18:51.370: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:18:51.370: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:18:51.370: INFO: rook-ceph-osd-prepare-worker-0-xz82f from rook-ceph started at 2019-12-13 19:03:42 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container provision ready: false, restart count 0
Dec 13 20:18:51.370: INFO: fluentd-es-v2.5.2-w2nk6 from kube-system started at 2019-12-13 19:07:51 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container fluentd-es ready: true, restart count 0
Dec 13 20:18:51.370: INFO: sonobuoy-systemd-logs-daemon-set-9616c2f784dc4b45-qpldg from sonobuoy started at 2019-12-13 19:12:22 +0000 UTC (2 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 13 20:18:51.370: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 13 20:18:51.370: INFO: grafana-5cd56df4cd-wqpht from monitoring started at 2019-12-13 19:41:23 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container grafana ready: true, restart count 0
Dec 13 20:18:51.370: INFO: rook-ceph-operator-c8ff6447d-6s7cz from rook-ceph started at 2019-12-13 18:55:45 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Dec 13 20:18:51.370: INFO: csi-cephfsplugin-n7682 from rook-ceph started at 2019-12-13 18:59:35 +0000 UTC (3 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 20:18:51.370: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:18:51.370: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:18:51.370: INFO: rook-ceph-osd-0-64bcd55b9f-hrsvl from rook-ceph started at 2019-12-13 19:03:50 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container osd ready: true, restart count 0
Dec 13 20:18:51.370: INFO: alertmanager-main-1 from monitoring started at 2019-12-13 20:14:55 +0000 UTC (2 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container alertmanager ready: true, restart count 0
Dec 13 20:18:51.370: INFO: 	Container config-reloader ready: true, restart count 0
Dec 13 20:18:51.370: INFO: elasticsearch-logging-0 from kube-system started at 2019-12-13 20:14:56 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Dec 13 20:18:51.370: INFO: metrics-server-7578984995-fr9dj from kube-system started at 2019-12-13 18:55:45 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container metrics-server ready: true, restart count 0
Dec 13 20:18:51.370: INFO: csi-cephfsplugin-provisioner-75c965db4f-dhmjp from rook-ceph started at 2019-12-13 18:59:36 +0000 UTC (4 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container csi-attacher ready: true, restart count 0
Dec 13 20:18:51.370: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 20:18:51.370: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 13 20:18:51.370: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:18:51.370: INFO: rook-ceph-mon-c-597fd9c9c7-r8ngb from rook-ceph started at 2019-12-13 19:02:59 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container mon ready: true, restart count 0
Dec 13 20:18:51.370: INFO: csi-rbdplugin-provisioner-56cbc4d585-lhtqr from rook-ceph started at 2019-12-13 20:14:30 +0000 UTC (5 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 13 20:18:51.370: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 20:18:51.370: INFO: 	Container csi-rbdplugin-attacher ready: true, restart count 0
Dec 13 20:18:51.370: INFO: 	Container csi-snapshotter ready: true, restart count 0
Dec 13 20:18:51.370: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:18:51.370: INFO: dashboard-metrics-scraper-6ccf7f6cc8-k4zf5 from kubernetes-dashboard started at 2019-12-13 20:14:31 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Dec 13 20:18:51.370: INFO: kube-proxy-rktzd from kube-system started at 2019-12-13 18:54:55 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 13 20:18:51.370: INFO: canal-sgqjs from kube-system started at 2019-12-13 18:55:18 +0000 UTC (2 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container calico-node ready: true, restart count 0
Dec 13 20:18:51.370: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 13 20:18:51.370: INFO: kata-deploy-2qbn2 from kube-system started at 2019-12-13 18:55:45 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container kube-kata ready: true, restart count 0
Dec 13 20:18:51.370: INFO: node-exporter-584q9 from monitoring started at 2019-12-13 19:05:14 +0000 UTC (2 container statuses recorded)
Dec 13 20:18:51.370: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 13 20:18:51.370: INFO: 	Container node-exporter ready: true, restart count 0
Dec 13 20:18:51.370: INFO: 
Logging pods the kubelet thinks is on node worker-1 before test
Dec 13 20:18:51.454: INFO: csi-cephfsplugin-6szlf from rook-ceph started at 2019-12-13 18:59:35 +0000 UTC (3 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 20:18:51.454: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:18:51.454: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:18:51.454: INFO: rook-ceph-osd-1-7d8886d496-jzxm4 from rook-ceph started at 2019-12-13 19:03:51 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container osd ready: true, restart count 0
Dec 13 20:18:51.454: INFO: nginx-ingress-controller-7fb85bc8bb-8f8dr from ingress-nginx started at 2019-12-13 19:08:29 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec 13 20:18:51.454: INFO: rook-ceph-mgr-a-5775778c4d-k745v from rook-ceph started at 2019-12-13 20:14:30 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container mgr ready: true, restart count 0
Dec 13 20:18:51.454: INFO: kibana-logging-7f6b4b96b4-pdq55 from kube-system started at 2019-12-13 20:14:30 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container kibana-logging ready: true, restart count 0
Dec 13 20:18:51.454: INFO: csi-cephfsplugin-provisioner-75c965db4f-6plm2 from rook-ceph started at 2019-12-13 20:14:31 +0000 UTC (4 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container csi-attacher ready: true, restart count 0
Dec 13 20:18:51.454: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 20:18:51.454: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 13 20:18:51.454: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:18:51.454: INFO: canal-5wqk9 from kube-system started at 2019-12-13 18:55:18 +0000 UTC (2 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container calico-node ready: true, restart count 0
Dec 13 20:18:51.454: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 13 20:18:51.454: INFO: fluentd-es-v2.5.2-s2ng4 from kube-system started at 2019-12-13 19:07:51 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container fluentd-es ready: true, restart count 0
Dec 13 20:18:51.454: INFO: sonobuoy from sonobuoy started at 2019-12-13 19:12:05 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 13 20:18:51.454: INFO: kube-state-metrics-79d4b9b497-wntkj from monitoring started at 2019-12-13 20:14:30 +0000 UTC (3 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec 13 20:18:51.454: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec 13 20:18:51.454: INFO: 	Container kube-state-metrics ready: true, restart count 0
Dec 13 20:18:51.454: INFO: alertmanager-main-0 from monitoring started at 2019-12-13 19:06:20 +0000 UTC (2 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container alertmanager ready: true, restart count 0
Dec 13 20:18:51.454: INFO: 	Container config-reloader ready: true, restart count 0
Dec 13 20:18:51.454: INFO: csi-rbdplugin-rhs8g from rook-ceph started at 2019-12-13 18:59:33 +0000 UTC (3 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 20:18:51.454: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:18:51.454: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:18:51.454: INFO: csi-rbdplugin-provisioner-56cbc4d585-j64wf from rook-ceph started at 2019-12-13 18:59:34 +0000 UTC (5 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 13 20:18:51.454: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 20:18:51.454: INFO: 	Container csi-rbdplugin-attacher ready: true, restart count 0
Dec 13 20:18:51.454: INFO: 	Container csi-snapshotter ready: true, restart count 0
Dec 13 20:18:51.454: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:18:51.454: INFO: node-exporter-7ggvt from monitoring started at 2019-12-13 19:05:14 +0000 UTC (2 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 13 20:18:51.454: INFO: 	Container node-exporter ready: true, restart count 0
Dec 13 20:18:51.454: INFO: prometheus-adapter-c676d8764-7pnjf from monitoring started at 2019-12-13 19:41:23 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container prometheus-adapter ready: true, restart count 0
Dec 13 20:18:51.454: INFO: prometheus-k8s-0 from monitoring started at 2019-12-13 20:14:55 +0000 UTC (3 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container prometheus ready: true, restart count 1
Dec 13 20:18:51.454: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec 13 20:18:51.454: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec 13 20:18:51.454: INFO: kube-proxy-dt9pl from kube-system started at 2019-12-13 18:55:17 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 13 20:18:51.454: INFO: rook-discover-m4tgh from rook-ceph started at 2019-12-13 18:59:33 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container rook-discover ready: true, restart count 0
Dec 13 20:18:51.454: INFO: rook-ceph-mon-a-c7664c8c4-jcx8w from rook-ceph started at 2019-12-13 19:02:27 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container mon ready: true, restart count 0
Dec 13 20:18:51.454: INFO: rook-ceph-osd-prepare-worker-1-2xccr from rook-ceph started at 2019-12-13 19:03:42 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container provision ready: false, restart count 0
Dec 13 20:18:51.454: INFO: elasticsearch-logging-1 from kube-system started at 2019-12-13 19:10:24 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Dec 13 20:18:51.454: INFO: sonobuoy-systemd-logs-daemon-set-9616c2f784dc4b45-f29sm from sonobuoy started at 2019-12-13 19:12:22 +0000 UTC (2 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 13 20:18:51.454: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 13 20:18:51.454: INFO: prometheus-operator-7559d67ff-shr6m from monitoring started at 2019-12-13 20:14:30 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container prometheus-operator ready: true, restart count 0
Dec 13 20:18:51.454: INFO: kata-deploy-qtlwp from kube-system started at 2019-12-13 18:55:57 +0000 UTC (1 container statuses recorded)
Dec 13 20:18:51.454: INFO: 	Container kube-kata ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-ab2c8b82-f8f2-45bc-9cf8-865c0028d725 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-ab2c8b82-f8f2-45bc-9cf8-865c0028d725 off the node master-0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ab2c8b82-f8f2-45bc-9cf8-865c0028d725
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:23:55.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8160" for this suite.
Dec 13 20:24:03.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:24:03.726: INFO: namespace sched-pred-8160 deletion completed in 8.151698358s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:312.547 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:24:03.726: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1970
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 13 20:24:03.870: INFO: Waiting up to 5m0s for pod "downwardapi-volume-18871087-5506-49e8-9519-fa72c2a95150" in namespace "downward-api-1970" to be "success or failure"
Dec 13 20:24:03.874: INFO: Pod "downwardapi-volume-18871087-5506-49e8-9519-fa72c2a95150": Phase="Pending", Reason="", readiness=false. Elapsed: 3.343659ms
Dec 13 20:24:05.876: INFO: Pod "downwardapi-volume-18871087-5506-49e8-9519-fa72c2a95150": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005509632s
STEP: Saw pod success
Dec 13 20:24:05.876: INFO: Pod "downwardapi-volume-18871087-5506-49e8-9519-fa72c2a95150" satisfied condition "success or failure"
Dec 13 20:24:05.878: INFO: Trying to get logs from node master-0 pod downwardapi-volume-18871087-5506-49e8-9519-fa72c2a95150 container client-container: <nil>
STEP: delete the pod
Dec 13 20:24:05.992: INFO: Waiting for pod downwardapi-volume-18871087-5506-49e8-9519-fa72c2a95150 to disappear
Dec 13 20:24:06.002: INFO: Pod downwardapi-volume-18871087-5506-49e8-9519-fa72c2a95150 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:24:06.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1970" for this suite.
Dec 13 20:24:12.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:24:12.081: INFO: namespace downward-api-1970 deletion completed in 6.076838215s

• [SLOW TEST:8.356 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:24:12.081: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-347
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Dec 13 20:24:14.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec pod-sharedvolume-ce7156a0-c658-465e-adec-e44c4a103a06 -c busybox-main-container --namespace=emptydir-347 -- cat /usr/share/volumeshare/shareddata.txt'
Dec 13 20:24:14.649: INFO: stderr: ""
Dec 13 20:24:14.649: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:24:14.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-347" for this suite.
Dec 13 20:24:20.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:24:20.740: INFO: namespace emptydir-347 deletion completed in 6.087186733s

• [SLOW TEST:8.659 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:24:20.741: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1646
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 13 20:24:21.118: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 13 20:24:23.124: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711865461, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711865461, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711865461, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711865461, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 13 20:24:26.146: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:24:38.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1646" for this suite.
Dec 13 20:24:44.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:24:44.314: INFO: namespace webhook-1646 deletion completed in 6.084229158s
STEP: Destroying namespace "webhook-1646-markers" for this suite.
Dec 13 20:24:50.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:24:50.406: INFO: namespace webhook-1646-markers deletion completed in 6.091895639s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:29.673 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:24:50.414: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8002
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Dec 13 20:24:50.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 cluster-info'
Dec 13 20:24:50.635: INFO: stderr: ""
Dec 13 20:24:50.635: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mElasticsearch\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy\x1b[0m\n\x1b[0;32mKibana\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kibana-logging/proxy\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:24:50.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8002" for this suite.
Dec 13 20:24:56.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:24:56.779: INFO: namespace kubectl-8002 deletion completed in 6.141599175s

• [SLOW TEST:6.365 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:24:56.779: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-6349
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec 13 20:24:57.148: INFO: Pod name wrapped-volume-race-761a8e1d-c08c-48c7-9949-9b5c3a162e46: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-761a8e1d-c08c-48c7-9949-9b5c3a162e46 in namespace emptydir-wrapper-6349, will wait for the garbage collector to delete the pods
Dec 13 20:25:13.266: INFO: Deleting ReplicationController wrapped-volume-race-761a8e1d-c08c-48c7-9949-9b5c3a162e46 took: 5.042613ms
Dec 13 20:25:15.966: INFO: Terminating ReplicationController wrapped-volume-race-761a8e1d-c08c-48c7-9949-9b5c3a162e46 pods took: 2.700314084s
STEP: Creating RC which spawns configmap-volume pods
Dec 13 20:25:56.193: INFO: Pod name wrapped-volume-race-7b151df4-08ad-4a40-8922-5aa7f7284ec4: Found 0 pods out of 5
Dec 13 20:26:01.197: INFO: Pod name wrapped-volume-race-7b151df4-08ad-4a40-8922-5aa7f7284ec4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7b151df4-08ad-4a40-8922-5aa7f7284ec4 in namespace emptydir-wrapper-6349, will wait for the garbage collector to delete the pods
Dec 13 20:26:13.275: INFO: Deleting ReplicationController wrapped-volume-race-7b151df4-08ad-4a40-8922-5aa7f7284ec4 took: 3.751702ms
Dec 13 20:26:15.976: INFO: Terminating ReplicationController wrapped-volume-race-7b151df4-08ad-4a40-8922-5aa7f7284ec4 pods took: 2.700219971s
STEP: Creating RC which spawns configmap-volume pods
Dec 13 20:26:55.791: INFO: Pod name wrapped-volume-race-dd28b271-0829-4a60-8077-60c16a890390: Found 0 pods out of 5
Dec 13 20:27:00.803: INFO: Pod name wrapped-volume-race-dd28b271-0829-4a60-8077-60c16a890390: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-dd28b271-0829-4a60-8077-60c16a890390 in namespace emptydir-wrapper-6349, will wait for the garbage collector to delete the pods
Dec 13 20:27:10.872: INFO: Deleting ReplicationController wrapped-volume-race-dd28b271-0829-4a60-8077-60c16a890390 took: 4.326935ms
Dec 13 20:27:11.476: INFO: Terminating ReplicationController wrapped-volume-race-dd28b271-0829-4a60-8077-60c16a890390 pods took: 603.538968ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:27:56.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6349" for this suite.
Dec 13 20:28:02.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:28:02.609: INFO: namespace emptydir-wrapper-6349 deletion completed in 6.100795496s

• [SLOW TEST:185.830 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:28:02.609: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9497
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Dec 13 20:28:02.784: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 20:28:06.277: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:28:19.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9497" for this suite.
Dec 13 20:28:25.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:28:26.017: INFO: namespace crd-publish-openapi-9497 deletion completed in 6.095004704s

• [SLOW TEST:23.408 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:28:26.017: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6657
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 20:28:26.532: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"032a3712-6c19-42e0-b062-dcc4f40e3d1c", Controller:(*bool)(0xc0022decaa), BlockOwnerDeletion:(*bool)(0xc0022decab)}}
Dec 13 20:28:26.544: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"97ee6bd2-2d76-428e-a869-f18a5908091e", Controller:(*bool)(0xc00540faee), BlockOwnerDeletion:(*bool)(0xc00540faef)}}
Dec 13 20:28:26.564: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"ab0e00b0-3a1c-4fd3-802f-654cfcdeb233", Controller:(*bool)(0xc0022def86), BlockOwnerDeletion:(*bool)(0xc0022def87)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:28:31.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6657" for this suite.
Dec 13 20:28:37.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:28:37.659: INFO: namespace gc-6657 deletion completed in 6.0742124s

• [SLOW TEST:11.642 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:28:37.659: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6667
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Dec 13 20:28:47.820: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:28:47.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1213 20:28:47.820654      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-6667" for this suite.
Dec 13 20:28:53.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:28:53.941: INFO: namespace gc-6667 deletion completed in 6.116029443s

• [SLOW TEST:16.282 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:28:53.942: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-153
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-71d45476-6b28-4829-8ebd-171578389d19
STEP: Creating a pod to test consume configMaps
Dec 13 20:28:54.092: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2bc0521b-68e2-4f9b-a1a0-5ac9520d3352" in namespace "projected-153" to be "success or failure"
Dec 13 20:28:54.110: INFO: Pod "pod-projected-configmaps-2bc0521b-68e2-4f9b-a1a0-5ac9520d3352": Phase="Pending", Reason="", readiness=false. Elapsed: 18.066459ms
Dec 13 20:28:56.112: INFO: Pod "pod-projected-configmaps-2bc0521b-68e2-4f9b-a1a0-5ac9520d3352": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020529338s
STEP: Saw pod success
Dec 13 20:28:56.112: INFO: Pod "pod-projected-configmaps-2bc0521b-68e2-4f9b-a1a0-5ac9520d3352" satisfied condition "success or failure"
Dec 13 20:28:56.114: INFO: Trying to get logs from node master-0 pod pod-projected-configmaps-2bc0521b-68e2-4f9b-a1a0-5ac9520d3352 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 20:28:56.150: INFO: Waiting for pod pod-projected-configmaps-2bc0521b-68e2-4f9b-a1a0-5ac9520d3352 to disappear
Dec 13 20:28:56.159: INFO: Pod pod-projected-configmaps-2bc0521b-68e2-4f9b-a1a0-5ac9520d3352 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:28:56.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-153" for this suite.
Dec 13 20:29:02.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:29:02.250: INFO: namespace projected-153 deletion completed in 6.088189091s

• [SLOW TEST:8.309 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:29:02.250: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7519
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 20:29:02.387: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-b074be95-6a00-4881-938c-fa8c3da36cb0" in namespace "security-context-test-7519" to be "success or failure"
Dec 13 20:29:02.394: INFO: Pod "alpine-nnp-false-b074be95-6a00-4881-938c-fa8c3da36cb0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.344636ms
Dec 13 20:29:04.396: INFO: Pod "alpine-nnp-false-b074be95-6a00-4881-938c-fa8c3da36cb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009851078s
Dec 13 20:29:06.398: INFO: Pod "alpine-nnp-false-b074be95-6a00-4881-938c-fa8c3da36cb0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011847687s
Dec 13 20:29:08.401: INFO: Pod "alpine-nnp-false-b074be95-6a00-4881-938c-fa8c3da36cb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014147145s
Dec 13 20:29:08.401: INFO: Pod "alpine-nnp-false-b074be95-6a00-4881-938c-fa8c3da36cb0" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:29:08.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7519" for this suite.
Dec 13 20:29:14.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:29:14.513: INFO: namespace security-context-test-7519 deletion completed in 6.105059988s

• [SLOW TEST:12.263 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:29:14.514: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-7309
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 13 20:29:15.147: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 13 20:29:18.164: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 20:29:18.166: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:29:19.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7309" for this suite.
Dec 13 20:29:25.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:29:25.480: INFO: namespace crd-webhook-7309 deletion completed in 6.147200827s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:11.012 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:29:25.526: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9674
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1213 20:29:35.751226      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 13 20:29:35.751: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:29:35.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9674" for this suite.
Dec 13 20:29:41.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:29:41.978: INFO: namespace gc-9674 deletion completed in 6.224667665s

• [SLOW TEST:16.453 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:29:41.978: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3272
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 13 20:29:42.202: INFO: Waiting up to 5m0s for pod "downwardapi-volume-57cc4d98-f69c-4122-b338-c4d5eff621d6" in namespace "projected-3272" to be "success or failure"
Dec 13 20:29:42.239: INFO: Pod "downwardapi-volume-57cc4d98-f69c-4122-b338-c4d5eff621d6": Phase="Pending", Reason="", readiness=false. Elapsed: 36.586438ms
Dec 13 20:29:44.241: INFO: Pod "downwardapi-volume-57cc4d98-f69c-4122-b338-c4d5eff621d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.038915772s
STEP: Saw pod success
Dec 13 20:29:44.241: INFO: Pod "downwardapi-volume-57cc4d98-f69c-4122-b338-c4d5eff621d6" satisfied condition "success or failure"
Dec 13 20:29:44.243: INFO: Trying to get logs from node master-0 pod downwardapi-volume-57cc4d98-f69c-4122-b338-c4d5eff621d6 container client-container: <nil>
STEP: delete the pod
Dec 13 20:29:44.259: INFO: Waiting for pod downwardapi-volume-57cc4d98-f69c-4122-b338-c4d5eff621d6 to disappear
Dec 13 20:29:44.262: INFO: Pod downwardapi-volume-57cc4d98-f69c-4122-b338-c4d5eff621d6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:29:44.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3272" for this suite.
Dec 13 20:29:50.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:29:50.380: INFO: namespace projected-3272 deletion completed in 6.114592427s

• [SLOW TEST:8.402 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:29:50.381: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5674
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 13 20:29:50.542: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 13 20:29:50.550: INFO: Waiting for terminating namespaces to be deleted...
Dec 13 20:29:50.552: INFO: 
Logging pods the kubelet thinks is on node master-0 before test
Dec 13 20:29:50.559: INFO: node-exporter-tk6xn from monitoring started at 2019-12-13 19:05:14 +0000 UTC (2 container statuses recorded)
Dec 13 20:29:50.559: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 13 20:29:50.559: INFO: 	Container node-exporter ready: true, restart count 0
Dec 13 20:29:50.559: INFO: etcd-master-0 from kube-system started at 2019-12-13 18:54:06 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.559: INFO: 	Container etcd ready: true, restart count 0
Dec 13 20:29:50.559: INFO: csi-rbdplugin-8rpjc from rook-ceph started at 2019-12-13 20:15:27 +0000 UTC (3 container statuses recorded)
Dec 13 20:29:50.559: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 20:29:50.559: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:29:50.559: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:29:50.559: INFO: csi-cephfsplugin-vvg2s from rook-ceph started at 2019-12-13 20:15:27 +0000 UTC (3 container statuses recorded)
Dec 13 20:29:50.559: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 20:29:50.559: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:29:50.559: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:29:50.559: INFO: kube-apiserver-master-0 from kube-system started at 2019-12-13 18:54:06 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.559: INFO: 	Container kube-apiserver ready: true, restart count 0
Dec 13 20:29:50.559: INFO: sonobuoy-e2e-job-8f6bded5e69344cd from sonobuoy started at 2019-12-13 19:12:21 +0000 UTC (2 container statuses recorded)
Dec 13 20:29:50.559: INFO: 	Container e2e ready: true, restart count 0
Dec 13 20:29:50.559: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 13 20:29:50.559: INFO: sonobuoy-systemd-logs-daemon-set-9616c2f784dc4b45-rxl5l from sonobuoy started at 2019-12-13 19:12:22 +0000 UTC (2 container statuses recorded)
Dec 13 20:29:50.559: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 13 20:29:50.559: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 13 20:29:50.559: INFO: rook-discover-p77wp from rook-ceph started at 2019-12-13 20:15:31 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.559: INFO: 	Container rook-discover ready: true, restart count 0
Dec 13 20:29:50.559: INFO: kube-scheduler-master-0 from kube-system started at 2019-12-13 18:54:06 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.559: INFO: 	Container kube-scheduler ready: true, restart count 0
Dec 13 20:29:50.559: INFO: kube-proxy-dfvlw from kube-system started at 2019-12-13 18:54:29 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.559: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 13 20:29:50.559: INFO: fluentd-es-v2.5.2-mfcr4 from kube-system started at 2019-12-13 20:15:27 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.559: INFO: 	Container fluentd-es ready: true, restart count 0
Dec 13 20:29:50.559: INFO: rook-ceph-mon-b-5b988bf97b-4xkn2 from rook-ceph started at 2019-12-13 20:15:27 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.559: INFO: 	Container mon ready: true, restart count 0
Dec 13 20:29:50.559: INFO: kube-controller-manager-master-0 from kube-system started at 2019-12-13 18:54:06 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.559: INFO: 	Container kube-controller-manager ready: true, restart count 0
Dec 13 20:29:50.559: INFO: canal-xbp9c from kube-system started at 2019-12-13 18:55:18 +0000 UTC (2 container statuses recorded)
Dec 13 20:29:50.559: INFO: 	Container calico-node ready: true, restart count 0
Dec 13 20:29:50.559: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 13 20:29:50.559: INFO: kata-deploy-m4sjm from kube-system started at 2019-12-13 20:15:27 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.559: INFO: 	Container kube-kata ready: true, restart count 0
Dec 13 20:29:50.559: INFO: rook-ceph-osd-2-58969bddc7-6zxpl from rook-ceph started at 2019-12-13 20:15:27 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.559: INFO: 	Container osd ready: true, restart count 0
Dec 13 20:29:50.559: INFO: 
Logging pods the kubelet thinks is on node worker-0 before test
Dec 13 20:29:50.581: INFO: kata-deploy-2qbn2 from kube-system started at 2019-12-13 18:55:45 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.581: INFO: 	Container kube-kata ready: true, restart count 0
Dec 13 20:29:50.581: INFO: node-exporter-584q9 from monitoring started at 2019-12-13 19:05:14 +0000 UTC (2 container statuses recorded)
Dec 13 20:29:50.581: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 13 20:29:50.581: INFO: 	Container node-exporter ready: true, restart count 0
Dec 13 20:29:50.581: INFO: kube-proxy-rktzd from kube-system started at 2019-12-13 18:54:55 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.581: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 13 20:29:50.581: INFO: canal-sgqjs from kube-system started at 2019-12-13 18:55:18 +0000 UTC (2 container statuses recorded)
Dec 13 20:29:50.581: INFO: 	Container calico-node ready: true, restart count 0
Dec 13 20:29:50.581: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 13 20:29:50.581: INFO: rook-discover-5t2vg from rook-ceph started at 2019-12-13 18:59:33 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.581: INFO: 	Container rook-discover ready: true, restart count 0
Dec 13 20:29:50.581: INFO: csi-rbdplugin-9vnm2 from rook-ceph started at 2019-12-13 18:59:33 +0000 UTC (3 container statuses recorded)
Dec 13 20:29:50.581: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 20:29:50.581: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:29:50.581: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:29:50.581: INFO: rook-ceph-osd-prepare-worker-0-xz82f from rook-ceph started at 2019-12-13 19:03:42 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.581: INFO: 	Container provision ready: false, restart count 0
Dec 13 20:29:50.581: INFO: kubernetes-dashboard-6865b885fc-7tv2c from kubernetes-dashboard started at 2019-12-13 19:05:45 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.582: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec 13 20:29:50.582: INFO: alertmanager-main-2 from monitoring started at 2019-12-13 19:06:20 +0000 UTC (2 container statuses recorded)
Dec 13 20:29:50.582: INFO: 	Container alertmanager ready: true, restart count 0
Dec 13 20:29:50.582: INFO: 	Container config-reloader ready: true, restart count 0
Dec 13 20:29:50.582: INFO: prometheus-k8s-1 from monitoring started at 2019-12-13 19:06:30 +0000 UTC (3 container statuses recorded)
Dec 13 20:29:50.582: INFO: 	Container prometheus ready: true, restart count 1
Dec 13 20:29:50.582: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec 13 20:29:50.582: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec 13 20:29:50.582: INFO: coredns-5644d7b6d9-rngbs from kube-system started at 2019-12-13 18:55:47 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.582: INFO: 	Container coredns ready: true, restart count 0
Dec 13 20:29:50.582: INFO: coredns-5644d7b6d9-tdq6s from kube-system started at 2019-12-13 18:55:47 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.582: INFO: 	Container coredns ready: true, restart count 0
Dec 13 20:29:50.582: INFO: grafana-5cd56df4cd-wqpht from monitoring started at 2019-12-13 19:41:23 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.582: INFO: 	Container grafana ready: true, restart count 0
Dec 13 20:29:50.582: INFO: fluentd-es-v2.5.2-w2nk6 from kube-system started at 2019-12-13 19:07:51 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.582: INFO: 	Container fluentd-es ready: true, restart count 0
Dec 13 20:29:50.582: INFO: sonobuoy-systemd-logs-daemon-set-9616c2f784dc4b45-qpldg from sonobuoy started at 2019-12-13 19:12:22 +0000 UTC (2 container statuses recorded)
Dec 13 20:29:50.582: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 13 20:29:50.582: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 13 20:29:50.582: INFO: rook-ceph-osd-0-64bcd55b9f-hrsvl from rook-ceph started at 2019-12-13 19:03:50 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.582: INFO: 	Container osd ready: true, restart count 0
Dec 13 20:29:50.582: INFO: alertmanager-main-1 from monitoring started at 2019-12-13 20:14:55 +0000 UTC (2 container statuses recorded)
Dec 13 20:29:50.582: INFO: 	Container alertmanager ready: true, restart count 0
Dec 13 20:29:50.582: INFO: 	Container config-reloader ready: true, restart count 0
Dec 13 20:29:50.582: INFO: elasticsearch-logging-0 from kube-system started at 2019-12-13 20:14:56 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.582: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Dec 13 20:29:50.582: INFO: rook-ceph-operator-c8ff6447d-6s7cz from rook-ceph started at 2019-12-13 18:55:45 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.582: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Dec 13 20:29:50.582: INFO: csi-cephfsplugin-n7682 from rook-ceph started at 2019-12-13 18:59:35 +0000 UTC (3 container statuses recorded)
Dec 13 20:29:50.582: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 20:29:50.582: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:29:50.582: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:29:50.582: INFO: rook-ceph-mon-c-597fd9c9c7-r8ngb from rook-ceph started at 2019-12-13 19:02:59 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.582: INFO: 	Container mon ready: true, restart count 0
Dec 13 20:29:50.582: INFO: csi-rbdplugin-provisioner-56cbc4d585-lhtqr from rook-ceph started at 2019-12-13 20:14:30 +0000 UTC (5 container statuses recorded)
Dec 13 20:29:50.582: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 13 20:29:50.582: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 20:29:50.582: INFO: 	Container csi-rbdplugin-attacher ready: true, restart count 0
Dec 13 20:29:50.582: INFO: 	Container csi-snapshotter ready: true, restart count 0
Dec 13 20:29:50.582: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:29:50.582: INFO: dashboard-metrics-scraper-6ccf7f6cc8-k4zf5 from kubernetes-dashboard started at 2019-12-13 20:14:31 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.582: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Dec 13 20:29:50.582: INFO: metrics-server-7578984995-fr9dj from kube-system started at 2019-12-13 18:55:45 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.582: INFO: 	Container metrics-server ready: true, restart count 0
Dec 13 20:29:50.582: INFO: csi-cephfsplugin-provisioner-75c965db4f-dhmjp from rook-ceph started at 2019-12-13 18:59:36 +0000 UTC (4 container statuses recorded)
Dec 13 20:29:50.582: INFO: 	Container csi-attacher ready: true, restart count 0
Dec 13 20:29:50.582: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 20:29:50.582: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 13 20:29:50.582: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:29:50.582: INFO: 
Logging pods the kubelet thinks is on node worker-1 before test
Dec 13 20:29:50.603: INFO: alertmanager-main-0 from monitoring started at 2019-12-13 19:06:20 +0000 UTC (2 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container alertmanager ready: true, restart count 0
Dec 13 20:29:50.604: INFO: 	Container config-reloader ready: true, restart count 0
Dec 13 20:29:50.604: INFO: fluentd-es-v2.5.2-s2ng4 from kube-system started at 2019-12-13 19:07:51 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container fluentd-es ready: true, restart count 0
Dec 13 20:29:50.604: INFO: sonobuoy from sonobuoy started at 2019-12-13 19:12:05 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 13 20:29:50.604: INFO: kube-state-metrics-79d4b9b497-wntkj from monitoring started at 2019-12-13 20:14:30 +0000 UTC (3 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec 13 20:29:50.604: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec 13 20:29:50.604: INFO: 	Container kube-state-metrics ready: true, restart count 0
Dec 13 20:29:50.604: INFO: kube-proxy-dt9pl from kube-system started at 2019-12-13 18:55:17 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 13 20:29:50.604: INFO: csi-rbdplugin-rhs8g from rook-ceph started at 2019-12-13 18:59:33 +0000 UTC (3 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 20:29:50.604: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:29:50.604: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:29:50.604: INFO: csi-rbdplugin-provisioner-56cbc4d585-j64wf from rook-ceph started at 2019-12-13 18:59:34 +0000 UTC (5 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 13 20:29:50.604: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 20:29:50.604: INFO: 	Container csi-rbdplugin-attacher ready: true, restart count 0
Dec 13 20:29:50.604: INFO: 	Container csi-snapshotter ready: true, restart count 0
Dec 13 20:29:50.604: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:29:50.604: INFO: node-exporter-7ggvt from monitoring started at 2019-12-13 19:05:14 +0000 UTC (2 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 13 20:29:50.604: INFO: 	Container node-exporter ready: true, restart count 0
Dec 13 20:29:50.604: INFO: prometheus-adapter-c676d8764-7pnjf from monitoring started at 2019-12-13 19:41:23 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container prometheus-adapter ready: true, restart count 0
Dec 13 20:29:50.604: INFO: prometheus-k8s-0 from monitoring started at 2019-12-13 20:14:55 +0000 UTC (3 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container prometheus ready: true, restart count 1
Dec 13 20:29:50.604: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec 13 20:29:50.604: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec 13 20:29:50.604: INFO: kata-deploy-qtlwp from kube-system started at 2019-12-13 18:55:57 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container kube-kata ready: true, restart count 0
Dec 13 20:29:50.604: INFO: rook-discover-m4tgh from rook-ceph started at 2019-12-13 18:59:33 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container rook-discover ready: true, restart count 0
Dec 13 20:29:50.604: INFO: rook-ceph-mon-a-c7664c8c4-jcx8w from rook-ceph started at 2019-12-13 19:02:27 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container mon ready: true, restart count 0
Dec 13 20:29:50.604: INFO: rook-ceph-osd-prepare-worker-1-2xccr from rook-ceph started at 2019-12-13 19:03:42 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container provision ready: false, restart count 0
Dec 13 20:29:50.604: INFO: elasticsearch-logging-1 from kube-system started at 2019-12-13 19:10:24 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Dec 13 20:29:50.604: INFO: sonobuoy-systemd-logs-daemon-set-9616c2f784dc4b45-f29sm from sonobuoy started at 2019-12-13 19:12:22 +0000 UTC (2 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 13 20:29:50.604: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 13 20:29:50.604: INFO: prometheus-operator-7559d67ff-shr6m from monitoring started at 2019-12-13 20:14:30 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container prometheus-operator ready: true, restart count 0
Dec 13 20:29:50.604: INFO: canal-5wqk9 from kube-system started at 2019-12-13 18:55:18 +0000 UTC (2 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container calico-node ready: true, restart count 0
Dec 13 20:29:50.604: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 13 20:29:50.604: INFO: csi-cephfsplugin-6szlf from rook-ceph started at 2019-12-13 18:59:35 +0000 UTC (3 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 20:29:50.604: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:29:50.604: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:29:50.604: INFO: rook-ceph-osd-1-7d8886d496-jzxm4 from rook-ceph started at 2019-12-13 19:03:51 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container osd ready: true, restart count 0
Dec 13 20:29:50.604: INFO: nginx-ingress-controller-7fb85bc8bb-8f8dr from ingress-nginx started at 2019-12-13 19:08:29 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec 13 20:29:50.604: INFO: rook-ceph-mgr-a-5775778c4d-k745v from rook-ceph started at 2019-12-13 20:14:30 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container mgr ready: true, restart count 0
Dec 13 20:29:50.604: INFO: kibana-logging-7f6b4b96b4-pdq55 from kube-system started at 2019-12-13 20:14:30 +0000 UTC (1 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container kibana-logging ready: true, restart count 0
Dec 13 20:29:50.604: INFO: csi-cephfsplugin-provisioner-75c965db4f-6plm2 from rook-ceph started at 2019-12-13 20:14:31 +0000 UTC (4 container statuses recorded)
Dec 13 20:29:50.604: INFO: 	Container csi-attacher ready: true, restart count 0
Dec 13 20:29:50.604: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 20:29:50.604: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 13 20:29:50.604: INFO: 	Container liveness-prometheus ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node master-0
STEP: verifying the node has the label node worker-0
STEP: verifying the node has the label node worker-1
Dec 13 20:29:50.690: INFO: Pod nginx-ingress-controller-7fb85bc8bb-8f8dr requesting resource cpu=0m on Node worker-1
Dec 13 20:29:50.690: INFO: Pod canal-5wqk9 requesting resource cpu=250m on Node worker-1
Dec 13 20:29:50.690: INFO: Pod canal-sgqjs requesting resource cpu=250m on Node worker-0
Dec 13 20:29:50.690: INFO: Pod canal-xbp9c requesting resource cpu=250m on Node master-0
Dec 13 20:29:50.691: INFO: Pod coredns-5644d7b6d9-rngbs requesting resource cpu=100m on Node worker-0
Dec 13 20:29:50.691: INFO: Pod coredns-5644d7b6d9-tdq6s requesting resource cpu=100m on Node worker-0
Dec 13 20:29:50.691: INFO: Pod elasticsearch-logging-0 requesting resource cpu=100m on Node worker-0
Dec 13 20:29:50.691: INFO: Pod elasticsearch-logging-1 requesting resource cpu=100m on Node worker-1
Dec 13 20:29:50.691: INFO: Pod etcd-master-0 requesting resource cpu=0m on Node master-0
Dec 13 20:29:50.691: INFO: Pod fluentd-es-v2.5.2-mfcr4 requesting resource cpu=100m on Node master-0
Dec 13 20:29:50.691: INFO: Pod fluentd-es-v2.5.2-s2ng4 requesting resource cpu=100m on Node worker-1
Dec 13 20:29:50.691: INFO: Pod fluentd-es-v2.5.2-w2nk6 requesting resource cpu=100m on Node worker-0
Dec 13 20:29:50.691: INFO: Pod kata-deploy-2qbn2 requesting resource cpu=0m on Node worker-0
Dec 13 20:29:50.691: INFO: Pod kata-deploy-m4sjm requesting resource cpu=0m on Node master-0
Dec 13 20:29:50.691: INFO: Pod kata-deploy-qtlwp requesting resource cpu=0m on Node worker-1
Dec 13 20:29:50.691: INFO: Pod kibana-logging-7f6b4b96b4-pdq55 requesting resource cpu=100m on Node worker-1
Dec 13 20:29:50.691: INFO: Pod kube-apiserver-master-0 requesting resource cpu=250m on Node master-0
Dec 13 20:29:50.691: INFO: Pod kube-controller-manager-master-0 requesting resource cpu=200m on Node master-0
Dec 13 20:29:50.691: INFO: Pod kube-proxy-dfvlw requesting resource cpu=0m on Node master-0
Dec 13 20:29:50.691: INFO: Pod kube-proxy-dt9pl requesting resource cpu=0m on Node worker-1
Dec 13 20:29:50.691: INFO: Pod kube-proxy-rktzd requesting resource cpu=0m on Node worker-0
Dec 13 20:29:50.691: INFO: Pod kube-scheduler-master-0 requesting resource cpu=100m on Node master-0
Dec 13 20:29:50.691: INFO: Pod metrics-server-7578984995-fr9dj requesting resource cpu=0m on Node worker-0
Dec 13 20:29:50.691: INFO: Pod dashboard-metrics-scraper-6ccf7f6cc8-k4zf5 requesting resource cpu=0m on Node worker-0
Dec 13 20:29:50.691: INFO: Pod kubernetes-dashboard-6865b885fc-7tv2c requesting resource cpu=0m on Node worker-0
Dec 13 20:29:50.691: INFO: Pod alertmanager-main-0 requesting resource cpu=100m on Node worker-1
Dec 13 20:29:50.691: INFO: Pod alertmanager-main-1 requesting resource cpu=100m on Node worker-0
Dec 13 20:29:50.691: INFO: Pod alertmanager-main-2 requesting resource cpu=100m on Node worker-0
Dec 13 20:29:50.691: INFO: Pod grafana-5cd56df4cd-wqpht requesting resource cpu=100m on Node worker-0
Dec 13 20:29:50.691: INFO: Pod kube-state-metrics-79d4b9b497-wntkj requesting resource cpu=120m on Node worker-1
Dec 13 20:29:50.691: INFO: Pod node-exporter-584q9 requesting resource cpu=112m on Node worker-0
Dec 13 20:29:50.691: INFO: Pod node-exporter-7ggvt requesting resource cpu=112m on Node worker-1
Dec 13 20:29:50.691: INFO: Pod node-exporter-tk6xn requesting resource cpu=112m on Node master-0
Dec 13 20:29:50.691: INFO: Pod prometheus-adapter-c676d8764-7pnjf requesting resource cpu=0m on Node worker-1
Dec 13 20:29:50.691: INFO: Pod prometheus-k8s-0 requesting resource cpu=200m on Node worker-1
Dec 13 20:29:50.691: INFO: Pod prometheus-k8s-1 requesting resource cpu=200m on Node worker-0
Dec 13 20:29:50.691: INFO: Pod prometheus-operator-7559d67ff-shr6m requesting resource cpu=100m on Node worker-1
Dec 13 20:29:50.691: INFO: Pod csi-cephfsplugin-6szlf requesting resource cpu=0m on Node worker-1
Dec 13 20:29:50.691: INFO: Pod csi-cephfsplugin-n7682 requesting resource cpu=0m on Node worker-0
Dec 13 20:29:50.691: INFO: Pod csi-cephfsplugin-provisioner-75c965db4f-6plm2 requesting resource cpu=0m on Node worker-1
Dec 13 20:29:50.691: INFO: Pod csi-cephfsplugin-provisioner-75c965db4f-dhmjp requesting resource cpu=0m on Node worker-0
Dec 13 20:29:50.691: INFO: Pod csi-cephfsplugin-vvg2s requesting resource cpu=0m on Node master-0
Dec 13 20:29:50.691: INFO: Pod csi-rbdplugin-8rpjc requesting resource cpu=0m on Node master-0
Dec 13 20:29:50.691: INFO: Pod csi-rbdplugin-9vnm2 requesting resource cpu=0m on Node worker-0
Dec 13 20:29:50.691: INFO: Pod csi-rbdplugin-provisioner-56cbc4d585-j64wf requesting resource cpu=0m on Node worker-1
Dec 13 20:29:50.691: INFO: Pod csi-rbdplugin-provisioner-56cbc4d585-lhtqr requesting resource cpu=0m on Node worker-0
Dec 13 20:29:50.691: INFO: Pod csi-rbdplugin-rhs8g requesting resource cpu=0m on Node worker-1
Dec 13 20:29:50.691: INFO: Pod rook-ceph-mgr-a-5775778c4d-k745v requesting resource cpu=0m on Node worker-1
Dec 13 20:29:50.691: INFO: Pod rook-ceph-mon-a-c7664c8c4-jcx8w requesting resource cpu=0m on Node worker-1
Dec 13 20:29:50.691: INFO: Pod rook-ceph-mon-b-5b988bf97b-4xkn2 requesting resource cpu=0m on Node master-0
Dec 13 20:29:50.691: INFO: Pod rook-ceph-mon-c-597fd9c9c7-r8ngb requesting resource cpu=0m on Node worker-0
Dec 13 20:29:50.691: INFO: Pod rook-ceph-operator-c8ff6447d-6s7cz requesting resource cpu=0m on Node worker-0
Dec 13 20:29:50.691: INFO: Pod rook-ceph-osd-0-64bcd55b9f-hrsvl requesting resource cpu=0m on Node worker-0
Dec 13 20:29:50.691: INFO: Pod rook-ceph-osd-1-7d8886d496-jzxm4 requesting resource cpu=0m on Node worker-1
Dec 13 20:29:50.691: INFO: Pod rook-ceph-osd-2-58969bddc7-6zxpl requesting resource cpu=0m on Node master-0
Dec 13 20:29:50.691: INFO: Pod rook-discover-5t2vg requesting resource cpu=0m on Node worker-0
Dec 13 20:29:50.691: INFO: Pod rook-discover-m4tgh requesting resource cpu=0m on Node worker-1
Dec 13 20:29:50.691: INFO: Pod rook-discover-p77wp requesting resource cpu=0m on Node master-0
Dec 13 20:29:50.691: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker-1
Dec 13 20:29:50.691: INFO: Pod sonobuoy-e2e-job-8f6bded5e69344cd requesting resource cpu=0m on Node master-0
Dec 13 20:29:50.691: INFO: Pod sonobuoy-systemd-logs-daemon-set-9616c2f784dc4b45-f29sm requesting resource cpu=0m on Node worker-1
Dec 13 20:29:50.691: INFO: Pod sonobuoy-systemd-logs-daemon-set-9616c2f784dc4b45-qpldg requesting resource cpu=0m on Node worker-0
Dec 13 20:29:50.691: INFO: Pod sonobuoy-systemd-logs-daemon-set-9616c2f784dc4b45-rxl5l requesting resource cpu=0m on Node master-0
STEP: Starting Pods to consume most of the cluster CPU.
Dec 13 20:29:50.691: INFO: Creating a pod which consumes cpu=4072m on Node worker-1
Dec 13 20:29:50.695: INFO: Creating a pod which consumes cpu=4191m on Node master-0
Dec 13 20:29:50.704: INFO: Creating a pod which consumes cpu=4016m on Node worker-0
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d764fb4f-9d9f-494c-8ec5-7828031a1ab4.15e0084bae22fa06], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5674/filler-pod-d764fb4f-9d9f-494c-8ec5-7828031a1ab4 to master-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d764fb4f-9d9f-494c-8ec5-7828031a1ab4.15e0084be1a5ea48], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d764fb4f-9d9f-494c-8ec5-7828031a1ab4.15e0084bf6c9827d], Reason = [Created], Message = [Created container filler-pod-d764fb4f-9d9f-494c-8ec5-7828031a1ab4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d764fb4f-9d9f-494c-8ec5-7828031a1ab4.15e0084bf9018c33], Reason = [Started], Message = [Started container filler-pod-d764fb4f-9d9f-494c-8ec5-7828031a1ab4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ec1a3d20-e733-4f29-b5ef-2ae87cfa8c70.15e0084bad3cde63], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5674/filler-pod-ec1a3d20-e733-4f29-b5ef-2ae87cfa8c70 to worker-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ec1a3d20-e733-4f29-b5ef-2ae87cfa8c70.15e0084bdcaaf1cc], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ec1a3d20-e733-4f29-b5ef-2ae87cfa8c70.15e0084bf246db6f], Reason = [Created], Message = [Created container filler-pod-ec1a3d20-e733-4f29-b5ef-2ae87cfa8c70]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ec1a3d20-e733-4f29-b5ef-2ae87cfa8c70.15e0084bf56eae2c], Reason = [Started], Message = [Started container filler-pod-ec1a3d20-e733-4f29-b5ef-2ae87cfa8c70]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ee00460a-0054-4e9c-b279-c9fe83eb5270.15e0084baf3affb5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5674/filler-pod-ee00460a-0054-4e9c-b279-c9fe83eb5270 to worker-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ee00460a-0054-4e9c-b279-c9fe83eb5270.15e0084bdf055c55], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ee00460a-0054-4e9c-b279-c9fe83eb5270.15e0084bf1e0584a], Reason = [Created], Message = [Created container filler-pod-ee00460a-0054-4e9c-b279-c9fe83eb5270]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ee00460a-0054-4e9c-b279-c9fe83eb5270.15e0084bf55e5935], Reason = [Started], Message = [Started container filler-pod-ee00460a-0054-4e9c-b279-c9fe83eb5270]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15e0084c9e041bb6], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node master-0
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker-0
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker-1
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:29:55.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5674" for this suite.
Dec 13 20:30:01.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:30:02.016: INFO: namespace sched-pred-5674 deletion completed in 6.216247413s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:11.636 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:30:02.016: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6259
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-02b02b8e-76f4-4a24-b583-bfa3d43892f9
STEP: Creating secret with name s-test-opt-upd-eeae16b0-29ff-4fc1-9691-48b4baf38d01
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-02b02b8e-76f4-4a24-b583-bfa3d43892f9
STEP: Updating secret s-test-opt-upd-eeae16b0-29ff-4fc1-9691-48b4baf38d01
STEP: Creating secret with name s-test-opt-create-4314f12c-ed65-4365-8d69-a25d6de34f62
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:30:06.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6259" for this suite.
Dec 13 20:30:18.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:30:18.423: INFO: namespace secrets-6259 deletion completed in 12.078840594s

• [SLOW TEST:16.407 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:30:18.423: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4843
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 13 20:30:20.561: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:30:20.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4843" for this suite.
Dec 13 20:30:26.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:30:26.690: INFO: namespace container-runtime-4843 deletion completed in 6.110630952s

• [SLOW TEST:8.267 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:30:26.690: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8629
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Dec 13 20:30:26.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-8629 -- logs-generator --log-lines-total 100 --run-duration 20s'
Dec 13 20:30:26.929: INFO: stderr: ""
Dec 13 20:30:26.929: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Dec 13 20:30:26.929: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Dec 13 20:30:26.929: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8629" to be "running and ready, or succeeded"
Dec 13 20:30:26.933: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.076868ms
Dec 13 20:30:28.935: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.005200431s
Dec 13 20:30:28.935: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Dec 13 20:30:28.935: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Dec 13 20:30:28.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 logs logs-generator logs-generator --namespace=kubectl-8629'
Dec 13 20:30:29.003: INFO: stderr: ""
Dec 13 20:30:29.003: INFO: stdout: "I1213 20:30:27.596795       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/gdr 330\nI1213 20:30:27.796900       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/jkhg 593\nI1213 20:30:27.997101       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/px6 328\nI1213 20:30:28.197287       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/gvh 361\nI1213 20:30:28.396982       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/c6p 592\nI1213 20:30:28.596983       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/9rj 533\nI1213 20:30:28.796936       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/s54 473\nI1213 20:30:28.996910       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/q6c 436\n"
STEP: limiting log lines
Dec 13 20:30:29.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 logs logs-generator logs-generator --namespace=kubectl-8629 --tail=1'
Dec 13 20:30:29.068: INFO: stderr: ""
Dec 13 20:30:29.068: INFO: stdout: "I1213 20:30:28.996910       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/q6c 436\n"
STEP: limiting log bytes
Dec 13 20:30:29.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 logs logs-generator logs-generator --namespace=kubectl-8629 --limit-bytes=1'
Dec 13 20:30:29.136: INFO: stderr: ""
Dec 13 20:30:29.136: INFO: stdout: "I"
STEP: exposing timestamps
Dec 13 20:30:29.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 logs logs-generator logs-generator --namespace=kubectl-8629 --tail=1 --timestamps'
Dec 13 20:30:29.280: INFO: stderr: ""
Dec 13 20:30:29.280: INFO: stdout: "2019-12-13T20:30:29.209213607Z I1213 20:30:29.201612       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/7zk 224\n"
STEP: restricting to a time range
Dec 13 20:30:31.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 logs logs-generator logs-generator --namespace=kubectl-8629 --since=1s'
Dec 13 20:30:31.854: INFO: stderr: ""
Dec 13 20:30:31.854: INFO: stdout: "I1213 20:30:30.997025       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/xvqz 530\nI1213 20:30:31.196934       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/67s2 415\nI1213 20:30:31.397039       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/5hw 541\nI1213 20:30:31.596938       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/2hwh 297\nI1213 20:30:31.796906       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/w8b 325\n"
Dec 13 20:30:31.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 logs logs-generator logs-generator --namespace=kubectl-8629 --since=24h'
Dec 13 20:30:31.926: INFO: stderr: ""
Dec 13 20:30:31.926: INFO: stdout: "I1213 20:30:27.596795       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/gdr 330\nI1213 20:30:27.796900       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/jkhg 593\nI1213 20:30:27.997101       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/px6 328\nI1213 20:30:28.197287       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/gvh 361\nI1213 20:30:28.396982       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/c6p 592\nI1213 20:30:28.596983       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/9rj 533\nI1213 20:30:28.796936       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/s54 473\nI1213 20:30:28.996910       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/q6c 436\nI1213 20:30:29.201612       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/7zk 224\nI1213 20:30:29.396914       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/fklw 585\nI1213 20:30:29.596925       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/2dvf 565\nI1213 20:30:29.796923       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/cdm 251\nI1213 20:30:29.996954       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/9g7 529\nI1213 20:30:30.196948       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/qjr 246\nI1213 20:30:30.396948       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/wkps 580\nI1213 20:30:30.596963       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/tgn 337\nI1213 20:30:30.796895       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/bz4l 553\nI1213 20:30:30.997025       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/xvqz 530\nI1213 20:30:31.196934       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/67s2 415\nI1213 20:30:31.397039       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/5hw 541\nI1213 20:30:31.596938       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/2hwh 297\nI1213 20:30:31.796906       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/w8b 325\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Dec 13 20:30:31.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 delete pod logs-generator --namespace=kubectl-8629'
Dec 13 20:30:34.130: INFO: stderr: ""
Dec 13 20:30:34.130: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:30:34.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8629" for this suite.
Dec 13 20:30:40.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:30:40.223: INFO: namespace kubectl-8629 deletion completed in 6.088820991s

• [SLOW TEST:13.533 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:30:40.223: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4379
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 13 20:30:44.389: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 13 20:30:44.401: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 13 20:30:46.401: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 13 20:30:46.403: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 13 20:30:48.401: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 13 20:30:48.403: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 13 20:30:50.401: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 13 20:30:50.403: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 13 20:30:52.401: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 13 20:30:52.403: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 13 20:30:54.401: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 13 20:30:54.403: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 13 20:30:56.404: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 13 20:30:56.413: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:30:56.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4379" for this suite.
Dec 13 20:31:08.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:31:08.526: INFO: namespace container-lifecycle-hook-4379 deletion completed in 12.109686636s

• [SLOW TEST:28.303 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:31:08.527: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3339
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 13 20:31:08.672: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e94e5608-5223-4233-b06c-775a34687c23" in namespace "projected-3339" to be "success or failure"
Dec 13 20:31:08.677: INFO: Pod "downwardapi-volume-e94e5608-5223-4233-b06c-775a34687c23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.439278ms
Dec 13 20:31:10.679: INFO: Pod "downwardapi-volume-e94e5608-5223-4233-b06c-775a34687c23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00673489s
STEP: Saw pod success
Dec 13 20:31:10.679: INFO: Pod "downwardapi-volume-e94e5608-5223-4233-b06c-775a34687c23" satisfied condition "success or failure"
Dec 13 20:31:10.681: INFO: Trying to get logs from node master-0 pod downwardapi-volume-e94e5608-5223-4233-b06c-775a34687c23 container client-container: <nil>
STEP: delete the pod
Dec 13 20:31:10.694: INFO: Waiting for pod downwardapi-volume-e94e5608-5223-4233-b06c-775a34687c23 to disappear
Dec 13 20:31:10.696: INFO: Pod downwardapi-volume-e94e5608-5223-4233-b06c-775a34687c23 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:31:10.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3339" for this suite.
Dec 13 20:31:16.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:31:16.788: INFO: namespace projected-3339 deletion completed in 6.089386411s

• [SLOW TEST:8.261 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:31:16.788: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1013
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec 13 20:31:16.926: INFO: PodSpec: initContainers in spec.initContainers
Dec 13 20:31:59.284: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-475e6366-c018-42a1-bf43-18fbda311ec9", GenerateName:"", Namespace:"init-container-1013", SelfLink:"/api/v1/namespaces/init-container-1013/pods/pod-init-475e6366-c018-42a1-bf43-18fbda311ec9", UID:"3cb309c4-348d-4ec7-a561-fed5af114358", ResourceVersion:"33584", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63711865876, loc:(*time.Location)(0x84bfb00)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"926595631"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.244.0.15/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-brtnh", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc004f6f0c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-brtnh", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-brtnh", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-brtnh", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004aae098), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"master-0", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001c04000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004aae120)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004aae140)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004aae148), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004aae14c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711865876, loc:(*time.Location)(0x84bfb00)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711865876, loc:(*time.Location)(0x84bfb00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711865876, loc:(*time.Location)(0x84bfb00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711865876, loc:(*time.Location)(0x84bfb00)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.122.2", PodIP:"10.244.0.15", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.0.15"}}, StartTime:(*v1.Time)(0xc0037ea060), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0008fe070)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0008fe0e0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"cri-o://1852c70a96355d7397f9835caff1b8ef86a7eabbe751e61cd5116eff35bf4730", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0037ea0a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0037ea080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc004aae1cf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:31:59.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1013" for this suite.
Dec 13 20:32:27.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:32:27.405: INFO: namespace init-container-1013 deletion completed in 28.101032208s

• [SLOW TEST:70.617 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:32:27.405: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2960
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-0b3a0f11-117c-436a-b6c8-28e3b5ed1679
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:32:29.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2960" for this suite.
Dec 13 20:32:41.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:32:41.660: INFO: namespace configmap-2960 deletion completed in 12.084706572s

• [SLOW TEST:14.254 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:32:41.660: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2568
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Dec 13 20:32:41.803: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 20:32:44.732: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:32:58.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2568" for this suite.
Dec 13 20:33:04.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:33:05.019: INFO: namespace crd-publish-openapi-2568 deletion completed in 6.097354147s

• [SLOW TEST:23.359 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:33:05.019: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2859
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:33:21.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2859" for this suite.
Dec 13 20:33:27.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:33:27.311: INFO: namespace resourcequota-2859 deletion completed in 6.081849597s

• [SLOW TEST:22.292 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:33:27.311: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4515
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:33:29.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4515" for this suite.
Dec 13 20:34:17.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:34:17.559: INFO: namespace kubelet-test-4515 deletion completed in 48.099688474s

• [SLOW TEST:50.248 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:34:17.559: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6009
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-a5a17888-73f0-451c-a1cd-3b45e2b4dbe4 in namespace container-probe-6009
Dec 13 20:34:19.781: INFO: Started pod busybox-a5a17888-73f0-451c-a1cd-3b45e2b4dbe4 in namespace container-probe-6009
STEP: checking the pod's current state and verifying that restartCount is present
Dec 13 20:34:19.783: INFO: Initial restart count of pod busybox-a5a17888-73f0-451c-a1cd-3b45e2b4dbe4 is 0
Dec 13 20:35:13.858: INFO: Restart count of pod container-probe-6009/busybox-a5a17888-73f0-451c-a1cd-3b45e2b4dbe4 is now 1 (54.074521507s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:35:13.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6009" for this suite.
Dec 13 20:35:19.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:35:19.972: INFO: namespace container-probe-6009 deletion completed in 6.08586156s

• [SLOW TEST:62.413 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:35:19.972: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7992
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 13 20:35:20.110: INFO: Waiting up to 5m0s for pod "pod-1f7d7f12-8c68-41cb-b67d-f63739cb9b92" in namespace "emptydir-7992" to be "success or failure"
Dec 13 20:35:20.127: INFO: Pod "pod-1f7d7f12-8c68-41cb-b67d-f63739cb9b92": Phase="Pending", Reason="", readiness=false. Elapsed: 17.19371ms
Dec 13 20:35:22.131: INFO: Pod "pod-1f7d7f12-8c68-41cb-b67d-f63739cb9b92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021196976s
STEP: Saw pod success
Dec 13 20:35:22.131: INFO: Pod "pod-1f7d7f12-8c68-41cb-b67d-f63739cb9b92" satisfied condition "success or failure"
Dec 13 20:35:22.134: INFO: Trying to get logs from node master-0 pod pod-1f7d7f12-8c68-41cb-b67d-f63739cb9b92 container test-container: <nil>
STEP: delete the pod
Dec 13 20:35:22.175: INFO: Waiting for pod pod-1f7d7f12-8c68-41cb-b67d-f63739cb9b92 to disappear
Dec 13 20:35:22.178: INFO: Pod pod-1f7d7f12-8c68-41cb-b67d-f63739cb9b92 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:35:22.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7992" for this suite.
Dec 13 20:35:28.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:35:28.375: INFO: namespace emptydir-7992 deletion completed in 6.189068472s

• [SLOW TEST:8.403 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:35:28.375: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9234
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-9234
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-9234
Dec 13 20:35:28.515: INFO: Found 0 stateful pods, waiting for 1
Dec 13 20:35:38.517: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 13 20:35:38.528: INFO: Deleting all statefulset in ns statefulset-9234
Dec 13 20:35:38.540: INFO: Scaling statefulset ss to 0
Dec 13 20:35:48.561: INFO: Waiting for statefulset status.replicas updated to 0
Dec 13 20:35:48.562: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:35:48.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9234" for this suite.
Dec 13 20:35:54.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:35:54.908: INFO: namespace statefulset-9234 deletion completed in 6.334507697s

• [SLOW TEST:26.533 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:35:54.908: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7446
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7446
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7446
STEP: creating replication controller externalsvc in namespace services-7446
I1213 20:35:55.206536      24 runners.go:184] Created replication controller with name: externalsvc, namespace: services-7446, replica count: 2
I1213 20:35:58.261675      24 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Dec 13 20:35:58.274: INFO: Creating new exec pod
Dec 13 20:36:00.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 exec --namespace=services-7446 execpodnz928 -- /bin/sh -x -c nslookup clusterip-service'
Dec 13 20:36:00.614: INFO: stderr: "+ nslookup clusterip-service\n"
Dec 13 20:36:00.614: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-7446.svc.cluster.local\tcanonical name = externalsvc.services-7446.svc.cluster.local.\nName:\texternalsvc.services-7446.svc.cluster.local\nAddress: 10.101.4.86\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7446, will wait for the garbage collector to delete the pods
Dec 13 20:36:00.669: INFO: Deleting ReplicationController externalsvc took: 3.381162ms
Dec 13 20:36:00.769: INFO: Terminating ReplicationController externalsvc pods took: 100.230144ms
Dec 13 20:36:15.806: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:36:15.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7446" for this suite.
Dec 13 20:36:21.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:36:21.929: INFO: namespace services-7446 deletion completed in 6.102197343s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:27.021 seconds]
[sig-network] Services
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:36:21.929: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2420
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 13 20:36:22.065: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a057dcff-d468-45c8-bc62-03cfc69d8960" in namespace "projected-2420" to be "success or failure"
Dec 13 20:36:22.067: INFO: Pod "downwardapi-volume-a057dcff-d468-45c8-bc62-03cfc69d8960": Phase="Pending", Reason="", readiness=false. Elapsed: 2.798471ms
Dec 13 20:36:24.069: INFO: Pod "downwardapi-volume-a057dcff-d468-45c8-bc62-03cfc69d8960": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004864755s
STEP: Saw pod success
Dec 13 20:36:24.069: INFO: Pod "downwardapi-volume-a057dcff-d468-45c8-bc62-03cfc69d8960" satisfied condition "success or failure"
Dec 13 20:36:24.071: INFO: Trying to get logs from node master-0 pod downwardapi-volume-a057dcff-d468-45c8-bc62-03cfc69d8960 container client-container: <nil>
STEP: delete the pod
Dec 13 20:36:24.090: INFO: Waiting for pod downwardapi-volume-a057dcff-d468-45c8-bc62-03cfc69d8960 to disappear
Dec 13 20:36:24.093: INFO: Pod downwardapi-volume-a057dcff-d468-45c8-bc62-03cfc69d8960 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:36:24.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2420" for this suite.
Dec 13 20:36:30.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:36:30.176: INFO: namespace projected-2420 deletion completed in 6.080204748s

• [SLOW TEST:8.247 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:36:30.177: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4211
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-wm27
STEP: Creating a pod to test atomic-volume-subpath
Dec 13 20:36:30.322: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-wm27" in namespace "subpath-4211" to be "success or failure"
Dec 13 20:36:30.331: INFO: Pod "pod-subpath-test-secret-wm27": Phase="Pending", Reason="", readiness=false. Elapsed: 8.729591ms
Dec 13 20:36:32.333: INFO: Pod "pod-subpath-test-secret-wm27": Phase="Running", Reason="", readiness=true. Elapsed: 2.010869034s
Dec 13 20:36:34.335: INFO: Pod "pod-subpath-test-secret-wm27": Phase="Running", Reason="", readiness=true. Elapsed: 4.013030542s
Dec 13 20:36:36.337: INFO: Pod "pod-subpath-test-secret-wm27": Phase="Running", Reason="", readiness=true. Elapsed: 6.015008112s
Dec 13 20:36:38.340: INFO: Pod "pod-subpath-test-secret-wm27": Phase="Running", Reason="", readiness=true. Elapsed: 8.017579375s
Dec 13 20:36:40.342: INFO: Pod "pod-subpath-test-secret-wm27": Phase="Running", Reason="", readiness=true. Elapsed: 10.020018792s
Dec 13 20:36:42.344: INFO: Pod "pod-subpath-test-secret-wm27": Phase="Running", Reason="", readiness=true. Elapsed: 12.022169444s
Dec 13 20:36:44.346: INFO: Pod "pod-subpath-test-secret-wm27": Phase="Running", Reason="", readiness=true. Elapsed: 14.023946502s
Dec 13 20:36:46.348: INFO: Pod "pod-subpath-test-secret-wm27": Phase="Running", Reason="", readiness=true. Elapsed: 16.025986417s
Dec 13 20:36:48.350: INFO: Pod "pod-subpath-test-secret-wm27": Phase="Running", Reason="", readiness=true. Elapsed: 18.027925533s
Dec 13 20:36:50.352: INFO: Pod "pod-subpath-test-secret-wm27": Phase="Running", Reason="", readiness=true. Elapsed: 20.030080631s
Dec 13 20:36:52.354: INFO: Pod "pod-subpath-test-secret-wm27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.032110797s
STEP: Saw pod success
Dec 13 20:36:52.354: INFO: Pod "pod-subpath-test-secret-wm27" satisfied condition "success or failure"
Dec 13 20:36:52.356: INFO: Trying to get logs from node master-0 pod pod-subpath-test-secret-wm27 container test-container-subpath-secret-wm27: <nil>
STEP: delete the pod
Dec 13 20:36:52.376: INFO: Waiting for pod pod-subpath-test-secret-wm27 to disappear
Dec 13 20:36:52.378: INFO: Pod pod-subpath-test-secret-wm27 no longer exists
STEP: Deleting pod pod-subpath-test-secret-wm27
Dec 13 20:36:52.378: INFO: Deleting pod "pod-subpath-test-secret-wm27" in namespace "subpath-4211"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:36:52.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4211" for this suite.
Dec 13 20:36:58.391: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:36:58.466: INFO: namespace subpath-4211 deletion completed in 6.083638377s

• [SLOW TEST:28.289 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:36:58.466: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8253
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec 13 20:36:58.611: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8253 /api/v1/namespaces/watch-8253/configmaps/e2e-watch-test-watch-closed 66e623d7-8148-4a27-9cf8-34fab70a5c37 35134 0 2019-12-13 20:36:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 13 20:36:58.611: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8253 /api/v1/namespaces/watch-8253/configmaps/e2e-watch-test-watch-closed 66e623d7-8148-4a27-9cf8-34fab70a5c37 35135 0 2019-12-13 20:36:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec 13 20:36:58.618: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8253 /api/v1/namespaces/watch-8253/configmaps/e2e-watch-test-watch-closed 66e623d7-8148-4a27-9cf8-34fab70a5c37 35136 0 2019-12-13 20:36:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 13 20:36:58.619: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8253 /api/v1/namespaces/watch-8253/configmaps/e2e-watch-test-watch-closed 66e623d7-8148-4a27-9cf8-34fab70a5c37 35137 0 2019-12-13 20:36:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:36:58.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8253" for this suite.
Dec 13 20:37:04.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:37:04.701: INFO: namespace watch-8253 deletion completed in 6.078904222s

• [SLOW TEST:6.235 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:37:04.701: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9069
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:37:20.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9069" for this suite.
Dec 13 20:37:26.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:37:27.003: INFO: namespace resourcequota-9069 deletion completed in 6.089316688s

• [SLOW TEST:22.302 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:37:27.004: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7907
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 20:37:27.146: INFO: (0) /api/v1/nodes/master-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 3.830661ms)
Dec 13 20:37:27.149: INFO: (1) /api/v1/nodes/master-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.796389ms)
Dec 13 20:37:27.152: INFO: (2) /api/v1/nodes/master-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.589931ms)
Dec 13 20:37:27.154: INFO: (3) /api/v1/nodes/master-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.581077ms)
Dec 13 20:37:27.157: INFO: (4) /api/v1/nodes/master-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.320828ms)
Dec 13 20:37:27.159: INFO: (5) /api/v1/nodes/master-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.507182ms)
Dec 13 20:37:27.161: INFO: (6) /api/v1/nodes/master-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.296297ms)
Dec 13 20:37:27.164: INFO: (7) /api/v1/nodes/master-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.48776ms)
Dec 13 20:37:27.167: INFO: (8) /api/v1/nodes/master-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.679023ms)
Dec 13 20:37:27.169: INFO: (9) /api/v1/nodes/master-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.537373ms)
Dec 13 20:37:27.171: INFO: (10) /api/v1/nodes/master-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.252126ms)
Dec 13 20:37:27.174: INFO: (11) /api/v1/nodes/master-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.455896ms)
Dec 13 20:37:27.177: INFO: (12) /api/v1/nodes/master-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.963738ms)
Dec 13 20:37:27.180: INFO: (13) /api/v1/nodes/master-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.569311ms)
Dec 13 20:37:27.183: INFO: (14) /api/v1/nodes/master-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.968781ms)
Dec 13 20:37:27.185: INFO: (15) /api/v1/nodes/master-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.488312ms)
Dec 13 20:37:27.188: INFO: (16) /api/v1/nodes/master-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.488525ms)
Dec 13 20:37:27.190: INFO: (17) /api/v1/nodes/master-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.476267ms)
Dec 13 20:37:27.192: INFO: (18) /api/v1/nodes/master-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.354919ms)
Dec 13 20:37:27.195: INFO: (19) /api/v1/nodes/master-0/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="crio/">crio/</a>
<a ... (200; 2.168824ms)
[AfterEach] version v1
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:37:27.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7907" for this suite.
Dec 13 20:37:33.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:37:33.410: INFO: namespace proxy-7907 deletion completed in 6.213174568s

• [SLOW TEST:6.407 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:37:33.411: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4089
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 13 20:37:34.299: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 13 20:37:37.314: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Dec 13 20:37:39.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 attach --namespace=webhook-4089 to-be-attached-pod -i -c=container1'
Dec 13 20:37:39.412: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:37:39.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4089" for this suite.
Dec 13 20:37:51.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:37:51.502: INFO: namespace webhook-4089 deletion completed in 12.084280496s
STEP: Destroying namespace "webhook-4089-markers" for this suite.
Dec 13 20:37:57.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:37:57.658: INFO: namespace webhook-4089-markers deletion completed in 6.155190578s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:24.257 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:37:57.668: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-492
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:38:08.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-492" for this suite.
Dec 13 20:38:14.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:38:14.993: INFO: namespace resourcequota-492 deletion completed in 6.100774131s

• [SLOW TEST:17.324 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:38:14.993: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6297
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 13 20:38:15.123: INFO: Waiting up to 5m0s for pod "pod-115402d7-afc1-4cdd-8fb2-6f4b2853ca14" in namespace "emptydir-6297" to be "success or failure"
Dec 13 20:38:15.129: INFO: Pod "pod-115402d7-afc1-4cdd-8fb2-6f4b2853ca14": Phase="Pending", Reason="", readiness=false. Elapsed: 5.389203ms
Dec 13 20:38:17.131: INFO: Pod "pod-115402d7-afc1-4cdd-8fb2-6f4b2853ca14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007768808s
STEP: Saw pod success
Dec 13 20:38:17.131: INFO: Pod "pod-115402d7-afc1-4cdd-8fb2-6f4b2853ca14" satisfied condition "success or failure"
Dec 13 20:38:17.133: INFO: Trying to get logs from node master-0 pod pod-115402d7-afc1-4cdd-8fb2-6f4b2853ca14 container test-container: <nil>
STEP: delete the pod
Dec 13 20:38:17.147: INFO: Waiting for pod pod-115402d7-afc1-4cdd-8fb2-6f4b2853ca14 to disappear
Dec 13 20:38:17.150: INFO: Pod pod-115402d7-afc1-4cdd-8fb2-6f4b2853ca14 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:38:17.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6297" for this suite.
Dec 13 20:38:23.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:38:23.242: INFO: namespace emptydir-6297 deletion completed in 6.088220491s

• [SLOW TEST:8.249 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:38:23.242: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5432
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 20:38:23.396: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec 13 20:38:23.407: INFO: Number of nodes with available pods: 0
Dec 13 20:38:23.407: INFO: Node master-0 is running more than one daemon pod
Dec 13 20:38:24.416: INFO: Number of nodes with available pods: 0
Dec 13 20:38:24.416: INFO: Node master-0 is running more than one daemon pod
Dec 13 20:38:25.424: INFO: Number of nodes with available pods: 2
Dec 13 20:38:25.424: INFO: Node master-0 is running more than one daemon pod
Dec 13 20:38:26.420: INFO: Number of nodes with available pods: 3
Dec 13 20:38:26.420: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec 13 20:38:26.506: INFO: Wrong image for pod: daemon-set-82kdz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:26.506: INFO: Wrong image for pod: daemon-set-wwxjv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:26.506: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:27.519: INFO: Wrong image for pod: daemon-set-82kdz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:27.519: INFO: Wrong image for pod: daemon-set-wwxjv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:27.519: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:28.519: INFO: Wrong image for pod: daemon-set-82kdz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:28.519: INFO: Wrong image for pod: daemon-set-wwxjv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:28.519: INFO: Pod daemon-set-wwxjv is not available
Dec 13 20:38:28.519: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:29.520: INFO: Wrong image for pod: daemon-set-82kdz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:29.520: INFO: Pod daemon-set-g6lh4 is not available
Dec 13 20:38:29.520: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:30.520: INFO: Wrong image for pod: daemon-set-82kdz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:30.520: INFO: Pod daemon-set-g6lh4 is not available
Dec 13 20:38:30.520: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:31.520: INFO: Wrong image for pod: daemon-set-82kdz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:31.520: INFO: Pod daemon-set-g6lh4 is not available
Dec 13 20:38:31.520: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:32.519: INFO: Wrong image for pod: daemon-set-82kdz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:32.519: INFO: Pod daemon-set-g6lh4 is not available
Dec 13 20:38:32.519: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:33.519: INFO: Wrong image for pod: daemon-set-82kdz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:33.520: INFO: Pod daemon-set-g6lh4 is not available
Dec 13 20:38:33.520: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:34.520: INFO: Wrong image for pod: daemon-set-82kdz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:34.520: INFO: Pod daemon-set-g6lh4 is not available
Dec 13 20:38:34.520: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:35.520: INFO: Wrong image for pod: daemon-set-82kdz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:35.520: INFO: Pod daemon-set-g6lh4 is not available
Dec 13 20:38:35.520: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:36.524: INFO: Wrong image for pod: daemon-set-82kdz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:36.524: INFO: Pod daemon-set-g6lh4 is not available
Dec 13 20:38:36.524: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:37.520: INFO: Wrong image for pod: daemon-set-82kdz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:37.520: INFO: Pod daemon-set-g6lh4 is not available
Dec 13 20:38:37.520: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:38.520: INFO: Wrong image for pod: daemon-set-82kdz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:38.520: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:39.520: INFO: Wrong image for pod: daemon-set-82kdz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:39.520: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:40.520: INFO: Wrong image for pod: daemon-set-82kdz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:40.520: INFO: Pod daemon-set-82kdz is not available
Dec 13 20:38:40.520: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:41.520: INFO: Wrong image for pod: daemon-set-82kdz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:41.520: INFO: Pod daemon-set-82kdz is not available
Dec 13 20:38:41.520: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:42.520: INFO: Wrong image for pod: daemon-set-82kdz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:42.520: INFO: Pod daemon-set-82kdz is not available
Dec 13 20:38:42.520: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:43.519: INFO: Wrong image for pod: daemon-set-82kdz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:43.519: INFO: Pod daemon-set-82kdz is not available
Dec 13 20:38:43.519: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:44.520: INFO: Wrong image for pod: daemon-set-82kdz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:44.520: INFO: Pod daemon-set-82kdz is not available
Dec 13 20:38:44.520: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:45.519: INFO: Wrong image for pod: daemon-set-82kdz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:45.519: INFO: Pod daemon-set-82kdz is not available
Dec 13 20:38:45.519: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:46.520: INFO: Pod daemon-set-brpv5 is not available
Dec 13 20:38:46.520: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:47.520: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:47.520: INFO: Pod daemon-set-xl4qh is not available
Dec 13 20:38:48.520: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:48.520: INFO: Pod daemon-set-xl4qh is not available
Dec 13 20:38:49.520: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:49.520: INFO: Pod daemon-set-xl4qh is not available
Dec 13 20:38:50.520: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:50.520: INFO: Pod daemon-set-xl4qh is not available
Dec 13 20:38:51.520: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:51.520: INFO: Pod daemon-set-xl4qh is not available
Dec 13 20:38:52.520: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:52.520: INFO: Pod daemon-set-xl4qh is not available
Dec 13 20:38:53.520: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:53.520: INFO: Pod daemon-set-xl4qh is not available
Dec 13 20:38:54.520: INFO: Wrong image for pod: daemon-set-xl4qh. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 13 20:38:54.520: INFO: Pod daemon-set-xl4qh is not available
Dec 13 20:38:55.520: INFO: Pod daemon-set-98tmk is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Dec 13 20:38:55.527: INFO: Number of nodes with available pods: 2
Dec 13 20:38:55.527: INFO: Node worker-0 is running more than one daemon pod
Dec 13 20:38:56.538: INFO: Number of nodes with available pods: 3
Dec 13 20:38:56.538: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5432, will wait for the garbage collector to delete the pods
Dec 13 20:38:56.641: INFO: Deleting DaemonSet.extensions daemon-set took: 11.41043ms
Dec 13 20:38:57.245: INFO: Terminating DaemonSet.extensions daemon-set pods took: 603.596813ms
Dec 13 20:39:04.947: INFO: Number of nodes with available pods: 0
Dec 13 20:39:04.947: INFO: Number of running nodes: 0, number of available pods: 0
Dec 13 20:39:04.948: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5432/daemonsets","resourceVersion":"35925"},"items":null}

Dec 13 20:39:04.950: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5432/pods","resourceVersion":"35925"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:39:04.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5432" for this suite.
Dec 13 20:39:10.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:39:11.036: INFO: namespace daemonsets-5432 deletion completed in 6.075581152s

• [SLOW TEST:47.794 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:39:11.036: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2769
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-fe09903b-0a47-402c-b9c0-a66b637b5856
STEP: Creating a pod to test consume configMaps
Dec 13 20:39:11.174: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ff25d1e7-501e-4fca-a7fc-6bb43429d2f2" in namespace "projected-2769" to be "success or failure"
Dec 13 20:39:11.181: INFO: Pod "pod-projected-configmaps-ff25d1e7-501e-4fca-a7fc-6bb43429d2f2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.641575ms
Dec 13 20:39:13.183: INFO: Pod "pod-projected-configmaps-ff25d1e7-501e-4fca-a7fc-6bb43429d2f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008541133s
STEP: Saw pod success
Dec 13 20:39:13.183: INFO: Pod "pod-projected-configmaps-ff25d1e7-501e-4fca-a7fc-6bb43429d2f2" satisfied condition "success or failure"
Dec 13 20:39:13.185: INFO: Trying to get logs from node master-0 pod pod-projected-configmaps-ff25d1e7-501e-4fca-a7fc-6bb43429d2f2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 20:39:13.201: INFO: Waiting for pod pod-projected-configmaps-ff25d1e7-501e-4fca-a7fc-6bb43429d2f2 to disappear
Dec 13 20:39:13.210: INFO: Pod pod-projected-configmaps-ff25d1e7-501e-4fca-a7fc-6bb43429d2f2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:39:13.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2769" for this suite.
Dec 13 20:39:19.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:39:19.298: INFO: namespace projected-2769 deletion completed in 6.08548947s

• [SLOW TEST:8.262 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:39:19.298: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7725
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec 13 20:39:21.978: INFO: Successfully updated pod "labelsupdate32cae4a1-abdc-4271-82f4-4b459e409ca7"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:39:25.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7725" for this suite.
Dec 13 20:39:38.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:39:38.133: INFO: namespace downward-api-7725 deletion completed in 12.135821832s

• [SLOW TEST:18.835 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:39:38.133: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7622
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Dec 13 20:39:38.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 create -f - --namespace=kubectl-7622'
Dec 13 20:39:38.515: INFO: stderr: ""
Dec 13 20:39:38.515: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 13 20:39:39.517: INFO: Selector matched 1 pods for map[app:redis]
Dec 13 20:39:39.517: INFO: Found 0 / 1
Dec 13 20:39:40.517: INFO: Selector matched 1 pods for map[app:redis]
Dec 13 20:39:40.517: INFO: Found 1 / 1
Dec 13 20:39:40.517: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec 13 20:39:40.519: INFO: Selector matched 1 pods for map[app:redis]
Dec 13 20:39:40.519: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 13 20:39:40.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 patch pod redis-master-sdkbk --namespace=kubectl-7622 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec 13 20:39:40.602: INFO: stderr: ""
Dec 13 20:39:40.602: INFO: stdout: "pod/redis-master-sdkbk patched\n"
STEP: checking annotations
Dec 13 20:39:40.605: INFO: Selector matched 1 pods for map[app:redis]
Dec 13 20:39:40.605: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:39:40.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7622" for this suite.
Dec 13 20:39:52.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:39:52.701: INFO: namespace kubectl-7622 deletion completed in 12.092423404s

• [SLOW TEST:14.568 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:39:52.702: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Dec 13 20:39:52.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 api-versions'
Dec 13 20:39:52.909: INFO: stderr: ""
Dec 13 20:39:52.909: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nceph.rook.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\nobjectbucket.io/v1alpha1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nrook.io/v1alpha2\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsnapshot.storage.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:39:52.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-112" for this suite.
Dec 13 20:39:58.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:39:58.996: INFO: namespace kubectl-112 deletion completed in 6.084407857s

• [SLOW TEST:6.295 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:39:58.996: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1489
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-8977e5a7-0dbf-4efb-a982-a08308b79728
STEP: Creating a pod to test consume secrets
Dec 13 20:39:59.181: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b79c46ac-08b7-4f68-adac-f88edbb8bea6" in namespace "projected-1489" to be "success or failure"
Dec 13 20:39:59.223: INFO: Pod "pod-projected-secrets-b79c46ac-08b7-4f68-adac-f88edbb8bea6": Phase="Pending", Reason="", readiness=false. Elapsed: 42.054263ms
Dec 13 20:40:01.225: INFO: Pod "pod-projected-secrets-b79c46ac-08b7-4f68-adac-f88edbb8bea6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.044582237s
STEP: Saw pod success
Dec 13 20:40:01.225: INFO: Pod "pod-projected-secrets-b79c46ac-08b7-4f68-adac-f88edbb8bea6" satisfied condition "success or failure"
Dec 13 20:40:01.228: INFO: Trying to get logs from node master-0 pod pod-projected-secrets-b79c46ac-08b7-4f68-adac-f88edbb8bea6 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 13 20:40:01.242: INFO: Waiting for pod pod-projected-secrets-b79c46ac-08b7-4f68-adac-f88edbb8bea6 to disappear
Dec 13 20:40:01.246: INFO: Pod pod-projected-secrets-b79c46ac-08b7-4f68-adac-f88edbb8bea6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:40:01.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1489" for this suite.
Dec 13 20:40:07.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:40:07.354: INFO: namespace projected-1489 deletion completed in 6.104806321s

• [SLOW TEST:8.357 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:40:07.354: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-312
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-4e8d16d8-59a6-4ae8-919b-d6e6dde9c626
STEP: Creating a pod to test consume secrets
Dec 13 20:40:07.499: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d5914095-cef9-497e-a8b3-88bdbd8614f7" in namespace "projected-312" to be "success or failure"
Dec 13 20:40:07.504: INFO: Pod "pod-projected-secrets-d5914095-cef9-497e-a8b3-88bdbd8614f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.729864ms
Dec 13 20:40:09.506: INFO: Pod "pod-projected-secrets-d5914095-cef9-497e-a8b3-88bdbd8614f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006626545s
STEP: Saw pod success
Dec 13 20:40:09.506: INFO: Pod "pod-projected-secrets-d5914095-cef9-497e-a8b3-88bdbd8614f7" satisfied condition "success or failure"
Dec 13 20:40:09.508: INFO: Trying to get logs from node master-0 pod pod-projected-secrets-d5914095-cef9-497e-a8b3-88bdbd8614f7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 13 20:40:09.527: INFO: Waiting for pod pod-projected-secrets-d5914095-cef9-497e-a8b3-88bdbd8614f7 to disappear
Dec 13 20:40:09.530: INFO: Pod pod-projected-secrets-d5914095-cef9-497e-a8b3-88bdbd8614f7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:40:09.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-312" for this suite.
Dec 13 20:40:15.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:40:15.623: INFO: namespace projected-312 deletion completed in 6.089787914s

• [SLOW TEST:8.269 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:40:15.623: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1335
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 20:40:15.760: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:40:16.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1335" for this suite.
Dec 13 20:40:22.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:40:22.424: INFO: namespace custom-resource-definition-1335 deletion completed in 6.126849376s

• [SLOW TEST:6.801 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:40:22.424: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8805
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 13 20:40:22.614: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0587ae80-3d46-4805-a4bd-cdb9539c5852" in namespace "projected-8805" to be "success or failure"
Dec 13 20:40:22.619: INFO: Pod "downwardapi-volume-0587ae80-3d46-4805-a4bd-cdb9539c5852": Phase="Pending", Reason="", readiness=false. Elapsed: 5.698847ms
Dec 13 20:40:24.621: INFO: Pod "downwardapi-volume-0587ae80-3d46-4805-a4bd-cdb9539c5852": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007853724s
STEP: Saw pod success
Dec 13 20:40:24.621: INFO: Pod "downwardapi-volume-0587ae80-3d46-4805-a4bd-cdb9539c5852" satisfied condition "success or failure"
Dec 13 20:40:24.623: INFO: Trying to get logs from node master-0 pod downwardapi-volume-0587ae80-3d46-4805-a4bd-cdb9539c5852 container client-container: <nil>
STEP: delete the pod
Dec 13 20:40:24.711: INFO: Waiting for pod downwardapi-volume-0587ae80-3d46-4805-a4bd-cdb9539c5852 to disappear
Dec 13 20:40:24.714: INFO: Pod downwardapi-volume-0587ae80-3d46-4805-a4bd-cdb9539c5852 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:40:24.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8805" for this suite.
Dec 13 20:40:30.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:40:30.819: INFO: namespace projected-8805 deletion completed in 6.10195506s

• [SLOW TEST:8.395 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:40:30.819: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4711
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-8365517f-751c-4e9f-aa3a-9b9d30fdc0ce
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:40:31.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4711" for this suite.
Dec 13 20:40:37.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:40:37.184: INFO: namespace configmap-4711 deletion completed in 6.173006193s

• [SLOW TEST:6.365 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:40:37.184: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8433
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 13 20:40:37.348: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 13 20:40:37.361: INFO: Waiting for terminating namespaces to be deleted...
Dec 13 20:40:37.363: INFO: 
Logging pods the kubelet thinks is on node master-0 before test
Dec 13 20:40:37.373: INFO: rook-ceph-mon-b-5b988bf97b-4xkn2 from rook-ceph started at 2019-12-13 20:15:27 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.373: INFO: 	Container mon ready: true, restart count 0
Dec 13 20:40:37.373: INFO: kube-controller-manager-master-0 from kube-system started at 2019-12-13 18:54:06 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.373: INFO: 	Container kube-controller-manager ready: true, restart count 0
Dec 13 20:40:37.373: INFO: canal-xbp9c from kube-system started at 2019-12-13 18:55:18 +0000 UTC (2 container statuses recorded)
Dec 13 20:40:37.373: INFO: 	Container calico-node ready: true, restart count 0
Dec 13 20:40:37.373: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 13 20:40:37.373: INFO: kata-deploy-m4sjm from kube-system started at 2019-12-13 20:15:27 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.373: INFO: 	Container kube-kata ready: true, restart count 0
Dec 13 20:40:37.373: INFO: rook-ceph-osd-2-58969bddc7-6zxpl from rook-ceph started at 2019-12-13 20:15:27 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.373: INFO: 	Container osd ready: true, restart count 0
Dec 13 20:40:37.373: INFO: node-exporter-tk6xn from monitoring started at 2019-12-13 19:05:14 +0000 UTC (2 container statuses recorded)
Dec 13 20:40:37.373: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 13 20:40:37.373: INFO: 	Container node-exporter ready: true, restart count 0
Dec 13 20:40:37.373: INFO: etcd-master-0 from kube-system started at 2019-12-13 18:54:06 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.373: INFO: 	Container etcd ready: true, restart count 0
Dec 13 20:40:37.373: INFO: csi-rbdplugin-8rpjc from rook-ceph started at 2019-12-13 20:15:27 +0000 UTC (3 container statuses recorded)
Dec 13 20:40:37.373: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 20:40:37.373: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:40:37.373: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:40:37.373: INFO: csi-cephfsplugin-vvg2s from rook-ceph started at 2019-12-13 20:15:27 +0000 UTC (3 container statuses recorded)
Dec 13 20:40:37.373: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 20:40:37.373: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:40:37.373: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:40:37.373: INFO: kube-apiserver-master-0 from kube-system started at 2019-12-13 18:54:06 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.373: INFO: 	Container kube-apiserver ready: true, restart count 0
Dec 13 20:40:37.373: INFO: sonobuoy-e2e-job-8f6bded5e69344cd from sonobuoy started at 2019-12-13 19:12:21 +0000 UTC (2 container statuses recorded)
Dec 13 20:40:37.376: INFO: 	Container e2e ready: true, restart count 0
Dec 13 20:40:37.376: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 13 20:40:37.376: INFO: sonobuoy-systemd-logs-daemon-set-9616c2f784dc4b45-rxl5l from sonobuoy started at 2019-12-13 19:12:22 +0000 UTC (2 container statuses recorded)
Dec 13 20:40:37.376: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 13 20:40:37.376: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 13 20:40:37.376: INFO: rook-discover-p77wp from rook-ceph started at 2019-12-13 20:15:31 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.376: INFO: 	Container rook-discover ready: true, restart count 0
Dec 13 20:40:37.376: INFO: kube-scheduler-master-0 from kube-system started at 2019-12-13 18:54:06 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.376: INFO: 	Container kube-scheduler ready: true, restart count 0
Dec 13 20:40:37.376: INFO: kube-proxy-dfvlw from kube-system started at 2019-12-13 18:54:29 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.376: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 13 20:40:37.376: INFO: fluentd-es-v2.5.2-mfcr4 from kube-system started at 2019-12-13 20:15:27 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.376: INFO: 	Container fluentd-es ready: true, restart count 0
Dec 13 20:40:37.376: INFO: 
Logging pods the kubelet thinks is on node worker-0 before test
Dec 13 20:40:37.401: INFO: alertmanager-main-2 from monitoring started at 2019-12-13 19:06:20 +0000 UTC (2 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container alertmanager ready: true, restart count 0
Dec 13 20:40:37.401: INFO: 	Container config-reloader ready: true, restart count 0
Dec 13 20:40:37.401: INFO: prometheus-k8s-1 from monitoring started at 2019-12-13 19:06:30 +0000 UTC (3 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container prometheus ready: true, restart count 1
Dec 13 20:40:37.401: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec 13 20:40:37.401: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec 13 20:40:37.401: INFO: coredns-5644d7b6d9-rngbs from kube-system started at 2019-12-13 18:55:47 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container coredns ready: true, restart count 0
Dec 13 20:40:37.401: INFO: coredns-5644d7b6d9-tdq6s from kube-system started at 2019-12-13 18:55:47 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container coredns ready: true, restart count 0
Dec 13 20:40:37.401: INFO: rook-discover-5t2vg from rook-ceph started at 2019-12-13 18:59:33 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container rook-discover ready: true, restart count 0
Dec 13 20:40:37.401: INFO: csi-rbdplugin-9vnm2 from rook-ceph started at 2019-12-13 18:59:33 +0000 UTC (3 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 20:40:37.401: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:40:37.401: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:40:37.401: INFO: rook-ceph-osd-prepare-worker-0-xz82f from rook-ceph started at 2019-12-13 19:03:42 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container provision ready: false, restart count 0
Dec 13 20:40:37.401: INFO: kubernetes-dashboard-6865b885fc-7tv2c from kubernetes-dashboard started at 2019-12-13 19:05:45 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec 13 20:40:37.401: INFO: fluentd-es-v2.5.2-w2nk6 from kube-system started at 2019-12-13 19:07:51 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container fluentd-es ready: true, restart count 0
Dec 13 20:40:37.401: INFO: sonobuoy-systemd-logs-daemon-set-9616c2f784dc4b45-qpldg from sonobuoy started at 2019-12-13 19:12:22 +0000 UTC (2 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 13 20:40:37.401: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 13 20:40:37.401: INFO: grafana-5cd56df4cd-wqpht from monitoring started at 2019-12-13 19:41:23 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container grafana ready: true, restart count 0
Dec 13 20:40:37.401: INFO: rook-ceph-operator-c8ff6447d-6s7cz from rook-ceph started at 2019-12-13 18:55:45 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Dec 13 20:40:37.401: INFO: csi-cephfsplugin-n7682 from rook-ceph started at 2019-12-13 18:59:35 +0000 UTC (3 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 20:40:37.401: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:40:37.401: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:40:37.401: INFO: rook-ceph-osd-0-64bcd55b9f-hrsvl from rook-ceph started at 2019-12-13 19:03:50 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container osd ready: true, restart count 0
Dec 13 20:40:37.401: INFO: alertmanager-main-1 from monitoring started at 2019-12-13 20:14:55 +0000 UTC (2 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container alertmanager ready: true, restart count 0
Dec 13 20:40:37.401: INFO: 	Container config-reloader ready: true, restart count 0
Dec 13 20:40:37.401: INFO: elasticsearch-logging-0 from kube-system started at 2019-12-13 20:14:56 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Dec 13 20:40:37.401: INFO: metrics-server-7578984995-fr9dj from kube-system started at 2019-12-13 18:55:45 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container metrics-server ready: true, restart count 0
Dec 13 20:40:37.401: INFO: csi-cephfsplugin-provisioner-75c965db4f-dhmjp from rook-ceph started at 2019-12-13 18:59:36 +0000 UTC (4 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container csi-attacher ready: true, restart count 0
Dec 13 20:40:37.401: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 20:40:37.401: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 13 20:40:37.401: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:40:37.401: INFO: rook-ceph-mon-c-597fd9c9c7-r8ngb from rook-ceph started at 2019-12-13 19:02:59 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container mon ready: true, restart count 0
Dec 13 20:40:37.401: INFO: csi-rbdplugin-provisioner-56cbc4d585-lhtqr from rook-ceph started at 2019-12-13 20:14:30 +0000 UTC (5 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 13 20:40:37.401: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 20:40:37.401: INFO: 	Container csi-rbdplugin-attacher ready: true, restart count 0
Dec 13 20:40:37.401: INFO: 	Container csi-snapshotter ready: true, restart count 0
Dec 13 20:40:37.401: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:40:37.401: INFO: dashboard-metrics-scraper-6ccf7f6cc8-k4zf5 from kubernetes-dashboard started at 2019-12-13 20:14:31 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Dec 13 20:40:37.401: INFO: kube-proxy-rktzd from kube-system started at 2019-12-13 18:54:55 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 13 20:40:37.401: INFO: canal-sgqjs from kube-system started at 2019-12-13 18:55:18 +0000 UTC (2 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container calico-node ready: true, restart count 0
Dec 13 20:40:37.401: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 13 20:40:37.401: INFO: kata-deploy-2qbn2 from kube-system started at 2019-12-13 18:55:45 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container kube-kata ready: true, restart count 0
Dec 13 20:40:37.401: INFO: node-exporter-584q9 from monitoring started at 2019-12-13 19:05:14 +0000 UTC (2 container statuses recorded)
Dec 13 20:40:37.401: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 13 20:40:37.401: INFO: 	Container node-exporter ready: true, restart count 0
Dec 13 20:40:37.401: INFO: 
Logging pods the kubelet thinks is on node worker-1 before test
Dec 13 20:40:37.421: INFO: prometheus-adapter-c676d8764-7pnjf from monitoring started at 2019-12-13 19:41:23 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container prometheus-adapter ready: true, restart count 0
Dec 13 20:40:37.421: INFO: prometheus-k8s-0 from monitoring started at 2019-12-13 20:14:55 +0000 UTC (3 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container prometheus ready: true, restart count 1
Dec 13 20:40:37.421: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec 13 20:40:37.421: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec 13 20:40:37.421: INFO: kube-proxy-dt9pl from kube-system started at 2019-12-13 18:55:17 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 13 20:40:37.421: INFO: csi-rbdplugin-rhs8g from rook-ceph started at 2019-12-13 18:59:33 +0000 UTC (3 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 20:40:37.421: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:40:37.421: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:40:37.421: INFO: csi-rbdplugin-provisioner-56cbc4d585-j64wf from rook-ceph started at 2019-12-13 18:59:34 +0000 UTC (5 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 13 20:40:37.421: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 20:40:37.421: INFO: 	Container csi-rbdplugin-attacher ready: true, restart count 0
Dec 13 20:40:37.421: INFO: 	Container csi-snapshotter ready: true, restart count 0
Dec 13 20:40:37.421: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:40:37.421: INFO: node-exporter-7ggvt from monitoring started at 2019-12-13 19:05:14 +0000 UTC (2 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 13 20:40:37.421: INFO: 	Container node-exporter ready: true, restart count 0
Dec 13 20:40:37.421: INFO: elasticsearch-logging-1 from kube-system started at 2019-12-13 19:10:24 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Dec 13 20:40:37.421: INFO: sonobuoy-systemd-logs-daemon-set-9616c2f784dc4b45-f29sm from sonobuoy started at 2019-12-13 19:12:22 +0000 UTC (2 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 13 20:40:37.421: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 13 20:40:37.421: INFO: prometheus-operator-7559d67ff-shr6m from monitoring started at 2019-12-13 20:14:30 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container prometheus-operator ready: true, restart count 0
Dec 13 20:40:37.421: INFO: kata-deploy-qtlwp from kube-system started at 2019-12-13 18:55:57 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container kube-kata ready: true, restart count 0
Dec 13 20:40:37.421: INFO: rook-discover-m4tgh from rook-ceph started at 2019-12-13 18:59:33 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container rook-discover ready: true, restart count 0
Dec 13 20:40:37.421: INFO: rook-ceph-mon-a-c7664c8c4-jcx8w from rook-ceph started at 2019-12-13 19:02:27 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container mon ready: true, restart count 0
Dec 13 20:40:37.421: INFO: rook-ceph-osd-prepare-worker-1-2xccr from rook-ceph started at 2019-12-13 19:03:42 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container provision ready: false, restart count 0
Dec 13 20:40:37.421: INFO: rook-ceph-mgr-a-5775778c4d-k745v from rook-ceph started at 2019-12-13 20:14:30 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container mgr ready: true, restart count 0
Dec 13 20:40:37.421: INFO: kibana-logging-7f6b4b96b4-pdq55 from kube-system started at 2019-12-13 20:14:30 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container kibana-logging ready: true, restart count 0
Dec 13 20:40:37.421: INFO: csi-cephfsplugin-provisioner-75c965db4f-6plm2 from rook-ceph started at 2019-12-13 20:14:31 +0000 UTC (4 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container csi-attacher ready: true, restart count 0
Dec 13 20:40:37.421: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 20:40:37.421: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 13 20:40:37.421: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:40:37.421: INFO: canal-5wqk9 from kube-system started at 2019-12-13 18:55:18 +0000 UTC (2 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container calico-node ready: true, restart count 0
Dec 13 20:40:37.421: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 13 20:40:37.421: INFO: csi-cephfsplugin-6szlf from rook-ceph started at 2019-12-13 18:59:35 +0000 UTC (3 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 20:40:37.421: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:40:37.421: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:40:37.421: INFO: rook-ceph-osd-1-7d8886d496-jzxm4 from rook-ceph started at 2019-12-13 19:03:51 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container osd ready: true, restart count 0
Dec 13 20:40:37.421: INFO: nginx-ingress-controller-7fb85bc8bb-8f8dr from ingress-nginx started at 2019-12-13 19:08:29 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec 13 20:40:37.421: INFO: alertmanager-main-0 from monitoring started at 2019-12-13 19:06:20 +0000 UTC (2 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container alertmanager ready: true, restart count 0
Dec 13 20:40:37.421: INFO: 	Container config-reloader ready: true, restart count 0
Dec 13 20:40:37.421: INFO: fluentd-es-v2.5.2-s2ng4 from kube-system started at 2019-12-13 19:07:51 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container fluentd-es ready: true, restart count 0
Dec 13 20:40:37.421: INFO: sonobuoy from sonobuoy started at 2019-12-13 19:12:05 +0000 UTC (1 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 13 20:40:37.421: INFO: kube-state-metrics-79d4b9b497-wntkj from monitoring started at 2019-12-13 20:14:30 +0000 UTC (3 container statuses recorded)
Dec 13 20:40:37.421: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec 13 20:40:37.421: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec 13 20:40:37.421: INFO: 	Container kube-state-metrics ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-70783c2e-6dad-438f-9052-8bffc37bd80c 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-70783c2e-6dad-438f-9052-8bffc37bd80c off the node master-0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-70783c2e-6dad-438f-9052-8bffc37bd80c
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:40:41.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8433" for this suite.
Dec 13 20:40:59.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:40:59.586: INFO: namespace sched-pred-8433 deletion completed in 18.085378871s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:22.402 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:40:59.587: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1486
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec 13 20:40:59.727: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1486 /api/v1/namespaces/watch-1486/configmaps/e2e-watch-test-label-changed 40107708-ed4e-473e-8399-44bb1d9b9a9e 36659 0 2019-12-13 20:40:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 13 20:40:59.727: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1486 /api/v1/namespaces/watch-1486/configmaps/e2e-watch-test-label-changed 40107708-ed4e-473e-8399-44bb1d9b9a9e 36660 0 2019-12-13 20:40:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 13 20:40:59.727: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1486 /api/v1/namespaces/watch-1486/configmaps/e2e-watch-test-label-changed 40107708-ed4e-473e-8399-44bb1d9b9a9e 36661 0 2019-12-13 20:40:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec 13 20:41:09.745: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1486 /api/v1/namespaces/watch-1486/configmaps/e2e-watch-test-label-changed 40107708-ed4e-473e-8399-44bb1d9b9a9e 36699 0 2019-12-13 20:40:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 13 20:41:09.745: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1486 /api/v1/namespaces/watch-1486/configmaps/e2e-watch-test-label-changed 40107708-ed4e-473e-8399-44bb1d9b9a9e 36701 0 2019-12-13 20:40:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Dec 13 20:41:09.745: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1486 /api/v1/namespaces/watch-1486/configmaps/e2e-watch-test-label-changed 40107708-ed4e-473e-8399-44bb1d9b9a9e 36702 0 2019-12-13 20:40:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:41:09.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1486" for this suite.
Dec 13 20:41:15.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:41:15.837: INFO: namespace watch-1486 deletion completed in 6.089485291s

• [SLOW TEST:16.250 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:41:15.837: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2428
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 20:41:15.972: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:41:19.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2428" for this suite.
Dec 13 20:42:08.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:42:08.084: INFO: namespace pods-2428 deletion completed in 48.088797361s

• [SLOW TEST:52.246 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:42:08.084: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7629
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 20:42:08.269: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-401c8d9c-822c-4e1e-bac6-9436dfc2a346" in namespace "security-context-test-7629" to be "success or failure"
Dec 13 20:42:08.277: INFO: Pod "busybox-readonly-false-401c8d9c-822c-4e1e-bac6-9436dfc2a346": Phase="Pending", Reason="", readiness=false. Elapsed: 7.942114ms
Dec 13 20:42:10.279: INFO: Pod "busybox-readonly-false-401c8d9c-822c-4e1e-bac6-9436dfc2a346": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009993371s
Dec 13 20:42:10.279: INFO: Pod "busybox-readonly-false-401c8d9c-822c-4e1e-bac6-9436dfc2a346" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:42:10.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7629" for this suite.
Dec 13 20:42:16.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:42:16.369: INFO: namespace security-context-test-7629 deletion completed in 6.086643927s

• [SLOW TEST:8.285 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:42:16.369: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2649
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 13 20:42:16.506: INFO: Waiting up to 5m0s for pod "downward-api-89dfb4b0-2494-4962-803c-9d40e85c26df" in namespace "downward-api-2649" to be "success or failure"
Dec 13 20:42:16.519: INFO: Pod "downward-api-89dfb4b0-2494-4962-803c-9d40e85c26df": Phase="Pending", Reason="", readiness=false. Elapsed: 12.589118ms
Dec 13 20:42:18.521: INFO: Pod "downward-api-89dfb4b0-2494-4962-803c-9d40e85c26df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014912696s
STEP: Saw pod success
Dec 13 20:42:18.521: INFO: Pod "downward-api-89dfb4b0-2494-4962-803c-9d40e85c26df" satisfied condition "success or failure"
Dec 13 20:42:18.523: INFO: Trying to get logs from node master-0 pod downward-api-89dfb4b0-2494-4962-803c-9d40e85c26df container dapi-container: <nil>
STEP: delete the pod
Dec 13 20:42:18.540: INFO: Waiting for pod downward-api-89dfb4b0-2494-4962-803c-9d40e85c26df to disappear
Dec 13 20:42:18.542: INFO: Pod downward-api-89dfb4b0-2494-4962-803c-9d40e85c26df no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:42:18.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2649" for this suite.
Dec 13 20:42:24.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:42:24.726: INFO: namespace downward-api-2649 deletion completed in 6.181490744s

• [SLOW TEST:8.357 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:42:24.726: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3975
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Dec 13 20:42:24.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 create -f - --namespace=kubectl-3975'
Dec 13 20:42:25.093: INFO: stderr: ""
Dec 13 20:42:25.093: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 13 20:42:25.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3975'
Dec 13 20:42:25.210: INFO: stderr: ""
Dec 13 20:42:25.210: INFO: stdout: "update-demo-nautilus-pz2p2 update-demo-nautilus-vdjf2 "
Dec 13 20:42:25.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-nautilus-pz2p2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3975'
Dec 13 20:42:25.304: INFO: stderr: ""
Dec 13 20:42:25.304: INFO: stdout: ""
Dec 13 20:42:25.304: INFO: update-demo-nautilus-pz2p2 is created but not running
Dec 13 20:42:30.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3975'
Dec 13 20:42:30.366: INFO: stderr: ""
Dec 13 20:42:30.366: INFO: stdout: "update-demo-nautilus-pz2p2 update-demo-nautilus-vdjf2 "
Dec 13 20:42:30.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-nautilus-pz2p2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3975'
Dec 13 20:42:30.532: INFO: stderr: ""
Dec 13 20:42:30.532: INFO: stdout: "true"
Dec 13 20:42:30.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-nautilus-pz2p2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3975'
Dec 13 20:42:30.592: INFO: stderr: ""
Dec 13 20:42:30.592: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 13 20:42:30.592: INFO: validating pod update-demo-nautilus-pz2p2
Dec 13 20:42:30.595: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 13 20:42:30.595: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 13 20:42:30.595: INFO: update-demo-nautilus-pz2p2 is verified up and running
Dec 13 20:42:30.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-nautilus-vdjf2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3975'
Dec 13 20:42:30.657: INFO: stderr: ""
Dec 13 20:42:30.657: INFO: stdout: "true"
Dec 13 20:42:30.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-nautilus-vdjf2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3975'
Dec 13 20:42:30.719: INFO: stderr: ""
Dec 13 20:42:30.719: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 13 20:42:30.719: INFO: validating pod update-demo-nautilus-vdjf2
Dec 13 20:42:30.723: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 13 20:42:30.723: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 13 20:42:30.723: INFO: update-demo-nautilus-vdjf2 is verified up and running
STEP: rolling-update to new replication controller
Dec 13 20:42:30.726: INFO: scanned /root for discovery docs: <nil>
Dec 13 20:42:30.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-3975'
Dec 13 20:42:54.042: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 13 20:42:54.042: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 13 20:42:54.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3975'
Dec 13 20:42:54.130: INFO: stderr: ""
Dec 13 20:42:54.130: INFO: stdout: "update-demo-kitten-rht78 update-demo-kitten-wqrk4 update-demo-nautilus-vdjf2 "
STEP: Replicas for name=update-demo: expected=2 actual=3
Dec 13 20:42:59.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3975'
Dec 13 20:42:59.212: INFO: stderr: ""
Dec 13 20:42:59.212: INFO: stdout: "update-demo-kitten-rht78 update-demo-kitten-wqrk4 "
Dec 13 20:42:59.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-kitten-rht78 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3975'
Dec 13 20:42:59.293: INFO: stderr: ""
Dec 13 20:42:59.293: INFO: stdout: "true"
Dec 13 20:42:59.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-kitten-rht78 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3975'
Dec 13 20:42:59.361: INFO: stderr: ""
Dec 13 20:42:59.361: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 13 20:42:59.361: INFO: validating pod update-demo-kitten-rht78
Dec 13 20:42:59.364: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 13 20:42:59.364: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 13 20:42:59.364: INFO: update-demo-kitten-rht78 is verified up and running
Dec 13 20:42:59.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-kitten-wqrk4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3975'
Dec 13 20:42:59.433: INFO: stderr: ""
Dec 13 20:42:59.434: INFO: stdout: "true"
Dec 13 20:42:59.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-kitten-wqrk4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3975'
Dec 13 20:42:59.492: INFO: stderr: ""
Dec 13 20:42:59.492: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 13 20:42:59.492: INFO: validating pod update-demo-kitten-wqrk4
Dec 13 20:42:59.496: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 13 20:42:59.496: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 13 20:42:59.496: INFO: update-demo-kitten-wqrk4 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:42:59.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3975" for this suite.
Dec 13 20:43:11.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:43:11.599: INFO: namespace kubectl-3975 deletion completed in 12.100739006s

• [SLOW TEST:46.873 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:43:11.599: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-8662
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8660
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6704
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:43:41.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8662" for this suite.
Dec 13 20:43:47.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:43:47.195: INFO: namespace namespaces-8662 deletion completed in 6.153264426s
STEP: Destroying namespace "nsdeletetest-8660" for this suite.
Dec 13 20:43:47.197: INFO: Namespace nsdeletetest-8660 was already deleted
STEP: Destroying namespace "nsdeletetest-6704" for this suite.
Dec 13 20:43:53.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:43:53.305: INFO: namespace nsdeletetest-6704 deletion completed in 6.107385726s

• [SLOW TEST:41.705 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:43:53.305: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2768
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 20:43:53.439: INFO: Creating ReplicaSet my-hostname-basic-93089249-1b20-4d83-b2ee-9ea6fcb2cf89
Dec 13 20:43:53.443: INFO: Pod name my-hostname-basic-93089249-1b20-4d83-b2ee-9ea6fcb2cf89: Found 0 pods out of 1
Dec 13 20:43:58.447: INFO: Pod name my-hostname-basic-93089249-1b20-4d83-b2ee-9ea6fcb2cf89: Found 1 pods out of 1
Dec 13 20:43:58.447: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-93089249-1b20-4d83-b2ee-9ea6fcb2cf89" is running
Dec 13 20:43:58.450: INFO: Pod "my-hostname-basic-93089249-1b20-4d83-b2ee-9ea6fcb2cf89-mrgvl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-13 20:43:53 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-13 20:43:54 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-13 20:43:54 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-13 20:43:53 +0000 UTC Reason: Message:}])
Dec 13 20:43:58.450: INFO: Trying to dial the pod
Dec 13 20:44:03.456: INFO: Controller my-hostname-basic-93089249-1b20-4d83-b2ee-9ea6fcb2cf89: Got expected result from replica 1 [my-hostname-basic-93089249-1b20-4d83-b2ee-9ea6fcb2cf89-mrgvl]: "my-hostname-basic-93089249-1b20-4d83-b2ee-9ea6fcb2cf89-mrgvl", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:44:03.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2768" for this suite.
Dec 13 20:44:09.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:44:09.578: INFO: namespace replicaset-2768 deletion completed in 6.119798247s

• [SLOW TEST:16.274 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:44:09.579: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3574
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:44:09.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3574" for this suite.
Dec 13 20:44:15.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:44:15.847: INFO: namespace custom-resource-definition-3574 deletion completed in 6.096581569s

• [SLOW TEST:6.268 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:44:15.847: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4029
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-7180c033-d3f7-42bb-a5a6-5b3498fc248d in namespace container-probe-4029
Dec 13 20:44:17.997: INFO: Started pod busybox-7180c033-d3f7-42bb-a5a6-5b3498fc248d in namespace container-probe-4029
STEP: checking the pod's current state and verifying that restartCount is present
Dec 13 20:44:18.002: INFO: Initial restart count of pod busybox-7180c033-d3f7-42bb-a5a6-5b3498fc248d is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:48:18.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4029" for this suite.
Dec 13 20:48:24.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:48:24.705: INFO: namespace container-probe-4029 deletion completed in 6.201168641s

• [SLOW TEST:248.857 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:48:24.705: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9392
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-9392/configmap-test-ac5ec45d-f685-46b7-9455-3e253ffd1b11
STEP: Creating a pod to test consume configMaps
Dec 13 20:48:24.848: INFO: Waiting up to 5m0s for pod "pod-configmaps-0dc9727d-21ef-4270-9586-1e1e3bb1d085" in namespace "configmap-9392" to be "success or failure"
Dec 13 20:48:24.861: INFO: Pod "pod-configmaps-0dc9727d-21ef-4270-9586-1e1e3bb1d085": Phase="Pending", Reason="", readiness=false. Elapsed: 12.722521ms
Dec 13 20:48:26.870: INFO: Pod "pod-configmaps-0dc9727d-21ef-4270-9586-1e1e3bb1d085": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02209829s
STEP: Saw pod success
Dec 13 20:48:26.870: INFO: Pod "pod-configmaps-0dc9727d-21ef-4270-9586-1e1e3bb1d085" satisfied condition "success or failure"
Dec 13 20:48:26.875: INFO: Trying to get logs from node master-0 pod pod-configmaps-0dc9727d-21ef-4270-9586-1e1e3bb1d085 container env-test: <nil>
STEP: delete the pod
Dec 13 20:48:26.926: INFO: Waiting for pod pod-configmaps-0dc9727d-21ef-4270-9586-1e1e3bb1d085 to disappear
Dec 13 20:48:26.942: INFO: Pod pod-configmaps-0dc9727d-21ef-4270-9586-1e1e3bb1d085 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:48:26.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9392" for this suite.
Dec 13 20:48:32.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:48:33.063: INFO: namespace configmap-9392 deletion completed in 6.110786222s

• [SLOW TEST:8.358 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:48:33.063: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5869
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-3394f217-1623-4ae2-a2cf-bdec03f80580
STEP: Creating a pod to test consume secrets
Dec 13 20:48:33.219: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c876b756-e855-4335-a27a-a9daae0459d7" in namespace "projected-5869" to be "success or failure"
Dec 13 20:48:33.232: INFO: Pod "pod-projected-secrets-c876b756-e855-4335-a27a-a9daae0459d7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.227335ms
Dec 13 20:48:35.235: INFO: Pod "pod-projected-secrets-c876b756-e855-4335-a27a-a9daae0459d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015460971s
STEP: Saw pod success
Dec 13 20:48:35.235: INFO: Pod "pod-projected-secrets-c876b756-e855-4335-a27a-a9daae0459d7" satisfied condition "success or failure"
Dec 13 20:48:35.237: INFO: Trying to get logs from node master-0 pod pod-projected-secrets-c876b756-e855-4335-a27a-a9daae0459d7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 13 20:48:35.252: INFO: Waiting for pod pod-projected-secrets-c876b756-e855-4335-a27a-a9daae0459d7 to disappear
Dec 13 20:48:35.262: INFO: Pod pod-projected-secrets-c876b756-e855-4335-a27a-a9daae0459d7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:48:35.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5869" for this suite.
Dec 13 20:48:41.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:48:41.350: INFO: namespace projected-5869 deletion completed in 6.083660562s

• [SLOW TEST:8.287 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:48:41.350: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3972
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 13 20:48:41.488: INFO: Waiting up to 5m0s for pod "pod-ffac5d2f-72c8-4654-a50e-2376fe715cb1" in namespace "emptydir-3972" to be "success or failure"
Dec 13 20:48:41.498: INFO: Pod "pod-ffac5d2f-72c8-4654-a50e-2376fe715cb1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018841ms
Dec 13 20:48:43.500: INFO: Pod "pod-ffac5d2f-72c8-4654-a50e-2376fe715cb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01199949s
STEP: Saw pod success
Dec 13 20:48:43.500: INFO: Pod "pod-ffac5d2f-72c8-4654-a50e-2376fe715cb1" satisfied condition "success or failure"
Dec 13 20:48:43.501: INFO: Trying to get logs from node master-0 pod pod-ffac5d2f-72c8-4654-a50e-2376fe715cb1 container test-container: <nil>
STEP: delete the pod
Dec 13 20:48:43.522: INFO: Waiting for pod pod-ffac5d2f-72c8-4654-a50e-2376fe715cb1 to disappear
Dec 13 20:48:43.525: INFO: Pod pod-ffac5d2f-72c8-4654-a50e-2376fe715cb1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:48:43.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3972" for this suite.
Dec 13 20:48:49.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:48:49.616: INFO: namespace emptydir-3972 deletion completed in 6.088390882s

• [SLOW TEST:8.266 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:48:49.616: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2596
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-44abe702-3ea4-458b-9af2-0bec4eb4bd47
STEP: Creating secret with name s-test-opt-upd-8f89d653-a3e2-496a-8814-b194bf7e17d1
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-44abe702-3ea4-458b-9af2-0bec4eb4bd47
STEP: Updating secret s-test-opt-upd-8f89d653-a3e2-496a-8814-b194bf7e17d1
STEP: Creating secret with name s-test-opt-create-d3284d82-0737-4907-b465-ed88c1fe57ba
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:48:53.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2596" for this suite.
Dec 13 20:49:07.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:49:07.952: INFO: namespace projected-2596 deletion completed in 14.123323039s

• [SLOW TEST:18.336 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:49:07.953: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8387
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec 13 20:49:10.623: INFO: Successfully updated pod "annotationupdatef6f7a140-5c5f-4360-867a-379e666e168e"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:49:14.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8387" for this suite.
Dec 13 20:49:26.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:49:26.904: INFO: namespace projected-8387 deletion completed in 12.263476968s

• [SLOW TEST:18.952 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:49:26.905: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6839
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Dec 13 20:49:27.093: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Dec 13 20:49:40.382: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 20:49:43.311: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:49:57.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6839" for this suite.
Dec 13 20:50:03.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:50:03.746: INFO: namespace crd-publish-openapi-6839 deletion completed in 6.117102431s

• [SLOW TEST:36.841 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:50:03.746: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2174
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 13 20:50:04.000: INFO: Waiting up to 5m0s for pod "pod-6afe811d-e1a5-44ee-ac72-54aaca7aa82d" in namespace "emptydir-2174" to be "success or failure"
Dec 13 20:50:04.040: INFO: Pod "pod-6afe811d-e1a5-44ee-ac72-54aaca7aa82d": Phase="Pending", Reason="", readiness=false. Elapsed: 39.260275ms
Dec 13 20:50:06.042: INFO: Pod "pod-6afe811d-e1a5-44ee-ac72-54aaca7aa82d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041595975s
STEP: Saw pod success
Dec 13 20:50:06.042: INFO: Pod "pod-6afe811d-e1a5-44ee-ac72-54aaca7aa82d" satisfied condition "success or failure"
Dec 13 20:50:06.044: INFO: Trying to get logs from node master-0 pod pod-6afe811d-e1a5-44ee-ac72-54aaca7aa82d container test-container: <nil>
STEP: delete the pod
Dec 13 20:50:06.059: INFO: Waiting for pod pod-6afe811d-e1a5-44ee-ac72-54aaca7aa82d to disappear
Dec 13 20:50:06.071: INFO: Pod pod-6afe811d-e1a5-44ee-ac72-54aaca7aa82d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:50:06.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2174" for this suite.
Dec 13 20:50:12.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:50:12.148: INFO: namespace emptydir-2174 deletion completed in 6.073600585s

• [SLOW TEST:8.401 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:50:12.148: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-694
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-fn66
STEP: Creating a pod to test atomic-volume-subpath
Dec 13 20:50:12.292: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-fn66" in namespace "subpath-694" to be "success or failure"
Dec 13 20:50:12.294: INFO: Pod "pod-subpath-test-downwardapi-fn66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.860796ms
Dec 13 20:50:14.297: INFO: Pod "pod-subpath-test-downwardapi-fn66": Phase="Running", Reason="", readiness=true. Elapsed: 2.005122705s
Dec 13 20:50:16.299: INFO: Pod "pod-subpath-test-downwardapi-fn66": Phase="Running", Reason="", readiness=true. Elapsed: 4.007427514s
Dec 13 20:50:18.301: INFO: Pod "pod-subpath-test-downwardapi-fn66": Phase="Running", Reason="", readiness=true. Elapsed: 6.009524231s
Dec 13 20:50:20.303: INFO: Pod "pod-subpath-test-downwardapi-fn66": Phase="Running", Reason="", readiness=true. Elapsed: 8.011631425s
Dec 13 20:50:22.305: INFO: Pod "pod-subpath-test-downwardapi-fn66": Phase="Running", Reason="", readiness=true. Elapsed: 10.013841133s
Dec 13 20:50:24.308: INFO: Pod "pod-subpath-test-downwardapi-fn66": Phase="Running", Reason="", readiness=true. Elapsed: 12.01622591s
Dec 13 20:50:26.314: INFO: Pod "pod-subpath-test-downwardapi-fn66": Phase="Running", Reason="", readiness=true. Elapsed: 14.022293495s
Dec 13 20:50:28.317: INFO: Pod "pod-subpath-test-downwardapi-fn66": Phase="Running", Reason="", readiness=true. Elapsed: 16.024952869s
Dec 13 20:50:30.319: INFO: Pod "pod-subpath-test-downwardapi-fn66": Phase="Running", Reason="", readiness=true. Elapsed: 18.027090057s
Dec 13 20:50:32.321: INFO: Pod "pod-subpath-test-downwardapi-fn66": Phase="Running", Reason="", readiness=true. Elapsed: 20.02907427s
Dec 13 20:50:34.323: INFO: Pod "pod-subpath-test-downwardapi-fn66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.031094375s
STEP: Saw pod success
Dec 13 20:50:34.323: INFO: Pod "pod-subpath-test-downwardapi-fn66" satisfied condition "success or failure"
Dec 13 20:50:34.325: INFO: Trying to get logs from node master-0 pod pod-subpath-test-downwardapi-fn66 container test-container-subpath-downwardapi-fn66: <nil>
STEP: delete the pod
Dec 13 20:50:34.342: INFO: Waiting for pod pod-subpath-test-downwardapi-fn66 to disappear
Dec 13 20:50:34.345: INFO: Pod pod-subpath-test-downwardapi-fn66 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-fn66
Dec 13 20:50:34.345: INFO: Deleting pod "pod-subpath-test-downwardapi-fn66" in namespace "subpath-694"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:50:34.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-694" for this suite.
Dec 13 20:50:40.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:50:40.434: INFO: namespace subpath-694 deletion completed in 6.08373646s

• [SLOW TEST:28.286 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:50:40.434: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-509
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:51:40.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-509" for this suite.
Dec 13 20:52:08.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:52:08.671: INFO: namespace container-probe-509 deletion completed in 28.095082359s

• [SLOW TEST:88.238 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:52:08.672: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8767
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 13 20:52:09.120: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 13 20:52:12.136: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Dec 13 20:52:12.147: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:52:12.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8767" for this suite.
Dec 13 20:52:18.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:52:18.369: INFO: namespace webhook-8767 deletion completed in 6.2079722s
STEP: Destroying namespace "webhook-8767-markers" for this suite.
Dec 13 20:52:24.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:52:24.511: INFO: namespace webhook-8767-markers deletion completed in 6.141633741s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:15.877 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:52:24.548: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8709
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-eef1e45f-d47f-4053-b1fa-3da33932305e
STEP: Creating a pod to test consume configMaps
Dec 13 20:52:24.814: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7e261d18-9f14-483a-a97e-f8a744cb74e8" in namespace "projected-8709" to be "success or failure"
Dec 13 20:52:24.820: INFO: Pod "pod-projected-configmaps-7e261d18-9f14-483a-a97e-f8a744cb74e8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.533614ms
Dec 13 20:52:26.824: INFO: Pod "pod-projected-configmaps-7e261d18-9f14-483a-a97e-f8a744cb74e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010867056s
STEP: Saw pod success
Dec 13 20:52:26.824: INFO: Pod "pod-projected-configmaps-7e261d18-9f14-483a-a97e-f8a744cb74e8" satisfied condition "success or failure"
Dec 13 20:52:26.827: INFO: Trying to get logs from node master-0 pod pod-projected-configmaps-7e261d18-9f14-483a-a97e-f8a744cb74e8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 20:52:26.903: INFO: Waiting for pod pod-projected-configmaps-7e261d18-9f14-483a-a97e-f8a744cb74e8 to disappear
Dec 13 20:52:26.951: INFO: Pod pod-projected-configmaps-7e261d18-9f14-483a-a97e-f8a744cb74e8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:52:26.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8709" for this suite.
Dec 13 20:52:32.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:52:33.066: INFO: namespace projected-8709 deletion completed in 6.110053876s

• [SLOW TEST:8.517 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:52:33.066: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1725
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1213 20:53:13.219252      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 13 20:53:13.219: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:53:13.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1725" for this suite.
Dec 13 20:53:23.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:53:23.703: INFO: namespace gc-1725 deletion completed in 10.48165275s

• [SLOW TEST:50.636 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:53:23.704: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-645
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 13 20:53:24.098: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ad21066c-6b3b-4c0b-a979-515e877d1adc" in namespace "downward-api-645" to be "success or failure"
Dec 13 20:53:24.217: INFO: Pod "downwardapi-volume-ad21066c-6b3b-4c0b-a979-515e877d1adc": Phase="Pending", Reason="", readiness=false. Elapsed: 118.949979ms
Dec 13 20:53:26.224: INFO: Pod "downwardapi-volume-ad21066c-6b3b-4c0b-a979-515e877d1adc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.125225712s
STEP: Saw pod success
Dec 13 20:53:26.224: INFO: Pod "downwardapi-volume-ad21066c-6b3b-4c0b-a979-515e877d1adc" satisfied condition "success or failure"
Dec 13 20:53:26.226: INFO: Trying to get logs from node master-0 pod downwardapi-volume-ad21066c-6b3b-4c0b-a979-515e877d1adc container client-container: <nil>
STEP: delete the pod
Dec 13 20:53:26.301: INFO: Waiting for pod downwardapi-volume-ad21066c-6b3b-4c0b-a979-515e877d1adc to disappear
Dec 13 20:53:26.314: INFO: Pod downwardapi-volume-ad21066c-6b3b-4c0b-a979-515e877d1adc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:53:26.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-645" for this suite.
Dec 13 20:53:32.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:53:32.672: INFO: namespace downward-api-645 deletion completed in 6.350394358s

• [SLOW TEST:8.968 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:53:32.672: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5668
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 13 20:53:34.829: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:53:34.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5668" for this suite.
Dec 13 20:53:40.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:53:40.968: INFO: namespace container-runtime-5668 deletion completed in 6.097960126s

• [SLOW TEST:8.296 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:53:40.968: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9051
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:53:54.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9051" for this suite.
Dec 13 20:54:00.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:54:00.243: INFO: namespace resourcequota-9051 deletion completed in 6.093034753s

• [SLOW TEST:19.275 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:54:00.243: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2324
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:54:02.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2324" for this suite.
Dec 13 20:54:20.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:54:20.532: INFO: namespace containers-2324 deletion completed in 18.132006116s

• [SLOW TEST:20.289 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:54:20.533: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6063
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 13 20:54:21.224: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 13 20:54:24.239: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 20:54:24.241: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8645-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:54:25.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6063" for this suite.
Dec 13 20:54:31.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:54:31.454: INFO: namespace webhook-6063 deletion completed in 6.126980004s
STEP: Destroying namespace "webhook-6063-markers" for this suite.
Dec 13 20:54:37.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:54:37.614: INFO: namespace webhook-6063-markers deletion completed in 6.15950967s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.095 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:54:37.628: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9094
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 13 20:54:37.827: INFO: Number of nodes with available pods: 0
Dec 13 20:54:37.827: INFO: Node master-0 is running more than one daemon pod
Dec 13 20:54:38.842: INFO: Number of nodes with available pods: 0
Dec 13 20:54:38.842: INFO: Node master-0 is running more than one daemon pod
Dec 13 20:54:39.833: INFO: Number of nodes with available pods: 0
Dec 13 20:54:39.833: INFO: Node master-0 is running more than one daemon pod
Dec 13 20:54:40.833: INFO: Number of nodes with available pods: 3
Dec 13 20:54:40.833: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec 13 20:54:40.857: INFO: Number of nodes with available pods: 2
Dec 13 20:54:40.857: INFO: Node worker-1 is running more than one daemon pod
Dec 13 20:54:41.864: INFO: Number of nodes with available pods: 2
Dec 13 20:54:41.864: INFO: Node worker-1 is running more than one daemon pod
Dec 13 20:54:42.871: INFO: Number of nodes with available pods: 2
Dec 13 20:54:42.871: INFO: Node worker-1 is running more than one daemon pod
Dec 13 20:54:43.863: INFO: Number of nodes with available pods: 2
Dec 13 20:54:43.863: INFO: Node worker-1 is running more than one daemon pod
Dec 13 20:54:44.862: INFO: Number of nodes with available pods: 2
Dec 13 20:54:44.862: INFO: Node worker-1 is running more than one daemon pod
Dec 13 20:54:45.862: INFO: Number of nodes with available pods: 2
Dec 13 20:54:45.862: INFO: Node worker-1 is running more than one daemon pod
Dec 13 20:54:46.890: INFO: Number of nodes with available pods: 2
Dec 13 20:54:46.890: INFO: Node worker-1 is running more than one daemon pod
Dec 13 20:54:47.864: INFO: Number of nodes with available pods: 2
Dec 13 20:54:47.864: INFO: Node worker-1 is running more than one daemon pod
Dec 13 20:54:48.862: INFO: Number of nodes with available pods: 3
Dec 13 20:54:48.862: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9094, will wait for the garbage collector to delete the pods
Dec 13 20:54:48.922: INFO: Deleting DaemonSet.extensions daemon-set took: 5.770948ms
Dec 13 20:54:49.524: INFO: Terminating DaemonSet.extensions daemon-set pods took: 601.552233ms
Dec 13 20:54:57.026: INFO: Number of nodes with available pods: 0
Dec 13 20:54:57.026: INFO: Number of running nodes: 0, number of available pods: 0
Dec 13 20:54:57.028: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9094/daemonsets","resourceVersion":"40900"},"items":null}

Dec 13 20:54:57.030: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9094/pods","resourceVersion":"40900"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:54:57.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9094" for this suite.
Dec 13 20:55:03.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:55:03.135: INFO: namespace daemonsets-9094 deletion completed in 6.092961943s

• [SLOW TEST:25.507 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:55:03.136: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3374
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 20:55:03.276: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-24e374a8-c8f0-446e-93d6-f60bdfdc481d" in namespace "security-context-test-3374" to be "success or failure"
Dec 13 20:55:03.284: INFO: Pod "busybox-privileged-false-24e374a8-c8f0-446e-93d6-f60bdfdc481d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.273754ms
Dec 13 20:55:05.286: INFO: Pod "busybox-privileged-false-24e374a8-c8f0-446e-93d6-f60bdfdc481d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010932396s
Dec 13 20:55:05.287: INFO: Pod "busybox-privileged-false-24e374a8-c8f0-446e-93d6-f60bdfdc481d" satisfied condition "success or failure"
Dec 13 20:55:05.292: INFO: Got logs for pod "busybox-privileged-false-24e374a8-c8f0-446e-93d6-f60bdfdc481d": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:55:05.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3374" for this suite.
Dec 13 20:55:11.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:55:11.388: INFO: namespace security-context-test-3374 deletion completed in 6.093304879s

• [SLOW TEST:8.252 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:55:11.388: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6732
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-8005a4bc-e876-4851-ba42-425d6014205b
STEP: Creating a pod to test consume configMaps
Dec 13 20:55:11.543: INFO: Waiting up to 5m0s for pod "pod-configmaps-ed083ad8-f555-4303-96eb-6ec9e105dc69" in namespace "configmap-6732" to be "success or failure"
Dec 13 20:55:11.553: INFO: Pod "pod-configmaps-ed083ad8-f555-4303-96eb-6ec9e105dc69": Phase="Pending", Reason="", readiness=false. Elapsed: 9.992037ms
Dec 13 20:55:13.556: INFO: Pod "pod-configmaps-ed083ad8-f555-4303-96eb-6ec9e105dc69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01280146s
STEP: Saw pod success
Dec 13 20:55:13.556: INFO: Pod "pod-configmaps-ed083ad8-f555-4303-96eb-6ec9e105dc69" satisfied condition "success or failure"
Dec 13 20:55:13.558: INFO: Trying to get logs from node master-0 pod pod-configmaps-ed083ad8-f555-4303-96eb-6ec9e105dc69 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 20:55:13.572: INFO: Waiting for pod pod-configmaps-ed083ad8-f555-4303-96eb-6ec9e105dc69 to disappear
Dec 13 20:55:13.575: INFO: Pod pod-configmaps-ed083ad8-f555-4303-96eb-6ec9e105dc69 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:55:13.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6732" for this suite.
Dec 13 20:55:19.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:55:19.676: INFO: namespace configmap-6732 deletion completed in 6.098631259s

• [SLOW TEST:8.288 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:55:19.677: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-370
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 20:55:19.821: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec 13 20:55:20.843: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:55:21.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-370" for this suite.
Dec 13 20:55:27.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:55:28.197: INFO: namespace replication-controller-370 deletion completed in 6.341210749s

• [SLOW TEST:8.520 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:55:28.197: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-7465
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7465
I1213 20:55:28.659922      24 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7465, replica count: 1
I1213 20:55:29.717801      24 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1213 20:55:30.717967      24 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 13 20:55:30.827: INFO: Created: latency-svc-kbn2n
Dec 13 20:55:30.837: INFO: Got endpoints: latency-svc-kbn2n [19.077268ms]
Dec 13 20:55:30.864: INFO: Created: latency-svc-mtwm8
Dec 13 20:55:30.875: INFO: Got endpoints: latency-svc-mtwm8 [37.15388ms]
Dec 13 20:55:30.887: INFO: Created: latency-svc-8llc9
Dec 13 20:55:30.908: INFO: Created: latency-svc-dp9j2
Dec 13 20:55:30.908: INFO: Got endpoints: latency-svc-8llc9 [70.374503ms]
Dec 13 20:55:30.929: INFO: Got endpoints: latency-svc-dp9j2 [91.258821ms]
Dec 13 20:55:30.937: INFO: Created: latency-svc-ml85q
Dec 13 20:55:30.947: INFO: Got endpoints: latency-svc-ml85q [110.056581ms]
Dec 13 20:55:30.961: INFO: Created: latency-svc-z2p7w
Dec 13 20:55:30.968: INFO: Got endpoints: latency-svc-z2p7w [130.399679ms]
Dec 13 20:55:30.981: INFO: Created: latency-svc-ngvjj
Dec 13 20:55:30.986: INFO: Got endpoints: latency-svc-ngvjj [149.304252ms]
Dec 13 20:55:31.000: INFO: Created: latency-svc-ztrqx
Dec 13 20:55:31.008: INFO: Got endpoints: latency-svc-ztrqx [170.259906ms]
Dec 13 20:55:31.024: INFO: Created: latency-svc-bwr6b
Dec 13 20:55:31.065: INFO: Got endpoints: latency-svc-bwr6b [227.419652ms]
Dec 13 20:55:31.065: INFO: Created: latency-svc-6sll7
Dec 13 20:55:31.078: INFO: Got endpoints: latency-svc-6sll7 [240.64337ms]
Dec 13 20:55:31.133: INFO: Created: latency-svc-bhlsb
Dec 13 20:55:31.138: INFO: Got endpoints: latency-svc-bhlsb [300.994401ms]
Dec 13 20:55:31.172: INFO: Created: latency-svc-f8xgj
Dec 13 20:55:31.179: INFO: Got endpoints: latency-svc-f8xgj [341.471994ms]
Dec 13 20:55:31.195: INFO: Created: latency-svc-vzn9b
Dec 13 20:55:31.202: INFO: Got endpoints: latency-svc-vzn9b [364.933508ms]
Dec 13 20:55:31.217: INFO: Created: latency-svc-bxzjr
Dec 13 20:55:31.226: INFO: Got endpoints: latency-svc-bxzjr [388.510622ms]
Dec 13 20:55:31.238: INFO: Created: latency-svc-vxcbz
Dec 13 20:55:31.248: INFO: Got endpoints: latency-svc-vxcbz [411.038948ms]
Dec 13 20:55:31.270: INFO: Created: latency-svc-kcpq7
Dec 13 20:55:31.281: INFO: Got endpoints: latency-svc-kcpq7 [443.056775ms]
Dec 13 20:55:31.295: INFO: Created: latency-svc-srb2q
Dec 13 20:55:31.300: INFO: Got endpoints: latency-svc-srb2q [51.187641ms]
Dec 13 20:55:31.322: INFO: Created: latency-svc-ngxn4
Dec 13 20:55:31.337: INFO: Got endpoints: latency-svc-ngxn4 [462.632113ms]
Dec 13 20:55:31.348: INFO: Created: latency-svc-dwlx7
Dec 13 20:55:31.365: INFO: Got endpoints: latency-svc-dwlx7 [457.25793ms]
Dec 13 20:55:31.365: INFO: Created: latency-svc-76krj
Dec 13 20:55:31.382: INFO: Got endpoints: latency-svc-76krj [452.943894ms]
Dec 13 20:55:31.429: INFO: Created: latency-svc-kmvkt
Dec 13 20:55:31.435: INFO: Got endpoints: latency-svc-kmvkt [487.765743ms]
Dec 13 20:55:31.446: INFO: Created: latency-svc-pxwpf
Dec 13 20:55:31.461: INFO: Got endpoints: latency-svc-pxwpf [493.599378ms]
Dec 13 20:55:31.462: INFO: Created: latency-svc-pt2v7
Dec 13 20:55:31.469: INFO: Got endpoints: latency-svc-pt2v7 [482.873049ms]
Dec 13 20:55:31.480: INFO: Created: latency-svc-rmpj8
Dec 13 20:55:31.485: INFO: Got endpoints: latency-svc-rmpj8 [477.626053ms]
Dec 13 20:55:31.501: INFO: Created: latency-svc-2lxh5
Dec 13 20:55:31.510: INFO: Got endpoints: latency-svc-2lxh5 [445.030796ms]
Dec 13 20:55:31.520: INFO: Created: latency-svc-ggvd9
Dec 13 20:55:31.536: INFO: Got endpoints: latency-svc-ggvd9 [458.512785ms]
Dec 13 20:55:31.537: INFO: Created: latency-svc-zfzvb
Dec 13 20:55:31.542: INFO: Got endpoints: latency-svc-zfzvb [404.259646ms]
Dec 13 20:55:31.552: INFO: Created: latency-svc-zfhcd
Dec 13 20:55:31.566: INFO: Got endpoints: latency-svc-zfhcd [386.750683ms]
Dec 13 20:55:31.577: INFO: Created: latency-svc-kk2gk
Dec 13 20:55:31.583: INFO: Got endpoints: latency-svc-kk2gk [380.846279ms]
Dec 13 20:55:31.595: INFO: Created: latency-svc-r6grs
Dec 13 20:55:31.602: INFO: Got endpoints: latency-svc-r6grs [376.598198ms]
Dec 13 20:55:31.621: INFO: Created: latency-svc-g6bm5
Dec 13 20:55:31.631: INFO: Got endpoints: latency-svc-g6bm5 [350.249101ms]
Dec 13 20:55:31.646: INFO: Created: latency-svc-86f55
Dec 13 20:55:31.651: INFO: Got endpoints: latency-svc-86f55 [351.675965ms]
Dec 13 20:55:31.670: INFO: Created: latency-svc-jzrpp
Dec 13 20:55:31.697: INFO: Created: latency-svc-tk7jj
Dec 13 20:55:31.697: INFO: Got endpoints: latency-svc-jzrpp [359.513878ms]
Dec 13 20:55:31.713: INFO: Got endpoints: latency-svc-tk7jj [347.328773ms]
Dec 13 20:55:31.743: INFO: Created: latency-svc-zdzm7
Dec 13 20:55:31.766: INFO: Got endpoints: latency-svc-zdzm7 [384.420807ms]
Dec 13 20:55:31.769: INFO: Created: latency-svc-gkdf5
Dec 13 20:55:31.780: INFO: Got endpoints: latency-svc-gkdf5 [344.989366ms]
Dec 13 20:55:31.796: INFO: Created: latency-svc-4p4w2
Dec 13 20:55:31.802: INFO: Got endpoints: latency-svc-4p4w2 [340.368938ms]
Dec 13 20:55:31.819: INFO: Created: latency-svc-whbhf
Dec 13 20:55:31.826: INFO: Got endpoints: latency-svc-whbhf [356.120144ms]
Dec 13 20:55:31.833: INFO: Created: latency-svc-5hxcx
Dec 13 20:55:31.849: INFO: Got endpoints: latency-svc-5hxcx [363.691851ms]
Dec 13 20:55:31.849: INFO: Created: latency-svc-twfv6
Dec 13 20:55:31.862: INFO: Got endpoints: latency-svc-twfv6 [351.803114ms]
Dec 13 20:55:31.885: INFO: Created: latency-svc-bds7m
Dec 13 20:55:31.889: INFO: Got endpoints: latency-svc-bds7m [352.43119ms]
Dec 13 20:55:31.906: INFO: Created: latency-svc-kpzpq
Dec 13 20:55:31.911: INFO: Got endpoints: latency-svc-kpzpq [369.216386ms]
Dec 13 20:55:31.919: INFO: Created: latency-svc-tsqmm
Dec 13 20:55:31.925: INFO: Got endpoints: latency-svc-tsqmm [359.640862ms]
Dec 13 20:55:31.945: INFO: Created: latency-svc-q472l
Dec 13 20:55:31.958: INFO: Got endpoints: latency-svc-q472l [374.768243ms]
Dec 13 20:55:31.958: INFO: Created: latency-svc-66m6t
Dec 13 20:55:31.964: INFO: Got endpoints: latency-svc-66m6t [361.16599ms]
Dec 13 20:55:31.972: INFO: Created: latency-svc-jnjcv
Dec 13 20:55:31.979: INFO: Got endpoints: latency-svc-jnjcv [348.483278ms]
Dec 13 20:55:31.997: INFO: Created: latency-svc-sr2cm
Dec 13 20:55:32.001: INFO: Got endpoints: latency-svc-sr2cm [349.745547ms]
Dec 13 20:55:32.033: INFO: Created: latency-svc-hnj9m
Dec 13 20:55:32.051: INFO: Created: latency-svc-cw8zk
Dec 13 20:55:32.052: INFO: Got endpoints: latency-svc-hnj9m [355.427715ms]
Dec 13 20:55:32.060: INFO: Got endpoints: latency-svc-cw8zk [347.825603ms]
Dec 13 20:55:32.074: INFO: Created: latency-svc-zj85h
Dec 13 20:55:32.078: INFO: Got endpoints: latency-svc-zj85h [312.162307ms]
Dec 13 20:55:32.093: INFO: Created: latency-svc-nd488
Dec 13 20:55:32.115: INFO: Got endpoints: latency-svc-nd488 [334.395288ms]
Dec 13 20:55:32.117: INFO: Created: latency-svc-8f6zb
Dec 13 20:55:32.123: INFO: Got endpoints: latency-svc-8f6zb [320.885929ms]
Dec 13 20:55:32.139: INFO: Created: latency-svc-vdmsz
Dec 13 20:55:32.145: INFO: Got endpoints: latency-svc-vdmsz [319.487132ms]
Dec 13 20:55:32.163: INFO: Created: latency-svc-2gr27
Dec 13 20:55:32.168: INFO: Got endpoints: latency-svc-2gr27 [318.649382ms]
Dec 13 20:55:32.180: INFO: Created: latency-svc-f8ftz
Dec 13 20:55:32.187: INFO: Got endpoints: latency-svc-f8ftz [324.917422ms]
Dec 13 20:55:32.200: INFO: Created: latency-svc-28mtd
Dec 13 20:55:32.206: INFO: Got endpoints: latency-svc-28mtd [317.489559ms]
Dec 13 20:55:32.229: INFO: Created: latency-svc-gbshx
Dec 13 20:55:32.234: INFO: Got endpoints: latency-svc-gbshx [322.618003ms]
Dec 13 20:55:32.250: INFO: Created: latency-svc-kzk6c
Dec 13 20:55:32.260: INFO: Got endpoints: latency-svc-kzk6c [334.942234ms]
Dec 13 20:55:32.261: INFO: Created: latency-svc-5smst
Dec 13 20:55:32.279: INFO: Created: latency-svc-dxvsn
Dec 13 20:55:32.292: INFO: Created: latency-svc-84njx
Dec 13 20:55:32.292: INFO: Got endpoints: latency-svc-5smst [333.933483ms]
Dec 13 20:55:32.309: INFO: Created: latency-svc-chc6k
Dec 13 20:55:32.366: INFO: Got endpoints: latency-svc-dxvsn [402.162993ms]
Dec 13 20:55:32.366: INFO: Created: latency-svc-lq7ss
Dec 13 20:55:32.394: INFO: Created: latency-svc-stlln
Dec 13 20:55:32.405: INFO: Got endpoints: latency-svc-84njx [425.154484ms]
Dec 13 20:55:32.424: INFO: Created: latency-svc-8fmhw
Dec 13 20:55:32.460: INFO: Created: latency-svc-qgzwm
Dec 13 20:55:32.462: INFO: Got endpoints: latency-svc-chc6k [460.759251ms]
Dec 13 20:55:32.494: INFO: Got endpoints: latency-svc-lq7ss [441.572561ms]
Dec 13 20:55:32.504: INFO: Created: latency-svc-v5lrp
Dec 13 20:55:32.522: INFO: Created: latency-svc-gfspf
Dec 13 20:55:32.546: INFO: Got endpoints: latency-svc-stlln [485.812339ms]
Dec 13 20:55:32.547: INFO: Created: latency-svc-55hr4
Dec 13 20:55:32.583: INFO: Created: latency-svc-gcvwc
Dec 13 20:55:32.645: INFO: Got endpoints: latency-svc-qgzwm [530.52751ms]
Dec 13 20:55:32.647: INFO: Got endpoints: latency-svc-8fmhw [568.232579ms]
Dec 13 20:55:32.676: INFO: Created: latency-svc-n8wr9
Dec 13 20:55:32.709: INFO: Created: latency-svc-gz54r
Dec 13 20:55:32.709: INFO: Got endpoints: latency-svc-v5lrp [585.773755ms]
Dec 13 20:55:32.744: INFO: Got endpoints: latency-svc-gfspf [598.903621ms]
Dec 13 20:55:32.759: INFO: Created: latency-svc-vg27p
Dec 13 20:55:32.791: INFO: Got endpoints: latency-svc-55hr4 [622.810993ms]
Dec 13 20:55:32.791: INFO: Created: latency-svc-ncjqr
Dec 13 20:55:32.873: INFO: Got endpoints: latency-svc-gcvwc [686.010529ms]
Dec 13 20:55:32.873: INFO: Created: latency-svc-6gwnr
Dec 13 20:55:32.905: INFO: Got endpoints: latency-svc-n8wr9 [698.959418ms]
Dec 13 20:55:32.906: INFO: Created: latency-svc-9mhc4
Dec 13 20:55:32.945: INFO: Got endpoints: latency-svc-gz54r [710.866847ms]
Dec 13 20:55:32.945: INFO: Created: latency-svc-kw5qw
Dec 13 20:55:33.000: INFO: Got endpoints: latency-svc-vg27p [740.054605ms]
Dec 13 20:55:33.008: INFO: Created: latency-svc-5svq8
Dec 13 20:55:33.019: INFO: Created: latency-svc-6xzk2
Dec 13 20:55:33.045: INFO: Created: latency-svc-9lggl
Dec 13 20:55:33.047: INFO: Got endpoints: latency-svc-ncjqr [751.412834ms]
Dec 13 20:55:33.086: INFO: Created: latency-svc-s79nf
Dec 13 20:55:33.086: INFO: Got endpoints: latency-svc-6gwnr [720.435277ms]
Dec 13 20:55:33.126: INFO: Created: latency-svc-x9bk2
Dec 13 20:55:33.154: INFO: Got endpoints: latency-svc-9mhc4 [749.937212ms]
Dec 13 20:55:33.158: INFO: Created: latency-svc-6sw2s
Dec 13 20:55:33.199: INFO: Got endpoints: latency-svc-kw5qw [737.330188ms]
Dec 13 20:55:33.200: INFO: Created: latency-svc-zj5h9
Dec 13 20:55:33.257: INFO: Got endpoints: latency-svc-5svq8 [762.925692ms]
Dec 13 20:55:33.259: INFO: Created: latency-svc-w8tts
Dec 13 20:55:33.284: INFO: Created: latency-svc-t78p4
Dec 13 20:55:33.294: INFO: Got endpoints: latency-svc-6xzk2 [747.256411ms]
Dec 13 20:55:33.310: INFO: Created: latency-svc-w6694
Dec 13 20:55:33.376: INFO: Created: latency-svc-jlj4z
Dec 13 20:55:33.377: INFO: Got endpoints: latency-svc-9lggl [731.331049ms]
Dec 13 20:55:33.418: INFO: Got endpoints: latency-svc-s79nf [771.727605ms]
Dec 13 20:55:33.419: INFO: Created: latency-svc-q8mfr
Dec 13 20:55:33.455: INFO: Created: latency-svc-d9p2n
Dec 13 20:55:33.455: INFO: Got endpoints: latency-svc-x9bk2 [746.438515ms]
Dec 13 20:55:33.506: INFO: Created: latency-svc-25f2n
Dec 13 20:55:33.506: INFO: Got endpoints: latency-svc-6sw2s [761.614328ms]
Dec 13 20:55:33.530: INFO: Created: latency-svc-6p5ll
Dec 13 20:55:33.551: INFO: Got endpoints: latency-svc-zj5h9 [759.930664ms]
Dec 13 20:55:33.551: INFO: Created: latency-svc-hwzsm
Dec 13 20:55:33.571: INFO: Created: latency-svc-gtj4k
Dec 13 20:55:33.619: INFO: Got endpoints: latency-svc-w8tts [745.78636ms]
Dec 13 20:55:33.619: INFO: Created: latency-svc-mvddh
Dec 13 20:55:33.645: INFO: Got endpoints: latency-svc-t78p4 [739.923564ms]
Dec 13 20:55:33.646: INFO: Created: latency-svc-mjgv5
Dec 13 20:55:33.700: INFO: Got endpoints: latency-svc-w6694 [755.231013ms]
Dec 13 20:55:33.701: INFO: Created: latency-svc-9fsz6
Dec 13 20:55:33.774: INFO: Got endpoints: latency-svc-jlj4z [773.898657ms]
Dec 13 20:55:33.775: INFO: Created: latency-svc-w7mx4
Dec 13 20:55:33.802: INFO: Got endpoints: latency-svc-q8mfr [754.731638ms]
Dec 13 20:55:33.802: INFO: Created: latency-svc-7msg8
Dec 13 20:55:33.847: INFO: Created: latency-svc-t2vk9
Dec 13 20:55:33.847: INFO: Got endpoints: latency-svc-d9p2n [760.339452ms]
Dec 13 20:55:33.893: INFO: Created: latency-svc-7qbxw
Dec 13 20:55:33.893: INFO: Got endpoints: latency-svc-25f2n [738.671627ms]
Dec 13 20:55:33.913: INFO: Created: latency-svc-pzh66
Dec 13 20:55:33.952: INFO: Created: latency-svc-4q47b
Dec 13 20:55:33.952: INFO: Got endpoints: latency-svc-6p5ll [752.519597ms]
Dec 13 20:55:33.971: INFO: Created: latency-svc-mdmsb
Dec 13 20:55:33.987: INFO: Got endpoints: latency-svc-hwzsm [730.315042ms]
Dec 13 20:55:33.988: INFO: Created: latency-svc-2rns8
Dec 13 20:55:34.037: INFO: Got endpoints: latency-svc-gtj4k [742.968464ms]
Dec 13 20:55:34.037: INFO: Created: latency-svc-h72cz
Dec 13 20:55:34.063: INFO: Created: latency-svc-l25vh
Dec 13 20:55:34.082: INFO: Got endpoints: latency-svc-mvddh [705.708854ms]
Dec 13 20:55:34.104: INFO: Created: latency-svc-ngrkc
Dec 13 20:55:34.148: INFO: Got endpoints: latency-svc-mjgv5 [729.198275ms]
Dec 13 20:55:34.171: INFO: Created: latency-svc-jkn6c
Dec 13 20:55:34.191: INFO: Created: latency-svc-fr9vf
Dec 13 20:55:34.191: INFO: Got endpoints: latency-svc-9fsz6 [736.028351ms]
Dec 13 20:55:34.220: INFO: Created: latency-svc-dcgjf
Dec 13 20:55:34.253: INFO: Got endpoints: latency-svc-w7mx4 [747.179397ms]
Dec 13 20:55:34.253: INFO: Created: latency-svc-5fd6k
Dec 13 20:55:34.299: INFO: Got endpoints: latency-svc-7msg8 [748.436498ms]
Dec 13 20:55:34.299: INFO: Created: latency-svc-qzvmm
Dec 13 20:55:34.334: INFO: Created: latency-svc-4rv5r
Dec 13 20:55:34.334: INFO: Got endpoints: latency-svc-t2vk9 [715.433773ms]
Dec 13 20:55:34.366: INFO: Created: latency-svc-c7btq
Dec 13 20:55:34.406: INFO: Got endpoints: latency-svc-7qbxw [760.161108ms]
Dec 13 20:55:34.430: INFO: Created: latency-svc-qtdhs
Dec 13 20:55:34.441: INFO: Got endpoints: latency-svc-pzh66 [740.847385ms]
Dec 13 20:55:34.470: INFO: Created: latency-svc-kwzs7
Dec 13 20:55:34.484: INFO: Got endpoints: latency-svc-4q47b [709.371278ms]
Dec 13 20:55:34.547: INFO: Got endpoints: latency-svc-mdmsb [744.906074ms]
Dec 13 20:55:34.547: INFO: Created: latency-svc-272gz
Dec 13 20:55:34.583: INFO: Created: latency-svc-v9qrn
Dec 13 20:55:34.607: INFO: Got endpoints: latency-svc-2rns8 [760.09558ms]
Dec 13 20:55:34.641: INFO: Got endpoints: latency-svc-h72cz [747.899009ms]
Dec 13 20:55:34.641: INFO: Created: latency-svc-k6n62
Dec 13 20:55:34.690: INFO: Got endpoints: latency-svc-l25vh [737.900693ms]
Dec 13 20:55:34.692: INFO: Created: latency-svc-g2q65
Dec 13 20:55:34.717: INFO: Created: latency-svc-zrtt9
Dec 13 20:55:34.731: INFO: Got endpoints: latency-svc-ngrkc [743.66435ms]
Dec 13 20:55:34.785: INFO: Created: latency-svc-pxxfb
Dec 13 20:55:34.786: INFO: Got endpoints: latency-svc-jkn6c [749.007211ms]
Dec 13 20:55:34.818: INFO: Created: latency-svc-rc6qb
Dec 13 20:55:34.832: INFO: Got endpoints: latency-svc-fr9vf [749.284717ms]
Dec 13 20:55:35.007: INFO: Created: latency-svc-xltzx
Dec 13 20:55:35.007: INFO: Got endpoints: latency-svc-qzvmm [754.064652ms]
Dec 13 20:55:35.008: INFO: Got endpoints: latency-svc-dcgjf [860.914499ms]
Dec 13 20:55:35.009: INFO: Got endpoints: latency-svc-5fd6k [817.556592ms]
Dec 13 20:55:35.032: INFO: Got endpoints: latency-svc-4rv5r [733.142018ms]
Dec 13 20:55:35.044: INFO: Created: latency-svc-rjtfz
Dec 13 20:55:35.061: INFO: Created: latency-svc-2hvpt
Dec 13 20:55:35.076: INFO: Created: latency-svc-xgvhj
Dec 13 20:55:35.086: INFO: Got endpoints: latency-svc-c7btq [751.628778ms]
Dec 13 20:55:35.123: INFO: Created: latency-svc-mls7k
Dec 13 20:55:35.139: INFO: Got endpoints: latency-svc-qtdhs [733.448857ms]
Dec 13 20:55:35.139: INFO: Created: latency-svc-j5k42
Dec 13 20:55:35.168: INFO: Created: latency-svc-j5b9z
Dec 13 20:55:35.181: INFO: Got endpoints: latency-svc-kwzs7 [739.677945ms]
Dec 13 20:55:35.205: INFO: Created: latency-svc-95nd2
Dec 13 20:55:35.247: INFO: Got endpoints: latency-svc-272gz [762.822956ms]
Dec 13 20:55:35.260: INFO: Created: latency-svc-pnj8m
Dec 13 20:55:35.284: INFO: Got endpoints: latency-svc-v9qrn [737.328628ms]
Dec 13 20:55:35.302: INFO: Created: latency-svc-l7krk
Dec 13 20:55:35.331: INFO: Got endpoints: latency-svc-k6n62 [723.859891ms]
Dec 13 20:55:35.357: INFO: Created: latency-svc-8vzqz
Dec 13 20:55:35.381: INFO: Got endpoints: latency-svc-g2q65 [739.975377ms]
Dec 13 20:55:35.396: INFO: Created: latency-svc-ngfmh
Dec 13 20:55:35.430: INFO: Got endpoints: latency-svc-zrtt9 [740.589957ms]
Dec 13 20:55:35.445: INFO: Created: latency-svc-lcnrc
Dec 13 20:55:35.481: INFO: Got endpoints: latency-svc-pxxfb [749.78907ms]
Dec 13 20:55:35.495: INFO: Created: latency-svc-znw9l
Dec 13 20:55:35.531: INFO: Got endpoints: latency-svc-rc6qb [745.137727ms]
Dec 13 20:55:35.543: INFO: Created: latency-svc-78m2k
Dec 13 20:55:35.581: INFO: Got endpoints: latency-svc-xltzx [749.230364ms]
Dec 13 20:55:35.595: INFO: Created: latency-svc-pgqt6
Dec 13 20:55:35.630: INFO: Got endpoints: latency-svc-rjtfz [622.804793ms]
Dec 13 20:55:35.641: INFO: Created: latency-svc-57464
Dec 13 20:55:35.680: INFO: Got endpoints: latency-svc-2hvpt [671.713805ms]
Dec 13 20:55:35.693: INFO: Created: latency-svc-dhqv9
Dec 13 20:55:35.730: INFO: Got endpoints: latency-svc-xgvhj [721.504979ms]
Dec 13 20:55:35.742: INFO: Created: latency-svc-29p28
Dec 13 20:55:35.780: INFO: Got endpoints: latency-svc-mls7k [747.691083ms]
Dec 13 20:55:35.791: INFO: Created: latency-svc-2zv7r
Dec 13 20:55:35.830: INFO: Got endpoints: latency-svc-j5k42 [744.478013ms]
Dec 13 20:55:35.840: INFO: Created: latency-svc-cjdnd
Dec 13 20:55:35.885: INFO: Got endpoints: latency-svc-j5b9z [746.412042ms]
Dec 13 20:55:35.895: INFO: Created: latency-svc-pfzqx
Dec 13 20:55:35.930: INFO: Got endpoints: latency-svc-95nd2 [749.502318ms]
Dec 13 20:55:35.940: INFO: Created: latency-svc-thr4d
Dec 13 20:55:35.980: INFO: Got endpoints: latency-svc-pnj8m [733.076621ms]
Dec 13 20:55:35.993: INFO: Created: latency-svc-7tcwx
Dec 13 20:55:36.030: INFO: Got endpoints: latency-svc-l7krk [745.578771ms]
Dec 13 20:55:36.042: INFO: Created: latency-svc-rcvbc
Dec 13 20:55:36.080: INFO: Got endpoints: latency-svc-8vzqz [748.942787ms]
Dec 13 20:55:36.098: INFO: Created: latency-svc-tn8xq
Dec 13 20:55:36.130: INFO: Got endpoints: latency-svc-ngfmh [748.677644ms]
Dec 13 20:55:36.140: INFO: Created: latency-svc-vz5mp
Dec 13 20:55:36.181: INFO: Got endpoints: latency-svc-lcnrc [750.370974ms]
Dec 13 20:55:36.208: INFO: Created: latency-svc-5t78b
Dec 13 20:55:36.230: INFO: Got endpoints: latency-svc-znw9l [749.322192ms]
Dec 13 20:55:36.239: INFO: Created: latency-svc-rkfm4
Dec 13 20:55:36.280: INFO: Got endpoints: latency-svc-78m2k [748.981346ms]
Dec 13 20:55:36.291: INFO: Created: latency-svc-h85cp
Dec 13 20:55:36.330: INFO: Got endpoints: latency-svc-pgqt6 [749.525162ms]
Dec 13 20:55:36.343: INFO: Created: latency-svc-ctf47
Dec 13 20:55:36.381: INFO: Got endpoints: latency-svc-57464 [750.91644ms]
Dec 13 20:55:36.393: INFO: Created: latency-svc-dtc5g
Dec 13 20:55:36.430: INFO: Got endpoints: latency-svc-dhqv9 [749.965769ms]
Dec 13 20:55:36.441: INFO: Created: latency-svc-h4qsh
Dec 13 20:55:36.480: INFO: Got endpoints: latency-svc-29p28 [749.73187ms]
Dec 13 20:55:36.491: INFO: Created: latency-svc-xzrgt
Dec 13 20:55:36.531: INFO: Got endpoints: latency-svc-2zv7r [750.490341ms]
Dec 13 20:55:36.542: INFO: Created: latency-svc-96ldv
Dec 13 20:55:36.580: INFO: Got endpoints: latency-svc-cjdnd [749.922017ms]
Dec 13 20:55:36.591: INFO: Created: latency-svc-64hrr
Dec 13 20:55:36.634: INFO: Got endpoints: latency-svc-pfzqx [748.784982ms]
Dec 13 20:55:36.647: INFO: Created: latency-svc-mctf6
Dec 13 20:55:36.681: INFO: Got endpoints: latency-svc-thr4d [750.845753ms]
Dec 13 20:55:36.693: INFO: Created: latency-svc-glpzn
Dec 13 20:55:36.733: INFO: Got endpoints: latency-svc-7tcwx [752.948474ms]
Dec 13 20:55:36.752: INFO: Created: latency-svc-hk7cb
Dec 13 20:55:36.781: INFO: Got endpoints: latency-svc-rcvbc [751.280783ms]
Dec 13 20:55:36.795: INFO: Created: latency-svc-xdhxz
Dec 13 20:55:36.832: INFO: Got endpoints: latency-svc-tn8xq [751.922541ms]
Dec 13 20:55:36.865: INFO: Created: latency-svc-7l65c
Dec 13 20:55:36.881: INFO: Got endpoints: latency-svc-vz5mp [751.350547ms]
Dec 13 20:55:36.897: INFO: Created: latency-svc-xjdqw
Dec 13 20:55:36.930: INFO: Got endpoints: latency-svc-5t78b [749.48722ms]
Dec 13 20:55:36.950: INFO: Created: latency-svc-ghxh4
Dec 13 20:55:36.984: INFO: Got endpoints: latency-svc-rkfm4 [754.411809ms]
Dec 13 20:55:37.001: INFO: Created: latency-svc-dc9zs
Dec 13 20:55:37.038: INFO: Got endpoints: latency-svc-h85cp [758.576483ms]
Dec 13 20:55:37.144: INFO: Created: latency-svc-jlm95
Dec 13 20:55:37.144: INFO: Got endpoints: latency-svc-dtc5g [762.797258ms]
Dec 13 20:55:37.144: INFO: Got endpoints: latency-svc-ctf47 [813.362206ms]
Dec 13 20:55:37.192: INFO: Got endpoints: latency-svc-h4qsh [761.776192ms]
Dec 13 20:55:37.199: INFO: Created: latency-svc-wsfpc
Dec 13 20:55:37.243: INFO: Got endpoints: latency-svc-xzrgt [762.908031ms]
Dec 13 20:55:37.243: INFO: Created: latency-svc-m9vxh
Dec 13 20:55:37.327: INFO: Got endpoints: latency-svc-96ldv [796.846726ms]
Dec 13 20:55:37.328: INFO: Created: latency-svc-plvz5
Dec 13 20:55:37.409: INFO: Created: latency-svc-ckx5s
Dec 13 20:55:37.409: INFO: Got endpoints: latency-svc-mctf6 [775.081224ms]
Dec 13 20:55:37.410: INFO: Got endpoints: latency-svc-64hrr [829.371095ms]
Dec 13 20:55:37.501: INFO: Got endpoints: latency-svc-glpzn [820.114065ms]
Dec 13 20:55:37.514: INFO: Got endpoints: latency-svc-hk7cb [780.912418ms]
Dec 13 20:55:37.525: INFO: Created: latency-svc-jfqsw
Dec 13 20:55:37.540: INFO: Got endpoints: latency-svc-xdhxz [758.475899ms]
Dec 13 20:55:37.590: INFO: Created: latency-svc-pcdzv
Dec 13 20:55:37.693: INFO: Got endpoints: latency-svc-ghxh4 [762.419533ms]
Dec 13 20:55:37.693: INFO: Got endpoints: latency-svc-7l65c [861.440786ms]
Dec 13 20:55:37.693: INFO: Got endpoints: latency-svc-xjdqw [811.995466ms]
Dec 13 20:55:37.697: INFO: Created: latency-svc-xk7b6
Dec 13 20:55:37.724: INFO: Created: latency-svc-hdb4b
Dec 13 20:55:37.750: INFO: Got endpoints: latency-svc-dc9zs [765.71635ms]
Dec 13 20:55:37.750: INFO: Created: latency-svc-s7rcq
Dec 13 20:55:37.818: INFO: Created: latency-svc-vjrlf
Dec 13 20:55:37.818: INFO: Got endpoints: latency-svc-jlm95 [779.726925ms]
Dec 13 20:55:37.877: INFO: Got endpoints: latency-svc-wsfpc [733.865775ms]
Dec 13 20:55:37.878: INFO: Created: latency-svc-q8mqf
Dec 13 20:55:37.967: INFO: Got endpoints: latency-svc-m9vxh [823.152436ms]
Dec 13 20:55:37.972: INFO: Got endpoints: latency-svc-plvz5 [780.23636ms]
Dec 13 20:55:37.991: INFO: Got endpoints: latency-svc-ckx5s [748.082937ms]
Dec 13 20:55:37.991: INFO: Created: latency-svc-rrr98
Dec 13 20:55:38.042: INFO: Got endpoints: latency-svc-jfqsw [714.7153ms]
Dec 13 20:55:38.042: INFO: Created: latency-svc-kktxk
Dec 13 20:55:38.061: INFO: Created: latency-svc-z5w97
Dec 13 20:55:38.092: INFO: Got endpoints: latency-svc-pcdzv [683.0425ms]
Dec 13 20:55:38.094: INFO: Created: latency-svc-dlnht
Dec 13 20:55:38.120: INFO: Created: latency-svc-cnfkq
Dec 13 20:55:38.145: INFO: Got endpoints: latency-svc-xk7b6 [734.999401ms]
Dec 13 20:55:38.145: INFO: Created: latency-svc-8mqck
Dec 13 20:55:38.159: INFO: Created: latency-svc-xqq5d
Dec 13 20:55:38.181: INFO: Created: latency-svc-c7lps
Dec 13 20:55:38.188: INFO: Got endpoints: latency-svc-hdb4b [686.16985ms]
Dec 13 20:55:38.226: INFO: Created: latency-svc-dchj6
Dec 13 20:55:38.226: INFO: Created: latency-svc-kdmxr
Dec 13 20:55:38.241: INFO: Got endpoints: latency-svc-s7rcq [727.183412ms]
Dec 13 20:55:38.255: INFO: Created: latency-svc-lsckd
Dec 13 20:55:38.271: INFO: Created: latency-svc-zqzfx
Dec 13 20:55:38.289: INFO: Got endpoints: latency-svc-vjrlf [749.749844ms]
Dec 13 20:55:38.290: INFO: Created: latency-svc-9q7qh
Dec 13 20:55:38.323: INFO: Created: latency-svc-7dq9f
Dec 13 20:55:38.334: INFO: Got endpoints: latency-svc-q8mqf [641.100748ms]
Dec 13 20:55:38.367: INFO: Created: latency-svc-m979x
Dec 13 20:55:38.383: INFO: Got endpoints: latency-svc-rrr98 [689.592311ms]
Dec 13 20:55:38.413: INFO: Created: latency-svc-2k9zf
Dec 13 20:55:38.437: INFO: Got endpoints: latency-svc-kktxk [744.108394ms]
Dec 13 20:55:38.457: INFO: Created: latency-svc-8hhvq
Dec 13 20:55:38.482: INFO: Got endpoints: latency-svc-z5w97 [731.773798ms]
Dec 13 20:55:38.503: INFO: Created: latency-svc-wsxpn
Dec 13 20:55:38.534: INFO: Got endpoints: latency-svc-dlnht [715.451ms]
Dec 13 20:55:38.562: INFO: Created: latency-svc-2t92g
Dec 13 20:55:38.587: INFO: Got endpoints: latency-svc-cnfkq [709.270345ms]
Dec 13 20:55:38.606: INFO: Created: latency-svc-chrqr
Dec 13 20:55:38.633: INFO: Got endpoints: latency-svc-8mqck [665.58283ms]
Dec 13 20:55:38.664: INFO: Created: latency-svc-cxhxc
Dec 13 20:55:38.682: INFO: Got endpoints: latency-svc-xqq5d [709.460459ms]
Dec 13 20:55:38.734: INFO: Got endpoints: latency-svc-c7lps [742.500735ms]
Dec 13 20:55:38.783: INFO: Got endpoints: latency-svc-kdmxr [740.445019ms]
Dec 13 20:55:38.833: INFO: Got endpoints: latency-svc-dchj6 [740.89003ms]
Dec 13 20:55:38.887: INFO: Got endpoints: latency-svc-lsckd [742.580474ms]
Dec 13 20:55:38.933: INFO: Got endpoints: latency-svc-zqzfx [745.775198ms]
Dec 13 20:55:38.981: INFO: Got endpoints: latency-svc-9q7qh [740.507866ms]
Dec 13 20:55:39.032: INFO: Got endpoints: latency-svc-7dq9f [742.687894ms]
Dec 13 20:55:39.087: INFO: Got endpoints: latency-svc-m979x [753.4471ms]
Dec 13 20:55:39.131: INFO: Got endpoints: latency-svc-2k9zf [748.705697ms]
Dec 13 20:55:39.186: INFO: Got endpoints: latency-svc-8hhvq [748.170067ms]
Dec 13 20:55:39.234: INFO: Got endpoints: latency-svc-wsxpn [751.765623ms]
Dec 13 20:55:39.283: INFO: Got endpoints: latency-svc-2t92g [749.177543ms]
Dec 13 20:55:39.338: INFO: Got endpoints: latency-svc-chrqr [751.148112ms]
Dec 13 20:55:39.387: INFO: Got endpoints: latency-svc-cxhxc [754.621771ms]
Dec 13 20:55:39.387: INFO: Latencies: [37.15388ms 51.187641ms 70.374503ms 91.258821ms 110.056581ms 130.399679ms 149.304252ms 170.259906ms 227.419652ms 240.64337ms 300.994401ms 312.162307ms 317.489559ms 318.649382ms 319.487132ms 320.885929ms 322.618003ms 324.917422ms 333.933483ms 334.395288ms 334.942234ms 340.368938ms 341.471994ms 344.989366ms 347.328773ms 347.825603ms 348.483278ms 349.745547ms 350.249101ms 351.675965ms 351.803114ms 352.43119ms 355.427715ms 356.120144ms 359.513878ms 359.640862ms 361.16599ms 363.691851ms 364.933508ms 369.216386ms 374.768243ms 376.598198ms 380.846279ms 384.420807ms 386.750683ms 388.510622ms 402.162993ms 404.259646ms 411.038948ms 425.154484ms 441.572561ms 443.056775ms 445.030796ms 452.943894ms 457.25793ms 458.512785ms 460.759251ms 462.632113ms 477.626053ms 482.873049ms 485.812339ms 487.765743ms 493.599378ms 530.52751ms 568.232579ms 585.773755ms 598.903621ms 622.804793ms 622.810993ms 641.100748ms 665.58283ms 671.713805ms 683.0425ms 686.010529ms 686.16985ms 689.592311ms 698.959418ms 705.708854ms 709.270345ms 709.371278ms 709.460459ms 710.866847ms 714.7153ms 715.433773ms 715.451ms 720.435277ms 721.504979ms 723.859891ms 727.183412ms 729.198275ms 730.315042ms 731.331049ms 731.773798ms 733.076621ms 733.142018ms 733.448857ms 733.865775ms 734.999401ms 736.028351ms 737.328628ms 737.330188ms 737.900693ms 738.671627ms 739.677945ms 739.923564ms 739.975377ms 740.054605ms 740.445019ms 740.507866ms 740.589957ms 740.847385ms 740.89003ms 742.500735ms 742.580474ms 742.687894ms 742.968464ms 743.66435ms 744.108394ms 744.478013ms 744.906074ms 745.137727ms 745.578771ms 745.775198ms 745.78636ms 746.412042ms 746.438515ms 747.179397ms 747.256411ms 747.691083ms 747.899009ms 748.082937ms 748.170067ms 748.436498ms 748.677644ms 748.705697ms 748.784982ms 748.942787ms 748.981346ms 749.007211ms 749.177543ms 749.230364ms 749.284717ms 749.322192ms 749.48722ms 749.502318ms 749.525162ms 749.73187ms 749.749844ms 749.78907ms 749.922017ms 749.937212ms 749.965769ms 750.370974ms 750.490341ms 750.845753ms 750.91644ms 751.148112ms 751.280783ms 751.350547ms 751.412834ms 751.628778ms 751.765623ms 751.922541ms 752.519597ms 752.948474ms 753.4471ms 754.064652ms 754.411809ms 754.621771ms 754.731638ms 755.231013ms 758.475899ms 758.576483ms 759.930664ms 760.09558ms 760.161108ms 760.339452ms 761.614328ms 761.776192ms 762.419533ms 762.797258ms 762.822956ms 762.908031ms 762.925692ms 765.71635ms 771.727605ms 773.898657ms 775.081224ms 779.726925ms 780.23636ms 780.912418ms 796.846726ms 811.995466ms 813.362206ms 817.556592ms 820.114065ms 823.152436ms 829.371095ms 860.914499ms 861.440786ms]
Dec 13 20:55:39.387: INFO: 50 %ile: 737.330188ms
Dec 13 20:55:39.387: INFO: 90 %ile: 762.797258ms
Dec 13 20:55:39.387: INFO: 99 %ile: 860.914499ms
Dec 13 20:55:39.387: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:55:39.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7465" for this suite.
Dec 13 20:55:55.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:55:55.499: INFO: namespace svc-latency-7465 deletion completed in 16.09887808s

• [SLOW TEST:27.302 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:55:55.500: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4602
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-4602/configmap-test-943e39c5-9c35-4ad1-a252-4051f08089f4
STEP: Creating a pod to test consume configMaps
Dec 13 20:55:55.634: INFO: Waiting up to 5m0s for pod "pod-configmaps-a41cbbe6-4dce-4348-a6d8-87d50060bf78" in namespace "configmap-4602" to be "success or failure"
Dec 13 20:55:55.639: INFO: Pod "pod-configmaps-a41cbbe6-4dce-4348-a6d8-87d50060bf78": Phase="Pending", Reason="", readiness=false. Elapsed: 5.314692ms
Dec 13 20:55:57.641: INFO: Pod "pod-configmaps-a41cbbe6-4dce-4348-a6d8-87d50060bf78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0074625s
STEP: Saw pod success
Dec 13 20:55:57.641: INFO: Pod "pod-configmaps-a41cbbe6-4dce-4348-a6d8-87d50060bf78" satisfied condition "success or failure"
Dec 13 20:55:57.643: INFO: Trying to get logs from node master-0 pod pod-configmaps-a41cbbe6-4dce-4348-a6d8-87d50060bf78 container env-test: <nil>
STEP: delete the pod
Dec 13 20:55:57.656: INFO: Waiting for pod pod-configmaps-a41cbbe6-4dce-4348-a6d8-87d50060bf78 to disappear
Dec 13 20:55:57.661: INFO: Pod pod-configmaps-a41cbbe6-4dce-4348-a6d8-87d50060bf78 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:55:57.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4602" for this suite.
Dec 13 20:56:03.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:56:03.827: INFO: namespace configmap-4602 deletion completed in 6.163612939s

• [SLOW TEST:8.328 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:56:03.827: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5767
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-f18cc8e2-3a8b-4b3b-a3d3-10c7476b0db5
STEP: Creating a pod to test consume secrets
Dec 13 20:56:04.018: INFO: Waiting up to 5m0s for pod "pod-secrets-7c87ade2-f898-4df9-9f16-c47eac3216e2" in namespace "secrets-5767" to be "success or failure"
Dec 13 20:56:04.025: INFO: Pod "pod-secrets-7c87ade2-f898-4df9-9f16-c47eac3216e2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.507329ms
Dec 13 20:56:06.027: INFO: Pod "pod-secrets-7c87ade2-f898-4df9-9f16-c47eac3216e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008669616s
STEP: Saw pod success
Dec 13 20:56:06.027: INFO: Pod "pod-secrets-7c87ade2-f898-4df9-9f16-c47eac3216e2" satisfied condition "success or failure"
Dec 13 20:56:06.028: INFO: Trying to get logs from node master-0 pod pod-secrets-7c87ade2-f898-4df9-9f16-c47eac3216e2 container secret-volume-test: <nil>
STEP: delete the pod
Dec 13 20:56:06.046: INFO: Waiting for pod pod-secrets-7c87ade2-f898-4df9-9f16-c47eac3216e2 to disappear
Dec 13 20:56:06.052: INFO: Pod pod-secrets-7c87ade2-f898-4df9-9f16-c47eac3216e2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:56:06.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5767" for this suite.
Dec 13 20:56:12.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:56:12.126: INFO: namespace secrets-5767 deletion completed in 6.071487193s

• [SLOW TEST:8.299 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:56:12.126: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-6133
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Dec 13 20:56:12.252: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Dec 13 20:56:12.562: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec 13 20:56:14.642: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711867372, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711867372, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711867372, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711867372, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 20:56:16.645: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711867372, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711867372, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711867372, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711867372, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 20:56:18.646: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711867372, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711867372, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711867372, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711867372, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 20:56:20.645: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711867372, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711867372, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711867372, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711867372, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 20:56:23.271: INFO: Waited 617.82761ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:56:24.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-6133" for this suite.
Dec 13 20:56:30.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:56:30.296: INFO: namespace aggregator-6133 deletion completed in 6.221667028s

• [SLOW TEST:18.170 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:56:30.296: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5154
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-f66c7e46-c709-4137-bd38-9e4f2f5a25c2
STEP: Creating a pod to test consume configMaps
Dec 13 20:56:30.486: INFO: Waiting up to 5m0s for pod "pod-configmaps-49d897fb-7cb0-44d7-81f4-80243a0d988f" in namespace "configmap-5154" to be "success or failure"
Dec 13 20:56:30.494: INFO: Pod "pod-configmaps-49d897fb-7cb0-44d7-81f4-80243a0d988f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.786313ms
Dec 13 20:56:32.496: INFO: Pod "pod-configmaps-49d897fb-7cb0-44d7-81f4-80243a0d988f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009941174s
STEP: Saw pod success
Dec 13 20:56:32.496: INFO: Pod "pod-configmaps-49d897fb-7cb0-44d7-81f4-80243a0d988f" satisfied condition "success or failure"
Dec 13 20:56:32.498: INFO: Trying to get logs from node master-0 pod pod-configmaps-49d897fb-7cb0-44d7-81f4-80243a0d988f container configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 20:56:32.514: INFO: Waiting for pod pod-configmaps-49d897fb-7cb0-44d7-81f4-80243a0d988f to disappear
Dec 13 20:56:32.519: INFO: Pod pod-configmaps-49d897fb-7cb0-44d7-81f4-80243a0d988f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:56:32.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5154" for this suite.
Dec 13 20:56:38.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:56:38.672: INFO: namespace configmap-5154 deletion completed in 6.149956583s

• [SLOW TEST:8.376 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:56:38.672: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5661
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 13 20:56:38.904: INFO: Waiting up to 5m0s for pod "pod-286d0367-c699-42a2-8185-c7271ffd1f45" in namespace "emptydir-5661" to be "success or failure"
Dec 13 20:56:38.922: INFO: Pod "pod-286d0367-c699-42a2-8185-c7271ffd1f45": Phase="Pending", Reason="", readiness=false. Elapsed: 18.041072ms
Dec 13 20:56:40.924: INFO: Pod "pod-286d0367-c699-42a2-8185-c7271ffd1f45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020031071s
STEP: Saw pod success
Dec 13 20:56:40.924: INFO: Pod "pod-286d0367-c699-42a2-8185-c7271ffd1f45" satisfied condition "success or failure"
Dec 13 20:56:40.925: INFO: Trying to get logs from node master-0 pod pod-286d0367-c699-42a2-8185-c7271ffd1f45 container test-container: <nil>
STEP: delete the pod
Dec 13 20:56:40.941: INFO: Waiting for pod pod-286d0367-c699-42a2-8185-c7271ffd1f45 to disappear
Dec 13 20:56:40.943: INFO: Pod pod-286d0367-c699-42a2-8185-c7271ffd1f45 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:56:40.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5661" for this suite.
Dec 13 20:56:46.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:56:47.033: INFO: namespace emptydir-5661 deletion completed in 6.087375987s

• [SLOW TEST:8.361 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:56:47.033: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1370
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 13 20:56:47.689: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 13 20:56:49.702: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711867407, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711867407, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711867407, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711867407, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 13 20:56:52.715: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:56:52.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1370" for this suite.
Dec 13 20:56:58.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:56:59.013: INFO: namespace webhook-1370 deletion completed in 6.082295375s
STEP: Destroying namespace "webhook-1370-markers" for this suite.
Dec 13 20:57:05.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:57:05.099: INFO: namespace webhook-1370-markers deletion completed in 6.086018102s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.074 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:57:05.108: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7854
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-7854
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 13 20:57:05.252: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 13 20:57:23.352: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.91:8080/dial?request=hostName&protocol=udp&host=10.244.2.44&port=8081&tries=1'] Namespace:pod-network-test-7854 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 20:57:23.352: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 20:57:23.424: INFO: Waiting for endpoints: map[]
Dec 13 20:57:23.427: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.91:8080/dial?request=hostName&protocol=udp&host=10.244.1.46&port=8081&tries=1'] Namespace:pod-network-test-7854 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 20:57:23.427: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 20:57:23.494: INFO: Waiting for endpoints: map[]
Dec 13 20:57:23.496: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.91:8080/dial?request=hostName&protocol=udp&host=10.244.0.90&port=8081&tries=1'] Namespace:pod-network-test-7854 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 20:57:23.496: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 20:57:23.558: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:57:23.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7854" for this suite.
Dec 13 20:57:35.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:57:35.711: INFO: namespace pod-network-test-7854 deletion completed in 12.150613522s

• [SLOW TEST:30.604 seconds]
[sig-network] Networking
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:57:35.712: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7988
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 20:57:35.869: INFO: Creating deployment "webserver-deployment"
Dec 13 20:57:35.872: INFO: Waiting for observed generation 1
Dec 13 20:57:37.883: INFO: Waiting for all required pods to come up
Dec 13 20:57:37.887: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec 13 20:57:43.899: INFO: Waiting for deployment "webserver-deployment" to complete
Dec 13 20:57:43.903: INFO: Updating deployment "webserver-deployment" with a non-existent image
Dec 13 20:57:43.907: INFO: Updating deployment webserver-deployment
Dec 13 20:57:43.907: INFO: Waiting for observed generation 2
Dec 13 20:57:45.911: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec 13 20:57:45.913: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec 13 20:57:45.915: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 13 20:57:45.923: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec 13 20:57:45.923: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec 13 20:57:45.925: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 13 20:57:45.928: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Dec 13 20:57:45.928: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Dec 13 20:57:45.932: INFO: Updating deployment webserver-deployment
Dec 13 20:57:45.932: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Dec 13 20:57:45.946: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec 13 20:57:45.967: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 13 20:57:46.101: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-7988 /apis/apps/v1/namespaces/deployment-7988/deployments/webserver-deployment 6fc93520-7fa0-4d94-ace8-afc77b0ef323 43513 3 2019-12-13 20:57:35 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00577eec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2019-12-13 20:57:44 +0000 UTC,LastTransitionTime:2019-12-13 20:57:35 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-12-13 20:57:45 +0000 UTC,LastTransitionTime:2019-12-13 20:57:45 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Dec 13 20:57:46.189: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-7988 /apis/apps/v1/namespaces/deployment-7988/replicasets/webserver-deployment-c7997dcc8 93094779-ecd1-4355-a64e-655fbd117ef6 43539 3 2019-12-13 20:57:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 6fc93520-7fa0-4d94-ace8-afc77b0ef323 0xc0024e0247 0xc0024e0248}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0024e02b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 13 20:57:46.189: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Dec 13 20:57:46.189: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-7988 /apis/apps/v1/namespaces/deployment-7988/replicasets/webserver-deployment-595b5b9587 01db697b-c16f-4001-a497-d919fb8f0eee 43535 3 2019-12-13 20:57:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 6fc93520-7fa0-4d94-ace8-afc77b0ef323 0xc0024e0187 0xc0024e0188}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0024e01e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Dec 13 20:57:46.197: INFO: Pod "webserver-deployment-595b5b9587-2gbd2" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2gbd2 webserver-deployment-595b5b9587- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-595b5b9587-2gbd2 e36059af-1f6f-4116-9a27-5b97a90abe11 43541 0 2019-12-13 20:57:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 01db697b-c16f-4001-a497-d919fb8f0eee 0xc00577f317 0xc00577f318}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.4,PodIP:,StartTime:2019-12-13 20:57:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.197: INFO: Pod "webserver-deployment-595b5b9587-58785" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-58785 webserver-deployment-595b5b9587- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-595b5b9587-58785 525985a5-aadb-4878-8dfd-71b0afe037a8 43358 0 2019-12-13 20:57:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.244.0.95/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 01db697b-c16f-4001-a497-d919fb8f0eee 0xc00577f487 0xc00577f488}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.2,PodIP:10.244.0.95,StartTime:2019-12-13 20:57:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-13 20:57:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://5571e68d842e06551f9e91b95ab3b7d8894bb5f933f50cd673a790e0d6513e96,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.95,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.197: INFO: Pod "webserver-deployment-595b5b9587-766z8" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-766z8 webserver-deployment-595b5b9587- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-595b5b9587-766z8 e0384ee3-4261-4126-b211-8120089da369 43361 0 2019-12-13 20:57:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.244.0.97/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 01db697b-c16f-4001-a497-d919fb8f0eee 0xc00577f610 0xc00577f611}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.2,PodIP:10.244.0.97,StartTime:2019-12-13 20:57:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-13 20:57:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://597fa05c360e0c4e4ff5ba0e9f89de2c9d305e5152b9a780e757628ccac90fcd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.198: INFO: Pod "webserver-deployment-595b5b9587-87xwd" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-87xwd webserver-deployment-595b5b9587- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-595b5b9587-87xwd b652ccf8-9cfc-415c-a099-531ad662aeae 43516 0 2019-12-13 20:57:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 01db697b-c16f-4001-a497-d919fb8f0eee 0xc00577f790 0xc00577f791}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.198: INFO: Pod "webserver-deployment-595b5b9587-9kt8x" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-9kt8x webserver-deployment-595b5b9587- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-595b5b9587-9kt8x e4302d6c-63a4-4839-9231-f9ae1f576562 43518 0 2019-12-13 20:57:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 01db697b-c16f-4001-a497-d919fb8f0eee 0xc00577f8a0 0xc00577f8a1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.198: INFO: Pod "webserver-deployment-595b5b9587-bjllh" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bjllh webserver-deployment-595b5b9587- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-595b5b9587-bjllh 49cb76e3-78a0-4f9e-be03-62ec1a7c9c25 43351 0 2019-12-13 20:57:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.244.0.94/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 01db697b-c16f-4001-a497-d919fb8f0eee 0xc00577f9c0 0xc00577f9c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.2,PodIP:10.244.0.94,StartTime:2019-12-13 20:57:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-13 20:57:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://320d77e956b4b83ddd480f86a5d687772b08ed94579ff739dd7731fc8e78060f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.94,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.198: INFO: Pod "webserver-deployment-595b5b9587-cj849" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-cj849 webserver-deployment-595b5b9587- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-595b5b9587-cj849 65e72c2b-a838-459f-a6b2-eba625a63281 43532 0 2019-12-13 20:57:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 01db697b-c16f-4001-a497-d919fb8f0eee 0xc00577fb30 0xc00577fb31}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.2,PodIP:,StartTime:2019-12-13 20:57:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.198: INFO: Pod "webserver-deployment-595b5b9587-dhg8l" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-dhg8l webserver-deployment-595b5b9587- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-595b5b9587-dhg8l cfa1d8e9-9950-44ec-9cb5-ae2bc89bf182 43514 0 2019-12-13 20:57:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 01db697b-c16f-4001-a497-d919fb8f0eee 0xc00577fc87 0xc00577fc88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.198: INFO: Pod "webserver-deployment-595b5b9587-jscc5" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jscc5 webserver-deployment-595b5b9587- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-595b5b9587-jscc5 3f02e8e6-d236-4bb1-8c61-ad71d5fa01b4 43379 0 2019-12-13 20:57:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.244.0.98/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 01db697b-c16f-4001-a497-d919fb8f0eee 0xc00577fdb0 0xc00577fdb1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.2,PodIP:10.244.0.98,StartTime:2019-12-13 20:57:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-13 20:57:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://861a519fffc2fb02687e5f05630c9953905b912d33c40d760cb7bd632d865826,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.98,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.199: INFO: Pod "webserver-deployment-595b5b9587-lk68t" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lk68t webserver-deployment-595b5b9587- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-595b5b9587-lk68t a4a312cb-3899-495f-9497-2de5d0b841bc 43390 0 2019-12-13 20:57:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.244.0.100/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 01db697b-c16f-4001-a497-d919fb8f0eee 0xc00577ff30 0xc00577ff31}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.2,PodIP:10.244.0.100,StartTime:2019-12-13 20:57:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-13 20:57:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://0f0f10d3155eb0f816ba62cb8c5d5fadbfd0ae865bd5cbd6535d88b6c0976df4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.199: INFO: Pod "webserver-deployment-595b5b9587-ls6cq" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-ls6cq webserver-deployment-595b5b9587- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-595b5b9587-ls6cq 06bf93a5-7e16-4bbc-9632-ceea556986b2 43524 0 2019-12-13 20:57:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 01db697b-c16f-4001-a497-d919fb8f0eee 0xc0033720a7 0xc0033720a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.199: INFO: Pod "webserver-deployment-595b5b9587-m5dvm" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-m5dvm webserver-deployment-595b5b9587- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-595b5b9587-m5dvm 340c8295-341b-438c-82be-c99a7975856b 43493 0 2019-12-13 20:57:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 01db697b-c16f-4001-a497-d919fb8f0eee 0xc0033721c0 0xc0033721c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.199: INFO: Pod "webserver-deployment-595b5b9587-mc6kw" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mc6kw webserver-deployment-595b5b9587- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-595b5b9587-mc6kw 41037bfc-7867-471f-acc8-f94cda4de15b 43530 0 2019-12-13 20:57:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 01db697b-c16f-4001-a497-d919fb8f0eee 0xc0033722d0 0xc0033722d1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.199: INFO: Pod "webserver-deployment-595b5b9587-nlg5h" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-nlg5h webserver-deployment-595b5b9587- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-595b5b9587-nlg5h b7d863eb-01a9-43c3-ac54-03e5ff7c719d 43523 0 2019-12-13 20:57:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 01db697b-c16f-4001-a497-d919fb8f0eee 0xc0033723e0 0xc0033723e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.199: INFO: Pod "webserver-deployment-595b5b9587-pl7kb" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pl7kb webserver-deployment-595b5b9587- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-595b5b9587-pl7kb ed651be4-376a-4a63-b597-1aa0b7209b27 43526 0 2019-12-13 20:57:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 01db697b-c16f-4001-a497-d919fb8f0eee 0xc0033724f0 0xc0033724f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.199: INFO: Pod "webserver-deployment-595b5b9587-pvvmm" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pvvmm webserver-deployment-595b5b9587- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-595b5b9587-pvvmm 399b7532-f69f-4a1a-b026-80dea51dd46f 43364 0 2019-12-13 20:57:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.244.0.92/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 01db697b-c16f-4001-a497-d919fb8f0eee 0xc003372610 0xc003372611}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.2,PodIP:10.244.0.92,StartTime:2019-12-13 20:57:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-13 20:57:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://424040e35ed05a165e92098a564a7ad4f43be33eabffa1c7fe3d36935e90adb9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.92,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.199: INFO: Pod "webserver-deployment-595b5b9587-vnc8x" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vnc8x webserver-deployment-595b5b9587- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-595b5b9587-vnc8x a4e82e69-50b5-47ec-aa59-ba8d6fd62a56 43531 0 2019-12-13 20:57:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 01db697b-c16f-4001-a497-d919fb8f0eee 0xc003372780 0xc003372781}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.200: INFO: Pod "webserver-deployment-595b5b9587-wtsbz" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-wtsbz webserver-deployment-595b5b9587- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-595b5b9587-wtsbz 457beb8d-2c27-469e-a7b2-6a29f3c06494 43507 0 2019-12-13 20:57:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 01db697b-c16f-4001-a497-d919fb8f0eee 0xc003372890 0xc003372891}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.200: INFO: Pod "webserver-deployment-595b5b9587-xncd6" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xncd6 webserver-deployment-595b5b9587- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-595b5b9587-xncd6 27c6b160-c7c0-483f-9309-f803fa0b6f45 43354 0 2019-12-13 20:57:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.244.0.93/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 01db697b-c16f-4001-a497-d919fb8f0eee 0xc0033729b0 0xc0033729b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.2,PodIP:10.244.0.93,StartTime:2019-12-13 20:57:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-13 20:57:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://021599a8b9441a421843094613aeac91155e9f29b33f486018c9a9ba406c03ec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.202: INFO: Pod "webserver-deployment-595b5b9587-xqmz8" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xqmz8 webserver-deployment-595b5b9587- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-595b5b9587-xqmz8 bdc29376-3e52-42ab-b989-570a4444071f 43375 0 2019-12-13 20:57:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.244.0.99/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 01db697b-c16f-4001-a497-d919fb8f0eee 0xc003372b30 0xc003372b31}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.2,PodIP:10.244.0.99,StartTime:2019-12-13 20:57:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-13 20:57:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://0668cf65bc1a80080bde4609f7d962f4b8737aca2fc1656ab9c00e89d00438df,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.99,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.202: INFO: Pod "webserver-deployment-c7997dcc8-2j8cn" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-2j8cn webserver-deployment-c7997dcc8- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-c7997dcc8-2j8cn 9fcb2483-50c8-431c-8268-5ca200f47cd2 43473 0 2019-12-13 20:57:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.244.0.103/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 93094779-ecd1-4355-a64e-655fbd117ef6 0xc003372cb0 0xc003372cb1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.2,PodIP:,StartTime:2019-12-13 20:57:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.202: INFO: Pod "webserver-deployment-c7997dcc8-824w9" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-824w9 webserver-deployment-c7997dcc8- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-c7997dcc8-824w9 d7c1c48a-5f2d-4496-b82f-af8c94b4e32b 43476 0 2019-12-13 20:57:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.244.0.105/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 93094779-ecd1-4355-a64e-655fbd117ef6 0xc003372eb7 0xc003372eb8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.2,PodIP:,StartTime:2019-12-13 20:57:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.202: INFO: Pod "webserver-deployment-c7997dcc8-8hxfw" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-8hxfw webserver-deployment-c7997dcc8- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-c7997dcc8-8hxfw 1b734345-7f79-44f4-b0b0-df10f92c4e13 43529 0 2019-12-13 20:57:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 93094779-ecd1-4355-a64e-655fbd117ef6 0xc003373237 0xc003373238}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.202: INFO: Pod "webserver-deployment-c7997dcc8-b4xgc" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-b4xgc webserver-deployment-c7997dcc8- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-c7997dcc8-b4xgc c9ffdf28-8c6b-41d5-adfa-7910edc77fd5 43472 0 2019-12-13 20:57:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.244.0.102/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 93094779-ecd1-4355-a64e-655fbd117ef6 0xc0033735b0 0xc0033735b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.2,PodIP:,StartTime:2019-12-13 20:57:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.202: INFO: Pod "webserver-deployment-c7997dcc8-cpp5d" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-cpp5d webserver-deployment-c7997dcc8- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-c7997dcc8-cpp5d 66fc0868-99f7-4289-a2eb-01be9d4c380f 43528 0 2019-12-13 20:57:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 93094779-ecd1-4355-a64e-655fbd117ef6 0xc0033738c7 0xc0033738c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.203: INFO: Pod "webserver-deployment-c7997dcc8-dvhzl" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-dvhzl webserver-deployment-c7997dcc8- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-c7997dcc8-dvhzl ad845777-657a-4f89-9f2a-5ba0d2825446 43537 0 2019-12-13 20:57:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 93094779-ecd1-4355-a64e-655fbd117ef6 0xc003373b40 0xc003373b41}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.203: INFO: Pod "webserver-deployment-c7997dcc8-gjf6m" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-gjf6m webserver-deployment-c7997dcc8- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-c7997dcc8-gjf6m 70ab7c83-e02d-4dbc-bb94-2fa5227ba6d3 43468 0 2019-12-13 20:57:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.244.2.45/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 93094779-ecd1-4355-a64e-655fbd117ef6 0xc003373e20 0xc003373e21}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.4,PodIP:,StartTime:2019-12-13 20:57:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.203: INFO: Pod "webserver-deployment-c7997dcc8-gxg4p" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-gxg4p webserver-deployment-c7997dcc8- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-c7997dcc8-gxg4p a401ac3b-24b5-4a42-b35d-ccd8edb6c818 43475 0 2019-12-13 20:57:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.244.0.104/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 93094779-ecd1-4355-a64e-655fbd117ef6 0xc003b2e0c7 0xc003b2e0c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.2,PodIP:,StartTime:2019-12-13 20:57:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.203: INFO: Pod "webserver-deployment-c7997dcc8-mlwxq" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-mlwxq webserver-deployment-c7997dcc8- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-c7997dcc8-mlwxq 9522ba5b-29c3-4c49-a7c8-d79739771a29 43500 0 2019-12-13 20:57:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 93094779-ecd1-4355-a64e-655fbd117ef6 0xc003b2e247 0xc003b2e248}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.203: INFO: Pod "webserver-deployment-c7997dcc8-qlkwh" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qlkwh webserver-deployment-c7997dcc8- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-c7997dcc8-qlkwh 35625fda-44af-4672-bd23-e64a84652cac 43542 0 2019-12-13 20:57:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 93094779-ecd1-4355-a64e-655fbd117ef6 0xc003b2e370 0xc003b2e371}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.3,PodIP:,StartTime:2019-12-13 20:57:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.203: INFO: Pod "webserver-deployment-c7997dcc8-qspsq" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qspsq webserver-deployment-c7997dcc8- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-c7997dcc8-qspsq a93838cf-cc7e-4944-a0a3-3dadcd410229 43515 0 2019-12-13 20:57:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 93094779-ecd1-4355-a64e-655fbd117ef6 0xc003b2e4e7 0xc003b2e4e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.204: INFO: Pod "webserver-deployment-c7997dcc8-tnl99" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-tnl99 webserver-deployment-c7997dcc8- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-c7997dcc8-tnl99 1740b0ca-01ed-46bb-a9bf-6a0e55a52262 43525 0 2019-12-13 20:57:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 93094779-ecd1-4355-a64e-655fbd117ef6 0xc003b2e610 0xc003b2e611}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 13 20:57:46.204: INFO: Pod "webserver-deployment-c7997dcc8-znqpp" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-znqpp webserver-deployment-c7997dcc8- deployment-7988 /api/v1/namespaces/deployment-7988/pods/webserver-deployment-c7997dcc8-znqpp a8cfb8b0-88f5-40ad-99f6-42e77c9e9d0b 43527 0 2019-12-13 20:57:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 93094779-ecd1-4355-a64e-655fbd117ef6 0xc003b2e730 0xc003b2e731}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsz2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsz2h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsz2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 20:57:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:57:46.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7988" for this suite.
Dec 13 20:57:58.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:57:59.619: INFO: namespace deployment-7988 deletion completed in 13.38011193s

• [SLOW TEST:23.907 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:57:59.619: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5830
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 13 20:57:59.973: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 13 20:58:00.019: INFO: Waiting for terminating namespaces to be deleted...
Dec 13 20:58:00.033: INFO: 
Logging pods the kubelet thinks is on node master-0 before test
Dec 13 20:58:00.078: INFO: kube-scheduler-master-0 from kube-system started at 2019-12-13 18:54:06 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.078: INFO: 	Container kube-scheduler ready: true, restart count 0
Dec 13 20:58:00.078: INFO: kube-proxy-dfvlw from kube-system started at 2019-12-13 18:54:29 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.078: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 13 20:58:00.078: INFO: fluentd-es-v2.5.2-mfcr4 from kube-system started at 2019-12-13 20:15:27 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.078: INFO: 	Container fluentd-es ready: true, restart count 0
Dec 13 20:58:00.078: INFO: rook-ceph-mon-b-5b988bf97b-4xkn2 from rook-ceph started at 2019-12-13 20:15:27 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.078: INFO: 	Container mon ready: true, restart count 0
Dec 13 20:58:00.078: INFO: kube-controller-manager-master-0 from kube-system started at 2019-12-13 18:54:06 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.078: INFO: 	Container kube-controller-manager ready: true, restart count 0
Dec 13 20:58:00.078: INFO: canal-xbp9c from kube-system started at 2019-12-13 18:55:18 +0000 UTC (2 container statuses recorded)
Dec 13 20:58:00.078: INFO: 	Container calico-node ready: true, restart count 0
Dec 13 20:58:00.078: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 13 20:58:00.078: INFO: kata-deploy-m4sjm from kube-system started at 2019-12-13 20:15:27 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.078: INFO: 	Container kube-kata ready: true, restart count 0
Dec 13 20:58:00.078: INFO: rook-ceph-osd-2-58969bddc7-6zxpl from rook-ceph started at 2019-12-13 20:15:27 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.078: INFO: 	Container osd ready: true, restart count 0
Dec 13 20:58:00.078: INFO: etcd-master-0 from kube-system started at 2019-12-13 18:54:06 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.078: INFO: 	Container etcd ready: true, restart count 0
Dec 13 20:58:00.078: INFO: node-exporter-tk6xn from monitoring started at 2019-12-13 19:05:14 +0000 UTC (2 container statuses recorded)
Dec 13 20:58:00.078: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 13 20:58:00.078: INFO: 	Container node-exporter ready: true, restart count 0
Dec 13 20:58:00.078: INFO: csi-rbdplugin-8rpjc from rook-ceph started at 2019-12-13 20:15:27 +0000 UTC (3 container statuses recorded)
Dec 13 20:58:00.078: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 20:58:00.078: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:58:00.078: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:58:00.078: INFO: csi-cephfsplugin-vvg2s from rook-ceph started at 2019-12-13 20:15:27 +0000 UTC (3 container statuses recorded)
Dec 13 20:58:00.078: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 20:58:00.078: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:58:00.078: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:58:00.078: INFO: kube-apiserver-master-0 from kube-system started at 2019-12-13 18:54:06 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.078: INFO: 	Container kube-apiserver ready: true, restart count 0
Dec 13 20:58:00.078: INFO: sonobuoy-e2e-job-8f6bded5e69344cd from sonobuoy started at 2019-12-13 19:12:21 +0000 UTC (2 container statuses recorded)
Dec 13 20:58:00.078: INFO: 	Container e2e ready: true, restart count 0
Dec 13 20:58:00.078: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 13 20:58:00.078: INFO: sonobuoy-systemd-logs-daemon-set-9616c2f784dc4b45-rxl5l from sonobuoy started at 2019-12-13 19:12:22 +0000 UTC (2 container statuses recorded)
Dec 13 20:58:00.078: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 13 20:58:00.078: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 13 20:58:00.078: INFO: rook-discover-p77wp from rook-ceph started at 2019-12-13 20:15:31 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.078: INFO: 	Container rook-discover ready: true, restart count 0
Dec 13 20:58:00.078: INFO: 
Logging pods the kubelet thinks is on node worker-0 before test
Dec 13 20:58:00.211: INFO: rook-ceph-osd-prepare-worker-0-xz82f from rook-ceph started at 2019-12-13 19:03:42 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.211: INFO: 	Container provision ready: false, restart count 0
Dec 13 20:58:00.211: INFO: kubernetes-dashboard-6865b885fc-7tv2c from kubernetes-dashboard started at 2019-12-13 19:05:45 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.211: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec 13 20:58:00.211: INFO: fluentd-es-v2.5.2-w2nk6 from kube-system started at 2019-12-13 19:07:51 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.211: INFO: 	Container fluentd-es ready: true, restart count 0
Dec 13 20:58:00.211: INFO: sonobuoy-systemd-logs-daemon-set-9616c2f784dc4b45-qpldg from sonobuoy started at 2019-12-13 19:12:22 +0000 UTC (2 container statuses recorded)
Dec 13 20:58:00.211: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 13 20:58:00.211: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 13 20:58:00.211: INFO: coredns-5644d7b6d9-tdq6s from kube-system started at 2019-12-13 18:55:47 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.211: INFO: 	Container coredns ready: true, restart count 0
Dec 13 20:58:00.211: INFO: rook-discover-5t2vg from rook-ceph started at 2019-12-13 18:59:33 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.211: INFO: 	Container rook-discover ready: true, restart count 0
Dec 13 20:58:00.211: INFO: csi-rbdplugin-9vnm2 from rook-ceph started at 2019-12-13 18:59:33 +0000 UTC (3 container statuses recorded)
Dec 13 20:58:00.211: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 20:58:00.211: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:58:00.211: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:58:00.211: INFO: rook-ceph-operator-c8ff6447d-6s7cz from rook-ceph started at 2019-12-13 18:55:45 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.211: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Dec 13 20:58:00.211: INFO: csi-cephfsplugin-n7682 from rook-ceph started at 2019-12-13 18:59:35 +0000 UTC (3 container statuses recorded)
Dec 13 20:58:00.211: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 20:58:00.211: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:58:00.211: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:58:00.211: INFO: rook-ceph-mon-c-597fd9c9c7-r8ngb from rook-ceph started at 2019-12-13 19:02:59 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.211: INFO: 	Container mon ready: true, restart count 0
Dec 13 20:58:00.211: INFO: dashboard-metrics-scraper-6ccf7f6cc8-k4zf5 from kubernetes-dashboard started at 2019-12-13 20:14:31 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.211: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Dec 13 20:58:00.211: INFO: canal-sgqjs from kube-system started at 2019-12-13 18:55:18 +0000 UTC (2 container statuses recorded)
Dec 13 20:58:00.211: INFO: 	Container calico-node ready: true, restart count 0
Dec 13 20:58:00.211: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 13 20:58:00.211: INFO: kata-deploy-2qbn2 from kube-system started at 2019-12-13 18:55:45 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.211: INFO: 	Container kube-kata ready: true, restart count 0
Dec 13 20:58:00.211: INFO: node-exporter-584q9 from monitoring started at 2019-12-13 19:05:14 +0000 UTC (2 container statuses recorded)
Dec 13 20:58:00.211: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 13 20:58:00.211: INFO: 	Container node-exporter ready: true, restart count 0
Dec 13 20:58:00.211: INFO: grafana-5cd56df4cd-wqpht from monitoring started at 2019-12-13 19:41:23 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.211: INFO: 	Container grafana ready: true, restart count 0
Dec 13 20:58:00.211: INFO: coredns-5644d7b6d9-rngbs from kube-system started at 2019-12-13 18:55:47 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.211: INFO: 	Container coredns ready: true, restart count 0
Dec 13 20:58:00.211: INFO: alertmanager-main-2 from monitoring started at 2019-12-13 19:06:20 +0000 UTC (2 container statuses recorded)
Dec 13 20:58:00.211: INFO: 	Container alertmanager ready: true, restart count 0
Dec 13 20:58:00.211: INFO: 	Container config-reloader ready: true, restart count 0
Dec 13 20:58:00.211: INFO: prometheus-k8s-1 from monitoring started at 2019-12-13 19:06:30 +0000 UTC (3 container statuses recorded)
Dec 13 20:58:00.211: INFO: 	Container prometheus ready: true, restart count 1
Dec 13 20:58:00.211: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec 13 20:58:00.211: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec 13 20:58:00.211: INFO: rook-ceph-osd-0-64bcd55b9f-hrsvl from rook-ceph started at 2019-12-13 19:03:50 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.211: INFO: 	Container osd ready: true, restart count 0
Dec 13 20:58:00.211: INFO: alertmanager-main-1 from monitoring started at 2019-12-13 20:14:55 +0000 UTC (2 container statuses recorded)
Dec 13 20:58:00.211: INFO: 	Container alertmanager ready: true, restart count 0
Dec 13 20:58:00.211: INFO: 	Container config-reloader ready: true, restart count 0
Dec 13 20:58:00.211: INFO: elasticsearch-logging-0 from kube-system started at 2019-12-13 20:14:56 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.211: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Dec 13 20:58:00.211: INFO: metrics-server-7578984995-fr9dj from kube-system started at 2019-12-13 18:55:45 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.211: INFO: 	Container metrics-server ready: true, restart count 0
Dec 13 20:58:00.211: INFO: csi-cephfsplugin-provisioner-75c965db4f-dhmjp from rook-ceph started at 2019-12-13 18:59:36 +0000 UTC (4 container statuses recorded)
Dec 13 20:58:00.212: INFO: 	Container csi-attacher ready: true, restart count 0
Dec 13 20:58:00.212: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 20:58:00.212: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 13 20:58:00.212: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:58:00.212: INFO: csi-rbdplugin-provisioner-56cbc4d585-lhtqr from rook-ceph started at 2019-12-13 20:14:30 +0000 UTC (5 container statuses recorded)
Dec 13 20:58:00.212: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 13 20:58:00.212: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 20:58:00.212: INFO: 	Container csi-rbdplugin-attacher ready: true, restart count 0
Dec 13 20:58:00.212: INFO: 	Container csi-snapshotter ready: true, restart count 0
Dec 13 20:58:00.212: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:58:00.212: INFO: kube-proxy-rktzd from kube-system started at 2019-12-13 18:54:55 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.212: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 13 20:58:00.212: INFO: 
Logging pods the kubelet thinks is on node worker-1 before test
Dec 13 20:58:00.359: INFO: nginx-ingress-controller-7fb85bc8bb-8f8dr from ingress-nginx started at 2019-12-13 19:08:29 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.359: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec 13 20:58:00.359: INFO: csi-cephfsplugin-6szlf from rook-ceph started at 2019-12-13 18:59:35 +0000 UTC (3 container statuses recorded)
Dec 13 20:58:00.359: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 20:58:00.359: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:58:00.359: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:58:00.359: INFO: fluentd-es-v2.5.2-s2ng4 from kube-system started at 2019-12-13 19:07:51 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.359: INFO: 	Container fluentd-es ready: true, restart count 0
Dec 13 20:58:00.359: INFO: alertmanager-main-0 from monitoring started at 2019-12-13 19:06:20 +0000 UTC (2 container statuses recorded)
Dec 13 20:58:00.359: INFO: 	Container alertmanager ready: true, restart count 0
Dec 13 20:58:00.359: INFO: 	Container config-reloader ready: true, restart count 0
Dec 13 20:58:00.359: INFO: node-exporter-7ggvt from monitoring started at 2019-12-13 19:05:14 +0000 UTC (2 container statuses recorded)
Dec 13 20:58:00.359: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 13 20:58:00.359: INFO: 	Container node-exporter ready: true, restart count 0
Dec 13 20:58:00.359: INFO: rook-ceph-osd-prepare-worker-1-2xccr from rook-ceph started at 2019-12-13 19:03:42 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.359: INFO: 	Container provision ready: false, restart count 0
Dec 13 20:58:00.359: INFO: elasticsearch-logging-1 from kube-system started at 2019-12-13 19:10:24 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.359: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Dec 13 20:58:00.359: INFO: sonobuoy-systemd-logs-daemon-set-9616c2f784dc4b45-f29sm from sonobuoy started at 2019-12-13 19:12:22 +0000 UTC (2 container statuses recorded)
Dec 13 20:58:00.359: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 13 20:58:00.359: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 13 20:58:00.359: INFO: prometheus-operator-7559d67ff-shr6m from monitoring started at 2019-12-13 20:14:30 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.359: INFO: 	Container prometheus-operator ready: true, restart count 0
Dec 13 20:58:00.359: INFO: rook-ceph-mon-a-c7664c8c4-jcx8w from rook-ceph started at 2019-12-13 19:02:27 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.359: INFO: 	Container mon ready: true, restart count 0
Dec 13 20:58:00.359: INFO: rook-ceph-osd-1-7d8886d496-jzxm4 from rook-ceph started at 2019-12-13 19:03:51 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.359: INFO: 	Container osd ready: true, restart count 0
Dec 13 20:58:00.359: INFO: rook-ceph-mgr-a-5775778c4d-k745v from rook-ceph started at 2019-12-13 20:14:30 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.359: INFO: 	Container mgr ready: true, restart count 0
Dec 13 20:58:00.359: INFO: kibana-logging-7f6b4b96b4-pdq55 from kube-system started at 2019-12-13 20:14:30 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.359: INFO: 	Container kibana-logging ready: true, restart count 0
Dec 13 20:58:00.359: INFO: csi-cephfsplugin-provisioner-75c965db4f-6plm2 from rook-ceph started at 2019-12-13 20:14:31 +0000 UTC (4 container statuses recorded)
Dec 13 20:58:00.360: INFO: 	Container csi-attacher ready: true, restart count 0
Dec 13 20:58:00.360: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 20:58:00.360: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 13 20:58:00.360: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:58:00.360: INFO: canal-5wqk9 from kube-system started at 2019-12-13 18:55:18 +0000 UTC (2 container statuses recorded)
Dec 13 20:58:00.360: INFO: 	Container calico-node ready: true, restart count 0
Dec 13 20:58:00.360: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 13 20:58:00.360: INFO: kube-state-metrics-79d4b9b497-wntkj from monitoring started at 2019-12-13 20:14:30 +0000 UTC (3 container statuses recorded)
Dec 13 20:58:00.360: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec 13 20:58:00.360: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec 13 20:58:00.360: INFO: 	Container kube-state-metrics ready: true, restart count 0
Dec 13 20:58:00.360: INFO: sonobuoy from sonobuoy started at 2019-12-13 19:12:05 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.360: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 13 20:58:00.360: INFO: csi-rbdplugin-rhs8g from rook-ceph started at 2019-12-13 18:59:33 +0000 UTC (3 container statuses recorded)
Dec 13 20:58:00.360: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 20:58:00.360: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 20:58:00.360: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:58:00.360: INFO: csi-rbdplugin-provisioner-56cbc4d585-j64wf from rook-ceph started at 2019-12-13 18:59:34 +0000 UTC (5 container statuses recorded)
Dec 13 20:58:00.360: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 13 20:58:00.360: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 20:58:00.360: INFO: 	Container csi-rbdplugin-attacher ready: true, restart count 0
Dec 13 20:58:00.360: INFO: 	Container csi-snapshotter ready: true, restart count 0
Dec 13 20:58:00.360: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 20:58:00.360: INFO: prometheus-adapter-c676d8764-7pnjf from monitoring started at 2019-12-13 19:41:23 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.360: INFO: 	Container prometheus-adapter ready: true, restart count 0
Dec 13 20:58:00.360: INFO: prometheus-k8s-0 from monitoring started at 2019-12-13 20:14:55 +0000 UTC (3 container statuses recorded)
Dec 13 20:58:00.360: INFO: 	Container prometheus ready: true, restart count 1
Dec 13 20:58:00.360: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec 13 20:58:00.360: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec 13 20:58:00.360: INFO: kube-proxy-dt9pl from kube-system started at 2019-12-13 18:55:17 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.360: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 13 20:58:00.360: INFO: rook-discover-m4tgh from rook-ceph started at 2019-12-13 18:59:33 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.360: INFO: 	Container rook-discover ready: true, restart count 0
Dec 13 20:58:00.360: INFO: kata-deploy-qtlwp from kube-system started at 2019-12-13 18:55:57 +0000 UTC (1 container statuses recorded)
Dec 13 20:58:00.360: INFO: 	Container kube-kata ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15e009d51c5b5e5b], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:58:01.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5830" for this suite.
Dec 13 20:58:09.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:58:09.651: INFO: namespace sched-pred-5830 deletion completed in 8.12057126s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:10.032 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 20:58:09.651: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2705
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 20:58:09.806: INFO: Create a RollingUpdate DaemonSet
Dec 13 20:58:09.808: INFO: Check that daemon pods launch on every node of the cluster
Dec 13 20:58:09.814: INFO: Number of nodes with available pods: 0
Dec 13 20:58:09.814: INFO: Node master-0 is running more than one daemon pod
Dec 13 20:58:10.821: INFO: Number of nodes with available pods: 0
Dec 13 20:58:10.821: INFO: Node master-0 is running more than one daemon pod
Dec 13 20:58:11.819: INFO: Number of nodes with available pods: 1
Dec 13 20:58:11.819: INFO: Node master-0 is running more than one daemon pod
Dec 13 20:58:12.828: INFO: Number of nodes with available pods: 3
Dec 13 20:58:12.828: INFO: Number of running nodes: 3, number of available pods: 3
Dec 13 20:58:12.828: INFO: Update the DaemonSet to trigger a rollout
Dec 13 20:58:12.832: INFO: Updating DaemonSet daemon-set
Dec 13 20:58:16.844: INFO: Roll back the DaemonSet before rollout is complete
Dec 13 20:58:16.849: INFO: Updating DaemonSet daemon-set
Dec 13 20:58:16.849: INFO: Make sure DaemonSet rollback is complete
Dec 13 20:58:16.852: INFO: Wrong image for pod: daemon-set-b46nl. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 13 20:58:16.853: INFO: Pod daemon-set-b46nl is not available
Dec 13 20:58:17.859: INFO: Wrong image for pod: daemon-set-b46nl. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 13 20:58:17.859: INFO: Pod daemon-set-b46nl is not available
Dec 13 20:58:18.867: INFO: Pod daemon-set-4tqcg is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2705, will wait for the garbage collector to delete the pods
Dec 13 20:58:18.943: INFO: Deleting DaemonSet.extensions daemon-set took: 4.523428ms
Dec 13 20:58:19.544: INFO: Terminating DaemonSet.extensions daemon-set pods took: 601.273942ms
Dec 13 20:59:55.746: INFO: Number of nodes with available pods: 0
Dec 13 20:59:55.747: INFO: Number of running nodes: 0, number of available pods: 0
Dec 13 20:59:55.748: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2705/daemonsets","resourceVersion":"44377"},"items":null}

Dec 13 20:59:55.750: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2705/pods","resourceVersion":"44377"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 20:59:55.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2705" for this suite.
Dec 13 21:00:01.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:00:01.857: INFO: namespace daemonsets-2705 deletion completed in 6.095408961s

• [SLOW TEST:112.206 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:00:01.857: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2444
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 13 21:00:01.992: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79d67013-c11a-48dc-8fb2-d61b5aab4051" in namespace "downward-api-2444" to be "success or failure"
Dec 13 21:00:01.998: INFO: Pod "downwardapi-volume-79d67013-c11a-48dc-8fb2-d61b5aab4051": Phase="Pending", Reason="", readiness=false. Elapsed: 6.373542ms
Dec 13 21:00:04.000: INFO: Pod "downwardapi-volume-79d67013-c11a-48dc-8fb2-d61b5aab4051": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008616582s
STEP: Saw pod success
Dec 13 21:00:04.000: INFO: Pod "downwardapi-volume-79d67013-c11a-48dc-8fb2-d61b5aab4051" satisfied condition "success or failure"
Dec 13 21:00:04.002: INFO: Trying to get logs from node master-0 pod downwardapi-volume-79d67013-c11a-48dc-8fb2-d61b5aab4051 container client-container: <nil>
STEP: delete the pod
Dec 13 21:00:04.023: INFO: Waiting for pod downwardapi-volume-79d67013-c11a-48dc-8fb2-d61b5aab4051 to disappear
Dec 13 21:00:04.028: INFO: Pod downwardapi-volume-79d67013-c11a-48dc-8fb2-d61b5aab4051 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:00:04.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2444" for this suite.
Dec 13 21:00:10.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:00:10.154: INFO: namespace downward-api-2444 deletion completed in 6.121406083s

• [SLOW TEST:8.297 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:00:10.154: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7486
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 21:00:10.319: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:00:12.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7486" for this suite.
Dec 13 21:00:56.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:00:56.655: INFO: namespace pods-7486 deletion completed in 44.251968289s

• [SLOW TEST:46.501 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:00:56.655: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8358
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Dec 13 21:00:56.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 create -f - --namespace=kubectl-8358'
Dec 13 21:00:58.383: INFO: stderr: ""
Dec 13 21:00:58.383: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 13 21:00:58.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8358'
Dec 13 21:00:58.500: INFO: stderr: ""
Dec 13 21:00:58.500: INFO: stdout: "update-demo-nautilus-sg9tn update-demo-nautilus-sjc6g "
Dec 13 21:00:58.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-nautilus-sg9tn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8358'
Dec 13 21:00:58.586: INFO: stderr: ""
Dec 13 21:00:58.586: INFO: stdout: ""
Dec 13 21:00:58.586: INFO: update-demo-nautilus-sg9tn is created but not running
Dec 13 21:01:03.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8358'
Dec 13 21:01:03.656: INFO: stderr: ""
Dec 13 21:01:03.656: INFO: stdout: "update-demo-nautilus-sg9tn update-demo-nautilus-sjc6g "
Dec 13 21:01:03.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-nautilus-sg9tn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8358'
Dec 13 21:01:03.745: INFO: stderr: ""
Dec 13 21:01:03.746: INFO: stdout: "true"
Dec 13 21:01:03.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-nautilus-sg9tn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8358'
Dec 13 21:01:03.811: INFO: stderr: ""
Dec 13 21:01:03.811: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 13 21:01:03.811: INFO: validating pod update-demo-nautilus-sg9tn
Dec 13 21:01:03.813: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 13 21:01:03.813: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 13 21:01:03.813: INFO: update-demo-nautilus-sg9tn is verified up and running
Dec 13 21:01:03.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-nautilus-sjc6g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8358'
Dec 13 21:01:03.872: INFO: stderr: ""
Dec 13 21:01:03.872: INFO: stdout: "true"
Dec 13 21:01:03.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods update-demo-nautilus-sjc6g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8358'
Dec 13 21:01:03.934: INFO: stderr: ""
Dec 13 21:01:03.934: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 13 21:01:03.934: INFO: validating pod update-demo-nautilus-sjc6g
Dec 13 21:01:03.938: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 13 21:01:03.938: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 13 21:01:03.938: INFO: update-demo-nautilus-sjc6g is verified up and running
STEP: using delete to clean up resources
Dec 13 21:01:03.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 delete --grace-period=0 --force -f - --namespace=kubectl-8358'
Dec 13 21:01:03.999: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 13 21:01:03.999: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 13 21:01:03.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8358'
Dec 13 21:01:04.077: INFO: stderr: "No resources found in kubectl-8358 namespace.\n"
Dec 13 21:01:04.077: INFO: stdout: ""
Dec 13 21:01:04.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods -l name=update-demo --namespace=kubectl-8358 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 13 21:01:04.139: INFO: stderr: ""
Dec 13 21:01:04.139: INFO: stdout: "update-demo-nautilus-sg9tn\nupdate-demo-nautilus-sjc6g\n"
Dec 13 21:01:04.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8358'
Dec 13 21:01:04.729: INFO: stderr: "No resources found in kubectl-8358 namespace.\n"
Dec 13 21:01:04.729: INFO: stdout: ""
Dec 13 21:01:04.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-728461388 get pods -l name=update-demo --namespace=kubectl-8358 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 13 21:01:04.839: INFO: stderr: ""
Dec 13 21:01:04.839: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:01:04.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8358" for this suite.
Dec 13 21:01:10.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:01:10.923: INFO: namespace kubectl-8358 deletion completed in 6.080557577s

• [SLOW TEST:14.268 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:01:10.923: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-4103
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:01:13.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4103" for this suite.
Dec 13 21:01:19.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:01:19.234: INFO: namespace emptydir-wrapper-4103 deletion completed in 6.107444273s

• [SLOW TEST:8.311 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:01:19.234: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3408
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 13 21:01:19.366: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 13 21:01:19.374: INFO: Waiting for terminating namespaces to be deleted...
Dec 13 21:01:19.375: INFO: 
Logging pods the kubelet thinks is on node master-0 before test
Dec 13 21:01:19.381: INFO: kube-apiserver-master-0 from kube-system started at 2019-12-13 18:54:06 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.381: INFO: 	Container kube-apiserver ready: true, restart count 0
Dec 13 21:01:19.381: INFO: sonobuoy-systemd-logs-daemon-set-9616c2f784dc4b45-rxl5l from sonobuoy started at 2019-12-13 19:12:22 +0000 UTC (2 container statuses recorded)
Dec 13 21:01:19.381: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 13 21:01:19.381: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 13 21:01:19.381: INFO: rook-discover-p77wp from rook-ceph started at 2019-12-13 20:15:31 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.381: INFO: 	Container rook-discover ready: true, restart count 0
Dec 13 21:01:19.381: INFO: sonobuoy-e2e-job-8f6bded5e69344cd from sonobuoy started at 2019-12-13 19:12:21 +0000 UTC (2 container statuses recorded)
Dec 13 21:01:19.381: INFO: 	Container e2e ready: true, restart count 0
Dec 13 21:01:19.381: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 13 21:01:19.381: INFO: kube-scheduler-master-0 from kube-system started at 2019-12-13 18:54:06 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.381: INFO: 	Container kube-scheduler ready: true, restart count 0
Dec 13 21:01:19.381: INFO: kube-proxy-dfvlw from kube-system started at 2019-12-13 18:54:29 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.381: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 13 21:01:19.381: INFO: fluentd-es-v2.5.2-mfcr4 from kube-system started at 2019-12-13 20:15:27 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.381: INFO: 	Container fluentd-es ready: true, restart count 0
Dec 13 21:01:19.381: INFO: kube-controller-manager-master-0 from kube-system started at 2019-12-13 18:54:06 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.381: INFO: 	Container kube-controller-manager ready: true, restart count 0
Dec 13 21:01:19.381: INFO: canal-xbp9c from kube-system started at 2019-12-13 18:55:18 +0000 UTC (2 container statuses recorded)
Dec 13 21:01:19.381: INFO: 	Container calico-node ready: true, restart count 0
Dec 13 21:01:19.381: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 13 21:01:19.381: INFO: kata-deploy-m4sjm from kube-system started at 2019-12-13 20:15:27 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.381: INFO: 	Container kube-kata ready: true, restart count 0
Dec 13 21:01:19.381: INFO: rook-ceph-osd-2-58969bddc7-6zxpl from rook-ceph started at 2019-12-13 20:15:27 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.381: INFO: 	Container osd ready: true, restart count 0
Dec 13 21:01:19.381: INFO: rook-ceph-mon-b-5b988bf97b-4xkn2 from rook-ceph started at 2019-12-13 20:15:27 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.381: INFO: 	Container mon ready: true, restart count 0
Dec 13 21:01:19.381: INFO: etcd-master-0 from kube-system started at 2019-12-13 18:54:06 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.381: INFO: 	Container etcd ready: true, restart count 0
Dec 13 21:01:19.381: INFO: node-exporter-tk6xn from monitoring started at 2019-12-13 19:05:14 +0000 UTC (2 container statuses recorded)
Dec 13 21:01:19.381: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 13 21:01:19.381: INFO: 	Container node-exporter ready: true, restart count 0
Dec 13 21:01:19.381: INFO: csi-rbdplugin-8rpjc from rook-ceph started at 2019-12-13 20:15:27 +0000 UTC (3 container statuses recorded)
Dec 13 21:01:19.381: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 21:01:19.381: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 21:01:19.381: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 21:01:19.381: INFO: csi-cephfsplugin-vvg2s from rook-ceph started at 2019-12-13 20:15:27 +0000 UTC (3 container statuses recorded)
Dec 13 21:01:19.381: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 21:01:19.381: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 21:01:19.381: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 21:01:19.381: INFO: 
Logging pods the kubelet thinks is on node worker-0 before test
Dec 13 21:01:19.396: INFO: metrics-server-7578984995-fr9dj from kube-system started at 2019-12-13 18:55:45 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container metrics-server ready: true, restart count 0
Dec 13 21:01:19.396: INFO: csi-cephfsplugin-provisioner-75c965db4f-dhmjp from rook-ceph started at 2019-12-13 18:59:36 +0000 UTC (4 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container csi-attacher ready: true, restart count 0
Dec 13 21:01:19.396: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 21:01:19.396: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 13 21:01:19.396: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 21:01:19.396: INFO: csi-rbdplugin-provisioner-56cbc4d585-lhtqr from rook-ceph started at 2019-12-13 20:14:30 +0000 UTC (5 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 13 21:01:19.396: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 21:01:19.396: INFO: 	Container csi-rbdplugin-attacher ready: true, restart count 0
Dec 13 21:01:19.396: INFO: 	Container csi-snapshotter ready: true, restart count 0
Dec 13 21:01:19.396: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 21:01:19.396: INFO: kube-proxy-rktzd from kube-system started at 2019-12-13 18:54:55 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 13 21:01:19.396: INFO: coredns-5644d7b6d9-tdq6s from kube-system started at 2019-12-13 18:55:47 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container coredns ready: true, restart count 0
Dec 13 21:01:19.396: INFO: rook-discover-5t2vg from rook-ceph started at 2019-12-13 18:59:33 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container rook-discover ready: true, restart count 0
Dec 13 21:01:19.396: INFO: csi-rbdplugin-9vnm2 from rook-ceph started at 2019-12-13 18:59:33 +0000 UTC (3 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 21:01:19.396: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 21:01:19.396: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 21:01:19.396: INFO: rook-ceph-osd-prepare-worker-0-xz82f from rook-ceph started at 2019-12-13 19:03:42 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container provision ready: false, restart count 0
Dec 13 21:01:19.396: INFO: kubernetes-dashboard-6865b885fc-7tv2c from kubernetes-dashboard started at 2019-12-13 19:05:45 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec 13 21:01:19.396: INFO: fluentd-es-v2.5.2-w2nk6 from kube-system started at 2019-12-13 19:07:51 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container fluentd-es ready: true, restart count 0
Dec 13 21:01:19.396: INFO: sonobuoy-systemd-logs-daemon-set-9616c2f784dc4b45-qpldg from sonobuoy started at 2019-12-13 19:12:22 +0000 UTC (2 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 13 21:01:19.396: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 13 21:01:19.396: INFO: rook-ceph-operator-c8ff6447d-6s7cz from rook-ceph started at 2019-12-13 18:55:45 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Dec 13 21:01:19.396: INFO: csi-cephfsplugin-n7682 from rook-ceph started at 2019-12-13 18:59:35 +0000 UTC (3 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 21:01:19.396: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 21:01:19.396: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 21:01:19.396: INFO: rook-ceph-mon-c-597fd9c9c7-r8ngb from rook-ceph started at 2019-12-13 19:02:59 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container mon ready: true, restart count 0
Dec 13 21:01:19.396: INFO: dashboard-metrics-scraper-6ccf7f6cc8-k4zf5 from kubernetes-dashboard started at 2019-12-13 20:14:31 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Dec 13 21:01:19.396: INFO: canal-sgqjs from kube-system started at 2019-12-13 18:55:18 +0000 UTC (2 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container calico-node ready: true, restart count 0
Dec 13 21:01:19.396: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 13 21:01:19.396: INFO: kata-deploy-2qbn2 from kube-system started at 2019-12-13 18:55:45 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container kube-kata ready: true, restart count 0
Dec 13 21:01:19.396: INFO: node-exporter-584q9 from monitoring started at 2019-12-13 19:05:14 +0000 UTC (2 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 13 21:01:19.396: INFO: 	Container node-exporter ready: true, restart count 0
Dec 13 21:01:19.396: INFO: coredns-5644d7b6d9-rngbs from kube-system started at 2019-12-13 18:55:47 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container coredns ready: true, restart count 0
Dec 13 21:01:19.396: INFO: alertmanager-main-2 from monitoring started at 2019-12-13 19:06:20 +0000 UTC (2 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container alertmanager ready: true, restart count 0
Dec 13 21:01:19.396: INFO: 	Container config-reloader ready: true, restart count 0
Dec 13 21:01:19.396: INFO: prometheus-k8s-1 from monitoring started at 2019-12-13 19:06:30 +0000 UTC (3 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container prometheus ready: true, restart count 1
Dec 13 21:01:19.396: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec 13 21:01:19.396: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec 13 21:01:19.396: INFO: grafana-5cd56df4cd-wqpht from monitoring started at 2019-12-13 19:41:23 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container grafana ready: true, restart count 0
Dec 13 21:01:19.396: INFO: rook-ceph-osd-0-64bcd55b9f-hrsvl from rook-ceph started at 2019-12-13 19:03:50 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container osd ready: true, restart count 0
Dec 13 21:01:19.396: INFO: alertmanager-main-1 from monitoring started at 2019-12-13 20:14:55 +0000 UTC (2 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container alertmanager ready: true, restart count 0
Dec 13 21:01:19.396: INFO: 	Container config-reloader ready: true, restart count 0
Dec 13 21:01:19.396: INFO: elasticsearch-logging-0 from kube-system started at 2019-12-13 20:14:56 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.396: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Dec 13 21:01:19.396: INFO: 
Logging pods the kubelet thinks is on node worker-1 before test
Dec 13 21:01:19.414: INFO: kata-deploy-qtlwp from kube-system started at 2019-12-13 18:55:57 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container kube-kata ready: true, restart count 0
Dec 13 21:01:19.414: INFO: rook-discover-m4tgh from rook-ceph started at 2019-12-13 18:59:33 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container rook-discover ready: true, restart count 0
Dec 13 21:01:19.414: INFO: csi-cephfsplugin-6szlf from rook-ceph started at 2019-12-13 18:59:35 +0000 UTC (3 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 21:01:19.414: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 21:01:19.414: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 21:01:19.414: INFO: nginx-ingress-controller-7fb85bc8bb-8f8dr from ingress-nginx started at 2019-12-13 19:08:29 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec 13 21:01:19.414: INFO: alertmanager-main-0 from monitoring started at 2019-12-13 19:06:20 +0000 UTC (2 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container alertmanager ready: true, restart count 0
Dec 13 21:01:19.414: INFO: 	Container config-reloader ready: true, restart count 0
Dec 13 21:01:19.414: INFO: fluentd-es-v2.5.2-s2ng4 from kube-system started at 2019-12-13 19:07:51 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container fluentd-es ready: true, restart count 0
Dec 13 21:01:19.414: INFO: node-exporter-7ggvt from monitoring started at 2019-12-13 19:05:14 +0000 UTC (2 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 13 21:01:19.414: INFO: 	Container node-exporter ready: true, restart count 0
Dec 13 21:01:19.414: INFO: rook-ceph-mon-a-c7664c8c4-jcx8w from rook-ceph started at 2019-12-13 19:02:27 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container mon ready: true, restart count 0
Dec 13 21:01:19.414: INFO: rook-ceph-osd-prepare-worker-1-2xccr from rook-ceph started at 2019-12-13 19:03:42 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container provision ready: false, restart count 0
Dec 13 21:01:19.414: INFO: elasticsearch-logging-1 from kube-system started at 2019-12-13 19:10:24 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Dec 13 21:01:19.414: INFO: sonobuoy-systemd-logs-daemon-set-9616c2f784dc4b45-f29sm from sonobuoy started at 2019-12-13 19:12:22 +0000 UTC (2 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 13 21:01:19.414: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 13 21:01:19.414: INFO: prometheus-operator-7559d67ff-shr6m from monitoring started at 2019-12-13 20:14:30 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container prometheus-operator ready: true, restart count 0
Dec 13 21:01:19.414: INFO: canal-5wqk9 from kube-system started at 2019-12-13 18:55:18 +0000 UTC (2 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container calico-node ready: true, restart count 0
Dec 13 21:01:19.414: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 13 21:01:19.414: INFO: rook-ceph-osd-1-7d8886d496-jzxm4 from rook-ceph started at 2019-12-13 19:03:51 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container osd ready: true, restart count 0
Dec 13 21:01:19.414: INFO: rook-ceph-mgr-a-5775778c4d-k745v from rook-ceph started at 2019-12-13 20:14:30 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container mgr ready: true, restart count 0
Dec 13 21:01:19.414: INFO: kibana-logging-7f6b4b96b4-pdq55 from kube-system started at 2019-12-13 20:14:30 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container kibana-logging ready: true, restart count 0
Dec 13 21:01:19.414: INFO: csi-cephfsplugin-provisioner-75c965db4f-6plm2 from rook-ceph started at 2019-12-13 20:14:31 +0000 UTC (4 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container csi-attacher ready: true, restart count 0
Dec 13 21:01:19.414: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 13 21:01:19.414: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 13 21:01:19.414: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 21:01:19.414: INFO: sonobuoy from sonobuoy started at 2019-12-13 19:12:05 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 13 21:01:19.414: INFO: kube-state-metrics-79d4b9b497-wntkj from monitoring started at 2019-12-13 20:14:30 +0000 UTC (3 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec 13 21:01:19.414: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec 13 21:01:19.414: INFO: 	Container kube-state-metrics ready: true, restart count 0
Dec 13 21:01:19.414: INFO: kube-proxy-dt9pl from kube-system started at 2019-12-13 18:55:17 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 13 21:01:19.414: INFO: csi-rbdplugin-rhs8g from rook-ceph started at 2019-12-13 18:59:33 +0000 UTC (3 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 21:01:19.414: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 13 21:01:19.414: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 21:01:19.414: INFO: csi-rbdplugin-provisioner-56cbc4d585-j64wf from rook-ceph started at 2019-12-13 18:59:34 +0000 UTC (5 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 13 21:01:19.414: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 13 21:01:19.414: INFO: 	Container csi-rbdplugin-attacher ready: true, restart count 0
Dec 13 21:01:19.414: INFO: 	Container csi-snapshotter ready: true, restart count 0
Dec 13 21:01:19.414: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 13 21:01:19.414: INFO: prometheus-adapter-c676d8764-7pnjf from monitoring started at 2019-12-13 19:41:23 +0000 UTC (1 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container prometheus-adapter ready: true, restart count 0
Dec 13 21:01:19.414: INFO: prometheus-k8s-0 from monitoring started at 2019-12-13 20:14:55 +0000 UTC (3 container statuses recorded)
Dec 13 21:01:19.414: INFO: 	Container prometheus ready: true, restart count 1
Dec 13 21:01:19.414: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec 13 21:01:19.414: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-32d589df-db2e-49ce-b58e-d4262421979f 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-32d589df-db2e-49ce-b58e-d4262421979f off the node master-0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-32d589df-db2e-49ce-b58e-d4262421979f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:01:33.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3408" for this suite.
Dec 13 21:01:51.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:01:51.594: INFO: namespace sched-pred-3408 deletion completed in 18.084875762s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:32.360 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:01:51.595: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-456
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 21:01:51.729: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec 13 21:01:56.732: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 13 21:01:56.732: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 13 21:01:58.752: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-456 /apis/apps/v1/namespaces/deployment-456/deployments/test-cleanup-deployment 21e7aab8-914f-4cd2-888c-69de8fe4d906 45140 1 2019-12-13 21:01:56 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002f7e828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-12-13 21:01:56 +0000 UTC,LastTransitionTime:2019-12-13 21:01:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-65db99849b" has successfully progressed.,LastUpdateTime:2019-12-13 21:01:57 +0000 UTC,LastTransitionTime:2019-12-13 21:01:56 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 13 21:01:58.755: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-456 /apis/apps/v1/namespaces/deployment-456/replicasets/test-cleanup-deployment-65db99849b e22a83fa-1c25-4300-ab86-5dfb7fa7b9ed 45129 1 2019-12-13 21:01:56 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 21e7aab8-914f-4cd2-888c-69de8fe4d906 0xc002f7fe27 0xc002f7fe28}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002f7ff68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 13 21:01:58.758: INFO: Pod "test-cleanup-deployment-65db99849b-wwnfl" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-65db99849b-wwnfl test-cleanup-deployment-65db99849b- deployment-456 /api/v1/namespaces/deployment-456/pods/test-cleanup-deployment-65db99849b-wwnfl 16311af0-2086-47a9-8374-98c84a83459b 45128 0 2019-12-13 21:01:56 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[cni.projectcalico.org/podIP:10.244.0.134/32] [{apps/v1 ReplicaSet test-cleanup-deployment-65db99849b e22a83fa-1c25-4300-ab86-5dfb7fa7b9ed 0xc004ab6cb7 0xc004ab6cb8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-s7fcq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-s7fcq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-s7fcq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 21:01:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 21:01:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 21:01:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 21:01:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.2,PodIP:10.244.0.134,StartTime:2019-12-13 21:01:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-13 21:01:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://f5823875e8505b9c51ca2be28e01e5727820b950fd63c6f227d1aee2c1c98fe8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.134,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:01:58.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-456" for this suite.
Dec 13 21:02:04.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:02:04.856: INFO: namespace deployment-456 deletion completed in 6.095752881s

• [SLOW TEST:13.262 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:02:04.857: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4879
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-b22e4aee-0876-40f6-8ec9-74f352eaeefc
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:02:04.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4879" for this suite.
Dec 13 21:02:11.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:02:11.073: INFO: namespace secrets-4879 deletion completed in 6.074741387s

• [SLOW TEST:6.216 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:02:11.073: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3646
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 13 21:02:11.324: INFO: Number of nodes with available pods: 0
Dec 13 21:02:11.324: INFO: Node master-0 is running more than one daemon pod
Dec 13 21:02:12.330: INFO: Number of nodes with available pods: 0
Dec 13 21:02:12.330: INFO: Node master-0 is running more than one daemon pod
Dec 13 21:02:13.329: INFO: Number of nodes with available pods: 3
Dec 13 21:02:13.329: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec 13 21:02:13.359: INFO: Number of nodes with available pods: 2
Dec 13 21:02:13.359: INFO: Node master-0 is running more than one daemon pod
Dec 13 21:02:14.364: INFO: Number of nodes with available pods: 2
Dec 13 21:02:14.364: INFO: Node master-0 is running more than one daemon pod
Dec 13 21:02:15.364: INFO: Number of nodes with available pods: 3
Dec 13 21:02:15.364: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3646, will wait for the garbage collector to delete the pods
Dec 13 21:02:15.423: INFO: Deleting DaemonSet.extensions daemon-set took: 4.246741ms
Dec 13 21:02:16.024: INFO: Terminating DaemonSet.extensions daemon-set pods took: 601.155916ms
Dec 13 21:02:26.927: INFO: Number of nodes with available pods: 0
Dec 13 21:02:26.927: INFO: Number of running nodes: 0, number of available pods: 0
Dec 13 21:02:26.929: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3646/daemonsets","resourceVersion":"45365"},"items":null}

Dec 13 21:02:26.931: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3646/pods","resourceVersion":"45365"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:02:26.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3646" for this suite.
Dec 13 21:02:32.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:02:33.065: INFO: namespace daemonsets-3646 deletion completed in 6.111296909s

• [SLOW TEST:21.992 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:02:33.065: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5638
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 13 21:02:37.226: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 13 21:02:37.235: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 13 21:02:39.235: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 13 21:02:39.237: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 13 21:02:41.236: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 13 21:02:41.238: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 13 21:02:43.236: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 13 21:02:43.238: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 13 21:02:45.235: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 13 21:02:45.238: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 13 21:02:47.236: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 13 21:02:47.237: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:02:47.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5638" for this suite.
Dec 13 21:03:15.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:03:15.377: INFO: namespace container-lifecycle-hook-5638 deletion completed in 28.133284179s

• [SLOW TEST:42.312 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:03:15.377: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3972
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec 13 21:03:15.749: INFO: Waiting up to 5m0s for pod "pod-4597348d-b5d4-423f-8b12-f3e5f9b5a2ba" in namespace "emptydir-3972" to be "success or failure"
Dec 13 21:03:15.754: INFO: Pod "pod-4597348d-b5d4-423f-8b12-f3e5f9b5a2ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.737221ms
Dec 13 21:03:17.757: INFO: Pod "pod-4597348d-b5d4-423f-8b12-f3e5f9b5a2ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008107633s
STEP: Saw pod success
Dec 13 21:03:17.757: INFO: Pod "pod-4597348d-b5d4-423f-8b12-f3e5f9b5a2ba" satisfied condition "success or failure"
Dec 13 21:03:17.759: INFO: Trying to get logs from node master-0 pod pod-4597348d-b5d4-423f-8b12-f3e5f9b5a2ba container test-container: <nil>
STEP: delete the pod
Dec 13 21:03:17.796: INFO: Waiting for pod pod-4597348d-b5d4-423f-8b12-f3e5f9b5a2ba to disappear
Dec 13 21:03:17.806: INFO: Pod pod-4597348d-b5d4-423f-8b12-f3e5f9b5a2ba no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:03:17.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3972" for this suite.
Dec 13 21:03:23.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:03:23.904: INFO: namespace emptydir-3972 deletion completed in 6.093529276s

• [SLOW TEST:8.526 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:03:23.904: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5433
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec 13 21:03:26.571: INFO: Successfully updated pod "labelsupdate0686cb6a-8d19-4051-b630-6b4f94af5659"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:03:30.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5433" for this suite.
Dec 13 21:03:42.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:03:42.721: INFO: namespace projected-5433 deletion completed in 12.091483983s

• [SLOW TEST:18.817 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:03:42.721: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4642
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-4059ea7d-f6ea-4e46-a0df-8afb72065dd5
STEP: Creating a pod to test consume configMaps
Dec 13 21:03:42.921: INFO: Waiting up to 5m0s for pod "pod-configmaps-800db94c-5c80-48fe-bf94-4f618c1ce55c" in namespace "configmap-4642" to be "success or failure"
Dec 13 21:03:42.934: INFO: Pod "pod-configmaps-800db94c-5c80-48fe-bf94-4f618c1ce55c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.156027ms
Dec 13 21:03:44.936: INFO: Pod "pod-configmaps-800db94c-5c80-48fe-bf94-4f618c1ce55c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015239124s
STEP: Saw pod success
Dec 13 21:03:44.936: INFO: Pod "pod-configmaps-800db94c-5c80-48fe-bf94-4f618c1ce55c" satisfied condition "success or failure"
Dec 13 21:03:44.938: INFO: Trying to get logs from node master-0 pod pod-configmaps-800db94c-5c80-48fe-bf94-4f618c1ce55c container configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 21:03:44.951: INFO: Waiting for pod pod-configmaps-800db94c-5c80-48fe-bf94-4f618c1ce55c to disappear
Dec 13 21:03:44.957: INFO: Pod pod-configmaps-800db94c-5c80-48fe-bf94-4f618c1ce55c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:03:44.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4642" for this suite.
Dec 13 21:03:50.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:03:51.055: INFO: namespace configmap-4642 deletion completed in 6.092962747s

• [SLOW TEST:8.334 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:03:51.055: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1315
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 21:03:51.216: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec 13 21:03:51.227: INFO: Number of nodes with available pods: 0
Dec 13 21:03:51.227: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec 13 21:03:51.247: INFO: Number of nodes with available pods: 0
Dec 13 21:03:51.247: INFO: Node master-0 is running more than one daemon pod
Dec 13 21:03:52.249: INFO: Number of nodes with available pods: 0
Dec 13 21:03:52.249: INFO: Node master-0 is running more than one daemon pod
Dec 13 21:03:53.251: INFO: Number of nodes with available pods: 1
Dec 13 21:03:53.251: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec 13 21:03:53.272: INFO: Number of nodes with available pods: 1
Dec 13 21:03:53.272: INFO: Number of running nodes: 0, number of available pods: 1
Dec 13 21:03:54.274: INFO: Number of nodes with available pods: 0
Dec 13 21:03:54.274: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec 13 21:03:54.279: INFO: Number of nodes with available pods: 0
Dec 13 21:03:54.279: INFO: Node master-0 is running more than one daemon pod
Dec 13 21:03:55.281: INFO: Number of nodes with available pods: 0
Dec 13 21:03:55.281: INFO: Node master-0 is running more than one daemon pod
Dec 13 21:03:56.281: INFO: Number of nodes with available pods: 0
Dec 13 21:03:56.281: INFO: Node master-0 is running more than one daemon pod
Dec 13 21:03:57.281: INFO: Number of nodes with available pods: 0
Dec 13 21:03:57.281: INFO: Node master-0 is running more than one daemon pod
Dec 13 21:03:58.281: INFO: Number of nodes with available pods: 0
Dec 13 21:03:58.281: INFO: Node master-0 is running more than one daemon pod
Dec 13 21:03:59.281: INFO: Number of nodes with available pods: 0
Dec 13 21:03:59.281: INFO: Node master-0 is running more than one daemon pod
Dec 13 21:04:00.281: INFO: Number of nodes with available pods: 0
Dec 13 21:04:00.282: INFO: Node master-0 is running more than one daemon pod
Dec 13 21:04:01.286: INFO: Number of nodes with available pods: 0
Dec 13 21:04:01.286: INFO: Node master-0 is running more than one daemon pod
Dec 13 21:04:02.282: INFO: Number of nodes with available pods: 0
Dec 13 21:04:02.282: INFO: Node master-0 is running more than one daemon pod
Dec 13 21:04:03.281: INFO: Number of nodes with available pods: 0
Dec 13 21:04:03.281: INFO: Node master-0 is running more than one daemon pod
Dec 13 21:04:04.281: INFO: Number of nodes with available pods: 0
Dec 13 21:04:04.281: INFO: Node master-0 is running more than one daemon pod
Dec 13 21:04:05.281: INFO: Number of nodes with available pods: 0
Dec 13 21:04:05.281: INFO: Node master-0 is running more than one daemon pod
Dec 13 21:04:06.282: INFO: Number of nodes with available pods: 0
Dec 13 21:04:06.282: INFO: Node master-0 is running more than one daemon pod
Dec 13 21:04:07.364: INFO: Number of nodes with available pods: 1
Dec 13 21:04:07.364: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1315, will wait for the garbage collector to delete the pods
Dec 13 21:04:07.488: INFO: Deleting DaemonSet.extensions daemon-set took: 3.533609ms
Dec 13 21:04:08.088: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.217821ms
Dec 13 21:04:15.790: INFO: Number of nodes with available pods: 0
Dec 13 21:04:15.790: INFO: Number of running nodes: 0, number of available pods: 0
Dec 13 21:04:15.791: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1315/daemonsets","resourceVersion":"45976"},"items":null}

Dec 13 21:04:15.793: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1315/pods","resourceVersion":"45976"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:04:15.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1315" for this suite.
Dec 13 21:04:21.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:04:21.902: INFO: namespace daemonsets-1315 deletion completed in 6.088569303s

• [SLOW TEST:30.847 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:04:21.902: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-692
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-692.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-692.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-692.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-692.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-692.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-692.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 13 21:04:26.095: INFO: DNS probes using dns-692/dns-test-330ed94e-959f-4490-907b-ccf4721536da succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:04:26.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-692" for this suite.
Dec 13 21:04:32.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:04:32.456: INFO: namespace dns-692 deletion completed in 6.126433576s

• [SLOW TEST:10.554 seconds]
[sig-network] DNS
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:04:32.457: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3891
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 13 21:04:32.598: INFO: Waiting up to 5m0s for pod "pod-d08181b7-cd65-4dbf-9a38-a054400b4c2e" in namespace "emptydir-3891" to be "success or failure"
Dec 13 21:04:32.608: INFO: Pod "pod-d08181b7-cd65-4dbf-9a38-a054400b4c2e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.824076ms
Dec 13 21:04:34.610: INFO: Pod "pod-d08181b7-cd65-4dbf-9a38-a054400b4c2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012184263s
STEP: Saw pod success
Dec 13 21:04:34.610: INFO: Pod "pod-d08181b7-cd65-4dbf-9a38-a054400b4c2e" satisfied condition "success or failure"
Dec 13 21:04:34.612: INFO: Trying to get logs from node master-0 pod pod-d08181b7-cd65-4dbf-9a38-a054400b4c2e container test-container: <nil>
STEP: delete the pod
Dec 13 21:04:34.627: INFO: Waiting for pod pod-d08181b7-cd65-4dbf-9a38-a054400b4c2e to disappear
Dec 13 21:04:34.633: INFO: Pod pod-d08181b7-cd65-4dbf-9a38-a054400b4c2e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:04:34.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3891" for this suite.
Dec 13 21:04:40.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:04:40.718: INFO: namespace emptydir-3891 deletion completed in 6.082598088s

• [SLOW TEST:8.262 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:04:40.719: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-610
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-mnlk
STEP: Creating a pod to test atomic-volume-subpath
Dec 13 21:04:40.862: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-mnlk" in namespace "subpath-610" to be "success or failure"
Dec 13 21:04:40.866: INFO: Pod "pod-subpath-test-projected-mnlk": Phase="Pending", Reason="", readiness=false. Elapsed: 3.527758ms
Dec 13 21:04:42.868: INFO: Pod "pod-subpath-test-projected-mnlk": Phase="Running", Reason="", readiness=true. Elapsed: 2.005623899s
Dec 13 21:04:44.870: INFO: Pod "pod-subpath-test-projected-mnlk": Phase="Running", Reason="", readiness=true. Elapsed: 4.007905757s
Dec 13 21:04:46.873: INFO: Pod "pod-subpath-test-projected-mnlk": Phase="Running", Reason="", readiness=true. Elapsed: 6.011082896s
Dec 13 21:04:48.876: INFO: Pod "pod-subpath-test-projected-mnlk": Phase="Running", Reason="", readiness=true. Elapsed: 8.013310678s
Dec 13 21:04:50.883: INFO: Pod "pod-subpath-test-projected-mnlk": Phase="Running", Reason="", readiness=true. Elapsed: 10.020483475s
Dec 13 21:04:52.885: INFO: Pod "pod-subpath-test-projected-mnlk": Phase="Running", Reason="", readiness=true. Elapsed: 12.02265911s
Dec 13 21:04:54.887: INFO: Pod "pod-subpath-test-projected-mnlk": Phase="Running", Reason="", readiness=true. Elapsed: 14.025060956s
Dec 13 21:04:56.896: INFO: Pod "pod-subpath-test-projected-mnlk": Phase="Running", Reason="", readiness=true. Elapsed: 16.033680022s
Dec 13 21:04:58.899: INFO: Pod "pod-subpath-test-projected-mnlk": Phase="Running", Reason="", readiness=true. Elapsed: 18.036694584s
Dec 13 21:05:00.901: INFO: Pod "pod-subpath-test-projected-mnlk": Phase="Running", Reason="", readiness=true. Elapsed: 20.039029208s
Dec 13 21:05:02.903: INFO: Pod "pod-subpath-test-projected-mnlk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.040987562s
STEP: Saw pod success
Dec 13 21:05:02.903: INFO: Pod "pod-subpath-test-projected-mnlk" satisfied condition "success or failure"
Dec 13 21:05:02.905: INFO: Trying to get logs from node master-0 pod pod-subpath-test-projected-mnlk container test-container-subpath-projected-mnlk: <nil>
STEP: delete the pod
Dec 13 21:05:02.917: INFO: Waiting for pod pod-subpath-test-projected-mnlk to disappear
Dec 13 21:05:02.923: INFO: Pod pod-subpath-test-projected-mnlk no longer exists
STEP: Deleting pod pod-subpath-test-projected-mnlk
Dec 13 21:05:02.923: INFO: Deleting pod "pod-subpath-test-projected-mnlk" in namespace "subpath-610"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:05:02.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-610" for this suite.
Dec 13 21:05:08.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:05:09.009: INFO: namespace subpath-610 deletion completed in 6.081818781s

• [SLOW TEST:28.291 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:05:09.010: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-281
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 13 21:05:09.148: INFO: Waiting up to 5m0s for pod "pod-a7b77a24-5b3c-4788-847f-fa8a69ac1979" in namespace "emptydir-281" to be "success or failure"
Dec 13 21:05:09.157: INFO: Pod "pod-a7b77a24-5b3c-4788-847f-fa8a69ac1979": Phase="Pending", Reason="", readiness=false. Elapsed: 8.873836ms
Dec 13 21:05:11.160: INFO: Pod "pod-a7b77a24-5b3c-4788-847f-fa8a69ac1979": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011977587s
STEP: Saw pod success
Dec 13 21:05:11.160: INFO: Pod "pod-a7b77a24-5b3c-4788-847f-fa8a69ac1979" satisfied condition "success or failure"
Dec 13 21:05:11.162: INFO: Trying to get logs from node master-0 pod pod-a7b77a24-5b3c-4788-847f-fa8a69ac1979 container test-container: <nil>
STEP: delete the pod
Dec 13 21:05:11.180: INFO: Waiting for pod pod-a7b77a24-5b3c-4788-847f-fa8a69ac1979 to disappear
Dec 13 21:05:11.184: INFO: Pod pod-a7b77a24-5b3c-4788-847f-fa8a69ac1979 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:05:11.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-281" for this suite.
Dec 13 21:05:17.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:05:17.289: INFO: namespace emptydir-281 deletion completed in 6.101342655s

• [SLOW TEST:8.279 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:05:17.289: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-702
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 13 21:05:17.445: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d4b16888-bd5e-4151-a072-abcde54904bf" in namespace "downward-api-702" to be "success or failure"
Dec 13 21:05:17.449: INFO: Pod "downwardapi-volume-d4b16888-bd5e-4151-a072-abcde54904bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003197ms
Dec 13 21:05:19.456: INFO: Pod "downwardapi-volume-d4b16888-bd5e-4151-a072-abcde54904bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011680642s
STEP: Saw pod success
Dec 13 21:05:19.456: INFO: Pod "downwardapi-volume-d4b16888-bd5e-4151-a072-abcde54904bf" satisfied condition "success or failure"
Dec 13 21:05:19.462: INFO: Trying to get logs from node master-0 pod downwardapi-volume-d4b16888-bd5e-4151-a072-abcde54904bf container client-container: <nil>
STEP: delete the pod
Dec 13 21:05:19.484: INFO: Waiting for pod downwardapi-volume-d4b16888-bd5e-4151-a072-abcde54904bf to disappear
Dec 13 21:05:19.490: INFO: Pod downwardapi-volume-d4b16888-bd5e-4151-a072-abcde54904bf no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:05:19.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-702" for this suite.
Dec 13 21:05:25.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:05:25.568: INFO: namespace downward-api-702 deletion completed in 6.075171393s

• [SLOW TEST:8.279 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:05:25.568: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9375
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:05:36.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9375" for this suite.
Dec 13 21:05:42.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:05:42.812: INFO: namespace resourcequota-9375 deletion completed in 6.086032972s

• [SLOW TEST:17.243 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:05:42.812: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-684
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-684.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-684.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-684.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-684.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 13 21:05:46.980: INFO: DNS probes using dns-test-120f9a94-a3ab-4ef8-a25d-4db4c3ec80ee succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-684.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-684.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-684.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-684.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 13 21:05:51.047: INFO: File wheezy_udp@dns-test-service-3.dns-684.svc.cluster.local from pod  dns-684/dns-test-1639c9f1-7001-41e0-8d8b-a485e703ca44 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 13 21:05:51.049: INFO: File jessie_udp@dns-test-service-3.dns-684.svc.cluster.local from pod  dns-684/dns-test-1639c9f1-7001-41e0-8d8b-a485e703ca44 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 13 21:05:51.049: INFO: Lookups using dns-684/dns-test-1639c9f1-7001-41e0-8d8b-a485e703ca44 failed for: [wheezy_udp@dns-test-service-3.dns-684.svc.cluster.local jessie_udp@dns-test-service-3.dns-684.svc.cluster.local]

Dec 13 21:05:56.058: INFO: File wheezy_udp@dns-test-service-3.dns-684.svc.cluster.local from pod  dns-684/dns-test-1639c9f1-7001-41e0-8d8b-a485e703ca44 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 13 21:05:56.071: INFO: File jessie_udp@dns-test-service-3.dns-684.svc.cluster.local from pod  dns-684/dns-test-1639c9f1-7001-41e0-8d8b-a485e703ca44 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 13 21:05:56.071: INFO: Lookups using dns-684/dns-test-1639c9f1-7001-41e0-8d8b-a485e703ca44 failed for: [wheezy_udp@dns-test-service-3.dns-684.svc.cluster.local jessie_udp@dns-test-service-3.dns-684.svc.cluster.local]

Dec 13 21:06:01.052: INFO: File wheezy_udp@dns-test-service-3.dns-684.svc.cluster.local from pod  dns-684/dns-test-1639c9f1-7001-41e0-8d8b-a485e703ca44 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 13 21:06:01.054: INFO: File jessie_udp@dns-test-service-3.dns-684.svc.cluster.local from pod  dns-684/dns-test-1639c9f1-7001-41e0-8d8b-a485e703ca44 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 13 21:06:01.054: INFO: Lookups using dns-684/dns-test-1639c9f1-7001-41e0-8d8b-a485e703ca44 failed for: [wheezy_udp@dns-test-service-3.dns-684.svc.cluster.local jessie_udp@dns-test-service-3.dns-684.svc.cluster.local]

Dec 13 21:06:06.052: INFO: File wheezy_udp@dns-test-service-3.dns-684.svc.cluster.local from pod  dns-684/dns-test-1639c9f1-7001-41e0-8d8b-a485e703ca44 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 13 21:06:06.054: INFO: File jessie_udp@dns-test-service-3.dns-684.svc.cluster.local from pod  dns-684/dns-test-1639c9f1-7001-41e0-8d8b-a485e703ca44 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 13 21:06:06.054: INFO: Lookups using dns-684/dns-test-1639c9f1-7001-41e0-8d8b-a485e703ca44 failed for: [wheezy_udp@dns-test-service-3.dns-684.svc.cluster.local jessie_udp@dns-test-service-3.dns-684.svc.cluster.local]

Dec 13 21:06:11.052: INFO: File wheezy_udp@dns-test-service-3.dns-684.svc.cluster.local from pod  dns-684/dns-test-1639c9f1-7001-41e0-8d8b-a485e703ca44 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 13 21:06:11.054: INFO: File jessie_udp@dns-test-service-3.dns-684.svc.cluster.local from pod  dns-684/dns-test-1639c9f1-7001-41e0-8d8b-a485e703ca44 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 13 21:06:11.054: INFO: Lookups using dns-684/dns-test-1639c9f1-7001-41e0-8d8b-a485e703ca44 failed for: [wheezy_udp@dns-test-service-3.dns-684.svc.cluster.local jessie_udp@dns-test-service-3.dns-684.svc.cluster.local]

Dec 13 21:06:16.063: INFO: DNS probes using dns-test-1639c9f1-7001-41e0-8d8b-a485e703ca44 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-684.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-684.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-684.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-684.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 13 21:06:20.166: INFO: DNS probes using dns-test-f8bf63fb-2ba1-4e00-9ec7-7756afac87d4 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:06:20.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-684" for this suite.
Dec 13 21:06:26.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:06:26.468: INFO: namespace dns-684 deletion completed in 6.190760703s

• [SLOW TEST:43.656 seconds]
[sig-network] DNS
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:06:26.468: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1794
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Dec 13 21:06:28.158: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W1213 21:06:28.158584      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 13 21:06:28.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1794" for this suite.
Dec 13 21:06:34.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:06:34.253: INFO: namespace gc-1794 deletion completed in 6.092228837s

• [SLOW TEST:7.785 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:06:34.253: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8378
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec 13 21:06:36.915: INFO: Successfully updated pod "annotationupdatebb1beb0d-7cb2-4d97-bd22-3fa6e23a6050"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:06:40.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8378" for this suite.
Dec 13 21:06:52.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:06:53.142: INFO: namespace downward-api-8378 deletion completed in 12.210015067s

• [SLOW TEST:18.889 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:06:53.142: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-7706
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec 13 21:07:01.436: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7706 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 21:07:01.436: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 21:07:01.505: INFO: Exec stderr: ""
Dec 13 21:07:01.505: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7706 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 21:07:01.505: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 21:07:01.677: INFO: Exec stderr: ""
Dec 13 21:07:01.677: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7706 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 21:07:01.677: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 21:07:01.750: INFO: Exec stderr: ""
Dec 13 21:07:01.750: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7706 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 21:07:01.750: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 21:07:01.816: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec 13 21:07:01.816: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7706 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 21:07:01.816: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 21:07:01.880: INFO: Exec stderr: ""
Dec 13 21:07:01.880: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7706 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 21:07:01.880: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 21:07:01.946: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec 13 21:07:01.946: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7706 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 21:07:01.946: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 21:07:02.006: INFO: Exec stderr: ""
Dec 13 21:07:02.006: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7706 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 21:07:02.006: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 21:07:02.066: INFO: Exec stderr: ""
Dec 13 21:07:02.066: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7706 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 21:07:02.066: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 21:07:02.134: INFO: Exec stderr: ""
Dec 13 21:07:02.134: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7706 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 21:07:02.134: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 21:07:02.195: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:07:02.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-7706" for this suite.
Dec 13 21:07:46.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:07:46.281: INFO: namespace e2e-kubelet-etc-hosts-7706 deletion completed in 44.084087748s

• [SLOW TEST:53.139 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:07:46.282: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5826
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-4596
STEP: Creating secret with name secret-test-465737dd-2b9e-4064-8b37-908cda483245
STEP: Creating a pod to test consume secrets
Dec 13 21:07:46.550: INFO: Waiting up to 5m0s for pod "pod-secrets-c691adec-b4e5-4de5-ac49-6194ee790848" in namespace "secrets-5826" to be "success or failure"
Dec 13 21:07:46.555: INFO: Pod "pod-secrets-c691adec-b4e5-4de5-ac49-6194ee790848": Phase="Pending", Reason="", readiness=false. Elapsed: 4.086304ms
Dec 13 21:07:48.558: INFO: Pod "pod-secrets-c691adec-b4e5-4de5-ac49-6194ee790848": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006416531s
STEP: Saw pod success
Dec 13 21:07:48.558: INFO: Pod "pod-secrets-c691adec-b4e5-4de5-ac49-6194ee790848" satisfied condition "success or failure"
Dec 13 21:07:48.560: INFO: Trying to get logs from node master-0 pod pod-secrets-c691adec-b4e5-4de5-ac49-6194ee790848 container secret-volume-test: <nil>
STEP: delete the pod
Dec 13 21:07:48.579: INFO: Waiting for pod pod-secrets-c691adec-b4e5-4de5-ac49-6194ee790848 to disappear
Dec 13 21:07:48.588: INFO: Pod pod-secrets-c691adec-b4e5-4de5-ac49-6194ee790848 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:07:48.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5826" for this suite.
Dec 13 21:07:54.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:07:54.785: INFO: namespace secrets-5826 deletion completed in 6.19471658s
STEP: Destroying namespace "secret-namespace-4596" for this suite.
Dec 13 21:08:00.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:08:00.874: INFO: namespace secret-namespace-4596 deletion completed in 6.088305184s

• [SLOW TEST:14.592 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:08:00.874: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6634
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec 13 21:08:01.030: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6634 /api/v1/namespaces/watch-6634/configmaps/e2e-watch-test-resource-version e561dbb2-d53b-4a96-a84e-3ba707f3c2d4 47311 0 2019-12-13 21:08:01 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 13 21:08:01.030: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6634 /api/v1/namespaces/watch-6634/configmaps/e2e-watch-test-resource-version e561dbb2-d53b-4a96-a84e-3ba707f3c2d4 47312 0 2019-12-13 21:08:01 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:08:01.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6634" for this suite.
Dec 13 21:08:07.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:08:07.227: INFO: namespace watch-6634 deletion completed in 6.193431383s

• [SLOW TEST:6.353 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:08:07.227: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8196
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 21:08:31.419: INFO: Container started at 2019-12-13 21:08:08 +0000 UTC, pod became ready at 2019-12-13 21:08:30 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:08:31.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8196" for this suite.
Dec 13 21:08:59.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:08:59.553: INFO: namespace container-probe-8196 deletion completed in 28.131638788s

• [SLOW TEST:52.327 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:08:59.554: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3229
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 13 21:09:00.457: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 13 21:09:03.478: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:09:03.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3229" for this suite.
Dec 13 21:09:09.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:09:09.829: INFO: namespace webhook-3229 deletion completed in 6.091867543s
STEP: Destroying namespace "webhook-3229-markers" for this suite.
Dec 13 21:09:15.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:09:15.920: INFO: namespace webhook-3229-markers deletion completed in 6.090413517s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:16.374 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:09:15.928: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4293
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 13 21:09:16.061: INFO: Creating deployment "test-recreate-deployment"
Dec 13 21:09:16.064: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec 13 21:09:16.070: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec 13 21:09:18.075: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec 13 21:09:18.076: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec 13 21:09:18.081: INFO: Updating deployment test-recreate-deployment
Dec 13 21:09:18.081: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 13 21:09:18.327: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4293 /apis/apps/v1/namespaces/deployment-4293/deployments/test-recreate-deployment 4244eac5-045e-4008-8138-cd90de6ae8f1 47745 2 2019-12-13 21:09:16 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005ca55e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-12-13 21:09:18 +0000 UTC,LastTransitionTime:2019-12-13 21:09:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2019-12-13 21:09:18 +0000 UTC,LastTransitionTime:2019-12-13 21:09:16 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Dec 13 21:09:18.334: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-4293 /apis/apps/v1/namespaces/deployment-4293/replicasets/test-recreate-deployment-5f94c574ff 11b3c3d5-272c-4e16-82b1-84c09a369844 47744 1 2019-12-13 21:09:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 4244eac5-045e-4008-8138-cd90de6ae8f1 0xc005ca59d7 0xc005ca59d8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005ca5a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 13 21:09:18.334: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec 13 21:09:18.334: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-4293 /apis/apps/v1/namespaces/deployment-4293/replicasets/test-recreate-deployment-68fc85c7bb 7e351c65-29d7-47ff-853a-cbc66c6efeb2 47733 2 2019-12-13 21:09:16 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 4244eac5-045e-4008-8138-cd90de6ae8f1 0xc005ca5aa7 0xc005ca5aa8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005ca5b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 13 21:09:18.343: INFO: Pod "test-recreate-deployment-5f94c574ff-rb65m" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-rb65m test-recreate-deployment-5f94c574ff- deployment-4293 /api/v1/namespaces/deployment-4293/pods/test-recreate-deployment-5f94c574ff-rb65m 92ee364b-609a-445a-9e9d-6e011645db2a 47743 0 2019-12-13 21:09:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 11b3c3d5-272c-4e16-82b1-84c09a369844 0xc000842637 0xc000842638}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-s54zg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-s54zg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-s54zg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 21:09:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 21:09:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 21:09:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-13 21:09:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.2,PodIP:,StartTime:2019-12-13 21:09:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:09:18.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4293" for this suite.
Dec 13 21:09:24.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:09:24.554: INFO: namespace deployment-4293 deletion completed in 6.207108385s

• [SLOW TEST:8.626 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:09:24.554: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3585
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-3585
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 13 21:09:24.729: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 13 21:09:42.850: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.51 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3585 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 21:09:42.850: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 21:09:43.932: INFO: Found all expected endpoints: [netserver-0]
Dec 13 21:09:43.934: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.161 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3585 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 21:09:43.934: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 21:09:45.038: INFO: Found all expected endpoints: [netserver-1]
Dec 13 21:09:45.040: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.52 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3585 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 21:09:45.040: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
Dec 13 21:09:46.138: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:09:46.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3585" for this suite.
Dec 13 21:09:58.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:09:58.232: INFO: namespace pod-network-test-3585 deletion completed in 12.091654619s

• [SLOW TEST:33.678 seconds]
[sig-network] Networking
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 13 21:09:58.232: INFO: >>> kubeConfig: /tmp/kubeconfig-728461388
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9944
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 13 21:09:58.366: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0f520ec2-da4b-4bde-8fdf-60b88ccafc20" in namespace "downward-api-9944" to be "success or failure"
Dec 13 21:09:58.371: INFO: Pod "downwardapi-volume-0f520ec2-da4b-4bde-8fdf-60b88ccafc20": Phase="Pending", Reason="", readiness=false. Elapsed: 5.14701ms
Dec 13 21:10:00.373: INFO: Pod "downwardapi-volume-0f520ec2-da4b-4bde-8fdf-60b88ccafc20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007469238s
STEP: Saw pod success
Dec 13 21:10:00.373: INFO: Pod "downwardapi-volume-0f520ec2-da4b-4bde-8fdf-60b88ccafc20" satisfied condition "success or failure"
Dec 13 21:10:00.375: INFO: Trying to get logs from node master-0 pod downwardapi-volume-0f520ec2-da4b-4bde-8fdf-60b88ccafc20 container client-container: <nil>
STEP: delete the pod
Dec 13 21:10:00.398: INFO: Waiting for pod downwardapi-volume-0f520ec2-da4b-4bde-8fdf-60b88ccafc20 to disappear
Dec 13 21:10:00.403: INFO: Pod downwardapi-volume-0f520ec2-da4b-4bde-8fdf-60b88ccafc20 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 13 21:10:00.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9944" for this suite.
Dec 13 21:10:06.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:10:06.493: INFO: namespace downward-api-9944 deletion completed in 6.087350054s

• [SLOW TEST:8.261 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.4-beta.0.50+d9a25890317058/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SDec 13 21:10:06.493: INFO: Running AfterSuite actions on all nodes
Dec 13 21:10:06.493: INFO: Running AfterSuite actions on node 1
Dec 13 21:10:06.493: INFO: Skipping dumping logs from cluster

Ran 276 of 4731 Specs in 7000.733 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4455 Skipped
PASS

Ginkgo ran 1 suite in 1h56m42.920740033s
Test Suite Passed
