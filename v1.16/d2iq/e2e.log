I0506 13:56:41.395536      25 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-926700516
I0506 13:56:41.395667      25 e2e.go:92] Starting e2e run "60d39153-42a8-47ca-8924-4d01d74f713f" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1588773400 - Will randomize all specs
Will run 276 of 4731 specs

May  6 13:56:41.404: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 13:56:41.406: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May  6 13:56:41.420: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May  6 13:56:41.447: INFO: 38 / 38 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May  6 13:56:41.447: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
May  6 13:56:41.447: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May  6 13:56:41.452: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
May  6 13:56:41.453: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'ebs-csi-node' (0 seconds elapsed)
May  6 13:56:41.453: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May  6 13:56:41.453: INFO: e2e test version: v1.16.8
May  6 13:56:41.453: INFO: kube-apiserver version: v1.16.8
May  6 13:56:41.453: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 13:56:41.456: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 13:56:41.457: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename statefulset
May  6 13:56:41.485: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
May  6 13:56:41.491: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-60
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-60
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-60
May  6 13:56:41.608: INFO: Found 0 stateful pods, waiting for 1
May  6 13:56:51.611: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May  6 13:56:51.624: INFO: Deleting all statefulset in ns statefulset-60
May  6 13:56:51.626: INFO: Scaling statefulset ss to 0
May  6 13:57:01.638: INFO: Waiting for statefulset status.replicas updated to 0
May  6 13:57:01.640: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 13:57:01.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-60" for this suite.
May  6 13:57:07.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 13:57:10.414: INFO: namespace statefulset-60 deletion completed in 8.762668741s

• [SLOW TEST:28.958 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 13:57:10.415: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8475
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-zhrm
STEP: Creating a pod to test atomic-volume-subpath
May  6 13:57:10.560: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zhrm" in namespace "subpath-8475" to be "success or failure"
May  6 13:57:10.562: INFO: Pod "pod-subpath-test-configmap-zhrm": Phase="Pending", Reason="", readiness=false. Elapsed: 1.916607ms
May  6 13:57:12.565: INFO: Pod "pod-subpath-test-configmap-zhrm": Phase="Running", Reason="", readiness=true. Elapsed: 2.004945421s
May  6 13:57:14.568: INFO: Pod "pod-subpath-test-configmap-zhrm": Phase="Running", Reason="", readiness=true. Elapsed: 4.007772723s
May  6 13:57:16.570: INFO: Pod "pod-subpath-test-configmap-zhrm": Phase="Running", Reason="", readiness=true. Elapsed: 6.010309995s
May  6 13:57:18.573: INFO: Pod "pod-subpath-test-configmap-zhrm": Phase="Running", Reason="", readiness=true. Elapsed: 8.012929273s
May  6 13:57:20.576: INFO: Pod "pod-subpath-test-configmap-zhrm": Phase="Running", Reason="", readiness=true. Elapsed: 10.015531785s
May  6 13:57:22.578: INFO: Pod "pod-subpath-test-configmap-zhrm": Phase="Running", Reason="", readiness=true. Elapsed: 12.018267816s
May  6 13:57:24.581: INFO: Pod "pod-subpath-test-configmap-zhrm": Phase="Running", Reason="", readiness=true. Elapsed: 14.021135891s
May  6 13:57:26.584: INFO: Pod "pod-subpath-test-configmap-zhrm": Phase="Running", Reason="", readiness=true. Elapsed: 16.023663425s
May  6 13:57:28.587: INFO: Pod "pod-subpath-test-configmap-zhrm": Phase="Running", Reason="", readiness=true. Elapsed: 18.026497868s
May  6 13:57:30.589: INFO: Pod "pod-subpath-test-configmap-zhrm": Phase="Running", Reason="", readiness=true. Elapsed: 20.02941237s
May  6 13:57:32.592: INFO: Pod "pod-subpath-test-configmap-zhrm": Phase="Running", Reason="", readiness=true. Elapsed: 22.032422896s
May  6 13:57:34.595: INFO: Pod "pod-subpath-test-configmap-zhrm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.035259979s
STEP: Saw pod success
May  6 13:57:34.595: INFO: Pod "pod-subpath-test-configmap-zhrm" satisfied condition "success or failure"
May  6 13:57:34.597: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-subpath-test-configmap-zhrm container test-container-subpath-configmap-zhrm: <nil>
STEP: delete the pod
May  6 13:57:34.616: INFO: Waiting for pod pod-subpath-test-configmap-zhrm to disappear
May  6 13:57:34.618: INFO: Pod pod-subpath-test-configmap-zhrm no longer exists
STEP: Deleting pod pod-subpath-test-configmap-zhrm
May  6 13:57:34.618: INFO: Deleting pod "pod-subpath-test-configmap-zhrm" in namespace "subpath-8475"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 13:57:34.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8475" for this suite.
May  6 13:57:40.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 13:57:43.382: INFO: namespace subpath-8475 deletion completed in 8.759433723s

• [SLOW TEST:32.967 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 13:57:43.382: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7291
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-aeeb8ba2-764f-4aa0-9044-61cde4ba4494
STEP: Creating secret with name secret-projected-all-test-volume-a7e5d5d4-b700-435e-9305-87c461c09850
STEP: Creating a pod to test Check all projections for projected volume plugin
May  6 13:57:43.525: INFO: Waiting up to 5m0s for pod "projected-volume-bb95d35c-a701-4419-ad42-3b595f18da95" in namespace "projected-7291" to be "success or failure"
May  6 13:57:43.527: INFO: Pod "projected-volume-bb95d35c-a701-4419-ad42-3b595f18da95": Phase="Pending", Reason="", readiness=false. Elapsed: 1.823601ms
May  6 13:57:45.530: INFO: Pod "projected-volume-bb95d35c-a701-4419-ad42-3b595f18da95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004357513s
May  6 13:57:47.536: INFO: Pod "projected-volume-bb95d35c-a701-4419-ad42-3b595f18da95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010978169s
STEP: Saw pod success
May  6 13:57:47.536: INFO: Pod "projected-volume-bb95d35c-a701-4419-ad42-3b595f18da95" satisfied condition "success or failure"
May  6 13:57:47.539: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod projected-volume-bb95d35c-a701-4419-ad42-3b595f18da95 container projected-all-volume-test: <nil>
STEP: delete the pod
May  6 13:57:47.551: INFO: Waiting for pod projected-volume-bb95d35c-a701-4419-ad42-3b595f18da95 to disappear
May  6 13:57:47.552: INFO: Pod projected-volume-bb95d35c-a701-4419-ad42-3b595f18da95 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 13:57:47.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7291" for this suite.
May  6 13:57:53.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 13:57:56.315: INFO: namespace projected-7291 deletion completed in 8.759559755s

• [SLOW TEST:12.933 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 13:57:56.315: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2528
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-2528
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May  6 13:57:56.445: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May  6 13:58:20.516: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.85.101:8080/dial?request=hostName&protocol=udp&host=192.168.121.63&port=8081&tries=1'] Namespace:pod-network-test-2528 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 13:58:20.516: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 13:58:20.688: INFO: Waiting for endpoints: map[]
May  6 13:58:20.691: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.85.101:8080/dial?request=hostName&protocol=udp&host=192.168.85.100&port=8081&tries=1'] Namespace:pod-network-test-2528 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 13:58:20.691: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 13:58:20.863: INFO: Waiting for endpoints: map[]
May  6 13:58:20.866: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.85.101:8080/dial?request=hostName&protocol=udp&host=192.168.187.32&port=8081&tries=1'] Namespace:pod-network-test-2528 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 13:58:20.866: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 13:58:21.044: INFO: Waiting for endpoints: map[]
May  6 13:58:21.046: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.85.101:8080/dial?request=hostName&protocol=udp&host=192.168.80.166&port=8081&tries=1'] Namespace:pod-network-test-2528 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 13:58:21.046: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 13:58:21.214: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 13:58:21.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2528" for this suite.
May  6 13:58:33.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 13:58:35.977: INFO: namespace pod-network-test-2528 deletion completed in 14.759676236s

• [SLOW TEST:39.662 seconds]
[sig-network] Networking
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 13:58:35.977: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6141
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-1081c2ba-0c96-4b49-8f09-15a03b22d883
STEP: Creating a pod to test consume configMaps
May  6 13:58:36.121: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-acba5602-8312-48ef-b140-90222b40fd90" in namespace "projected-6141" to be "success or failure"
May  6 13:58:36.123: INFO: Pod "pod-projected-configmaps-acba5602-8312-48ef-b140-90222b40fd90": Phase="Pending", Reason="", readiness=false. Elapsed: 1.810037ms
May  6 13:58:38.130: INFO: Pod "pod-projected-configmaps-acba5602-8312-48ef-b140-90222b40fd90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009315726s
STEP: Saw pod success
May  6 13:58:38.130: INFO: Pod "pod-projected-configmaps-acba5602-8312-48ef-b140-90222b40fd90" satisfied condition "success or failure"
May  6 13:58:38.132: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-projected-configmaps-acba5602-8312-48ef-b140-90222b40fd90 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  6 13:58:38.147: INFO: Waiting for pod pod-projected-configmaps-acba5602-8312-48ef-b140-90222b40fd90 to disappear
May  6 13:58:38.150: INFO: Pod pod-projected-configmaps-acba5602-8312-48ef-b140-90222b40fd90 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 13:58:38.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6141" for this suite.
May  6 13:58:44.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 13:58:47.107: INFO: namespace projected-6141 deletion completed in 8.953986482s

• [SLOW TEST:11.130 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 13:58:47.108: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6897
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
May  6 13:58:53.308: INFO: 10 pods remaining
May  6 13:58:53.308: INFO: 10 pods has nil DeletionTimestamp
May  6 13:58:53.308: INFO: 
STEP: Gathering metrics
May  6 13:58:54.311: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0506 13:58:54.311114      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 13:58:54.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6897" for this suite.
May  6 13:59:00.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 13:59:03.077: INFO: namespace gc-6897 deletion completed in 8.763283933s

• [SLOW TEST:15.969 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 13:59:03.077: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3810
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May  6 13:59:03.215: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3810 /api/v1/namespaces/watch-3810/configmaps/e2e-watch-test-watch-closed c0fe8b42-37fd-45cc-9a05-3df723966787 83915 0 2020-05-06 13:59:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May  6 13:59:03.215: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3810 /api/v1/namespaces/watch-3810/configmaps/e2e-watch-test-watch-closed c0fe8b42-37fd-45cc-9a05-3df723966787 83916 0 2020-05-06 13:59:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May  6 13:59:03.225: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3810 /api/v1/namespaces/watch-3810/configmaps/e2e-watch-test-watch-closed c0fe8b42-37fd-45cc-9a05-3df723966787 83917 0 2020-05-06 13:59:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May  6 13:59:03.225: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3810 /api/v1/namespaces/watch-3810/configmaps/e2e-watch-test-watch-closed c0fe8b42-37fd-45cc-9a05-3df723966787 83918 0 2020-05-06 13:59:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 13:59:03.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3810" for this suite.
May  6 13:59:09.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 13:59:11.986: INFO: namespace watch-3810 deletion completed in 8.758941823s

• [SLOW TEST:8.909 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 13:59:11.986: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-904
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 13:59:12.123: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May  6 13:59:17.126: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May  6 13:59:17.126: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May  6 13:59:17.139: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-904 /apis/apps/v1/namespaces/deployment-904/deployments/test-cleanup-deployment 9ce527cb-b28d-4da8-9df0-b83062be8861 84022 1 2020-05-06 13:59:17 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003138868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

May  6 13:59:17.141: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
May  6 13:59:17.141: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May  6 13:59:17.141: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-904 /apis/apps/v1/namespaces/deployment-904/replicasets/test-cleanup-controller 727c0a4d-f517-4f8a-b7fe-bcaea825f78d 84023 1 2020-05-06 13:59:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 9ce527cb-b28d-4da8-9df0-b83062be8861 0xc003138be7 0xc003138be8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003138c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  6 13:59:17.143: INFO: Pod "test-cleanup-controller-w4j6n" is available:
&Pod{ObjectMeta:{test-cleanup-controller-w4j6n test-cleanup-controller- deployment-904 /api/v1/namespaces/deployment-904/pods/test-cleanup-controller-w4j6n ad146feb-adbd-4275-86b2-88bc6c2eb2a6 83998 0 2020-05-06 13:59:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:192.168.85.110/32 cni.projectcalico.org/podIPs:192.168.85.110/32] [{apps/v1 ReplicaSet test-cleanup-controller 727c0a4d-f517-4f8a-b7fe-bcaea825f78d 0xc003138f57 0xc003138f58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-l4q6s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-l4q6s,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-l4q6s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-80.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 13:59:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 13:59:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 13:59:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 13:59:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.80,PodIP:192.168.85.110,StartTime:2020-05-06 13:59:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 13:59:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://4640db5a10a37941701de5229ed54ece9af4e6497a92335c20110fce1f84e525,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.85.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 13:59:17.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-904" for this suite.
May  6 13:59:23.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 13:59:26.039: INFO: namespace deployment-904 deletion completed in 8.892946249s

• [SLOW TEST:14.053 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 13:59:26.039: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7333
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May  6 13:59:26.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-7333'
May  6 13:59:26.400: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May  6 13:59:26.400: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
May  6 13:59:28.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 delete deployment e2e-test-httpd-deployment --namespace=kubectl-7333'
May  6 13:59:28.520: INFO: stderr: ""
May  6 13:59:28.520: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 13:59:28.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7333" for this suite.
May  6 13:59:34.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 13:59:37.283: INFO: namespace kubectl-7333 deletion completed in 8.759635504s

• [SLOW TEST:11.244 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 13:59:37.284: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8172
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 13:59:37.783: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 13:59:40.800: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 13:59:40.803: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 13:59:41.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8172" for this suite.
May  6 13:59:47.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 13:59:50.660: INFO: namespace webhook-8172 deletion completed in 8.761913501s
STEP: Destroying namespace "webhook-8172-markers" for this suite.
May  6 13:59:56.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 13:59:59.447: INFO: namespace webhook-8172-markers deletion completed in 8.786839016s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:22.173 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 13:59:59.457: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3124
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May  6 13:59:59.607: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 13:59:59.607: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 13:59:59.607: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 13:59:59.609: INFO: Number of nodes with available pods: 0
May  6 13:59:59.609: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 14:00:00.613: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:00.613: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:00.613: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:00.616: INFO: Number of nodes with available pods: 0
May  6 14:00:00.616: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 14:00:01.613: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:01.613: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:01.613: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:01.619: INFO: Number of nodes with available pods: 2
May  6 14:00:01.619: INFO: Node ip-10-0-131-234.us-west-2.compute.internal is running more than one daemon pod
May  6 14:00:02.613: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:02.613: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:02.613: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:02.615: INFO: Number of nodes with available pods: 4
May  6 14:00:02.615: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Stop a daemon pod, check that the daemon pod is revived.
May  6 14:00:02.627: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:02.627: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:02.627: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:02.629: INFO: Number of nodes with available pods: 3
May  6 14:00:02.629: INFO: Node ip-10-0-129-108.us-west-2.compute.internal is running more than one daemon pod
May  6 14:00:03.633: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:03.633: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:03.633: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:03.635: INFO: Number of nodes with available pods: 3
May  6 14:00:03.635: INFO: Node ip-10-0-129-108.us-west-2.compute.internal is running more than one daemon pod
May  6 14:00:04.633: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:04.633: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:04.633: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:04.635: INFO: Number of nodes with available pods: 3
May  6 14:00:04.635: INFO: Node ip-10-0-129-108.us-west-2.compute.internal is running more than one daemon pod
May  6 14:00:05.633: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:05.633: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:05.633: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:05.635: INFO: Number of nodes with available pods: 3
May  6 14:00:05.635: INFO: Node ip-10-0-129-108.us-west-2.compute.internal is running more than one daemon pod
May  6 14:00:06.632: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:06.633: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:06.633: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:06.635: INFO: Number of nodes with available pods: 3
May  6 14:00:06.635: INFO: Node ip-10-0-129-108.us-west-2.compute.internal is running more than one daemon pod
May  6 14:00:07.633: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:07.633: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:07.633: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:00:07.636: INFO: Number of nodes with available pods: 4
May  6 14:00:07.636: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3124, will wait for the garbage collector to delete the pods
May  6 14:00:07.695: INFO: Deleting DaemonSet.extensions daemon-set took: 5.022862ms
May  6 14:00:09.095: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.400158396s
May  6 14:00:17.198: INFO: Number of nodes with available pods: 0
May  6 14:00:17.198: INFO: Number of running nodes: 0, number of available pods: 0
May  6 14:00:17.202: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3124/daemonsets","resourceVersion":"84635"},"items":null}

May  6 14:00:17.203: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3124/pods","resourceVersion":"84635"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:00:17.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3124" for this suite.
May  6 14:00:23.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:00:25.976: INFO: namespace daemonsets-3124 deletion completed in 8.759692512s

• [SLOW TEST:26.519 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:00:25.976: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-999
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3193
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4911
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:00:39.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-999" for this suite.
May  6 14:00:45.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:00:48.204: INFO: namespace namespaces-999 deletion completed in 8.761500631s
STEP: Destroying namespace "nsdeletetest-3193" for this suite.
May  6 14:00:48.206: INFO: Namespace nsdeletetest-3193 was already deleted
STEP: Destroying namespace "nsdeletetest-4911" for this suite.
May  6 14:00:54.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:00:56.966: INFO: namespace nsdeletetest-4911 deletion completed in 8.760950176s

• [SLOW TEST:30.991 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:00:56.967: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-380
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-380.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-380.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-380.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-380.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-380.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-380.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-380.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-380.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-380.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-380.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-380.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-380.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-380.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-380.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-380.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-380.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-380.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-380.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  6 14:01:01.142: INFO: DNS probes using dns-380/dns-test-3cbb18e8-81e1-4f1d-a2d3-cb3fdc94138a succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:01:01.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-380" for this suite.
May  6 14:01:07.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:01:09.929: INFO: namespace dns-380 deletion completed in 8.762736922s

• [SLOW TEST:12.963 seconds]
[sig-network] DNS
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:01:09.929: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1101
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-1f09f2a3-6c9c-4927-bcf6-7b15c24ee744
STEP: Creating a pod to test consume configMaps
May  6 14:01:10.071: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8199d5ba-fdd2-45a3-863f-699e641a902a" in namespace "projected-1101" to be "success or failure"
May  6 14:01:10.072: INFO: Pod "pod-projected-configmaps-8199d5ba-fdd2-45a3-863f-699e641a902a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.800414ms
May  6 14:01:12.075: INFO: Pod "pod-projected-configmaps-8199d5ba-fdd2-45a3-863f-699e641a902a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004170995s
STEP: Saw pod success
May  6 14:01:12.075: INFO: Pod "pod-projected-configmaps-8199d5ba-fdd2-45a3-863f-699e641a902a" satisfied condition "success or failure"
May  6 14:01:12.077: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-projected-configmaps-8199d5ba-fdd2-45a3-863f-699e641a902a container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  6 14:01:12.094: INFO: Waiting for pod pod-projected-configmaps-8199d5ba-fdd2-45a3-863f-699e641a902a to disappear
May  6 14:01:12.096: INFO: Pod pod-projected-configmaps-8199d5ba-fdd2-45a3-863f-699e641a902a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:01:12.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1101" for this suite.
May  6 14:01:18.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:01:20.858: INFO: namespace projected-1101 deletion completed in 8.759919192s

• [SLOW TEST:10.929 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:01:20.859: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1472
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-26a0ebc7-65b5-4b7e-a8ca-bcadeae42def
STEP: Creating a pod to test consume configMaps
May  6 14:01:21.004: INFO: Waiting up to 5m0s for pod "pod-configmaps-71fe630d-8826-4ed2-9ae5-215f1b9ca43a" in namespace "configmap-1472" to be "success or failure"
May  6 14:01:21.008: INFO: Pod "pod-configmaps-71fe630d-8826-4ed2-9ae5-215f1b9ca43a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.869736ms
May  6 14:01:23.011: INFO: Pod "pod-configmaps-71fe630d-8826-4ed2-9ae5-215f1b9ca43a": Phase="Running", Reason="", readiness=true. Elapsed: 2.006744073s
May  6 14:01:25.014: INFO: Pod "pod-configmaps-71fe630d-8826-4ed2-9ae5-215f1b9ca43a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00941724s
STEP: Saw pod success
May  6 14:01:25.014: INFO: Pod "pod-configmaps-71fe630d-8826-4ed2-9ae5-215f1b9ca43a" satisfied condition "success or failure"
May  6 14:01:25.016: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-configmaps-71fe630d-8826-4ed2-9ae5-215f1b9ca43a container configmap-volume-test: <nil>
STEP: delete the pod
May  6 14:01:25.027: INFO: Waiting for pod pod-configmaps-71fe630d-8826-4ed2-9ae5-215f1b9ca43a to disappear
May  6 14:01:25.028: INFO: Pod pod-configmaps-71fe630d-8826-4ed2-9ae5-215f1b9ca43a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:01:25.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1472" for this suite.
May  6 14:01:31.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:01:33.801: INFO: namespace configmap-1472 deletion completed in 8.769520652s

• [SLOW TEST:12.942 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:01:33.801: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2114
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:01:50.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2114" for this suite.
May  6 14:01:56.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:01:59.743: INFO: namespace resourcequota-2114 deletion completed in 8.768947262s

• [SLOW TEST:25.942 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:01:59.743: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-329
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 14:02:00.277: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 14:02:03.291: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:02:03.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-329" for this suite.
May  6 14:02:09.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:02:12.193: INFO: namespace webhook-329 deletion completed in 8.769057904s
STEP: Destroying namespace "webhook-329-markers" for this suite.
May  6 14:02:18.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:02:20.959: INFO: namespace webhook-329-markers deletion completed in 8.765936749s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.226 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:02:20.969: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6283
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-0adedfd1-8989-4dbf-b9c7-be119144dd30
STEP: Creating a pod to test consume configMaps
May  6 14:02:21.121: INFO: Waiting up to 5m0s for pod "pod-configmaps-a443e080-d6ea-4e0e-a4b6-9cd16cdb7d8d" in namespace "configmap-6283" to be "success or failure"
May  6 14:02:21.123: INFO: Pod "pod-configmaps-a443e080-d6ea-4e0e-a4b6-9cd16cdb7d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.975901ms
May  6 14:02:23.126: INFO: Pod "pod-configmaps-a443e080-d6ea-4e0e-a4b6-9cd16cdb7d8d": Phase="Running", Reason="", readiness=true. Elapsed: 2.004488962s
May  6 14:02:25.128: INFO: Pod "pod-configmaps-a443e080-d6ea-4e0e-a4b6-9cd16cdb7d8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007150831s
STEP: Saw pod success
May  6 14:02:25.128: INFO: Pod "pod-configmaps-a443e080-d6ea-4e0e-a4b6-9cd16cdb7d8d" satisfied condition "success or failure"
May  6 14:02:25.130: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-configmaps-a443e080-d6ea-4e0e-a4b6-9cd16cdb7d8d container configmap-volume-test: <nil>
STEP: delete the pod
May  6 14:02:25.142: INFO: Waiting for pod pod-configmaps-a443e080-d6ea-4e0e-a4b6-9cd16cdb7d8d to disappear
May  6 14:02:25.144: INFO: Pod pod-configmaps-a443e080-d6ea-4e0e-a4b6-9cd16cdb7d8d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:02:25.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6283" for this suite.
May  6 14:02:31.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:02:33.912: INFO: namespace configmap-6283 deletion completed in 8.759569805s

• [SLOW TEST:12.943 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:02:33.913: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3425
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:02:38.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3425" for this suite.
May  6 14:03:24.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:03:26.830: INFO: namespace kubelet-test-3425 deletion completed in 48.760421075s

• [SLOW TEST:52.917 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:03:26.830: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7315
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:03:30.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7315" for this suite.
May  6 14:04:18.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:04:21.745: INFO: namespace kubelet-test-7315 deletion completed in 50.761290195s

• [SLOW TEST:54.915 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:04:21.745: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7884
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 14:04:21.902: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"3f25ef16-345a-454a-8ac1-c756aadad746", Controller:(*bool)(0xc002ecd6ee), BlockOwnerDeletion:(*bool)(0xc002ecd6ef)}}
May  6 14:04:21.910: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"db8a1174-ae22-4d0e-8b4b-11db5492b6cb", Controller:(*bool)(0xc002d8bd1e), BlockOwnerDeletion:(*bool)(0xc002d8bd1f)}}
May  6 14:04:21.917: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"622cd807-254f-497d-bef7-26af630641c6", Controller:(*bool)(0xc002ecd89e), BlockOwnerDeletion:(*bool)(0xc002ecd89f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:04:26.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7884" for this suite.
May  6 14:04:32.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:04:35.691: INFO: namespace gc-7884 deletion completed in 8.761189563s

• [SLOW TEST:13.946 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:04:35.692: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5920
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May  6 14:04:35.822: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  6 14:04:35.831: INFO: Waiting for terminating namespaces to be deleted...
May  6 14:04:35.833: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-129-1.us-west-2.compute.internal before test
May  6 14:04:35.845: INFO: elasticsearch-kubeaddons-data-1 from kubeaddons started at 2020-05-06 11:01:19 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.845: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 14:04:35.845: INFO: coredns-5644d7b6d9-5jf9p from kube-system started at 2020-05-06 10:56:00 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.845: INFO: 	Container coredns ready: true, restart count 0
May  6 14:04:35.845: INFO: kommander-kubeaddons-kubeaddons-catalog-6654f856df-ccmkl from kommander started at 2020-05-06 11:00:36 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.845: INFO: 	Container kubeaddons-catalog ready: true, restart count 0
May  6 14:04:35.845: INFO: kcl-utility-apiserver-5d448f665b-gf6gv from kommander started at 2020-05-06 11:00:36 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.845: INFO: 	Container server ready: true, restart count 0
May  6 14:04:35.845: INFO: minio-3 from velero started at 2020-05-06 10:59:47 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.845: INFO: 	Container minio ready: true, restart count 0
May  6 14:04:35.845: INFO: sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-9wkcg from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 14:04:35.845: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 14:04:35.845: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 14:04:35.845: INFO: reloader-kubeaddons-reloader-7d4bd64cfb-xnxz8 from kubeaddons started at 2020-05-06 10:56:23 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.845: INFO: 	Container reloader-kubeaddons-reloader ready: true, restart count 0
May  6 14:04:35.845: INFO: kubefed-controller-manager-55fd5b4dff-bvsl6 from kommander started at 2020-05-06 11:00:46 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.845: INFO: 	Container controller-manager ready: true, restart count 0
May  6 14:04:35.845: INFO: prometheus-kubeaddons-grafana-99b49bd54-8h7z8 from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (2 container statuses recorded)
May  6 14:04:35.845: INFO: 	Container grafana ready: true, restart count 0
May  6 14:04:35.845: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
May  6 14:04:35.845: INFO: fluentbit-kubeaddons-fluent-bit-5rbrr from kubeaddons started at 2020-05-06 11:02:11 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.845: INFO: 	Container fluent-bit ready: true, restart count 0
May  6 14:04:35.845: INFO: cert-manager-kubeaddons-7d7f98fbc6-qpl75 from cert-manager started at 2020-05-06 10:56:59 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.845: INFO: 	Container cert-manager ready: true, restart count 0
May  6 14:04:35.845: INFO: ebs-csi-node-f2cbn from kube-system started at 2020-05-06 10:57:49 +0000 UTC (3 container statuses recorded)
May  6 14:04:35.845: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 14:04:35.845: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 14:04:35.845: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  6 14:04:35.845: INFO: traefik-kubeaddons-6f97669977-2sxh8 from kubeaddons started at 2020-05-06 10:58:20 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.845: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
May  6 14:04:35.845: INFO: prometheus-kubeaddons-prometheus-node-exporter-b6z8v from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.845: INFO: 	Container node-exporter ready: true, restart count 0
May  6 14:04:35.845: INFO: kcl-cm-6588c896f9-4b2gk from kommander started at 2020-05-06 11:00:36 +0000 UTC (2 container statuses recorded)
May  6 14:04:35.845: INFO: 	Container controller-manager ready: true, restart count 0
May  6 14:04:35.845: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  6 14:04:35.845: INFO: kube-proxy-v9c2p from kube-system started at 2020-05-06 10:55:20 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.845: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 14:04:35.845: INFO: calico-node-wpbh7 from kube-system started at 2020-05-06 10:55:38 +0000 UTC (2 container statuses recorded)
May  6 14:04:35.845: INFO: 	Container bird-metrics ready: true, restart count 0
May  6 14:04:35.845: INFO: 	Container calico-node ready: true, restart count 0
May  6 14:04:35.845: INFO: cert-manager-kubeaddons-webhook-77fbc6d59b-fbbjd from cert-manager started at 2020-05-06 10:56:59 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.845: INFO: 	Container cert-manager ready: true, restart count 1
May  6 14:04:35.845: INFO: elasticsearch-kubeaddons-master-1 from kubeaddons started at 2020-05-06 10:59:19 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.845: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 14:04:35.845: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-129-108.us-west-2.compute.internal before test
May  6 14:04:35.859: INFO: kommander-kubeaddons-grafana-645f957f8f-96sf5 from kommander started at 2020-05-06 11:00:36 +0000 UTC (2 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container grafana ready: true, restart count 0
May  6 14:04:35.859: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
May  6 14:04:35.859: INFO: kubefed-admission-webhook-5fb847574f-6mjm2 from kommander started at 2020-05-06 11:00:41 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container admission-webhook ready: true, restart count 0
May  6 14:04:35.859: INFO: fluentbit-kubeaddons-fluent-bit-t6xpz from kubeaddons started at 2020-05-06 11:02:11 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container fluent-bit ready: true, restart count 0
May  6 14:04:35.859: INFO: elasticsearch-kubeaddons-client-d4f5db58d-7qrn8 from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 14:04:35.859: INFO: calico-node-z679x from kube-system started at 2020-05-06 10:55:38 +0000 UTC (2 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container bird-metrics ready: true, restart count 0
May  6 14:04:35.859: INFO: 	Container calico-node ready: true, restart count 0
May  6 14:04:35.859: INFO: prometheus-kubeaddons-prometheus-node-exporter-2wrg9 from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container node-exporter ready: true, restart count 0
May  6 14:04:35.859: INFO: kommander-kubeaddons-karma-7688944fcf-hpnzl from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container karma ready: true, restart count 0
May  6 14:04:35.859: INFO: external-dns-kubeaddons-6c65ff8d88-sm9p2 from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container external-dns ready: true, restart count 0
May  6 14:04:35.859: INFO: calico-kube-controllers-84f666455-5t7xm from kube-system started at 2020-05-06 10:55:49 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  6 14:04:35.859: INFO: minio-2 from velero started at 2020-05-06 10:59:44 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container minio ready: true, restart count 0
May  6 14:04:35.859: INFO: traefik-kubeaddons-6f97669977-9fwr9 from kubeaddons started at 2020-05-06 10:57:49 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
May  6 14:04:35.859: INFO: elasticsearch-kubeaddons-data-0 from kubeaddons started at 2020-05-06 10:58:15 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 14:04:35.859: INFO: elasticsearchexporter-kubeaddons-elasticsearch-exporter-845tdq8 from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container elasticsearch-exporter ready: true, restart count 0
May  6 14:04:35.859: INFO: kube-proxy-4lqz5 from kube-system started at 2020-05-06 10:55:19 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 14:04:35.859: INFO: coredns-5644d7b6d9-hmtfs from kube-system started at 2020-05-06 10:56:00 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container coredns ready: true, restart count 0
May  6 14:04:35.859: INFO: sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-zqwx9 from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 14:04:35.859: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 14:04:35.859: INFO: kcl-tfcb-9c88c7bc9-lbgz6 from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container server ready: true, restart count 0
May  6 14:04:35.859: INFO: kcl-webhook-fb4dfc7d7-vv757 from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container webhook ready: true, restart count 0
May  6 14:04:35.859: INFO: kubernetes-dashboard-549989bcdf-2d767 from kubeaddons started at 2020-05-06 10:56:34 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
May  6 14:04:35.859: INFO: kibana-kubeaddons-649897648c-mzc2h from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (2 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container initialize-kibana-index ready: true, restart count 0
May  6 14:04:35.859: INFO: 	Container kibana ready: true, restart count 0
May  6 14:04:35.859: INFO: kubeaddons-controller-manager-77cf76b857-k89ml from kubeaddons started at 2020-05-06 10:55:49 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container manager ready: true, restart count 0
May  6 14:04:35.859: INFO: ebs-csi-node-vvbxt from kube-system started at 2020-05-06 10:57:49 +0000 UTC (3 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 14:04:35.859: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 14:04:35.859: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  6 14:04:35.859: INFO: cert-manager-kubeaddons-cainjector-6dcd94769b-ng6xl from cert-manager started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container cainjector ready: true, restart count 0
May  6 14:04:35.859: INFO: elasticsearch-kubeaddons-master-2 from kubeaddons started at 2020-05-06 11:00:07 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.859: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 14:04:35.859: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-131-234.us-west-2.compute.internal before test
May  6 14:04:35.872: INFO: dex-kubeaddons-dex-controller-6b6c9fbd7f-pxz8h from kubeaddons started at 2020-05-06 10:58:46 +0000 UTC (2 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  6 14:04:35.872: INFO: 	Container manager ready: true, restart count 0
May  6 14:04:35.872: INFO: kubefed-controller-manager-55fd5b4dff-g9m6m from kommander started at 2020-05-06 11:00:41 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container controller-manager ready: true, restart count 0
May  6 14:04:35.872: INFO: traefik-forward-auth-kubeaddons-8bdddfcd-wdrjp from kubeaddons started at 2020-05-06 10:59:51 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container traefik-forward-auth ready: true, restart count 0
May  6 14:04:35.872: INFO: traefik-kubeaddons-1.72.17-5pttd from kubeaddons started at 2020-05-06 10:57:38 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container traefik ready: false, restart count 0
May  6 14:04:35.872: INFO: prometheus-kubeaddons-kube-state-metrics-78c65cc7bc-66zk8 from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container kube-state-metrics ready: true, restart count 0
May  6 14:04:35.872: INFO: kube-oidc-proxy-kubeaddons-6fbd5c8fc4-xtmsg from kubeaddons started at 2020-05-06 10:59:18 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container kube-oidc-proxy ready: true, restart count 0
May  6 14:04:35.872: INFO: alertmanager-prometheus-kubeaddons-prom-alertmanager-0 from kubeaddons started at 2020-05-06 10:59:03 +0000 UTC (2 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container alertmanager ready: true, restart count 0
May  6 14:04:35.872: INFO: 	Container config-reloader ready: true, restart count 0
May  6 14:04:35.872: INFO: tiller-deploy-969865475-m2ph8 from kube-system started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container tiller ready: true, restart count 0
May  6 14:04:35.872: INFO: prometheusadapter-kubeaddons-prometheus-adapter-6b8975fc48pwzsr from kubeaddons started at 2020-05-06 10:59:55 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container prometheus-adapter ready: true, restart count 0
May  6 14:04:35.872: INFO: dstorageclass-controller-manager-5c966c767f-4bf6x from kubeaddons started at 2020-05-06 10:57:33 +0000 UTC (2 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  6 14:04:35.872: INFO: 	Container manager ready: true, restart count 0
May  6 14:04:35.872: INFO: opsportal-kubeaddons-kommander-ui-679db5d9cb-62psh from kubeaddons started at 2020-05-06 10:56:45 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container opsportal-kubeaddons-kommander-ui ready: true, restart count 0
May  6 14:04:35.872: INFO: dex-kubeaddons-79b77778bc-84p4b from kubeaddons started at 2020-05-06 10:59:20 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container main ready: true, restart count 1
May  6 14:04:35.872: INFO: velero-kubeaddons-578b4667ff-6x2nw from velero started at 2020-05-06 10:59:40 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container velero ready: true, restart count 4
May  6 14:04:35.872: INFO: dex-k8s-authenticator-kubeaddons-6697855d4c-xbqks from kubeaddons started at 2020-05-06 10:59:29 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container dex-k8s-authenticator ready: true, restart count 0
May  6 14:04:35.872: INFO: fluentbit-kubeaddons-fluent-bit-wnqvd from kubeaddons started at 2020-05-06 11:02:11 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container fluent-bit ready: true, restart count 0
May  6 14:04:35.872: INFO: opsportal-landing-6f6865b688-vb67f from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container opsportal-landing ready: true, restart count 0
May  6 14:04:35.872: INFO: kommander-kubeaddons-kommander-ui-7b9b585d95-rgnz6 from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container kommander-kubeaddons-kommander-ui ready: true, restart count 0
May  6 14:04:35.872: INFO: ebs-csi-node-ffszp from kube-system started at 2020-05-06 10:57:49 +0000 UTC (3 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 14:04:35.872: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 14:04:35.872: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  6 14:04:35.872: INFO: prometheus-kubeaddons-prom-operator-cc56c6c-m8kfx from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (2 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container prometheus-operator ready: true, restart count 0
May  6 14:04:35.872: INFO: 	Container tls-proxy ready: true, restart count 0
May  6 14:04:35.872: INFO: prometheus-kubeaddons-prometheus-node-exporter-4jtfq from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container node-exporter ready: true, restart count 0
May  6 14:04:35.872: INFO: kommander-kubeaddons-thanos-query-7597bf4957-tmdgd from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container thanos-query ready: true, restart count 0
May  6 14:04:35.872: INFO: minio-0 from velero started at 2020-05-06 10:59:44 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container minio ready: true, restart count 0
May  6 14:04:35.872: INFO: kube-proxy-ptcmp from kube-system started at 2020-05-06 10:55:19 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 14:04:35.872: INFO: calico-node-gm4h2 from kube-system started at 2020-05-06 10:55:38 +0000 UTC (2 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container bird-metrics ready: true, restart count 0
May  6 14:04:35.872: INFO: 	Container calico-node ready: true, restart count 0
May  6 14:04:35.872: INFO: elasticsearch-kubeaddons-client-d4f5db58d-7g4sd from kubeaddons started at 2020-05-06 10:57:52 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 14:04:35.872: INFO: gatekeeper-kubeaddons-fdc87db85-5rj6d from kubeaddons started at 2020-05-06 10:57:40 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container manager ready: true, restart count 0
May  6 14:04:35.872: INFO: sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-hqgql from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 14:04:35.872: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 14:04:35.872: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 14:04:35.872: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-131-80.us-west-2.compute.internal before test
May  6 14:04:35.878: INFO: fluentbit-kubeaddons-fluent-bit-w966k from kubeaddons started at 2020-05-06 13:31:20 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.878: INFO: 	Container fluent-bit ready: true, restart count 0
May  6 14:04:35.878: INFO: ebs-csi-controller-0 from kube-system started at 2020-05-06 13:31:16 +0000 UTC (5 container statuses recorded)
May  6 14:04:35.878: INFO: 	Container csi-attacher ready: true, restart count 0
May  6 14:04:35.878: INFO: 	Container csi-provisioner ready: true, restart count 0
May  6 14:04:35.878: INFO: 	Container csi-snapshotter ready: true, restart count 0
May  6 14:04:35.878: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 14:04:35.878: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 14:04:35.878: INFO: minio-1 from velero started at 2020-05-06 13:31:21 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.878: INFO: 	Container minio ready: true, restart count 0
May  6 14:04:35.878: INFO: prometheus-kubeaddons-prometheus-node-exporter-2qwk2 from kubeaddons started at 2020-05-06 13:32:37 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.878: INFO: 	Container node-exporter ready: true, restart count 0
May  6 14:04:35.878: INFO: sonobuoy from sonobuoy started at 2020-05-06 13:56:26 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.878: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  6 14:04:35.878: INFO: sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-cvh6w from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 14:04:35.878: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 14:04:35.878: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 14:04:35.878: INFO: calico-node-mrkdh from kube-system started at 2020-05-06 10:55:38 +0000 UTC (2 container statuses recorded)
May  6 14:04:35.878: INFO: 	Container bird-metrics ready: true, restart count 0
May  6 14:04:35.878: INFO: 	Container calico-node ready: true, restart count 0
May  6 14:04:35.878: INFO: sonobuoy-e2e-job-7496ec37786a4f0c from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 14:04:35.878: INFO: 	Container e2e ready: true, restart count 0
May  6 14:04:35.878: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 14:04:35.878: INFO: prometheus-prometheus-kubeaddons-prom-prometheus-0 from kubeaddons started at 2020-05-06 13:31:17 +0000 UTC (4 container statuses recorded)
May  6 14:04:35.878: INFO: 	Container prometheus ready: true, restart count 0
May  6 14:04:35.878: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May  6 14:04:35.878: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May  6 14:04:35.878: INFO: 	Container thanos-sidecar ready: true, restart count 0
May  6 14:04:35.878: INFO: ebs-csi-node-722lq from kube-system started at 2020-05-06 10:57:49 +0000 UTC (3 container statuses recorded)
May  6 14:04:35.878: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 14:04:35.878: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 14:04:35.878: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  6 14:04:35.878: INFO: kube-proxy-vmxhd from kube-system started at 2020-05-06 10:55:19 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.878: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 14:04:35.878: INFO: elasticsearch-kubeaddons-master-0 from kubeaddons started at 2020-05-06 13:31:19 +0000 UTC (1 container statuses recorded)
May  6 14:04:35.878: INFO: 	Container elasticsearch ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.160c756c2595fdb2], Reason = [FailedScheduling], Message = [0/7 nodes are available: 7 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:04:36.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5920" for this suite.
May  6 14:04:42.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:04:45.669: INFO: namespace sched-pred-5920 deletion completed in 8.760499482s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:9.977 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:04:45.669: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5821
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 14:04:45.798: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:04:46.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5821" for this suite.
May  6 14:04:52.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:04:55.645: INFO: namespace custom-resource-definition-5821 deletion completed in 8.812436357s

• [SLOW TEST:9.976 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:04:55.645: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1359
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 14:04:55.783: INFO: Waiting up to 5m0s for pod "busybox-user-65534-5e40ff7d-6cc9-4be2-87fe-b6c58547266b" in namespace "security-context-test-1359" to be "success or failure"
May  6 14:04:55.784: INFO: Pod "busybox-user-65534-5e40ff7d-6cc9-4be2-87fe-b6c58547266b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.803307ms
May  6 14:04:57.787: INFO: Pod "busybox-user-65534-5e40ff7d-6cc9-4be2-87fe-b6c58547266b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00425463s
May  6 14:04:57.787: INFO: Pod "busybox-user-65534-5e40ff7d-6cc9-4be2-87fe-b6c58547266b" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:04:57.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1359" for this suite.
May  6 14:05:03.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:05:06.550: INFO: namespace security-context-test-1359 deletion completed in 8.760098292s

• [SLOW TEST:10.905 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:05:06.550: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9033
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-8vt7
STEP: Creating a pod to test atomic-volume-subpath
May  6 14:05:06.696: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8vt7" in namespace "subpath-9033" to be "success or failure"
May  6 14:05:06.700: INFO: Pod "pod-subpath-test-configmap-8vt7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.172454ms
May  6 14:05:08.703: INFO: Pod "pod-subpath-test-configmap-8vt7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006826069s
May  6 14:05:10.706: INFO: Pod "pod-subpath-test-configmap-8vt7": Phase="Running", Reason="", readiness=true. Elapsed: 4.009639331s
May  6 14:05:12.708: INFO: Pod "pod-subpath-test-configmap-8vt7": Phase="Running", Reason="", readiness=true. Elapsed: 6.012378173s
May  6 14:05:14.711: INFO: Pod "pod-subpath-test-configmap-8vt7": Phase="Running", Reason="", readiness=true. Elapsed: 8.015405412s
May  6 14:05:16.714: INFO: Pod "pod-subpath-test-configmap-8vt7": Phase="Running", Reason="", readiness=true. Elapsed: 10.018136706s
May  6 14:05:18.717: INFO: Pod "pod-subpath-test-configmap-8vt7": Phase="Running", Reason="", readiness=true. Elapsed: 12.020542548s
May  6 14:05:20.719: INFO: Pod "pod-subpath-test-configmap-8vt7": Phase="Running", Reason="", readiness=true. Elapsed: 14.023301931s
May  6 14:05:22.722: INFO: Pod "pod-subpath-test-configmap-8vt7": Phase="Running", Reason="", readiness=true. Elapsed: 16.025836965s
May  6 14:05:24.725: INFO: Pod "pod-subpath-test-configmap-8vt7": Phase="Running", Reason="", readiness=true. Elapsed: 18.02860238s
May  6 14:05:26.727: INFO: Pod "pod-subpath-test-configmap-8vt7": Phase="Running", Reason="", readiness=true. Elapsed: 20.031106658s
May  6 14:05:28.730: INFO: Pod "pod-subpath-test-configmap-8vt7": Phase="Running", Reason="", readiness=true. Elapsed: 22.033441945s
May  6 14:05:30.732: INFO: Pod "pod-subpath-test-configmap-8vt7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.036152756s
STEP: Saw pod success
May  6 14:05:30.732: INFO: Pod "pod-subpath-test-configmap-8vt7" satisfied condition "success or failure"
May  6 14:05:30.734: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-subpath-test-configmap-8vt7 container test-container-subpath-configmap-8vt7: <nil>
STEP: delete the pod
May  6 14:05:30.746: INFO: Waiting for pod pod-subpath-test-configmap-8vt7 to disappear
May  6 14:05:30.748: INFO: Pod pod-subpath-test-configmap-8vt7 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-8vt7
May  6 14:05:30.748: INFO: Deleting pod "pod-subpath-test-configmap-8vt7" in namespace "subpath-9033"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:05:30.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9033" for this suite.
May  6 14:05:36.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:05:39.512: INFO: namespace subpath-9033 deletion completed in 8.759952749s

• [SLOW TEST:32.962 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:05:39.512: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9045
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May  6 14:05:39.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-9045'
May  6 14:05:39.720: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May  6 14:05:39.720: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
May  6 14:05:39.725: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-54m6l]
May  6 14:05:39.725: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-54m6l" in namespace "kubectl-9045" to be "running and ready"
May  6 14:05:39.727: INFO: Pod "e2e-test-httpd-rc-54m6l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.11277ms
May  6 14:05:41.729: INFO: Pod "e2e-test-httpd-rc-54m6l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004828825s
May  6 14:05:43.732: INFO: Pod "e2e-test-httpd-rc-54m6l": Phase="Running", Reason="", readiness=true. Elapsed: 4.007483426s
May  6 14:05:43.732: INFO: Pod "e2e-test-httpd-rc-54m6l" satisfied condition "running and ready"
May  6 14:05:43.732: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-54m6l]
May  6 14:05:43.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 logs rc/e2e-test-httpd-rc --namespace=kubectl-9045'
May  6 14:05:43.835: INFO: stderr: ""
May  6 14:05:43.835: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.85.72. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.85.72. Set the 'ServerName' directive globally to suppress this message\n[Wed May 06 14:05:41.446033 2020] [mpm_event:notice] [pid 1:tid 140178381605736] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Wed May 06 14:05:41.446073 2020] [core:notice] [pid 1:tid 140178381605736] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
May  6 14:05:43.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 delete rc e2e-test-httpd-rc --namespace=kubectl-9045'
May  6 14:05:43.940: INFO: stderr: ""
May  6 14:05:43.940: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:05:43.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9045" for this suite.
May  6 14:05:49.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:05:52.703: INFO: namespace kubectl-9045 deletion completed in 8.759766646s

• [SLOW TEST:13.190 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:05:52.703: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-8015
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May  6 14:05:53.327: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May  6 14:05:55.333: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724370753, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724370753, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724370753, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724370753, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 14:05:58.342: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 14:05:58.345: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:05:59.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8015" for this suite.
May  6 14:06:05.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:06:08.245: INFO: namespace crd-webhook-8015 deletion completed in 8.765786934s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:15.552 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:06:08.255: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7038
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-103dc5bb-cd08-423c-baee-c52cd38dc599
STEP: Creating a pod to test consume configMaps
May  6 14:06:08.396: INFO: Waiting up to 5m0s for pod "pod-configmaps-20c2699a-3a22-43a8-80c5-ccb741fccd9c" in namespace "configmap-7038" to be "success or failure"
May  6 14:06:08.398: INFO: Pod "pod-configmaps-20c2699a-3a22-43a8-80c5-ccb741fccd9c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.865585ms
May  6 14:06:10.400: INFO: Pod "pod-configmaps-20c2699a-3a22-43a8-80c5-ccb741fccd9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004379634s
STEP: Saw pod success
May  6 14:06:10.400: INFO: Pod "pod-configmaps-20c2699a-3a22-43a8-80c5-ccb741fccd9c" satisfied condition "success or failure"
May  6 14:06:10.402: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-configmaps-20c2699a-3a22-43a8-80c5-ccb741fccd9c container configmap-volume-test: <nil>
STEP: delete the pod
May  6 14:06:10.414: INFO: Waiting for pod pod-configmaps-20c2699a-3a22-43a8-80c5-ccb741fccd9c to disappear
May  6 14:06:10.416: INFO: Pod pod-configmaps-20c2699a-3a22-43a8-80c5-ccb741fccd9c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:06:10.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7038" for this suite.
May  6 14:06:16.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:06:19.178: INFO: namespace configmap-7038 deletion completed in 8.759147364s

• [SLOW TEST:10.923 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:06:19.179: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6891
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-md85
STEP: Creating a pod to test atomic-volume-subpath
May  6 14:06:19.350: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-md85" in namespace "subpath-6891" to be "success or failure"
May  6 14:06:19.353: INFO: Pod "pod-subpath-test-downwardapi-md85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.580249ms
May  6 14:06:21.356: INFO: Pod "pod-subpath-test-downwardapi-md85": Phase="Running", Reason="", readiness=true. Elapsed: 2.005296249s
May  6 14:06:23.358: INFO: Pod "pod-subpath-test-downwardapi-md85": Phase="Running", Reason="", readiness=true. Elapsed: 4.007907505s
May  6 14:06:25.361: INFO: Pod "pod-subpath-test-downwardapi-md85": Phase="Running", Reason="", readiness=true. Elapsed: 6.010367934s
May  6 14:06:27.363: INFO: Pod "pod-subpath-test-downwardapi-md85": Phase="Running", Reason="", readiness=true. Elapsed: 8.012948829s
May  6 14:06:29.366: INFO: Pod "pod-subpath-test-downwardapi-md85": Phase="Running", Reason="", readiness=true. Elapsed: 10.015592563s
May  6 14:06:31.369: INFO: Pod "pod-subpath-test-downwardapi-md85": Phase="Running", Reason="", readiness=true. Elapsed: 12.01837569s
May  6 14:06:33.371: INFO: Pod "pod-subpath-test-downwardapi-md85": Phase="Running", Reason="", readiness=true. Elapsed: 14.021054503s
May  6 14:06:35.374: INFO: Pod "pod-subpath-test-downwardapi-md85": Phase="Running", Reason="", readiness=true. Elapsed: 16.023551155s
May  6 14:06:37.377: INFO: Pod "pod-subpath-test-downwardapi-md85": Phase="Running", Reason="", readiness=true. Elapsed: 18.026311667s
May  6 14:06:39.380: INFO: Pod "pod-subpath-test-downwardapi-md85": Phase="Running", Reason="", readiness=true. Elapsed: 20.029310253s
May  6 14:06:41.382: INFO: Pod "pod-subpath-test-downwardapi-md85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.031954425s
STEP: Saw pod success
May  6 14:06:41.382: INFO: Pod "pod-subpath-test-downwardapi-md85" satisfied condition "success or failure"
May  6 14:06:41.384: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-subpath-test-downwardapi-md85 container test-container-subpath-downwardapi-md85: <nil>
STEP: delete the pod
May  6 14:06:41.397: INFO: Waiting for pod pod-subpath-test-downwardapi-md85 to disappear
May  6 14:06:41.398: INFO: Pod pod-subpath-test-downwardapi-md85 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-md85
May  6 14:06:41.398: INFO: Deleting pod "pod-subpath-test-downwardapi-md85" in namespace "subpath-6891"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:06:41.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6891" for this suite.
May  6 14:06:47.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:06:50.162: INFO: namespace subpath-6891 deletion completed in 8.759204126s

• [SLOW TEST:30.983 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:06:50.162: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8504
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8504.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8504.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8504.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8504.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8504.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8504.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8504.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8504.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8504.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8504.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8504.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8504.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8504.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 60.27.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.27.60_udp@PTR;check="$$(dig +tcp +noall +answer +search 60.27.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.27.60_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8504.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8504.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8504.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8504.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8504.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8504.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8504.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8504.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8504.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8504.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8504.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8504.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8504.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 60.27.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.27.60_udp@PTR;check="$$(dig +tcp +noall +answer +search 60.27.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.27.60_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  6 14:06:54.370: INFO: DNS probes using dns-8504/dns-test-bec80721-3dba-4ab1-952f-284bd03560e4 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:06:54.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8504" for this suite.
May  6 14:07:00.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:07:03.177: INFO: namespace dns-8504 deletion completed in 8.771776947s

• [SLOW TEST:13.015 seconds]
[sig-network] DNS
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:07:03.177: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2170
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2170.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2170.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2170.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2170.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  6 14:07:07.332: INFO: DNS probes using dns-test-c1119c52-973f-44dc-9da9-8dd25ff60b94 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2170.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2170.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2170.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2170.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  6 14:07:11.367: INFO: DNS probes using dns-test-eba8375b-5b36-4a50-a735-e362b1747b67 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2170.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2170.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2170.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2170.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  6 14:07:15.403: INFO: DNS probes using dns-test-eed0505d-f3e6-49ce-92fc-f013a0253904 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:07:15.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2170" for this suite.
May  6 14:07:21.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:07:24.188: INFO: namespace dns-2170 deletion completed in 8.760796451s

• [SLOW TEST:21.010 seconds]
[sig-network] DNS
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:07:24.188: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-8753
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
May  6 14:07:24.321: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
May  6 14:07:24.738: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May  6 14:07:26.763: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724370844, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724370844, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724370844, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724370844, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 14:07:29.786: INFO: Waited 1.015418787s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:07:31.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8753" for this suite.
May  6 14:07:37.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:07:40.245: INFO: namespace aggregator-8753 deletion completed in 8.830524045s

• [SLOW TEST:16.057 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:07:40.245: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8071
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-8071
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8071 to expose endpoints map[]
May  6 14:07:40.385: INFO: Get endpoints failed (1.785608ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
May  6 14:07:41.387: INFO: successfully validated that service endpoint-test2 in namespace services-8071 exposes endpoints map[] (1.004260093s elapsed)
STEP: Creating pod pod1 in namespace services-8071
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8071 to expose endpoints map[pod1:[80]]
May  6 14:07:43.407: INFO: successfully validated that service endpoint-test2 in namespace services-8071 exposes endpoints map[pod1:[80]] (2.013040763s elapsed)
STEP: Creating pod pod2 in namespace services-8071
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8071 to expose endpoints map[pod1:[80] pod2:[80]]
May  6 14:07:45.432: INFO: successfully validated that service endpoint-test2 in namespace services-8071 exposes endpoints map[pod1:[80] pod2:[80]] (2.019136174s elapsed)
STEP: Deleting pod pod1 in namespace services-8071
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8071 to expose endpoints map[pod2:[80]]
May  6 14:07:46.447: INFO: successfully validated that service endpoint-test2 in namespace services-8071 exposes endpoints map[pod2:[80]] (1.011039776s elapsed)
STEP: Deleting pod pod2 in namespace services-8071
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8071 to expose endpoints map[]
May  6 14:07:47.456: INFO: successfully validated that service endpoint-test2 in namespace services-8071 exposes endpoints map[] (1.004173171s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:07:47.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8071" for this suite.
May  6 14:07:59.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:08:02.233: INFO: namespace services-8071 deletion completed in 14.761003793s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:21.988 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:08:02.233: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4640
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
May  6 14:08:04.897: INFO: Successfully updated pod "annotationupdate1f53b07c-0a58-43ac-ab79-741c31fc4d82"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:08:06.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4640" for this suite.
May  6 14:08:18.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:08:21.675: INFO: namespace projected-4640 deletion completed in 14.760832542s

• [SLOW TEST:19.441 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:08:21.675: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6385
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May  6 14:08:21.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-6385'
May  6 14:08:21.887: INFO: stderr: ""
May  6 14:08:21.887: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
May  6 14:08:26.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pod e2e-test-httpd-pod --namespace=kubectl-6385 -o json'
May  6 14:08:27.045: INFO: stderr: ""
May  6 14:08:27.045: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"192.168.85.77/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.85.77/32\"\n        },\n        \"creationTimestamp\": \"2020-05-06T14:08:21Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6385\",\n        \"resourceVersion\": \"88395\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6385/pods/e2e-test-httpd-pod\",\n        \"uid\": \"ebda0349-3733-466b-bb54-ebd2e4d5d45f\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"Always\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-29kn6\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-131-80.us-west-2.compute.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-29kn6\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-29kn6\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-05-06T14:08:21Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-05-06T14:08:24Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-05-06T14:08:24Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-05-06T14:08:21Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://41f113bbb768d1bc1351def8faf4fd1eee4c921db54f597749dfb1adc1c95d72\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-05-06T14:08:23Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.131.80\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.85.77\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.85.77\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-05-06T14:08:21Z\"\n    }\n}\n"
STEP: replace the image in the pod
May  6 14:08:27.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 replace -f - --namespace=kubectl-6385'
May  6 14:08:27.302: INFO: stderr: ""
May  6 14:08:27.302: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
May  6 14:08:27.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 delete pods e2e-test-httpd-pod --namespace=kubectl-6385'
May  6 14:08:37.183: INFO: stderr: ""
May  6 14:08:37.183: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:08:37.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6385" for this suite.
May  6 14:08:43.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:08:45.946: INFO: namespace kubectl-6385 deletion completed in 8.759891106s

• [SLOW TEST:24.272 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:08:45.947: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5176
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 14:08:46.077: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:08:48.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5176" for this suite.
May  6 14:09:30.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:09:32.869: INFO: namespace pods-5176 deletion completed in 44.761512659s

• [SLOW TEST:46.922 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:09:32.869: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8618
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 14:09:33.008: INFO: Waiting up to 5m0s for pod "downwardapi-volume-05ad9cf1-7596-4455-98d0-c58714b574ea" in namespace "downward-api-8618" to be "success or failure"
May  6 14:09:33.010: INFO: Pod "downwardapi-volume-05ad9cf1-7596-4455-98d0-c58714b574ea": Phase="Pending", Reason="", readiness=false. Elapsed: 1.852325ms
May  6 14:09:35.012: INFO: Pod "downwardapi-volume-05ad9cf1-7596-4455-98d0-c58714b574ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004433197s
STEP: Saw pod success
May  6 14:09:35.012: INFO: Pod "downwardapi-volume-05ad9cf1-7596-4455-98d0-c58714b574ea" satisfied condition "success or failure"
May  6 14:09:35.014: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod downwardapi-volume-05ad9cf1-7596-4455-98d0-c58714b574ea container client-container: <nil>
STEP: delete the pod
May  6 14:09:35.027: INFO: Waiting for pod downwardapi-volume-05ad9cf1-7596-4455-98d0-c58714b574ea to disappear
May  6 14:09:35.029: INFO: Pod downwardapi-volume-05ad9cf1-7596-4455-98d0-c58714b574ea no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:09:35.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8618" for this suite.
May  6 14:09:41.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:09:43.792: INFO: namespace downward-api-8618 deletion completed in 8.759397643s

• [SLOW TEST:10.923 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:09:43.792: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3801
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-fb1de070-c46d-48cd-af6a-f98a624f76f3
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:09:43.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3801" for this suite.
May  6 14:09:49.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:09:52.688: INFO: namespace secrets-3801 deletion completed in 8.761568185s

• [SLOW TEST:8.896 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:09:52.688: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2274
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 14:09:52.826: INFO: Waiting up to 5m0s for pod "downwardapi-volume-190c047a-313e-485a-96d3-a8e2901af953" in namespace "downward-api-2274" to be "success or failure"
May  6 14:09:52.828: INFO: Pod "downwardapi-volume-190c047a-313e-485a-96d3-a8e2901af953": Phase="Pending", Reason="", readiness=false. Elapsed: 1.904563ms
May  6 14:09:54.830: INFO: Pod "downwardapi-volume-190c047a-313e-485a-96d3-a8e2901af953": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004258088s
STEP: Saw pod success
May  6 14:09:54.830: INFO: Pod "downwardapi-volume-190c047a-313e-485a-96d3-a8e2901af953" satisfied condition "success or failure"
May  6 14:09:54.832: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod downwardapi-volume-190c047a-313e-485a-96d3-a8e2901af953 container client-container: <nil>
STEP: delete the pod
May  6 14:09:54.844: INFO: Waiting for pod downwardapi-volume-190c047a-313e-485a-96d3-a8e2901af953 to disappear
May  6 14:09:54.845: INFO: Pod downwardapi-volume-190c047a-313e-485a-96d3-a8e2901af953 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:09:54.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2274" for this suite.
May  6 14:10:00.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:10:03.608: INFO: namespace downward-api-2274 deletion completed in 8.759520731s

• [SLOW TEST:10.920 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:10:03.608: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2218
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-2218
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2218
STEP: creating replication controller externalsvc in namespace services-2218
I0506 14:10:03.763972      25 runners.go:184] Created replication controller with name: externalsvc, namespace: services-2218, replica count: 2
I0506 14:10:06.814265      25 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
May  6 14:10:06.829: INFO: Creating new exec pod
May  6 14:10:08.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=services-2218 execpodp8jb9 -- /bin/sh -x -c nslookup nodeport-service'
May  6 14:10:09.306: INFO: stderr: "+ nslookup nodeport-service\n"
May  6 14:10:09.306: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nnodeport-service.services-2218.svc.cluster.local\tcanonical name = externalsvc.services-2218.svc.cluster.local.\nName:\texternalsvc.services-2218.svc.cluster.local\nAddress: 10.0.61.125\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2218, will wait for the garbage collector to delete the pods
May  6 14:10:09.365: INFO: Deleting ReplicationController externalsvc took: 5.729667ms
May  6 14:10:10.765: INFO: Terminating ReplicationController externalsvc pods took: 1.400183465s
May  6 14:10:17.279: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:10:17.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2218" for this suite.
May  6 14:10:23.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:10:26.053: INFO: namespace services-2218 deletion completed in 8.760397976s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:22.445 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:10:26.053: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-695
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-8b8686ba-49bc-438f-bff0-072fe341a760
STEP: Creating a pod to test consume configMaps
May  6 14:10:26.194: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-109f0b9c-a69b-4e8c-a6c8-e2303efc2aab" in namespace "projected-695" to be "success or failure"
May  6 14:10:26.195: INFO: Pod "pod-projected-configmaps-109f0b9c-a69b-4e8c-a6c8-e2303efc2aab": Phase="Pending", Reason="", readiness=false. Elapsed: 1.873295ms
May  6 14:10:28.199: INFO: Pod "pod-projected-configmaps-109f0b9c-a69b-4e8c-a6c8-e2303efc2aab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004960554s
STEP: Saw pod success
May  6 14:10:28.199: INFO: Pod "pod-projected-configmaps-109f0b9c-a69b-4e8c-a6c8-e2303efc2aab" satisfied condition "success or failure"
May  6 14:10:28.201: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-projected-configmaps-109f0b9c-a69b-4e8c-a6c8-e2303efc2aab container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  6 14:10:28.213: INFO: Waiting for pod pod-projected-configmaps-109f0b9c-a69b-4e8c-a6c8-e2303efc2aab to disappear
May  6 14:10:28.214: INFO: Pod pod-projected-configmaps-109f0b9c-a69b-4e8c-a6c8-e2303efc2aab no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:10:28.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-695" for this suite.
May  6 14:10:34.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:10:36.979: INFO: namespace projected-695 deletion completed in 8.762118908s

• [SLOW TEST:10.927 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:10:36.980: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7280
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7280
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7280
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7280
May  6 14:10:37.119: INFO: Found 0 stateful pods, waiting for 1
May  6 14:10:47.123: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May  6 14:10:47.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=statefulset-7280 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 14:10:47.391: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 14:10:47.391: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 14:10:47.391: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 14:10:47.394: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May  6 14:10:57.397: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  6 14:10:57.397: INFO: Waiting for statefulset status.replicas updated to 0
May  6 14:10:57.406: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999777s
May  6 14:10:58.408: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997933391s
May  6 14:10:59.411: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.995168564s
May  6 14:11:00.414: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.992324812s
May  6 14:11:01.417: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.989542631s
May  6 14:11:02.420: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.986619147s
May  6 14:11:03.423: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.983682511s
May  6 14:11:04.425: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.981001347s
May  6 14:11:05.428: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.978266342s
May  6 14:11:06.431: INFO: Verifying statefulset ss doesn't scale past 1 for another 975.61825ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7280
May  6 14:11:07.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=statefulset-7280 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 14:11:07.702: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  6 14:11:07.702: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 14:11:07.702: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 14:11:07.705: INFO: Found 1 stateful pods, waiting for 3
May  6 14:11:17.708: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  6 14:11:17.708: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May  6 14:11:17.708: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May  6 14:11:17.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=statefulset-7280 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 14:11:17.975: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 14:11:17.975: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 14:11:17.975: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 14:11:17.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=statefulset-7280 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 14:11:18.266: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 14:11:18.266: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 14:11:18.266: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 14:11:18.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=statefulset-7280 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 14:11:18.533: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 14:11:18.533: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 14:11:18.533: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 14:11:18.533: INFO: Waiting for statefulset status.replicas updated to 0
May  6 14:11:18.536: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May  6 14:11:28.542: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  6 14:11:28.542: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May  6 14:11:28.542: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May  6 14:11:28.549: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999786s
May  6 14:11:29.552: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997771056s
May  6 14:11:30.556: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994822844s
May  6 14:11:31.559: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.9916488s
May  6 14:11:32.561: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.988551984s
May  6 14:11:33.564: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.985764465s
May  6 14:11:34.567: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.982723476s
May  6 14:11:35.570: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.979779687s
May  6 14:11:36.573: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.976899738s
May  6 14:11:37.576: INFO: Verifying statefulset ss doesn't scale past 3 for another 973.839511ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7280
May  6 14:11:38.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=statefulset-7280 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 14:11:38.840: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  6 14:11:38.840: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 14:11:38.840: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 14:11:38.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=statefulset-7280 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 14:11:39.123: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  6 14:11:39.123: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 14:11:39.123: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 14:11:39.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=statefulset-7280 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 14:11:39.387: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  6 14:11:39.387: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 14:11:39.388: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 14:11:39.388: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May  6 14:11:49.398: INFO: Deleting all statefulset in ns statefulset-7280
May  6 14:11:49.401: INFO: Scaling statefulset ss to 0
May  6 14:11:49.406: INFO: Waiting for statefulset status.replicas updated to 0
May  6 14:11:49.408: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:11:49.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7280" for this suite.
May  6 14:11:55.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:11:58.179: INFO: namespace statefulset-7280 deletion completed in 8.760371189s

• [SLOW TEST:81.200 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:11:58.179: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4029
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 14:11:58.323: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2dd3fe09-fefe-445b-8a50-f160ee0bc0d0" in namespace "downward-api-4029" to be "success or failure"
May  6 14:11:58.325: INFO: Pod "downwardapi-volume-2dd3fe09-fefe-445b-8a50-f160ee0bc0d0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.786942ms
May  6 14:12:00.328: INFO: Pod "downwardapi-volume-2dd3fe09-fefe-445b-8a50-f160ee0bc0d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004749466s
STEP: Saw pod success
May  6 14:12:00.328: INFO: Pod "downwardapi-volume-2dd3fe09-fefe-445b-8a50-f160ee0bc0d0" satisfied condition "success or failure"
May  6 14:12:00.330: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod downwardapi-volume-2dd3fe09-fefe-445b-8a50-f160ee0bc0d0 container client-container: <nil>
STEP: delete the pod
May  6 14:12:00.350: INFO: Waiting for pod downwardapi-volume-2dd3fe09-fefe-445b-8a50-f160ee0bc0d0 to disappear
May  6 14:12:00.352: INFO: Pod downwardapi-volume-2dd3fe09-fefe-445b-8a50-f160ee0bc0d0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:12:00.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4029" for this suite.
May  6 14:12:06.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:12:09.115: INFO: namespace downward-api-4029 deletion completed in 8.760321482s

• [SLOW TEST:10.936 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:12:09.116: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1604
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 14:12:29.260: INFO: Container started at 2020-05-06 14:12:10 +0000 UTC, pod became ready at 2020-05-06 14:12:27 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:12:29.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1604" for this suite.
May  6 14:12:57.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:13:00.023: INFO: namespace container-probe-1604 deletion completed in 30.760185018s

• [SLOW TEST:50.907 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:13:00.023: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6061
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
May  6 14:13:00.162: INFO: Waiting up to 5m0s for pod "client-containers-4e7cb807-7fa4-4558-ae15-2c9b35a7b748" in namespace "containers-6061" to be "success or failure"
May  6 14:13:00.164: INFO: Pod "client-containers-4e7cb807-7fa4-4558-ae15-2c9b35a7b748": Phase="Pending", Reason="", readiness=false. Elapsed: 1.920048ms
May  6 14:13:02.166: INFO: Pod "client-containers-4e7cb807-7fa4-4558-ae15-2c9b35a7b748": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004459131s
STEP: Saw pod success
May  6 14:13:02.166: INFO: Pod "client-containers-4e7cb807-7fa4-4558-ae15-2c9b35a7b748" satisfied condition "success or failure"
May  6 14:13:02.168: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod client-containers-4e7cb807-7fa4-4558-ae15-2c9b35a7b748 container test-container: <nil>
STEP: delete the pod
May  6 14:13:02.179: INFO: Waiting for pod client-containers-4e7cb807-7fa4-4558-ae15-2c9b35a7b748 to disappear
May  6 14:13:02.181: INFO: Pod client-containers-4e7cb807-7fa4-4558-ae15-2c9b35a7b748 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:13:02.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6061" for this suite.
May  6 14:13:08.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:13:10.943: INFO: namespace containers-6061 deletion completed in 8.759222627s

• [SLOW TEST:10.920 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:13:10.943: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7400
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:13:13.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7400" for this suite.
May  6 14:13:59.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:14:01.866: INFO: namespace kubelet-test-7400 deletion completed in 48.759256817s

• [SLOW TEST:50.923 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:14:01.866: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2561
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:14:07.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2561" for this suite.
May  6 14:14:13.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:14:16.173: INFO: namespace watch-2561 deletion completed in 8.852652828s

• [SLOW TEST:14.307 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:14:16.173: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5052
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
May  6 14:14:16.307: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 14:14:21.155: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:14:38.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5052" for this suite.
May  6 14:14:44.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:14:46.983: INFO: namespace crd-publish-openapi-5052 deletion completed in 8.759717122s

• [SLOW TEST:30.810 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:14:46.983: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6754
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:15:47.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6754" for this suite.
May  6 14:15:59.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:16:01.894: INFO: namespace container-probe-6754 deletion completed in 14.763804522s

• [SLOW TEST:74.910 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:16:01.894: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8231
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-8231
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May  6 14:16:02.025: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May  6 14:16:24.096: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.85.66:8080/dial?request=hostName&protocol=http&host=192.168.85.69&port=8080&tries=1'] Namespace:pod-network-test-8231 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 14:16:24.096: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 14:16:24.267: INFO: Waiting for endpoints: map[]
May  6 14:16:24.269: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.85.66:8080/dial?request=hostName&protocol=http&host=192.168.121.9&port=8080&tries=1'] Namespace:pod-network-test-8231 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 14:16:24.269: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 14:16:24.438: INFO: Waiting for endpoints: map[]
May  6 14:16:24.440: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.85.66:8080/dial?request=hostName&protocol=http&host=192.168.80.172&port=8080&tries=1'] Namespace:pod-network-test-8231 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 14:16:24.440: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 14:16:24.615: INFO: Waiting for endpoints: map[]
May  6 14:16:24.617: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.85.66:8080/dial?request=hostName&protocol=http&host=192.168.187.37&port=8080&tries=1'] Namespace:pod-network-test-8231 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 14:16:24.617: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 14:16:24.783: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:16:24.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8231" for this suite.
May  6 14:16:36.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:16:39.548: INFO: namespace pod-network-test-8231 deletion completed in 14.760637162s

• [SLOW TEST:37.654 seconds]
[sig-network] Networking
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:16:39.548: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6214
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-f6e06ee1-756e-4c2e-acc8-36862ded374f
STEP: Creating a pod to test consume secrets
May  6 14:16:39.692: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7cbc543d-f41d-419b-bd21-fadde5975a54" in namespace "projected-6214" to be "success or failure"
May  6 14:16:39.694: INFO: Pod "pod-projected-secrets-7cbc543d-f41d-419b-bd21-fadde5975a54": Phase="Pending", Reason="", readiness=false. Elapsed: 1.823039ms
May  6 14:16:41.697: INFO: Pod "pod-projected-secrets-7cbc543d-f41d-419b-bd21-fadde5975a54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004933601s
STEP: Saw pod success
May  6 14:16:41.697: INFO: Pod "pod-projected-secrets-7cbc543d-f41d-419b-bd21-fadde5975a54" satisfied condition "success or failure"
May  6 14:16:41.699: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-projected-secrets-7cbc543d-f41d-419b-bd21-fadde5975a54 container projected-secret-volume-test: <nil>
STEP: delete the pod
May  6 14:16:41.718: INFO: Waiting for pod pod-projected-secrets-7cbc543d-f41d-419b-bd21-fadde5975a54 to disappear
May  6 14:16:41.720: INFO: Pod pod-projected-secrets-7cbc543d-f41d-419b-bd21-fadde5975a54 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:16:41.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6214" for this suite.
May  6 14:16:47.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:16:50.496: INFO: namespace projected-6214 deletion completed in 8.773442104s

• [SLOW TEST:10.948 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:16:50.496: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-7147
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-7147, will wait for the garbage collector to delete the pods
May  6 14:16:54.690: INFO: Deleting Job.batch foo took: 5.234851ms
May  6 14:16:56.090: INFO: Terminating Job.batch foo pods took: 1.400157641s
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:17:37.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7147" for this suite.
May  6 14:17:43.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:17:46.058: INFO: namespace job-7147 deletion completed in 8.761593683s

• [SLOW TEST:55.562 seconds]
[sig-apps] Job
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:17:46.058: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8310
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 14:17:46.207: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May  6 14:17:46.213: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:46.213: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:46.213: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:46.215: INFO: Number of nodes with available pods: 0
May  6 14:17:46.215: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 14:17:47.218: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:47.218: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:47.218: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:47.221: INFO: Number of nodes with available pods: 0
May  6 14:17:47.221: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 14:17:48.219: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:48.219: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:48.219: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:48.221: INFO: Number of nodes with available pods: 1
May  6 14:17:48.221: INFO: Node ip-10-0-129-108.us-west-2.compute.internal is running more than one daemon pod
May  6 14:17:49.218: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:49.219: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:49.219: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:49.221: INFO: Number of nodes with available pods: 4
May  6 14:17:49.221: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May  6 14:17:49.243: INFO: Wrong image for pod: daemon-set-46nvk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:49.243: INFO: Wrong image for pod: daemon-set-64blr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:49.243: INFO: Wrong image for pod: daemon-set-hjr9g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:49.243: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:49.246: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:49.246: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:49.246: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:50.249: INFO: Wrong image for pod: daemon-set-46nvk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:50.249: INFO: Wrong image for pod: daemon-set-64blr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:50.249: INFO: Wrong image for pod: daemon-set-hjr9g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:50.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:50.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:50.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:50.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:51.249: INFO: Wrong image for pod: daemon-set-46nvk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:51.249: INFO: Wrong image for pod: daemon-set-64blr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:51.249: INFO: Wrong image for pod: daemon-set-hjr9g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:51.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:51.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:51.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:51.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:52.249: INFO: Wrong image for pod: daemon-set-46nvk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:52.249: INFO: Wrong image for pod: daemon-set-64blr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:52.249: INFO: Pod daemon-set-64blr is not available
May  6 14:17:52.249: INFO: Wrong image for pod: daemon-set-hjr9g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:52.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:52.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:52.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:52.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:53.249: INFO: Wrong image for pod: daemon-set-46nvk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:53.249: INFO: Wrong image for pod: daemon-set-64blr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:53.249: INFO: Pod daemon-set-64blr is not available
May  6 14:17:53.249: INFO: Wrong image for pod: daemon-set-hjr9g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:53.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:53.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:53.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:53.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:54.249: INFO: Wrong image for pod: daemon-set-46nvk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:54.249: INFO: Wrong image for pod: daemon-set-64blr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:54.249: INFO: Pod daemon-set-64blr is not available
May  6 14:17:54.249: INFO: Wrong image for pod: daemon-set-hjr9g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:54.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:54.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:54.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:54.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:55.250: INFO: Wrong image for pod: daemon-set-46nvk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:55.250: INFO: Wrong image for pod: daemon-set-64blr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:55.250: INFO: Pod daemon-set-64blr is not available
May  6 14:17:55.250: INFO: Wrong image for pod: daemon-set-hjr9g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:55.250: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:55.253: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:55.253: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:55.253: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:56.249: INFO: Wrong image for pod: daemon-set-46nvk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:56.249: INFO: Wrong image for pod: daemon-set-64blr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:56.249: INFO: Pod daemon-set-64blr is not available
May  6 14:17:56.249: INFO: Wrong image for pod: daemon-set-hjr9g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:56.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:56.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:56.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:56.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:57.249: INFO: Pod daemon-set-2kt27 is not available
May  6 14:17:57.249: INFO: Wrong image for pod: daemon-set-46nvk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:57.249: INFO: Wrong image for pod: daemon-set-hjr9g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:57.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:57.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:57.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:57.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:58.249: INFO: Pod daemon-set-2kt27 is not available
May  6 14:17:58.249: INFO: Wrong image for pod: daemon-set-46nvk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:58.249: INFO: Wrong image for pod: daemon-set-hjr9g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:58.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:58.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:58.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:58.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:59.249: INFO: Wrong image for pod: daemon-set-46nvk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:59.249: INFO: Wrong image for pod: daemon-set-hjr9g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:59.249: INFO: Pod daemon-set-hjr9g is not available
May  6 14:17:59.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:17:59.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:59.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:17:59.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:00.249: INFO: Wrong image for pod: daemon-set-46nvk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:00.249: INFO: Pod daemon-set-rvvsl is not available
May  6 14:18:00.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:00.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:00.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:00.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:01.249: INFO: Wrong image for pod: daemon-set-46nvk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:01.249: INFO: Pod daemon-set-rvvsl is not available
May  6 14:18:01.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:01.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:01.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:01.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:02.249: INFO: Wrong image for pod: daemon-set-46nvk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:02.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:02.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:02.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:02.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:03.249: INFO: Wrong image for pod: daemon-set-46nvk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:03.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:03.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:03.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:03.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:04.249: INFO: Wrong image for pod: daemon-set-46nvk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:04.249: INFO: Pod daemon-set-46nvk is not available
May  6 14:18:04.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:04.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:04.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:04.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:05.249: INFO: Wrong image for pod: daemon-set-46nvk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:05.249: INFO: Pod daemon-set-46nvk is not available
May  6 14:18:05.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:05.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:05.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:05.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:06.249: INFO: Wrong image for pod: daemon-set-46nvk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:06.249: INFO: Pod daemon-set-46nvk is not available
May  6 14:18:06.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:06.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:06.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:06.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:07.249: INFO: Pod daemon-set-6hpmf is not available
May  6 14:18:07.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:07.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:07.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:07.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:08.249: INFO: Pod daemon-set-6hpmf is not available
May  6 14:18:08.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:08.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:08.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:08.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:09.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:09.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:09.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:09.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:10.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:10.249: INFO: Pod daemon-set-wn2dw is not available
May  6 14:18:10.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:10.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:10.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:11.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:11.249: INFO: Pod daemon-set-wn2dw is not available
May  6 14:18:11.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:11.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:11.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:12.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:12.249: INFO: Pod daemon-set-wn2dw is not available
May  6 14:18:12.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:12.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:12.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:13.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:13.249: INFO: Pod daemon-set-wn2dw is not available
May  6 14:18:13.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:13.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:13.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:14.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:14.249: INFO: Pod daemon-set-wn2dw is not available
May  6 14:18:14.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:14.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:14.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:15.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:15.249: INFO: Pod daemon-set-wn2dw is not available
May  6 14:18:15.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:15.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:15.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:16.249: INFO: Wrong image for pod: daemon-set-wn2dw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 14:18:16.249: INFO: Pod daemon-set-wn2dw is not available
May  6 14:18:16.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:16.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:16.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:17.249: INFO: Pod daemon-set-vhnpg is not available
May  6 14:18:17.252: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:17.252: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:17.252: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
May  6 14:18:17.254: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:17.254: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:17.254: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:17.257: INFO: Number of nodes with available pods: 3
May  6 14:18:17.257: INFO: Node ip-10-0-129-108.us-west-2.compute.internal is running more than one daemon pod
May  6 14:18:18.261: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:18.261: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:18.261: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:18.263: INFO: Number of nodes with available pods: 3
May  6 14:18:18.263: INFO: Node ip-10-0-129-108.us-west-2.compute.internal is running more than one daemon pod
May  6 14:18:19.260: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:19.260: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:19.260: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:19.263: INFO: Number of nodes with available pods: 3
May  6 14:18:19.263: INFO: Node ip-10-0-129-108.us-west-2.compute.internal is running more than one daemon pod
May  6 14:18:20.260: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:20.260: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:20.260: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:18:20.263: INFO: Number of nodes with available pods: 4
May  6 14:18:20.263: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8310, will wait for the garbage collector to delete the pods
May  6 14:18:20.328: INFO: Deleting DaemonSet.extensions daemon-set took: 4.797366ms
May  6 14:18:21.728: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.400143691s
May  6 14:18:24.131: INFO: Number of nodes with available pods: 0
May  6 14:18:24.131: INFO: Number of running nodes: 0, number of available pods: 0
May  6 14:18:24.132: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8310/daemonsets","resourceVersion":"92767"},"items":null}

May  6 14:18:24.134: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8310/pods","resourceVersion":"92767"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:18:24.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8310" for this suite.
May  6 14:18:30.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:18:32.907: INFO: namespace daemonsets-8310 deletion completed in 8.759708054s

• [SLOW TEST:46.849 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:18:32.907: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6900
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 14:18:33.036: INFO: Creating deployment "webserver-deployment"
May  6 14:18:33.039: INFO: Waiting for observed generation 1
May  6 14:18:35.043: INFO: Waiting for all required pods to come up
May  6 14:18:35.046: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
May  6 14:18:39.052: INFO: Waiting for deployment "webserver-deployment" to complete
May  6 14:18:39.056: INFO: Updating deployment "webserver-deployment" with a non-existent image
May  6 14:18:39.061: INFO: Updating deployment webserver-deployment
May  6 14:18:39.061: INFO: Waiting for observed generation 2
May  6 14:18:41.065: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May  6 14:18:41.067: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May  6 14:18:41.069: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May  6 14:18:41.074: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May  6 14:18:41.074: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May  6 14:18:41.076: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May  6 14:18:41.079: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May  6 14:18:41.079: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May  6 14:18:41.083: INFO: Updating deployment webserver-deployment
May  6 14:18:41.083: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May  6 14:18:41.088: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May  6 14:18:41.089: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May  6 14:18:41.094: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6900 /apis/apps/v1/namespaces/deployment-6900/deployments/webserver-deployment 9f20ef63-90e3-4ffb-b6aa-49c3cd7a417c 93178 3 2020-05-06 14:18:33 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003f89948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-05-06 14:18:36 +0000 UTC,LastTransitionTime:2020-05-06 14:18:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-05-06 14:18:39 +0000 UTC,LastTransitionTime:2020-05-06 14:18:33 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May  6 14:18:41.098: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-6900 /apis/apps/v1/namespaces/deployment-6900/replicasets/webserver-deployment-c7997dcc8 70712163-9586-4740-9495-acb32ce9ead2 93181 3 2020-05-06 14:18:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 9f20ef63-90e3-4ffb-b6aa-49c3cd7a417c 0xc003f89e57 0xc003f89e58}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003f89ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  6 14:18:41.098: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May  6 14:18:41.098: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-6900 /apis/apps/v1/namespaces/deployment-6900/replicasets/webserver-deployment-595b5b9587 8514e2b2-9568-4662-b33f-d8157f83b6e1 93179 3 2020-05-06 14:18:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 9f20ef63-90e3-4ffb-b6aa-49c3cd7a417c 0xc003f89d97 0xc003f89d98}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003f89df8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May  6 14:18:41.103: INFO: Pod "webserver-deployment-595b5b9587-46x8x" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-46x8x webserver-deployment-595b5b9587- deployment-6900 /api/v1/namespaces/deployment-6900/pods/webserver-deployment-595b5b9587-46x8x 62feb384-1766-417b-b42b-644daec02e44 93187 0 2020-05-06 14:18:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8514e2b2-9568-4662-b33f-d8157f83b6e1 0xc002edc497 0xc002edc498}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-24nxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-24nxn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-24nxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 14:18:41.103: INFO: Pod "webserver-deployment-595b5b9587-6kmx6" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6kmx6 webserver-deployment-595b5b9587- deployment-6900 /api/v1/namespaces/deployment-6900/pods/webserver-deployment-595b5b9587-6kmx6 069b7ce3-ab9e-4e64-a248-ec664b5c6d3c 93024 0 2020-05-06 14:18:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.187.43/32 cni.projectcalico.org/podIPs:192.168.187.43/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8514e2b2-9568-4662-b33f-d8157f83b6e1 0xc002edc5a7 0xc002edc5a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-24nxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-24nxn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-24nxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-129-108.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.129.108,PodIP:192.168.187.43,StartTime:2020-05-06 14:18:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 14:18:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://aedce1cdc7a8a2e7a19b38f537d4c1c64b8e8e467aad1251965fc6124024613a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.187.43,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 14:18:41.104: INFO: Pod "webserver-deployment-595b5b9587-7srq9" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7srq9 webserver-deployment-595b5b9587- deployment-6900 /api/v1/namespaces/deployment-6900/pods/webserver-deployment-595b5b9587-7srq9 1d6c38bb-a8cb-4e62-980c-384c1dbba038 92996 0 2020-05-06 14:18:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.187.40/32 cni.projectcalico.org/podIPs:192.168.187.40/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8514e2b2-9568-4662-b33f-d8157f83b6e1 0xc002edc767 0xc002edc768}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-24nxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-24nxn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-24nxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-129-108.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.129.108,PodIP:192.168.187.40,StartTime:2020-05-06 14:18:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 14:18:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://e867eeff19ffc2e0610810b8d6cf74382b1040f1301c3509c2255f6f340a073c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.187.40,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 14:18:41.104: INFO: Pod "webserver-deployment-595b5b9587-bmpvs" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bmpvs webserver-deployment-595b5b9587- deployment-6900 /api/v1/namespaces/deployment-6900/pods/webserver-deployment-595b5b9587-bmpvs cc2e31eb-f8de-43fe-8594-fcb84ba272e0 93000 0 2020-05-06 14:18:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.80.175/32 cni.projectcalico.org/podIPs:192.168.80.175/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8514e2b2-9568-4662-b33f-d8157f83b6e1 0xc002edc9b7 0xc002edc9b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-24nxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-24nxn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-24nxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-234.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.234,PodIP:192.168.80.175,StartTime:2020-05-06 14:18:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 14:18:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://6eb4a280019d20cfe3077ae6f6d0188774cc3ab253e4fff4645c0490571e705a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.80.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 14:18:41.104: INFO: Pod "webserver-deployment-595b5b9587-cs27m" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-cs27m webserver-deployment-595b5b9587- deployment-6900 /api/v1/namespaces/deployment-6900/pods/webserver-deployment-595b5b9587-cs27m 5936c263-db9c-4a27-9948-f0877a212809 93045 0 2020-05-06 14:18:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.85.103/32 cni.projectcalico.org/podIPs:192.168.85.103/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8514e2b2-9568-4662-b33f-d8157f83b6e1 0xc002edcb27 0xc002edcb28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-24nxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-24nxn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-24nxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-80.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.80,PodIP:192.168.85.103,StartTime:2020-05-06 14:18:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 14:18:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://1847f253db101c428be3abca2c1c173563ecd0d41d401c73a949fbe275a0bcb7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.85.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 14:18:41.104: INFO: Pod "webserver-deployment-595b5b9587-d4jnh" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-d4jnh webserver-deployment-595b5b9587- deployment-6900 /api/v1/namespaces/deployment-6900/pods/webserver-deployment-595b5b9587-d4jnh 49f9fb04-61eb-4123-b0da-86f454624644 93030 0 2020-05-06 14:18:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.85.102/32 cni.projectcalico.org/podIPs:192.168.85.102/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8514e2b2-9568-4662-b33f-d8157f83b6e1 0xc002edce17 0xc002edce18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-24nxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-24nxn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-24nxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-80.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.80,PodIP:192.168.85.102,StartTime:2020-05-06 14:18:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 14:18:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://415b586f724722881658d858fae8f54c79e6bc4ffe97eb9522f25eb87587ccc3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.85.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 14:18:41.104: INFO: Pod "webserver-deployment-595b5b9587-mvv7h" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mvv7h webserver-deployment-595b5b9587- deployment-6900 /api/v1/namespaces/deployment-6900/pods/webserver-deployment-595b5b9587-mvv7h da8c4e25-156c-4e07-b745-1dcb728f3f47 93185 0 2020-05-06 14:18:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8514e2b2-9568-4662-b33f-d8157f83b6e1 0xc002edd027 0xc002edd028}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-24nxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-24nxn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-24nxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-129-1.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 14:18:41.104: INFO: Pod "webserver-deployment-595b5b9587-pd8tp" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pd8tp webserver-deployment-595b5b9587- deployment-6900 /api/v1/namespaces/deployment-6900/pods/webserver-deployment-595b5b9587-pd8tp aac474f3-184f-4f43-a213-63c6da8b69ee 93027 0 2020-05-06 14:18:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.85.101/32 cni.projectcalico.org/podIPs:192.168.85.101/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8514e2b2-9568-4662-b33f-d8157f83b6e1 0xc002edd1b0 0xc002edd1b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-24nxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-24nxn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-24nxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-80.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.80,PodIP:192.168.85.101,StartTime:2020-05-06 14:18:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 14:18:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://559515a6fc306461a45171a4f8127ceedb9f5068038d7c1f05298540026a00fa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.85.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 14:18:41.104: INFO: Pod "webserver-deployment-595b5b9587-pfhvr" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pfhvr webserver-deployment-595b5b9587- deployment-6900 /api/v1/namespaces/deployment-6900/pods/webserver-deployment-595b5b9587-pfhvr f5855e96-3e32-4c6b-b05f-7cfbae54362e 93029 0 2020-05-06 14:18:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.80.176/32 cni.projectcalico.org/podIPs:192.168.80.176/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8514e2b2-9568-4662-b33f-d8157f83b6e1 0xc002edd407 0xc002edd408}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-24nxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-24nxn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-24nxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-234.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.234,PodIP:192.168.80.176,StartTime:2020-05-06 14:18:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 14:18:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://6d0fe3bb61a1f958f0fd56077dda5f29834647bd3237cc1735b1b5f76d927fc5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.80.176,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 14:18:41.104: INFO: Pod "webserver-deployment-595b5b9587-qxqwc" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qxqwc webserver-deployment-595b5b9587- deployment-6900 /api/v1/namespaces/deployment-6900/pods/webserver-deployment-595b5b9587-qxqwc cb10b684-9364-4a94-acfc-948d0e1c9c18 93017 0 2020-05-06 14:18:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.121.12/32 cni.projectcalico.org/podIPs:192.168.121.12/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8514e2b2-9568-4662-b33f-d8157f83b6e1 0xc002edd597 0xc002edd598}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-24nxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-24nxn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-24nxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-129-1.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.129.1,PodIP:192.168.121.12,StartTime:2020-05-06 14:18:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 14:18:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://085d73f52e6ba81daa92a5b375d54508d753ae72bf75b5601b05e4b9e1dfe179,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.121.12,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 14:18:41.104: INFO: Pod "webserver-deployment-c7997dcc8-45r8p" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-45r8p webserver-deployment-c7997dcc8- deployment-6900 /api/v1/namespaces/deployment-6900/pods/webserver-deployment-c7997dcc8-45r8p 97626c11-c12f-4f0f-adb1-e19a1edb3674 93150 0 2020-05-06 14:18:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.85.107/32 cni.projectcalico.org/podIPs:192.168.85.107/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 70712163-9586-4740-9495-acb32ce9ead2 0xc002edd700 0xc002edd701}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-24nxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-24nxn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-24nxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-80.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.80,PodIP:,StartTime:2020-05-06 14:18:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 14:18:41.104: INFO: Pod "webserver-deployment-c7997dcc8-4khll" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-4khll webserver-deployment-c7997dcc8- deployment-6900 /api/v1/namespaces/deployment-6900/pods/webserver-deployment-c7997dcc8-4khll 1da32b0c-e5a8-44ad-bf5f-f5997d86b5a7 93173 0 2020-05-06 14:18:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.85.105/32 cni.projectcalico.org/podIPs:192.168.85.105/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 70712163-9586-4740-9495-acb32ce9ead2 0xc002edd860 0xc002edd861}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-24nxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-24nxn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-24nxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-80.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.80,PodIP:192.168.85.105,StartTime:2020-05-06 14:18:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to resolve image "docker.io/library/webserver:404": no available registry endpoint: pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.85.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 14:18:41.105: INFO: Pod "webserver-deployment-c7997dcc8-dfkt4" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-dfkt4 webserver-deployment-c7997dcc8- deployment-6900 /api/v1/namespaces/deployment-6900/pods/webserver-deployment-c7997dcc8-dfkt4 92b11165-b2cb-40e2-8309-2476e54b74ee 93151 0 2020-05-06 14:18:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.85.106/32 cni.projectcalico.org/podIPs:192.168.85.106/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 70712163-9586-4740-9495-acb32ce9ead2 0xc002edd9f0 0xc002edd9f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-24nxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-24nxn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-24nxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-80.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.80,PodIP:,StartTime:2020-05-06 14:18:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 14:18:41.105: INFO: Pod "webserver-deployment-c7997dcc8-hcjt5" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-hcjt5 webserver-deployment-c7997dcc8- deployment-6900 /api/v1/namespaces/deployment-6900/pods/webserver-deployment-c7997dcc8-hcjt5 84323168-b258-453d-9be0-03f0079f3cf3 93186 0 2020-05-06 14:18:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 70712163-9586-4740-9495-acb32ce9ead2 0xc002eddb70 0xc002eddb71}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-24nxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-24nxn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-24nxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 14:18:41.105: INFO: Pod "webserver-deployment-c7997dcc8-mrxvl" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-mrxvl webserver-deployment-c7997dcc8- deployment-6900 /api/v1/namespaces/deployment-6900/pods/webserver-deployment-c7997dcc8-mrxvl ef1ce1e3-7cdf-494c-b78f-d73d4a78c5f9 93169 0 2020-05-06 14:18:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.80.177/32 cni.projectcalico.org/podIPs:192.168.80.177/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 70712163-9586-4740-9495-acb32ce9ead2 0xc002eddc77 0xc002eddc78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-24nxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-24nxn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-24nxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-234.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.234,PodIP:192.168.80.177,StartTime:2020-05-06 14:18:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to resolve image "docker.io/library/webserver:404": no available registry endpoint: pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.80.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 14:18:41.105: INFO: Pod "webserver-deployment-c7997dcc8-rkbbp" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-rkbbp webserver-deployment-c7997dcc8- deployment-6900 /api/v1/namespaces/deployment-6900/pods/webserver-deployment-c7997dcc8-rkbbp fbc1fd66-cc0c-434f-b351-ad4503785994 93166 0 2020-05-06 14:18:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.187.42/32 cni.projectcalico.org/podIPs:192.168.187.42/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 70712163-9586-4740-9495-acb32ce9ead2 0xc002edde27 0xc002edde28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-24nxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-24nxn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-24nxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-129-108.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 14:18:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.129.108,PodIP:192.168.187.42,StartTime:2020-05-06 14:18:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to resolve image "docker.io/library/webserver:404": no available registry endpoint: pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.187.42,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:18:41.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6900" for this suite.
May  6 14:18:49.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:18:51.916: INFO: namespace deployment-6900 deletion completed in 10.799043785s

• [SLOW TEST:19.009 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:18:51.916: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3728
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-st97c in namespace proxy-3728
I0506 14:18:52.057179      25 runners.go:184] Created replication controller with name: proxy-service-st97c, namespace: proxy-3728, replica count: 1
I0506 14:18:53.107424      25 runners.go:184] proxy-service-st97c Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0506 14:18:54.107554      25 runners.go:184] proxy-service-st97c Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0506 14:18:55.107697      25 runners.go:184] proxy-service-st97c Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0506 14:18:56.107821      25 runners.go:184] proxy-service-st97c Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0506 14:18:57.107958      25 runners.go:184] proxy-service-st97c Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0506 14:18:58.108096      25 runners.go:184] proxy-service-st97c Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0506 14:18:59.108220      25 runners.go:184] proxy-service-st97c Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0506 14:19:00.108348      25 runners.go:184] proxy-service-st97c Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0506 14:19:01.108489      25 runners.go:184] proxy-service-st97c Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  6 14:19:01.111: INFO: setup took 9.064020204s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May  6 14:19:01.119: INFO: (0) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/rewriteme">test</a> (200; 8.131303ms)
May  6 14:19:01.119: INFO: (0) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/rewriteme">test<... (200; 8.211381ms)
May  6 14:19:01.119: INFO: (0) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/rewriteme">... (200; 8.275639ms)
May  6 14:19:01.119: INFO: (0) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:160/proxy/: foo (200; 8.379226ms)
May  6 14:19:01.119: INFO: (0) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname2/proxy/: bar (200; 8.410047ms)
May  6 14:19:01.119: INFO: (0) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:162/proxy/: bar (200; 8.428387ms)
May  6 14:19:01.120: INFO: (0) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:162/proxy/: bar (200; 8.678926ms)
May  6 14:19:01.120: INFO: (0) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname1/proxy/: foo (200; 8.817527ms)
May  6 14:19:01.120: INFO: (0) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname2/proxy/: bar (200; 8.831507ms)
May  6 14:19:01.120: INFO: (0) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:160/proxy/: foo (200; 8.834142ms)
May  6 14:19:01.120: INFO: (0) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname1/proxy/: foo (200; 8.895531ms)
May  6 14:19:01.121: INFO: (0) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/tlsrewritem... (200; 10.164354ms)
May  6 14:19:01.123: INFO: (0) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:460/proxy/: tls baz (200; 12.271746ms)
May  6 14:19:01.123: INFO: (0) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname1/proxy/: tls baz (200; 12.176718ms)
May  6 14:19:01.123: INFO: (0) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:462/proxy/: tls qux (200; 12.412587ms)
May  6 14:19:01.123: INFO: (0) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname2/proxy/: tls qux (200; 12.52266ms)
May  6 14:19:01.127: INFO: (1) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/tlsrewritem... (200; 3.495006ms)
May  6 14:19:01.128: INFO: (1) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:162/proxy/: bar (200; 4.250921ms)
May  6 14:19:01.128: INFO: (1) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/rewriteme">... (200; 4.450665ms)
May  6 14:19:01.128: INFO: (1) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/rewriteme">test</a> (200; 4.725715ms)
May  6 14:19:01.128: INFO: (1) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:160/proxy/: foo (200; 4.788063ms)
May  6 14:19:01.128: INFO: (1) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:160/proxy/: foo (200; 4.79168ms)
May  6 14:19:01.129: INFO: (1) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:462/proxy/: tls qux (200; 4.978704ms)
May  6 14:19:01.129: INFO: (1) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:162/proxy/: bar (200; 5.101512ms)
May  6 14:19:01.129: INFO: (1) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:460/proxy/: tls baz (200; 5.06824ms)
May  6 14:19:01.129: INFO: (1) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/rewriteme">test<... (200; 5.167405ms)
May  6 14:19:01.130: INFO: (1) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname2/proxy/: tls qux (200; 6.693403ms)
May  6 14:19:01.131: INFO: (1) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname1/proxy/: tls baz (200; 7.420503ms)
May  6 14:19:01.131: INFO: (1) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname1/proxy/: foo (200; 7.315542ms)
May  6 14:19:01.131: INFO: (1) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname2/proxy/: bar (200; 7.320675ms)
May  6 14:19:01.131: INFO: (1) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname2/proxy/: bar (200; 7.322261ms)
May  6 14:19:01.131: INFO: (1) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname1/proxy/: foo (200; 7.548138ms)
May  6 14:19:01.136: INFO: (2) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/rewriteme">test<... (200; 4.905753ms)
May  6 14:19:01.136: INFO: (2) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/tlsrewritem... (200; 5.242301ms)
May  6 14:19:01.137: INFO: (2) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:160/proxy/: foo (200; 5.541783ms)
May  6 14:19:01.137: INFO: (2) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:162/proxy/: bar (200; 5.555913ms)
May  6 14:19:01.137: INFO: (2) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:462/proxy/: tls qux (200; 5.594519ms)
May  6 14:19:01.137: INFO: (2) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:460/proxy/: tls baz (200; 5.562007ms)
May  6 14:19:01.137: INFO: (2) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/rewriteme">test</a> (200; 5.617142ms)
May  6 14:19:01.137: INFO: (2) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname2/proxy/: bar (200; 5.671562ms)
May  6 14:19:01.137: INFO: (2) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:160/proxy/: foo (200; 5.829234ms)
May  6 14:19:01.137: INFO: (2) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:162/proxy/: bar (200; 5.774111ms)
May  6 14:19:01.137: INFO: (2) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname1/proxy/: foo (200; 5.928524ms)
May  6 14:19:01.137: INFO: (2) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/rewriteme">... (200; 5.963922ms)
May  6 14:19:01.138: INFO: (2) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname1/proxy/: tls baz (200; 6.858741ms)
May  6 14:19:01.138: INFO: (2) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname2/proxy/: bar (200; 7.041458ms)
May  6 14:19:01.139: INFO: (2) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname2/proxy/: tls qux (200; 7.744817ms)
May  6 14:19:01.139: INFO: (2) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname1/proxy/: foo (200; 7.732501ms)
May  6 14:19:01.143: INFO: (3) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:462/proxy/: tls qux (200; 4.328748ms)
May  6 14:19:01.143: INFO: (3) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:460/proxy/: tls baz (200; 4.421977ms)
May  6 14:19:01.144: INFO: (3) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/rewriteme">... (200; 5.130674ms)
May  6 14:19:01.144: INFO: (3) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/tlsrewritem... (200; 5.280923ms)
May  6 14:19:01.144: INFO: (3) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:162/proxy/: bar (200; 5.349273ms)
May  6 14:19:01.144: INFO: (3) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:160/proxy/: foo (200; 5.577405ms)
May  6 14:19:01.145: INFO: (3) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:160/proxy/: foo (200; 5.658846ms)
May  6 14:19:01.145: INFO: (3) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/rewriteme">test</a> (200; 5.721464ms)
May  6 14:19:01.145: INFO: (3) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/rewriteme">test<... (200; 5.7581ms)
May  6 14:19:01.145: INFO: (3) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname2/proxy/: bar (200; 5.802396ms)
May  6 14:19:01.145: INFO: (3) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:162/proxy/: bar (200; 5.812509ms)
May  6 14:19:01.146: INFO: (3) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname1/proxy/: foo (200; 6.639853ms)
May  6 14:19:01.147: INFO: (3) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname1/proxy/: foo (200; 8.459413ms)
May  6 14:19:01.148: INFO: (3) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname2/proxy/: bar (200; 8.589727ms)
May  6 14:19:01.148: INFO: (3) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname2/proxy/: tls qux (200; 8.594278ms)
May  6 14:19:01.148: INFO: (3) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname1/proxy/: tls baz (200; 8.648638ms)
May  6 14:19:01.152: INFO: (4) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:160/proxy/: foo (200; 4.119999ms)
May  6 14:19:01.152: INFO: (4) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:162/proxy/: bar (200; 4.331136ms)
May  6 14:19:01.152: INFO: (4) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/rewriteme">... (200; 4.491912ms)
May  6 14:19:01.152: INFO: (4) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/rewriteme">test</a> (200; 4.413374ms)
May  6 14:19:01.152: INFO: (4) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:162/proxy/: bar (200; 4.5068ms)
May  6 14:19:01.152: INFO: (4) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:462/proxy/: tls qux (200; 4.749199ms)
May  6 14:19:01.153: INFO: (4) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/tlsrewritem... (200; 4.735147ms)
May  6 14:19:01.153: INFO: (4) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:160/proxy/: foo (200; 4.628824ms)
May  6 14:19:01.153: INFO: (4) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:460/proxy/: tls baz (200; 4.710862ms)
May  6 14:19:01.153: INFO: (4) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/rewriteme">test<... (200; 4.693408ms)
May  6 14:19:01.153: INFO: (4) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname1/proxy/: foo (200; 5.51092ms)
May  6 14:19:01.154: INFO: (4) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname2/proxy/: bar (200; 6.29925ms)
May  6 14:19:01.155: INFO: (4) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname1/proxy/: foo (200; 7.363455ms)
May  6 14:19:01.155: INFO: (4) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname1/proxy/: tls baz (200; 7.297552ms)
May  6 14:19:01.155: INFO: (4) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname2/proxy/: bar (200; 7.441876ms)
May  6 14:19:01.155: INFO: (4) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname2/proxy/: tls qux (200; 7.492807ms)
May  6 14:19:01.160: INFO: (5) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:162/proxy/: bar (200; 4.636703ms)
May  6 14:19:01.160: INFO: (5) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/rewriteme">test<... (200; 4.590001ms)
May  6 14:19:01.160: INFO: (5) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:162/proxy/: bar (200; 4.850451ms)
May  6 14:19:01.161: INFO: (5) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:160/proxy/: foo (200; 5.22306ms)
May  6 14:19:01.161: INFO: (5) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/rewriteme">test</a> (200; 5.205872ms)
May  6 14:19:01.161: INFO: (5) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:460/proxy/: tls baz (200; 5.392014ms)
May  6 14:19:01.161: INFO: (5) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:160/proxy/: foo (200; 5.366806ms)
May  6 14:19:01.161: INFO: (5) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/tlsrewritem... (200; 5.235428ms)
May  6 14:19:01.161: INFO: (5) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/rewriteme">... (200; 5.344024ms)
May  6 14:19:01.161: INFO: (5) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname2/proxy/: tls qux (200; 5.55032ms)
May  6 14:19:01.161: INFO: (5) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:462/proxy/: tls qux (200; 5.615079ms)
May  6 14:19:01.162: INFO: (5) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname2/proxy/: bar (200; 6.311192ms)
May  6 14:19:01.163: INFO: (5) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname1/proxy/: foo (200; 7.430235ms)
May  6 14:19:01.163: INFO: (5) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname1/proxy/: tls baz (200; 7.377192ms)
May  6 14:19:01.163: INFO: (5) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname1/proxy/: foo (200; 7.307229ms)
May  6 14:19:01.163: INFO: (5) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname2/proxy/: bar (200; 7.478363ms)
May  6 14:19:01.168: INFO: (6) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/rewriteme">test</a> (200; 4.81567ms)
May  6 14:19:01.168: INFO: (6) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:460/proxy/: tls baz (200; 5.154599ms)
May  6 14:19:01.168: INFO: (6) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:160/proxy/: foo (200; 5.293998ms)
May  6 14:19:01.168: INFO: (6) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:162/proxy/: bar (200; 5.362659ms)
May  6 14:19:01.168: INFO: (6) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/rewriteme">test<... (200; 5.429442ms)
May  6 14:19:01.169: INFO: (6) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:160/proxy/: foo (200; 5.760825ms)
May  6 14:19:01.169: INFO: (6) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/tlsrewritem... (200; 5.753946ms)
May  6 14:19:01.169: INFO: (6) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname2/proxy/: tls qux (200; 5.77146ms)
May  6 14:19:01.169: INFO: (6) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:162/proxy/: bar (200; 5.828247ms)
May  6 14:19:01.169: INFO: (6) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/rewriteme">... (200; 6.09921ms)
May  6 14:19:01.169: INFO: (6) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:462/proxy/: tls qux (200; 6.064088ms)
May  6 14:19:01.170: INFO: (6) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname1/proxy/: foo (200; 6.722658ms)
May  6 14:19:01.170: INFO: (6) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname1/proxy/: foo (200; 7.386387ms)
May  6 14:19:01.170: INFO: (6) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname2/proxy/: bar (200; 7.349134ms)
May  6 14:19:01.170: INFO: (6) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname2/proxy/: bar (200; 7.379848ms)
May  6 14:19:01.170: INFO: (6) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname1/proxy/: tls baz (200; 7.362784ms)
May  6 14:19:01.174: INFO: (7) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/tlsrewritem... (200; 3.103103ms)
May  6 14:19:01.174: INFO: (7) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/rewriteme">test<... (200; 3.151385ms)
May  6 14:19:01.174: INFO: (7) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:162/proxy/: bar (200; 3.564066ms)
May  6 14:19:01.174: INFO: (7) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:160/proxy/: foo (200; 3.565589ms)
May  6 14:19:01.176: INFO: (7) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:160/proxy/: foo (200; 5.448274ms)
May  6 14:19:01.176: INFO: (7) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/rewriteme">test</a> (200; 5.401006ms)
May  6 14:19:01.176: INFO: (7) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:162/proxy/: bar (200; 5.749945ms)
May  6 14:19:01.176: INFO: (7) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:462/proxy/: tls qux (200; 5.804784ms)
May  6 14:19:01.176: INFO: (7) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/rewriteme">... (200; 5.642885ms)
May  6 14:19:01.177: INFO: (7) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:460/proxy/: tls baz (200; 6.096763ms)
May  6 14:19:01.178: INFO: (7) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname1/proxy/: foo (200; 7.056638ms)
May  6 14:19:01.178: INFO: (7) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname1/proxy/: foo (200; 7.599651ms)
May  6 14:19:01.179: INFO: (7) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname1/proxy/: tls baz (200; 7.973375ms)
May  6 14:19:01.179: INFO: (7) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname2/proxy/: bar (200; 8.154131ms)
May  6 14:19:01.179: INFO: (7) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname2/proxy/: tls qux (200; 8.126914ms)
May  6 14:19:01.179: INFO: (7) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname2/proxy/: bar (200; 8.051403ms)
May  6 14:19:01.183: INFO: (8) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/rewriteme">... (200; 4.367787ms)
May  6 14:19:01.184: INFO: (8) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname1/proxy/: tls baz (200; 4.774723ms)
May  6 14:19:01.184: INFO: (8) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:462/proxy/: tls qux (200; 4.880818ms)
May  6 14:19:01.184: INFO: (8) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/tlsrewritem... (200; 4.851603ms)
May  6 14:19:01.184: INFO: (8) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:162/proxy/: bar (200; 4.993678ms)
May  6 14:19:01.184: INFO: (8) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:162/proxy/: bar (200; 4.840884ms)
May  6 14:19:01.184: INFO: (8) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:160/proxy/: foo (200; 5.133375ms)
May  6 14:19:01.184: INFO: (8) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:160/proxy/: foo (200; 5.264003ms)
May  6 14:19:01.184: INFO: (8) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/rewriteme">test<... (200; 5.09478ms)
May  6 14:19:01.184: INFO: (8) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/rewriteme">test</a> (200; 5.264276ms)
May  6 14:19:01.184: INFO: (8) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:460/proxy/: tls baz (200; 5.353173ms)
May  6 14:19:01.185: INFO: (8) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname1/proxy/: foo (200; 6.598288ms)
May  6 14:19:01.186: INFO: (8) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname2/proxy/: bar (200; 7.562923ms)
May  6 14:19:01.187: INFO: (8) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname2/proxy/: bar (200; 7.738352ms)
May  6 14:19:01.187: INFO: (8) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname2/proxy/: tls qux (200; 7.667531ms)
May  6 14:19:01.187: INFO: (8) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname1/proxy/: foo (200; 7.696756ms)
May  6 14:19:01.190: INFO: (9) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:162/proxy/: bar (200; 3.352008ms)
May  6 14:19:01.191: INFO: (9) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/rewriteme">test<... (200; 3.967343ms)
May  6 14:19:01.191: INFO: (9) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/rewriteme">... (200; 4.043858ms)
May  6 14:19:01.191: INFO: (9) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:160/proxy/: foo (200; 4.186484ms)
May  6 14:19:01.191: INFO: (9) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:162/proxy/: bar (200; 4.190952ms)
May  6 14:19:01.191: INFO: (9) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/rewriteme">test</a> (200; 4.268275ms)
May  6 14:19:01.192: INFO: (9) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:160/proxy/: foo (200; 5.419904ms)
May  6 14:19:01.193: INFO: (9) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname2/proxy/: bar (200; 5.835983ms)
May  6 14:19:01.193: INFO: (9) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:462/proxy/: tls qux (200; 5.936757ms)
May  6 14:19:01.193: INFO: (9) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:460/proxy/: tls baz (200; 5.836104ms)
May  6 14:19:01.193: INFO: (9) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/tlsrewritem... (200; 5.796559ms)
May  6 14:19:01.193: INFO: (9) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname1/proxy/: foo (200; 6.236555ms)
May  6 14:19:01.194: INFO: (9) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname2/proxy/: tls qux (200; 7.447726ms)
May  6 14:19:01.194: INFO: (9) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname2/proxy/: bar (200; 7.66054ms)
May  6 14:19:01.195: INFO: (9) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname1/proxy/: tls baz (200; 8.067482ms)
May  6 14:19:01.195: INFO: (9) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname1/proxy/: foo (200; 8.045279ms)
May  6 14:19:01.198: INFO: (10) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:460/proxy/: tls baz (200; 3.406538ms)
May  6 14:19:01.200: INFO: (10) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:160/proxy/: foo (200; 5.027016ms)
May  6 14:19:01.200: INFO: (10) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:162/proxy/: bar (200; 5.043705ms)
May  6 14:19:01.200: INFO: (10) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/rewriteme">... (200; 5.262355ms)
May  6 14:19:01.200: INFO: (10) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/rewriteme">test<... (200; 5.290004ms)
May  6 14:19:01.200: INFO: (10) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:160/proxy/: foo (200; 5.378657ms)
May  6 14:19:01.200: INFO: (10) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:462/proxy/: tls qux (200; 5.336982ms)
May  6 14:19:01.200: INFO: (10) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:162/proxy/: bar (200; 5.317497ms)
May  6 14:19:01.200: INFO: (10) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/tlsrewritem... (200; 5.482016ms)
May  6 14:19:01.200: INFO: (10) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/rewriteme">test</a> (200; 5.386554ms)
May  6 14:19:01.202: INFO: (10) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname2/proxy/: bar (200; 6.630546ms)
May  6 14:19:01.203: INFO: (10) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname1/proxy/: foo (200; 7.685618ms)
May  6 14:19:01.203: INFO: (10) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname1/proxy/: foo (200; 7.641232ms)
May  6 14:19:01.203: INFO: (10) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname2/proxy/: tls qux (200; 7.727341ms)
May  6 14:19:01.203: INFO: (10) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname1/proxy/: tls baz (200; 7.780804ms)
May  6 14:19:01.203: INFO: (10) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname2/proxy/: bar (200; 7.770768ms)
May  6 14:19:01.206: INFO: (11) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/rewriteme">... (200; 2.978574ms)
May  6 14:19:01.207: INFO: (11) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:160/proxy/: foo (200; 3.624985ms)
May  6 14:19:01.207: INFO: (11) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:162/proxy/: bar (200; 3.73462ms)
May  6 14:19:01.207: INFO: (11) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:462/proxy/: tls qux (200; 3.874507ms)
May  6 14:19:01.207: INFO: (11) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/rewriteme">test<... (200; 4.027434ms)
May  6 14:19:01.208: INFO: (11) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/tlsrewritem... (200; 4.56435ms)
May  6 14:19:01.208: INFO: (11) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/rewriteme">test</a> (200; 4.736448ms)
May  6 14:19:01.208: INFO: (11) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:162/proxy/: bar (200; 5.026062ms)
May  6 14:19:01.208: INFO: (11) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:460/proxy/: tls baz (200; 5.002177ms)
May  6 14:19:01.209: INFO: (11) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:160/proxy/: foo (200; 5.395116ms)
May  6 14:19:01.209: INFO: (11) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname2/proxy/: bar (200; 5.812685ms)
May  6 14:19:01.209: INFO: (11) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname1/proxy/: foo (200; 5.86799ms)
May  6 14:19:01.210: INFO: (11) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname2/proxy/: bar (200; 7.1115ms)
May  6 14:19:01.210: INFO: (11) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname2/proxy/: tls qux (200; 7.218087ms)
May  6 14:19:01.210: INFO: (11) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname1/proxy/: tls baz (200; 7.086516ms)
May  6 14:19:01.211: INFO: (11) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname1/proxy/: foo (200; 7.290843ms)
May  6 14:19:01.214: INFO: (12) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:162/proxy/: bar (200; 3.125972ms)
May  6 14:19:01.215: INFO: (12) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:462/proxy/: tls qux (200; 4.157428ms)
May  6 14:19:01.217: INFO: (12) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:162/proxy/: bar (200; 5.886092ms)
May  6 14:19:01.217: INFO: (12) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/rewriteme">... (200; 5.905764ms)
May  6 14:19:01.217: INFO: (12) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:460/proxy/: tls baz (200; 6.269621ms)
May  6 14:19:01.217: INFO: (12) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:160/proxy/: foo (200; 6.346828ms)
May  6 14:19:01.217: INFO: (12) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/rewriteme">test<... (200; 6.431536ms)
May  6 14:19:01.217: INFO: (12) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:160/proxy/: foo (200; 6.610976ms)
May  6 14:19:01.217: INFO: (12) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/rewriteme">test</a> (200; 6.665901ms)
May  6 14:19:01.218: INFO: (12) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/tlsrewritem... (200; 7.626292ms)
May  6 14:19:01.218: INFO: (12) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname1/proxy/: foo (200; 7.616219ms)
May  6 14:19:01.219: INFO: (12) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname2/proxy/: bar (200; 8.800019ms)
May  6 14:19:01.220: INFO: (12) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname2/proxy/: tls qux (200; 8.695498ms)
May  6 14:19:01.220: INFO: (12) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname1/proxy/: tls baz (200; 9.012161ms)
May  6 14:19:01.220: INFO: (12) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname1/proxy/: foo (200; 8.930864ms)
May  6 14:19:01.220: INFO: (12) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname2/proxy/: bar (200; 8.912135ms)
May  6 14:19:01.224: INFO: (13) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/rewriteme">... (200; 4.052028ms)
May  6 14:19:01.224: INFO: (13) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:462/proxy/: tls qux (200; 4.261313ms)
May  6 14:19:01.224: INFO: (13) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:162/proxy/: bar (200; 4.491329ms)
May  6 14:19:01.225: INFO: (13) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:160/proxy/: foo (200; 4.692955ms)
May  6 14:19:01.225: INFO: (13) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:160/proxy/: foo (200; 4.75722ms)
May  6 14:19:01.225: INFO: (13) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/rewriteme">test</a> (200; 4.873497ms)
May  6 14:19:01.225: INFO: (13) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:162/proxy/: bar (200; 4.812589ms)
May  6 14:19:01.225: INFO: (13) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname1/proxy/: foo (200; 5.107217ms)
May  6 14:19:01.225: INFO: (13) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/rewriteme">test<... (200; 5.298862ms)
May  6 14:19:01.225: INFO: (13) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/tlsrewritem... (200; 5.3498ms)
May  6 14:19:01.225: INFO: (13) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:460/proxy/: tls baz (200; 5.520525ms)
May  6 14:19:01.227: INFO: (13) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname2/proxy/: bar (200; 6.626927ms)
May  6 14:19:01.227: INFO: (13) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname2/proxy/: tls qux (200; 7.278631ms)
May  6 14:19:01.227: INFO: (13) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname1/proxy/: tls baz (200; 7.54323ms)
May  6 14:19:01.227: INFO: (13) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname2/proxy/: bar (200; 7.576706ms)
May  6 14:19:01.227: INFO: (13) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname1/proxy/: foo (200; 7.493844ms)
May  6 14:19:01.233: INFO: (14) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:160/proxy/: foo (200; 5.02203ms)
May  6 14:19:01.233: INFO: (14) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:162/proxy/: bar (200; 5.183683ms)
May  6 14:19:01.233: INFO: (14) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/tlsrewritem... (200; 5.381828ms)
May  6 14:19:01.233: INFO: (14) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/rewriteme">test<... (200; 5.648977ms)
May  6 14:19:01.233: INFO: (14) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:160/proxy/: foo (200; 5.588903ms)
May  6 14:19:01.233: INFO: (14) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/rewriteme">... (200; 5.642511ms)
May  6 14:19:01.233: INFO: (14) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:162/proxy/: bar (200; 5.781981ms)
May  6 14:19:01.233: INFO: (14) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:462/proxy/: tls qux (200; 5.706985ms)
May  6 14:19:01.233: INFO: (14) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/rewriteme">test</a> (200; 5.849495ms)
May  6 14:19:01.233: INFO: (14) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname1/proxy/: foo (200; 5.893585ms)
May  6 14:19:01.234: INFO: (14) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:460/proxy/: tls baz (200; 6.046017ms)
May  6 14:19:01.234: INFO: (14) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname2/proxy/: bar (200; 6.544165ms)
May  6 14:19:01.235: INFO: (14) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname2/proxy/: bar (200; 7.454293ms)
May  6 14:19:01.235: INFO: (14) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname1/proxy/: tls baz (200; 7.563253ms)
May  6 14:19:01.235: INFO: (14) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname1/proxy/: foo (200; 7.635061ms)
May  6 14:19:01.235: INFO: (14) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname2/proxy/: tls qux (200; 7.640966ms)
May  6 14:19:01.241: INFO: (15) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/rewriteme">test</a> (200; 5.139132ms)
May  6 14:19:01.241: INFO: (15) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname1/proxy/: tls baz (200; 5.381327ms)
May  6 14:19:01.241: INFO: (15) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:160/proxy/: foo (200; 5.153546ms)
May  6 14:19:01.241: INFO: (15) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/rewriteme">test<... (200; 5.37538ms)
May  6 14:19:01.241: INFO: (15) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:162/proxy/: bar (200; 5.25265ms)
May  6 14:19:01.241: INFO: (15) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:162/proxy/: bar (200; 5.544346ms)
May  6 14:19:01.241: INFO: (15) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/tlsrewritem... (200; 5.845005ms)
May  6 14:19:01.241: INFO: (15) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/rewriteme">... (200; 5.756134ms)
May  6 14:19:01.241: INFO: (15) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:462/proxy/: tls qux (200; 5.601217ms)
May  6 14:19:01.241: INFO: (15) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:460/proxy/: tls baz (200; 5.768356ms)
May  6 14:19:01.241: INFO: (15) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:160/proxy/: foo (200; 5.588947ms)
May  6 14:19:01.242: INFO: (15) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname2/proxy/: bar (200; 6.499569ms)
May  6 14:19:01.243: INFO: (15) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname1/proxy/: foo (200; 7.209819ms)
May  6 14:19:01.243: INFO: (15) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname1/proxy/: foo (200; 7.651073ms)
May  6 14:19:01.243: INFO: (15) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname2/proxy/: tls qux (200; 8.134275ms)
May  6 14:19:01.244: INFO: (15) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname2/proxy/: bar (200; 8.060211ms)
May  6 14:19:01.248: INFO: (16) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:160/proxy/: foo (200; 4.483054ms)
May  6 14:19:01.249: INFO: (16) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/rewriteme">test<... (200; 4.895093ms)
May  6 14:19:01.249: INFO: (16) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:462/proxy/: tls qux (200; 5.262017ms)
May  6 14:19:01.249: INFO: (16) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:160/proxy/: foo (200; 5.296381ms)
May  6 14:19:01.249: INFO: (16) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/tlsrewritem... (200; 5.415109ms)
May  6 14:19:01.249: INFO: (16) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:162/proxy/: bar (200; 5.258665ms)
May  6 14:19:01.249: INFO: (16) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:162/proxy/: bar (200; 5.412686ms)
May  6 14:19:01.249: INFO: (16) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:460/proxy/: tls baz (200; 5.215693ms)
May  6 14:19:01.249: INFO: (16) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/rewriteme">test</a> (200; 5.335313ms)
May  6 14:19:01.249: INFO: (16) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/rewriteme">... (200; 5.342208ms)
May  6 14:19:01.250: INFO: (16) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname1/proxy/: foo (200; 6.542539ms)
May  6 14:19:01.251: INFO: (16) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname2/proxy/: tls qux (200; 6.973161ms)
May  6 14:19:01.252: INFO: (16) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname1/proxy/: foo (200; 7.583777ms)
May  6 14:19:01.252: INFO: (16) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname2/proxy/: bar (200; 7.826457ms)
May  6 14:19:01.252: INFO: (16) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname2/proxy/: bar (200; 8.109839ms)
May  6 14:19:01.252: INFO: (16) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname1/proxy/: tls baz (200; 8.229912ms)
May  6 14:19:01.256: INFO: (17) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:162/proxy/: bar (200; 4.078142ms)
May  6 14:19:01.256: INFO: (17) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/rewriteme">test</a> (200; 4.018774ms)
May  6 14:19:01.257: INFO: (17) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/rewriteme">... (200; 4.444638ms)
May  6 14:19:01.257: INFO: (17) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:160/proxy/: foo (200; 4.478997ms)
May  6 14:19:01.257: INFO: (17) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/tlsrewritem... (200; 4.618774ms)
May  6 14:19:01.257: INFO: (17) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:460/proxy/: tls baz (200; 4.691247ms)
May  6 14:19:01.258: INFO: (17) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:160/proxy/: foo (200; 5.51192ms)
May  6 14:19:01.258: INFO: (17) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname1/proxy/: foo (200; 6.277191ms)
May  6 14:19:01.258: INFO: (17) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname1/proxy/: foo (200; 6.19374ms)
May  6 14:19:01.259: INFO: (17) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:462/proxy/: tls qux (200; 6.327255ms)
May  6 14:19:01.259: INFO: (17) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:162/proxy/: bar (200; 6.488739ms)
May  6 14:19:01.259: INFO: (17) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/rewriteme">test<... (200; 6.70148ms)
May  6 14:19:01.259: INFO: (17) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname2/proxy/: tls qux (200; 7.186512ms)
May  6 14:19:01.260: INFO: (17) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname2/proxy/: bar (200; 8.141068ms)
May  6 14:19:01.260: INFO: (17) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname1/proxy/: tls baz (200; 8.176212ms)
May  6 14:19:01.260: INFO: (17) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname2/proxy/: bar (200; 8.30174ms)
May  6 14:19:01.264: INFO: (18) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:162/proxy/: bar (200; 3.192937ms)
May  6 14:19:01.266: INFO: (18) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/rewriteme">... (200; 4.792191ms)
May  6 14:19:01.266: INFO: (18) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/rewriteme">test<... (200; 4.9808ms)
May  6 14:19:01.266: INFO: (18) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:460/proxy/: tls baz (200; 5.077981ms)
May  6 14:19:01.266: INFO: (18) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/rewriteme">test</a> (200; 5.102052ms)
May  6 14:19:01.266: INFO: (18) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:160/proxy/: foo (200; 5.646684ms)
May  6 14:19:01.266: INFO: (18) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:162/proxy/: bar (200; 5.547346ms)
May  6 14:19:01.266: INFO: (18) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:462/proxy/: tls qux (200; 5.600856ms)
May  6 14:19:01.266: INFO: (18) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname1/proxy/: tls baz (200; 5.368196ms)
May  6 14:19:01.266: INFO: (18) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/tlsrewritem... (200; 5.544712ms)
May  6 14:19:01.266: INFO: (18) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:160/proxy/: foo (200; 5.343827ms)
May  6 14:19:01.267: INFO: (18) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname1/proxy/: foo (200; 6.285237ms)
May  6 14:19:01.268: INFO: (18) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname1/proxy/: foo (200; 7.522279ms)
May  6 14:19:01.268: INFO: (18) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname2/proxy/: bar (200; 7.205133ms)
May  6 14:19:01.268: INFO: (18) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname2/proxy/: tls qux (200; 7.440431ms)
May  6 14:19:01.268: INFO: (18) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname2/proxy/: bar (200; 7.472764ms)
May  6 14:19:01.274: INFO: (19) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:162/proxy/: bar (200; 6.167523ms)
May  6 14:19:01.275: INFO: (19) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:1080/proxy/rewriteme">... (200; 6.585464ms)
May  6 14:19:01.275: INFO: (19) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:1080/proxy/rewriteme">test<... (200; 6.617323ms)
May  6 14:19:01.275: INFO: (19) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:443/proxy/tlsrewritem... (200; 6.732178ms)
May  6 14:19:01.275: INFO: (19) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname2/proxy/: bar (200; 6.751809ms)
May  6 14:19:01.275: INFO: (19) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/: <a href="/api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd/proxy/rewriteme">test</a> (200; 7.020835ms)
May  6 14:19:01.275: INFO: (19) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:460/proxy/: tls baz (200; 7.174128ms)
May  6 14:19:01.275: INFO: (19) /api/v1/namespaces/proxy-3728/pods/https:proxy-service-st97c-kxksd:462/proxy/: tls qux (200; 7.257211ms)
May  6 14:19:01.275: INFO: (19) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname2/proxy/: bar (200; 7.254327ms)
May  6 14:19:01.276: INFO: (19) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:160/proxy/: foo (200; 7.372232ms)
May  6 14:19:01.276: INFO: (19) /api/v1/namespaces/proxy-3728/pods/proxy-service-st97c-kxksd:160/proxy/: foo (200; 7.51076ms)
May  6 14:19:01.276: INFO: (19) /api/v1/namespaces/proxy-3728/pods/http:proxy-service-st97c-kxksd:162/proxy/: bar (200; 7.674876ms)
May  6 14:19:01.277: INFO: (19) /api/v1/namespaces/proxy-3728/services/proxy-service-st97c:portname1/proxy/: foo (200; 8.682091ms)
May  6 14:19:01.277: INFO: (19) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname2/proxy/: tls qux (200; 8.718639ms)
May  6 14:19:01.277: INFO: (19) /api/v1/namespaces/proxy-3728/services/https:proxy-service-st97c:tlsportname1/proxy/: tls baz (200; 8.773819ms)
May  6 14:19:01.277: INFO: (19) /api/v1/namespaces/proxy-3728/services/http:proxy-service-st97c:portname1/proxy/: foo (200; 8.846323ms)
STEP: deleting ReplicationController proxy-service-st97c in namespace proxy-3728, will wait for the garbage collector to delete the pods
May  6 14:19:01.334: INFO: Deleting ReplicationController proxy-service-st97c took: 5.341069ms
May  6 14:19:02.734: INFO: Terminating ReplicationController proxy-service-st97c pods took: 1.400147436s
[AfterEach] version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:19:07.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3728" for this suite.
May  6 14:19:13.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:19:13.426: INFO: namespace proxy-3728 deletion completed in 6.187769194s

• [SLOW TEST:21.510 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:19:13.426: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5166
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-09f6a253-bf43-4642-b0e7-5a996b8ac0b1 in namespace container-probe-5166
May  6 14:19:15.574: INFO: Started pod liveness-09f6a253-bf43-4642-b0e7-5a996b8ac0b1 in namespace container-probe-5166
STEP: checking the pod's current state and verifying that restartCount is present
May  6 14:19:15.576: INFO: Initial restart count of pod liveness-09f6a253-bf43-4642-b0e7-5a996b8ac0b1 is 0
May  6 14:19:37.607: INFO: Restart count of pod container-probe-5166/liveness-09f6a253-bf43-4642-b0e7-5a996b8ac0b1 is now 1 (22.031072812s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:19:37.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5166" for this suite.
May  6 14:19:43.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:19:46.379: INFO: namespace container-probe-5166 deletion completed in 8.760543256s

• [SLOW TEST:32.953 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:19:46.379: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3748
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:19:46.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3748" for this suite.
May  6 14:19:52.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:19:55.304: INFO: namespace resourcequota-3748 deletion completed in 8.768453713s

• [SLOW TEST:8.924 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:19:55.304: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9982
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
May  6 14:19:55.442: INFO: Waiting up to 5m0s for pod "pod-84878b66-cdf6-4cc3-8aff-ba8f2a10faf9" in namespace "emptydir-9982" to be "success or failure"
May  6 14:19:55.443: INFO: Pod "pod-84878b66-cdf6-4cc3-8aff-ba8f2a10faf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.81945ms
May  6 14:19:57.446: INFO: Pod "pod-84878b66-cdf6-4cc3-8aff-ba8f2a10faf9": Phase="Running", Reason="", readiness=true. Elapsed: 2.004243455s
May  6 14:19:59.448: INFO: Pod "pod-84878b66-cdf6-4cc3-8aff-ba8f2a10faf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006834507s
STEP: Saw pod success
May  6 14:19:59.448: INFO: Pod "pod-84878b66-cdf6-4cc3-8aff-ba8f2a10faf9" satisfied condition "success or failure"
May  6 14:19:59.450: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-84878b66-cdf6-4cc3-8aff-ba8f2a10faf9 container test-container: <nil>
STEP: delete the pod
May  6 14:19:59.469: INFO: Waiting for pod pod-84878b66-cdf6-4cc3-8aff-ba8f2a10faf9 to disappear
May  6 14:19:59.471: INFO: Pod pod-84878b66-cdf6-4cc3-8aff-ba8f2a10faf9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:19:59.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9982" for this suite.
May  6 14:20:05.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:20:08.233: INFO: namespace emptydir-9982 deletion completed in 8.759265111s

• [SLOW TEST:12.929 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:20:08.233: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7298
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 14:20:08.373: INFO: Waiting up to 5m0s for pod "downwardapi-volume-47f32b1d-c4c7-4c19-9879-28653b0db136" in namespace "projected-7298" to be "success or failure"
May  6 14:20:08.376: INFO: Pod "downwardapi-volume-47f32b1d-c4c7-4c19-9879-28653b0db136": Phase="Pending", Reason="", readiness=false. Elapsed: 2.551662ms
May  6 14:20:10.379: INFO: Pod "downwardapi-volume-47f32b1d-c4c7-4c19-9879-28653b0db136": Phase="Running", Reason="", readiness=true. Elapsed: 2.00528523s
May  6 14:20:12.381: INFO: Pod "downwardapi-volume-47f32b1d-c4c7-4c19-9879-28653b0db136": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007958596s
STEP: Saw pod success
May  6 14:20:12.381: INFO: Pod "downwardapi-volume-47f32b1d-c4c7-4c19-9879-28653b0db136" satisfied condition "success or failure"
May  6 14:20:12.383: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod downwardapi-volume-47f32b1d-c4c7-4c19-9879-28653b0db136 container client-container: <nil>
STEP: delete the pod
May  6 14:20:12.395: INFO: Waiting for pod downwardapi-volume-47f32b1d-c4c7-4c19-9879-28653b0db136 to disappear
May  6 14:20:12.397: INFO: Pod downwardapi-volume-47f32b1d-c4c7-4c19-9879-28653b0db136 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:20:12.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7298" for this suite.
May  6 14:20:18.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:20:21.159: INFO: namespace projected-7298 deletion completed in 8.759246287s

• [SLOW TEST:12.926 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:20:21.159: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7200
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
May  6 14:20:21.297: INFO: Waiting up to 5m0s for pod "pod-89a8f0fe-d207-4319-960b-235c048ea4bd" in namespace "emptydir-7200" to be "success or failure"
May  6 14:20:21.299: INFO: Pod "pod-89a8f0fe-d207-4319-960b-235c048ea4bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.738019ms
May  6 14:20:23.301: INFO: Pod "pod-89a8f0fe-d207-4319-960b-235c048ea4bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004186224s
STEP: Saw pod success
May  6 14:20:23.301: INFO: Pod "pod-89a8f0fe-d207-4319-960b-235c048ea4bd" satisfied condition "success or failure"
May  6 14:20:23.303: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-89a8f0fe-d207-4319-960b-235c048ea4bd container test-container: <nil>
STEP: delete the pod
May  6 14:20:23.314: INFO: Waiting for pod pod-89a8f0fe-d207-4319-960b-235c048ea4bd to disappear
May  6 14:20:23.316: INFO: Pod pod-89a8f0fe-d207-4319-960b-235c048ea4bd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:20:23.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7200" for this suite.
May  6 14:20:29.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:20:32.089: INFO: namespace emptydir-7200 deletion completed in 8.770760033s

• [SLOW TEST:10.930 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:20:32.089: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-4629
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May  6 14:20:32.572: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 14:20:35.584: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 14:20:35.587: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:20:36.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4629" for this suite.
May  6 14:20:42.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:20:45.475: INFO: namespace crd-webhook-4629 deletion completed in 8.773120024s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:13.395 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:20:45.485: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-454
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-664dd8e2-4a56-4f3d-ab9f-d2fe0bbf9102
STEP: Creating configMap with name cm-test-opt-upd-1b8612e2-e7d8-4e0c-9b10-a2026ea3be9b
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-664dd8e2-4a56-4f3d-ab9f-d2fe0bbf9102
STEP: Updating configmap cm-test-opt-upd-1b8612e2-e7d8-4e0c-9b10-a2026ea3be9b
STEP: Creating configMap with name cm-test-opt-create-e62b2053-b48d-4c25-8a20-9d36d3dcafb0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:22:05.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-454" for this suite.
May  6 14:22:17.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:22:20.629: INFO: namespace projected-454 deletion completed in 14.759622958s

• [SLOW TEST:95.144 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:22:20.629: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-9878
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:22:22.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9878" for this suite.
May  6 14:22:28.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:22:31.569: INFO: namespace emptydir-wrapper-9878 deletion completed in 8.760964137s

• [SLOW TEST:10.940 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:22:31.569: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4103
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May  6 14:22:31.700: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  6 14:22:31.709: INFO: Waiting for terminating namespaces to be deleted...
May  6 14:22:31.711: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-129-1.us-west-2.compute.internal before test
May  6 14:22:31.724: INFO: fluentbit-kubeaddons-fluent-bit-5rbrr from kubeaddons started at 2020-05-06 11:02:11 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.724: INFO: 	Container fluent-bit ready: true, restart count 0
May  6 14:22:31.724: INFO: traefik-kubeaddons-6f97669977-2sxh8 from kubeaddons started at 2020-05-06 10:58:20 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.724: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
May  6 14:22:31.724: INFO: prometheus-kubeaddons-prometheus-node-exporter-b6z8v from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.724: INFO: 	Container node-exporter ready: true, restart count 0
May  6 14:22:31.724: INFO: kcl-cm-6588c896f9-4b2gk from kommander started at 2020-05-06 11:00:36 +0000 UTC (2 container statuses recorded)
May  6 14:22:31.724: INFO: 	Container controller-manager ready: true, restart count 0
May  6 14:22:31.724: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  6 14:22:31.724: INFO: kube-proxy-v9c2p from kube-system started at 2020-05-06 10:55:20 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.724: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 14:22:31.724: INFO: calico-node-wpbh7 from kube-system started at 2020-05-06 10:55:38 +0000 UTC (2 container statuses recorded)
May  6 14:22:31.724: INFO: 	Container bird-metrics ready: true, restart count 0
May  6 14:22:31.724: INFO: 	Container calico-node ready: true, restart count 0
May  6 14:22:31.724: INFO: cert-manager-kubeaddons-webhook-77fbc6d59b-fbbjd from cert-manager started at 2020-05-06 10:56:59 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.724: INFO: 	Container cert-manager ready: true, restart count 1
May  6 14:22:31.724: INFO: cert-manager-kubeaddons-7d7f98fbc6-qpl75 from cert-manager started at 2020-05-06 10:56:59 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.724: INFO: 	Container cert-manager ready: true, restart count 0
May  6 14:22:31.724: INFO: ebs-csi-node-f2cbn from kube-system started at 2020-05-06 10:57:49 +0000 UTC (3 container statuses recorded)
May  6 14:22:31.724: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 14:22:31.724: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 14:22:31.724: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  6 14:22:31.724: INFO: elasticsearch-kubeaddons-master-1 from kubeaddons started at 2020-05-06 10:59:19 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.724: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 14:22:31.724: INFO: coredns-5644d7b6d9-5jf9p from kube-system started at 2020-05-06 10:56:00 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.724: INFO: 	Container coredns ready: true, restart count 0
May  6 14:22:31.724: INFO: kommander-kubeaddons-kubeaddons-catalog-6654f856df-ccmkl from kommander started at 2020-05-06 11:00:36 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.724: INFO: 	Container kubeaddons-catalog ready: true, restart count 0
May  6 14:22:31.724: INFO: kcl-utility-apiserver-5d448f665b-gf6gv from kommander started at 2020-05-06 11:00:36 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.724: INFO: 	Container server ready: true, restart count 0
May  6 14:22:31.724: INFO: elasticsearch-kubeaddons-data-1 from kubeaddons started at 2020-05-06 11:01:19 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.724: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 14:22:31.724: INFO: reloader-kubeaddons-reloader-7d4bd64cfb-xnxz8 from kubeaddons started at 2020-05-06 10:56:23 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.724: INFO: 	Container reloader-kubeaddons-reloader ready: true, restart count 0
May  6 14:22:31.724: INFO: kubefed-controller-manager-55fd5b4dff-bvsl6 from kommander started at 2020-05-06 11:00:46 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.724: INFO: 	Container controller-manager ready: true, restart count 0
May  6 14:22:31.724: INFO: prometheus-kubeaddons-grafana-99b49bd54-8h7z8 from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (2 container statuses recorded)
May  6 14:22:31.724: INFO: 	Container grafana ready: true, restart count 0
May  6 14:22:31.724: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
May  6 14:22:31.724: INFO: minio-3 from velero started at 2020-05-06 10:59:47 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.724: INFO: 	Container minio ready: true, restart count 0
May  6 14:22:31.724: INFO: sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-9wkcg from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 14:22:31.724: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 14:22:31.724: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 14:22:31.724: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-129-108.us-west-2.compute.internal before test
May  6 14:22:31.738: INFO: elasticsearchexporter-kubeaddons-elasticsearch-exporter-845tdq8 from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container elasticsearch-exporter ready: true, restart count 0
May  6 14:22:31.738: INFO: kube-proxy-4lqz5 from kube-system started at 2020-05-06 10:55:19 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 14:22:31.738: INFO: coredns-5644d7b6d9-hmtfs from kube-system started at 2020-05-06 10:56:00 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container coredns ready: true, restart count 0
May  6 14:22:31.738: INFO: sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-zqwx9 from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 14:22:31.738: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 14:22:31.738: INFO: kcl-tfcb-9c88c7bc9-lbgz6 from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container server ready: true, restart count 0
May  6 14:22:31.738: INFO: kcl-webhook-fb4dfc7d7-vv757 from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container webhook ready: true, restart count 0
May  6 14:22:31.738: INFO: kubernetes-dashboard-549989bcdf-2d767 from kubeaddons started at 2020-05-06 10:56:34 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
May  6 14:22:31.738: INFO: kibana-kubeaddons-649897648c-mzc2h from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (2 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container initialize-kibana-index ready: true, restart count 0
May  6 14:22:31.738: INFO: 	Container kibana ready: true, restart count 0
May  6 14:22:31.738: INFO: kubeaddons-controller-manager-77cf76b857-k89ml from kubeaddons started at 2020-05-06 10:55:49 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container manager ready: true, restart count 0
May  6 14:22:31.738: INFO: ebs-csi-node-vvbxt from kube-system started at 2020-05-06 10:57:49 +0000 UTC (3 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 14:22:31.738: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 14:22:31.738: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  6 14:22:31.738: INFO: cert-manager-kubeaddons-cainjector-6dcd94769b-ng6xl from cert-manager started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container cainjector ready: true, restart count 0
May  6 14:22:31.738: INFO: elasticsearch-kubeaddons-master-2 from kubeaddons started at 2020-05-06 11:00:07 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 14:22:31.738: INFO: kommander-kubeaddons-grafana-645f957f8f-96sf5 from kommander started at 2020-05-06 11:00:36 +0000 UTC (2 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container grafana ready: true, restart count 0
May  6 14:22:31.738: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
May  6 14:22:31.738: INFO: kubefed-admission-webhook-5fb847574f-6mjm2 from kommander started at 2020-05-06 11:00:41 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container admission-webhook ready: true, restart count 0
May  6 14:22:31.738: INFO: fluentbit-kubeaddons-fluent-bit-t6xpz from kubeaddons started at 2020-05-06 11:02:11 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container fluent-bit ready: true, restart count 0
May  6 14:22:31.738: INFO: elasticsearch-kubeaddons-client-d4f5db58d-7qrn8 from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 14:22:31.738: INFO: calico-node-z679x from kube-system started at 2020-05-06 10:55:38 +0000 UTC (2 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container bird-metrics ready: true, restart count 0
May  6 14:22:31.738: INFO: 	Container calico-node ready: true, restart count 0
May  6 14:22:31.738: INFO: prometheus-kubeaddons-prometheus-node-exporter-2wrg9 from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container node-exporter ready: true, restart count 0
May  6 14:22:31.738: INFO: kommander-kubeaddons-karma-7688944fcf-hpnzl from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container karma ready: true, restart count 0
May  6 14:22:31.738: INFO: external-dns-kubeaddons-6c65ff8d88-sm9p2 from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container external-dns ready: true, restart count 0
May  6 14:22:31.738: INFO: calico-kube-controllers-84f666455-5t7xm from kube-system started at 2020-05-06 10:55:49 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  6 14:22:31.738: INFO: minio-2 from velero started at 2020-05-06 10:59:44 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container minio ready: true, restart count 0
May  6 14:22:31.738: INFO: traefik-kubeaddons-6f97669977-9fwr9 from kubeaddons started at 2020-05-06 10:57:49 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
May  6 14:22:31.738: INFO: elasticsearch-kubeaddons-data-0 from kubeaddons started at 2020-05-06 10:58:15 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.738: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 14:22:31.738: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-131-234.us-west-2.compute.internal before test
May  6 14:22:31.752: INFO: alertmanager-prometheus-kubeaddons-prom-alertmanager-0 from kubeaddons started at 2020-05-06 10:59:03 +0000 UTC (2 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container alertmanager ready: true, restart count 0
May  6 14:22:31.752: INFO: 	Container config-reloader ready: true, restart count 0
May  6 14:22:31.752: INFO: tiller-deploy-969865475-m2ph8 from kube-system started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container tiller ready: true, restart count 0
May  6 14:22:31.752: INFO: prometheusadapter-kubeaddons-prometheus-adapter-6b8975fc48pwzsr from kubeaddons started at 2020-05-06 10:59:55 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container prometheus-adapter ready: true, restart count 0
May  6 14:22:31.752: INFO: dstorageclass-controller-manager-5c966c767f-4bf6x from kubeaddons started at 2020-05-06 10:57:33 +0000 UTC (2 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  6 14:22:31.752: INFO: 	Container manager ready: true, restart count 0
May  6 14:22:31.752: INFO: opsportal-kubeaddons-kommander-ui-679db5d9cb-62psh from kubeaddons started at 2020-05-06 10:56:45 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container opsportal-kubeaddons-kommander-ui ready: true, restart count 0
May  6 14:22:31.752: INFO: dex-kubeaddons-79b77778bc-84p4b from kubeaddons started at 2020-05-06 10:59:20 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container main ready: true, restart count 1
May  6 14:22:31.752: INFO: opsportal-landing-6f6865b688-vb67f from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container opsportal-landing ready: true, restart count 0
May  6 14:22:31.752: INFO: kommander-kubeaddons-kommander-ui-7b9b585d95-rgnz6 from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container kommander-kubeaddons-kommander-ui ready: true, restart count 0
May  6 14:22:31.752: INFO: ebs-csi-node-ffszp from kube-system started at 2020-05-06 10:57:49 +0000 UTC (3 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 14:22:31.752: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 14:22:31.752: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  6 14:22:31.752: INFO: prometheus-kubeaddons-prom-operator-cc56c6c-m8kfx from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (2 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container prometheus-operator ready: true, restart count 0
May  6 14:22:31.752: INFO: 	Container tls-proxy ready: true, restart count 0
May  6 14:22:31.752: INFO: prometheus-kubeaddons-prometheus-node-exporter-4jtfq from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container node-exporter ready: true, restart count 0
May  6 14:22:31.752: INFO: velero-kubeaddons-578b4667ff-6x2nw from velero started at 2020-05-06 10:59:40 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container velero ready: true, restart count 4
May  6 14:22:31.752: INFO: dex-k8s-authenticator-kubeaddons-6697855d4c-xbqks from kubeaddons started at 2020-05-06 10:59:29 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container dex-k8s-authenticator ready: true, restart count 0
May  6 14:22:31.752: INFO: fluentbit-kubeaddons-fluent-bit-wnqvd from kubeaddons started at 2020-05-06 11:02:11 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container fluent-bit ready: true, restart count 0
May  6 14:22:31.752: INFO: kommander-kubeaddons-thanos-query-7597bf4957-tmdgd from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container thanos-query ready: true, restart count 0
May  6 14:22:31.752: INFO: kube-proxy-ptcmp from kube-system started at 2020-05-06 10:55:19 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 14:22:31.752: INFO: calico-node-gm4h2 from kube-system started at 2020-05-06 10:55:38 +0000 UTC (2 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container bird-metrics ready: true, restart count 0
May  6 14:22:31.752: INFO: 	Container calico-node ready: true, restart count 0
May  6 14:22:31.752: INFO: elasticsearch-kubeaddons-client-d4f5db58d-7g4sd from kubeaddons started at 2020-05-06 10:57:52 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 14:22:31.752: INFO: minio-0 from velero started at 2020-05-06 10:59:44 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container minio ready: true, restart count 0
May  6 14:22:31.752: INFO: gatekeeper-kubeaddons-fdc87db85-5rj6d from kubeaddons started at 2020-05-06 10:57:40 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container manager ready: true, restart count 0
May  6 14:22:31.752: INFO: sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-hqgql from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 14:22:31.752: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 14:22:31.752: INFO: dex-kubeaddons-dex-controller-6b6c9fbd7f-pxz8h from kubeaddons started at 2020-05-06 10:58:46 +0000 UTC (2 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  6 14:22:31.752: INFO: 	Container manager ready: true, restart count 0
May  6 14:22:31.752: INFO: kubefed-controller-manager-55fd5b4dff-g9m6m from kommander started at 2020-05-06 11:00:41 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container controller-manager ready: true, restart count 0
May  6 14:22:31.752: INFO: traefik-kubeaddons-1.72.17-5pttd from kubeaddons started at 2020-05-06 10:57:38 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container traefik ready: false, restart count 0
May  6 14:22:31.752: INFO: prometheus-kubeaddons-kube-state-metrics-78c65cc7bc-66zk8 from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container kube-state-metrics ready: true, restart count 0
May  6 14:22:31.752: INFO: kube-oidc-proxy-kubeaddons-6fbd5c8fc4-xtmsg from kubeaddons started at 2020-05-06 10:59:18 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container kube-oidc-proxy ready: true, restart count 0
May  6 14:22:31.752: INFO: traefik-forward-auth-kubeaddons-8bdddfcd-wdrjp from kubeaddons started at 2020-05-06 10:59:51 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.752: INFO: 	Container traefik-forward-auth ready: true, restart count 0
May  6 14:22:31.752: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-131-80.us-west-2.compute.internal before test
May  6 14:22:31.757: INFO: sonobuoy from sonobuoy started at 2020-05-06 13:56:26 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.757: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  6 14:22:31.757: INFO: sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-cvh6w from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 14:22:31.757: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 14:22:31.757: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 14:22:31.757: INFO: minio-1 from velero started at 2020-05-06 13:31:21 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.757: INFO: 	Container minio ready: true, restart count 0
May  6 14:22:31.757: INFO: prometheus-kubeaddons-prometheus-node-exporter-2qwk2 from kubeaddons started at 2020-05-06 13:32:37 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.757: INFO: 	Container node-exporter ready: true, restart count 0
May  6 14:22:31.757: INFO: calico-node-mrkdh from kube-system started at 2020-05-06 10:55:38 +0000 UTC (2 container statuses recorded)
May  6 14:22:31.757: INFO: 	Container bird-metrics ready: true, restart count 0
May  6 14:22:31.757: INFO: 	Container calico-node ready: true, restart count 0
May  6 14:22:31.757: INFO: sonobuoy-e2e-job-7496ec37786a4f0c from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 14:22:31.757: INFO: 	Container e2e ready: true, restart count 0
May  6 14:22:31.757: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 14:22:31.757: INFO: prometheus-prometheus-kubeaddons-prom-prometheus-0 from kubeaddons started at 2020-05-06 13:31:17 +0000 UTC (4 container statuses recorded)
May  6 14:22:31.757: INFO: 	Container prometheus ready: true, restart count 0
May  6 14:22:31.757: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May  6 14:22:31.757: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May  6 14:22:31.757: INFO: 	Container thanos-sidecar ready: true, restart count 0
May  6 14:22:31.757: INFO: ebs-csi-node-722lq from kube-system started at 2020-05-06 10:57:49 +0000 UTC (3 container statuses recorded)
May  6 14:22:31.757: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 14:22:31.757: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 14:22:31.757: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  6 14:22:31.757: INFO: kube-proxy-vmxhd from kube-system started at 2020-05-06 10:55:19 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.757: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 14:22:31.757: INFO: elasticsearch-kubeaddons-master-0 from kubeaddons started at 2020-05-06 13:31:19 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.757: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 14:22:31.757: INFO: ebs-csi-controller-0 from kube-system started at 2020-05-06 13:31:16 +0000 UTC (5 container statuses recorded)
May  6 14:22:31.757: INFO: 	Container csi-attacher ready: true, restart count 0
May  6 14:22:31.757: INFO: 	Container csi-provisioner ready: true, restart count 0
May  6 14:22:31.757: INFO: 	Container csi-snapshotter ready: true, restart count 0
May  6 14:22:31.757: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 14:22:31.757: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 14:22:31.757: INFO: fluentbit-kubeaddons-fluent-bit-w966k from kubeaddons started at 2020-05-06 13:31:20 +0000 UTC (1 container statuses recorded)
May  6 14:22:31.757: INFO: 	Container fluent-bit ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node ip-10-0-129-1.us-west-2.compute.internal
STEP: verifying the node has the label node ip-10-0-129-108.us-west-2.compute.internal
STEP: verifying the node has the label node ip-10-0-131-234.us-west-2.compute.internal
STEP: verifying the node has the label node ip-10-0-131-80.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod cert-manager-kubeaddons-7d7f98fbc6-qpl75 requesting resource cpu=0m on Node ip-10-0-129-1.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod cert-manager-kubeaddons-cainjector-6dcd94769b-ng6xl requesting resource cpu=0m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod cert-manager-kubeaddons-webhook-77fbc6d59b-fbbjd requesting resource cpu=0m on Node ip-10-0-129-1.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod kcl-cm-6588c896f9-4b2gk requesting resource cpu=500m on Node ip-10-0-129-1.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod kcl-tfcb-9c88c7bc9-lbgz6 requesting resource cpu=100m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod kcl-utility-apiserver-5d448f665b-gf6gv requesting resource cpu=100m on Node ip-10-0-129-1.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod kcl-webhook-fb4dfc7d7-vv757 requesting resource cpu=100m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod kommander-kubeaddons-grafana-645f957f8f-96sf5 requesting resource cpu=0m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod kommander-kubeaddons-karma-7688944fcf-hpnzl requesting resource cpu=0m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod kommander-kubeaddons-kommander-ui-7b9b585d95-rgnz6 requesting resource cpu=100m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod kommander-kubeaddons-kubeaddons-catalog-6654f856df-ccmkl requesting resource cpu=100m on Node ip-10-0-129-1.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod kommander-kubeaddons-thanos-query-7597bf4957-tmdgd requesting resource cpu=0m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod kubefed-admission-webhook-5fb847574f-6mjm2 requesting resource cpu=0m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod kubefed-controller-manager-55fd5b4dff-bvsl6 requesting resource cpu=100m on Node ip-10-0-129-1.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod kubefed-controller-manager-55fd5b4dff-g9m6m requesting resource cpu=100m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod calico-kube-controllers-84f666455-5t7xm requesting resource cpu=30m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod calico-node-gm4h2 requesting resource cpu=300m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod calico-node-mrkdh requesting resource cpu=300m on Node ip-10-0-131-80.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod calico-node-wpbh7 requesting resource cpu=300m on Node ip-10-0-129-1.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod calico-node-z679x requesting resource cpu=300m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod coredns-5644d7b6d9-5jf9p requesting resource cpu=100m on Node ip-10-0-129-1.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod coredns-5644d7b6d9-hmtfs requesting resource cpu=100m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod ebs-csi-controller-0 requesting resource cpu=0m on Node ip-10-0-131-80.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod ebs-csi-node-722lq requesting resource cpu=0m on Node ip-10-0-131-80.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod ebs-csi-node-f2cbn requesting resource cpu=0m on Node ip-10-0-129-1.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod ebs-csi-node-ffszp requesting resource cpu=0m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod ebs-csi-node-vvbxt requesting resource cpu=0m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod kube-proxy-4lqz5 requesting resource cpu=0m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod kube-proxy-ptcmp requesting resource cpu=0m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod kube-proxy-v9c2p requesting resource cpu=0m on Node ip-10-0-129-1.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod kube-proxy-vmxhd requesting resource cpu=0m on Node ip-10-0-131-80.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod tiller-deploy-969865475-m2ph8 requesting resource cpu=0m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod alertmanager-prometheus-kubeaddons-prom-alertmanager-0 requesting resource cpu=110m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod dex-k8s-authenticator-kubeaddons-6697855d4c-xbqks requesting resource cpu=100m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod dex-kubeaddons-79b77778bc-84p4b requesting resource cpu=100m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod dex-kubeaddons-dex-controller-6b6c9fbd7f-pxz8h requesting resource cpu=100m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod dstorageclass-controller-manager-5c966c767f-4bf6x requesting resource cpu=0m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod elasticsearch-kubeaddons-client-d4f5db58d-7g4sd requesting resource cpu=100m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod elasticsearch-kubeaddons-client-d4f5db58d-7qrn8 requesting resource cpu=100m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod elasticsearch-kubeaddons-data-0 requesting resource cpu=1000m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod elasticsearch-kubeaddons-data-1 requesting resource cpu=1000m on Node ip-10-0-129-1.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod elasticsearch-kubeaddons-master-0 requesting resource cpu=500m on Node ip-10-0-131-80.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod elasticsearch-kubeaddons-master-1 requesting resource cpu=500m on Node ip-10-0-129-1.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod elasticsearch-kubeaddons-master-2 requesting resource cpu=500m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod elasticsearchexporter-kubeaddons-elasticsearch-exporter-845tdq8 requesting resource cpu=0m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod external-dns-kubeaddons-6c65ff8d88-sm9p2 requesting resource cpu=0m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod fluentbit-kubeaddons-fluent-bit-5rbrr requesting resource cpu=200m on Node ip-10-0-129-1.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod fluentbit-kubeaddons-fluent-bit-t6xpz requesting resource cpu=200m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod fluentbit-kubeaddons-fluent-bit-w966k requesting resource cpu=200m on Node ip-10-0-131-80.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod fluentbit-kubeaddons-fluent-bit-wnqvd requesting resource cpu=200m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod gatekeeper-kubeaddons-fdc87db85-5rj6d requesting resource cpu=200m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod kibana-kubeaddons-649897648c-mzc2h requesting resource cpu=100m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod kube-oidc-proxy-kubeaddons-6fbd5c8fc4-xtmsg requesting resource cpu=0m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod kubeaddons-controller-manager-77cf76b857-k89ml requesting resource cpu=100m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod kubernetes-dashboard-549989bcdf-2d767 requesting resource cpu=250m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod opsportal-kubeaddons-kommander-ui-679db5d9cb-62psh requesting resource cpu=100m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod opsportal-landing-6f6865b688-vb67f requesting resource cpu=100m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod prometheus-kubeaddons-grafana-99b49bd54-8h7z8 requesting resource cpu=200m on Node ip-10-0-129-1.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod prometheus-kubeaddons-kube-state-metrics-78c65cc7bc-66zk8 requesting resource cpu=0m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod prometheus-kubeaddons-prom-operator-cc56c6c-m8kfx requesting resource cpu=0m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod prometheus-kubeaddons-prometheus-node-exporter-2qwk2 requesting resource cpu=0m on Node ip-10-0-131-80.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod prometheus-kubeaddons-prometheus-node-exporter-2wrg9 requesting resource cpu=0m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod prometheus-kubeaddons-prometheus-node-exporter-4jtfq requesting resource cpu=0m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod prometheus-kubeaddons-prometheus-node-exporter-b6z8v requesting resource cpu=0m on Node ip-10-0-129-1.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod prometheus-prometheus-kubeaddons-prom-prometheus-0 requesting resource cpu=500m on Node ip-10-0-131-80.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod prometheusadapter-kubeaddons-prometheus-adapter-6b8975fc48pwzsr requesting resource cpu=1000m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod reloader-kubeaddons-reloader-7d4bd64cfb-xnxz8 requesting resource cpu=100m on Node ip-10-0-129-1.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod traefik-forward-auth-kubeaddons-8bdddfcd-wdrjp requesting resource cpu=100m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod traefik-kubeaddons-6f97669977-2sxh8 requesting resource cpu=500m on Node ip-10-0-129-1.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod traefik-kubeaddons-6f97669977-9fwr9 requesting resource cpu=500m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-131-80.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod sonobuoy-e2e-job-7496ec37786a4f0c requesting resource cpu=0m on Node ip-10-0-131-80.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-9wkcg requesting resource cpu=0m on Node ip-10-0-129-1.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-cvh6w requesting resource cpu=0m on Node ip-10-0-131-80.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-hqgql requesting resource cpu=0m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-zqwx9 requesting resource cpu=0m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod minio-0 requesting resource cpu=250m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod minio-1 requesting resource cpu=250m on Node ip-10-0-131-80.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod minio-2 requesting resource cpu=250m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod minio-3 requesting resource cpu=250m on Node ip-10-0-129-1.us-west-2.compute.internal
May  6 14:22:31.822: INFO: Pod velero-kubeaddons-578b4667ff-6x2nw requesting resource cpu=0m on Node ip-10-0-131-234.us-west-2.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
May  6 14:22:31.822: INFO: Creating a pod which consumes cpu=2835m on Node ip-10-0-129-1.us-west-2.compute.internal
May  6 14:22:31.829: INFO: Creating a pod which consumes cpu=3059m on Node ip-10-0-129-108.us-west-2.compute.internal
May  6 14:22:31.835: INFO: Creating a pod which consumes cpu=3528m on Node ip-10-0-131-234.us-west-2.compute.internal
May  6 14:22:31.844: INFO: Creating a pod which consumes cpu=4375m on Node ip-10-0-131-80.us-west-2.compute.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1812f2c7-8a07-471f-92d7-b9efb45bb435.160c7666a8a700b3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4103/filler-pod-1812f2c7-8a07-471f-92d7-b9efb45bb435 to ip-10-0-129-108.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1812f2c7-8a07-471f-92d7-b9efb45bb435.160c7666ce7e28b2], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1812f2c7-8a07-471f-92d7-b9efb45bb435.160c7666e3a14c9d], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1812f2c7-8a07-471f-92d7-b9efb45bb435.160c7666e67523dd], Reason = [Created], Message = [Created container filler-pod-1812f2c7-8a07-471f-92d7-b9efb45bb435]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1812f2c7-8a07-471f-92d7-b9efb45bb435.160c7666f07fa656], Reason = [Started], Message = [Started container filler-pod-1812f2c7-8a07-471f-92d7-b9efb45bb435]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6a00bcc9-e260-4dbe-9549-8bd24fd9f05c.160c7666a9575085], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4103/filler-pod-6a00bcc9-e260-4dbe-9549-8bd24fd9f05c to ip-10-0-131-80.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6a00bcc9-e260-4dbe-9549-8bd24fd9f05c.160c7666cce2d07e], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6a00bcc9-e260-4dbe-9549-8bd24fd9f05c.160c7666e529fa7f], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6a00bcc9-e260-4dbe-9549-8bd24fd9f05c.160c7666e7d9fb4e], Reason = [Created], Message = [Created container filler-pod-6a00bcc9-e260-4dbe-9549-8bd24fd9f05c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6a00bcc9-e260-4dbe-9549-8bd24fd9f05c.160c7666efdb0998], Reason = [Started], Message = [Started container filler-pod-6a00bcc9-e260-4dbe-9549-8bd24fd9f05c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7c34ee69-e400-4533-855c-991fc346ecb8.160c7666a90a64cb], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4103/filler-pod-7c34ee69-e400-4533-855c-991fc346ecb8 to ip-10-0-131-234.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7c34ee69-e400-4533-855c-991fc346ecb8.160c7666ced17b80], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7c34ee69-e400-4533-855c-991fc346ecb8.160c7666e50fc0a4], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7c34ee69-e400-4533-855c-991fc346ecb8.160c7666e7e175d6], Reason = [Created], Message = [Created container filler-pod-7c34ee69-e400-4533-855c-991fc346ecb8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7c34ee69-e400-4533-855c-991fc346ecb8.160c7666f27e70c6], Reason = [Started], Message = [Started container filler-pod-7c34ee69-e400-4533-855c-991fc346ecb8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d3722fd1-c991-40d5-9f19-13f1cbfa7898.160c7666a81d4eba], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4103/filler-pod-d3722fd1-c991-40d5-9f19-13f1cbfa7898 to ip-10-0-129-1.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d3722fd1-c991-40d5-9f19-13f1cbfa7898.160c7666cb36ce88], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d3722fd1-c991-40d5-9f19-13f1cbfa7898.160c7666e3fd3af7], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d3722fd1-c991-40d5-9f19-13f1cbfa7898.160c7666e762233f], Reason = [Created], Message = [Created container filler-pod-d3722fd1-c991-40d5-9f19-13f1cbfa7898]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d3722fd1-c991-40d5-9f19-13f1cbfa7898.160c7666efe0d9b6], Reason = [Started], Message = [Started container filler-pod-d3722fd1-c991-40d5-9f19-13f1cbfa7898]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.160c7667218a2435], Reason = [FailedScheduling], Message = [0/7 nodes are available: 7 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.160c766721fc871e], Reason = [FailedScheduling], Message = [0/7 nodes are available: 7 Insufficient cpu.]
STEP: removing the label node off the node ip-10-0-129-1.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-129-108.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-131-234.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-131-80.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:22:34.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4103" for this suite.
May  6 14:22:40.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:22:43.684: INFO: namespace sched-pred-4103 deletion completed in 8.769857006s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:12.115 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:22:43.684: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-131
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
May  6 14:22:43.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 --namespace=kubectl-131 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
May  6 14:22:46.175: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
May  6 14:22:46.175: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:22:48.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-131" for this suite.
May  6 14:22:54.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:22:56.950: INFO: namespace kubectl-131 deletion completed in 8.766807018s

• [SLOW TEST:13.266 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:22:56.950: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2755
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
May  6 14:22:59.107: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-926700516 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
May  6 14:23:09.166: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:23:09.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2755" for this suite.
May  6 14:23:15.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:23:17.930: INFO: namespace pods-2755 deletion completed in 8.759535844s

• [SLOW TEST:20.980 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:23:17.931: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-547
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:23:25.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-547" for this suite.
May  6 14:23:31.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:23:33.836: INFO: namespace resourcequota-547 deletion completed in 8.760259148s

• [SLOW TEST:15.905 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:23:33.836: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6447
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:23:33.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6447" for this suite.
May  6 14:23:39.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:23:42.732: INFO: namespace services-6447 deletion completed in 8.759008529s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:8.897 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:23:42.733: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-322
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
May  6 14:23:42.863: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:23:47.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-322" for this suite.
May  6 14:23:59.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:24:01.922: INFO: namespace init-container-322 deletion completed in 14.763974237s

• [SLOW TEST:19.189 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:24:01.922: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-5975
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-5975
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-5975
STEP: Deleting pre-stop pod
May  6 14:24:11.085: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:24:11.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5975" for this suite.
May  6 14:24:47.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:24:49.854: INFO: namespace prestop-5975 deletion completed in 38.761159928s

• [SLOW TEST:47.932 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:24:49.855: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-3228
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
May  6 14:24:50.036: INFO: Waiting up to 1m0s for all nodes to be ready
May  6 14:25:50.079: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 14:25:50.081: INFO: Starting informer...
STEP: Starting pod...
May  6 14:25:50.296: INFO: Pod is running on ip-10-0-131-80.us-west-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
May  6 14:25:50.306: INFO: Pod wasn't evicted. Proceeding
May  6 14:25:50.306: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
May  6 14:27:05.319: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:27:05.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-3228" for this suite.
May  6 14:27:33.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:27:36.082: INFO: namespace taint-single-pod-3228 deletion completed in 30.75977422s

• [SLOW TEST:166.228 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:27:36.082: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6499
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-6499/configmap-test-6ded2a1d-2223-4282-a05d-1e8ee19239f9
STEP: Creating a pod to test consume configMaps
May  6 14:27:36.228: INFO: Waiting up to 5m0s for pod "pod-configmaps-89e2a78b-8351-48b6-a241-c4355647b47c" in namespace "configmap-6499" to be "success or failure"
May  6 14:27:36.230: INFO: Pod "pod-configmaps-89e2a78b-8351-48b6-a241-c4355647b47c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.773608ms
May  6 14:27:38.233: INFO: Pod "pod-configmaps-89e2a78b-8351-48b6-a241-c4355647b47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004441951s
STEP: Saw pod success
May  6 14:27:38.233: INFO: Pod "pod-configmaps-89e2a78b-8351-48b6-a241-c4355647b47c" satisfied condition "success or failure"
May  6 14:27:38.235: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-configmaps-89e2a78b-8351-48b6-a241-c4355647b47c container env-test: <nil>
STEP: delete the pod
May  6 14:27:38.253: INFO: Waiting for pod pod-configmaps-89e2a78b-8351-48b6-a241-c4355647b47c to disappear
May  6 14:27:38.255: INFO: Pod pod-configmaps-89e2a78b-8351-48b6-a241-c4355647b47c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:27:38.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6499" for this suite.
May  6 14:27:44.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:27:47.018: INFO: namespace configmap-6499 deletion completed in 8.759967993s

• [SLOW TEST:10.936 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:27:47.018: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9872
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 14:27:47.159: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f31a08de-7783-49d5-b0c2-5754f40d9c71" in namespace "projected-9872" to be "success or failure"
May  6 14:27:47.164: INFO: Pod "downwardapi-volume-f31a08de-7783-49d5-b0c2-5754f40d9c71": Phase="Pending", Reason="", readiness=false. Elapsed: 5.212679ms
May  6 14:27:49.167: INFO: Pod "downwardapi-volume-f31a08de-7783-49d5-b0c2-5754f40d9c71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008242966s
May  6 14:27:51.170: INFO: Pod "downwardapi-volume-f31a08de-7783-49d5-b0c2-5754f40d9c71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011007707s
STEP: Saw pod success
May  6 14:27:51.170: INFO: Pod "downwardapi-volume-f31a08de-7783-49d5-b0c2-5754f40d9c71" satisfied condition "success or failure"
May  6 14:27:51.172: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod downwardapi-volume-f31a08de-7783-49d5-b0c2-5754f40d9c71 container client-container: <nil>
STEP: delete the pod
May  6 14:27:51.184: INFO: Waiting for pod downwardapi-volume-f31a08de-7783-49d5-b0c2-5754f40d9c71 to disappear
May  6 14:27:51.185: INFO: Pod downwardapi-volume-f31a08de-7783-49d5-b0c2-5754f40d9c71 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:27:51.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9872" for this suite.
May  6 14:27:57.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:27:59.948: INFO: namespace projected-9872 deletion completed in 8.759760349s

• [SLOW TEST:12.930 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:27:59.948: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5237
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-2b512b9c-b561-4068-9cbe-f7c33124dbe5
STEP: Creating a pod to test consume configMaps
May  6 14:28:00.088: INFO: Waiting up to 5m0s for pod "pod-configmaps-124b653b-c43a-4034-9ba2-9ce5d7906673" in namespace "configmap-5237" to be "success or failure"
May  6 14:28:00.090: INFO: Pod "pod-configmaps-124b653b-c43a-4034-9ba2-9ce5d7906673": Phase="Pending", Reason="", readiness=false. Elapsed: 1.786742ms
May  6 14:28:02.093: INFO: Pod "pod-configmaps-124b653b-c43a-4034-9ba2-9ce5d7906673": Phase="Running", Reason="", readiness=true. Elapsed: 2.004505929s
May  6 14:28:04.095: INFO: Pod "pod-configmaps-124b653b-c43a-4034-9ba2-9ce5d7906673": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007143711s
STEP: Saw pod success
May  6 14:28:04.095: INFO: Pod "pod-configmaps-124b653b-c43a-4034-9ba2-9ce5d7906673" satisfied condition "success or failure"
May  6 14:28:04.097: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-configmaps-124b653b-c43a-4034-9ba2-9ce5d7906673 container configmap-volume-test: <nil>
STEP: delete the pod
May  6 14:28:04.112: INFO: Waiting for pod pod-configmaps-124b653b-c43a-4034-9ba2-9ce5d7906673 to disappear
May  6 14:28:04.113: INFO: Pod pod-configmaps-124b653b-c43a-4034-9ba2-9ce5d7906673 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:28:04.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5237" for this suite.
May  6 14:28:10.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:28:12.876: INFO: namespace configmap-5237 deletion completed in 8.75940147s

• [SLOW TEST:12.928 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:28:12.876: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4989
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 14:28:13.014: INFO: Waiting up to 5m0s for pod "downwardapi-volume-685dc74f-4433-43db-b0a3-2843d802e84b" in namespace "downward-api-4989" to be "success or failure"
May  6 14:28:13.016: INFO: Pod "downwardapi-volume-685dc74f-4433-43db-b0a3-2843d802e84b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.909225ms
May  6 14:28:15.019: INFO: Pod "downwardapi-volume-685dc74f-4433-43db-b0a3-2843d802e84b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004627114s
STEP: Saw pod success
May  6 14:28:15.019: INFO: Pod "downwardapi-volume-685dc74f-4433-43db-b0a3-2843d802e84b" satisfied condition "success or failure"
May  6 14:28:15.021: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod downwardapi-volume-685dc74f-4433-43db-b0a3-2843d802e84b container client-container: <nil>
STEP: delete the pod
May  6 14:28:15.033: INFO: Waiting for pod downwardapi-volume-685dc74f-4433-43db-b0a3-2843d802e84b to disappear
May  6 14:28:15.035: INFO: Pod downwardapi-volume-685dc74f-4433-43db-b0a3-2843d802e84b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:28:15.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4989" for this suite.
May  6 14:28:21.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:28:23.797: INFO: namespace downward-api-4989 deletion completed in 8.759146302s

• [SLOW TEST:10.921 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:28:23.797: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8170
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 14:28:24.166: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 14:28:27.180: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 14:28:27.182: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3393-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:28:28.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8170" for this suite.
May  6 14:28:34.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:28:37.075: INFO: namespace webhook-8170 deletion completed in 8.79112201s
STEP: Destroying namespace "webhook-8170-markers" for this suite.
May  6 14:28:43.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:28:45.836: INFO: namespace webhook-8170-markers deletion completed in 8.760019307s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:22.049 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:28:45.846: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5369
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
May  6 14:28:45.987: INFO: Waiting up to 5m0s for pod "pod-ab9c6d3a-e8c7-4b65-90c6-dbfaeeb7445f" in namespace "emptydir-5369" to be "success or failure"
May  6 14:28:45.988: INFO: Pod "pod-ab9c6d3a-e8c7-4b65-90c6-dbfaeeb7445f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.721188ms
May  6 14:28:47.991: INFO: Pod "pod-ab9c6d3a-e8c7-4b65-90c6-dbfaeeb7445f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004354707s
STEP: Saw pod success
May  6 14:28:47.991: INFO: Pod "pod-ab9c6d3a-e8c7-4b65-90c6-dbfaeeb7445f" satisfied condition "success or failure"
May  6 14:28:47.993: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-ab9c6d3a-e8c7-4b65-90c6-dbfaeeb7445f container test-container: <nil>
STEP: delete the pod
May  6 14:28:48.004: INFO: Waiting for pod pod-ab9c6d3a-e8c7-4b65-90c6-dbfaeeb7445f to disappear
May  6 14:28:48.006: INFO: Pod pod-ab9c6d3a-e8c7-4b65-90c6-dbfaeeb7445f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:28:48.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5369" for this suite.
May  6 14:28:54.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:28:56.770: INFO: namespace emptydir-5369 deletion completed in 8.759433913s

• [SLOW TEST:10.924 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:28:56.770: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2936
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May  6 14:29:00.958: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  6 14:29:00.960: INFO: Pod pod-with-poststart-http-hook still exists
May  6 14:29:02.960: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  6 14:29:02.963: INFO: Pod pod-with-poststart-http-hook still exists
May  6 14:29:04.960: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  6 14:29:04.963: INFO: Pod pod-with-poststart-http-hook still exists
May  6 14:29:06.960: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  6 14:29:06.964: INFO: Pod pod-with-poststart-http-hook still exists
May  6 14:29:08.960: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  6 14:29:08.963: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:29:08.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2936" for this suite.
May  6 14:29:20.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:29:23.739: INFO: namespace container-lifecycle-hook-2936 deletion completed in 14.767061147s

• [SLOW TEST:26.969 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:29:23.739: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9345
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-9345
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9345 to expose endpoints map[]
May  6 14:29:23.878: INFO: Get endpoints failed (1.892961ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
May  6 14:29:24.880: INFO: successfully validated that service multi-endpoint-test in namespace services-9345 exposes endpoints map[] (1.00434801s elapsed)
STEP: Creating pod pod1 in namespace services-9345
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9345 to expose endpoints map[pod1:[100]]
May  6 14:29:26.902: INFO: successfully validated that service multi-endpoint-test in namespace services-9345 exposes endpoints map[pod1:[100]] (2.014069049s elapsed)
STEP: Creating pod pod2 in namespace services-9345
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9345 to expose endpoints map[pod1:[100] pod2:[101]]
May  6 14:29:28.929: INFO: successfully validated that service multi-endpoint-test in namespace services-9345 exposes endpoints map[pod1:[100] pod2:[101]] (2.019714911s elapsed)
STEP: Deleting pod pod1 in namespace services-9345
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9345 to expose endpoints map[pod2:[101]]
May  6 14:29:29.941: INFO: successfully validated that service multi-endpoint-test in namespace services-9345 exposes endpoints map[pod2:[101]] (1.007805188s elapsed)
STEP: Deleting pod pod2 in namespace services-9345
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9345 to expose endpoints map[]
May  6 14:29:30.949: INFO: successfully validated that service multi-endpoint-test in namespace services-9345 exposes endpoints map[] (1.003839517s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:29:30.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9345" for this suite.
May  6 14:29:42.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:29:45.744: INFO: namespace services-9345 deletion completed in 14.761155781s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:22.005 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:29:45.744: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4172
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-058dccfe-5eb5-45fa-b696-c2794efc23ee in namespace container-probe-4172
May  6 14:29:49.886: INFO: Started pod busybox-058dccfe-5eb5-45fa-b696-c2794efc23ee in namespace container-probe-4172
STEP: checking the pod's current state and verifying that restartCount is present
May  6 14:29:49.888: INFO: Initial restart count of pod busybox-058dccfe-5eb5-45fa-b696-c2794efc23ee is 0
May  6 14:30:43.961: INFO: Restart count of pod container-probe-4172/busybox-058dccfe-5eb5-45fa-b696-c2794efc23ee is now 1 (54.073433063s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:30:43.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4172" for this suite.
May  6 14:30:49.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:30:52.733: INFO: namespace container-probe-4172 deletion completed in 8.76138916s

• [SLOW TEST:66.990 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:30:52.734: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-60
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-3976351e-d4fb-48c8-87ec-81eb910fc08c
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:30:56.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-60" for this suite.
May  6 14:31:08.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:31:11.664: INFO: namespace configmap-60 deletion completed in 14.763368624s

• [SLOW TEST:18.931 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:31:11.664: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-5276
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 14:31:11.805: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-dcce1ea2-ae36-4554-a038-16f5e1ac8985" in namespace "security-context-test-5276" to be "success or failure"
May  6 14:31:11.807: INFO: Pod "busybox-readonly-false-dcce1ea2-ae36-4554-a038-16f5e1ac8985": Phase="Pending", Reason="", readiness=false. Elapsed: 1.982619ms
May  6 14:31:13.809: INFO: Pod "busybox-readonly-false-dcce1ea2-ae36-4554-a038-16f5e1ac8985": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004675445s
May  6 14:31:13.809: INFO: Pod "busybox-readonly-false-dcce1ea2-ae36-4554-a038-16f5e1ac8985" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:31:13.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5276" for this suite.
May  6 14:31:19.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:31:22.572: INFO: namespace security-context-test-5276 deletion completed in 8.759892716s

• [SLOW TEST:10.908 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:31:22.573: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-454
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
May  6 14:31:22.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 create -f - --namespace=kubectl-454'
May  6 14:31:22.962: INFO: stderr: ""
May  6 14:31:22.962: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May  6 14:31:23.965: INFO: Selector matched 1 pods for map[app:redis]
May  6 14:31:23.965: INFO: Found 0 / 1
May  6 14:31:24.965: INFO: Selector matched 1 pods for map[app:redis]
May  6 14:31:24.965: INFO: Found 1 / 1
May  6 14:31:24.965: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May  6 14:31:24.967: INFO: Selector matched 1 pods for map[app:redis]
May  6 14:31:24.967: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  6 14:31:24.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 patch pod redis-master-twkp2 --namespace=kubectl-454 -p {"metadata":{"annotations":{"x":"y"}}}'
May  6 14:31:25.079: INFO: stderr: ""
May  6 14:31:25.079: INFO: stdout: "pod/redis-master-twkp2 patched\n"
STEP: checking annotations
May  6 14:31:25.082: INFO: Selector matched 1 pods for map[app:redis]
May  6 14:31:25.082: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:31:25.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-454" for this suite.
May  6 14:31:53.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:31:55.845: INFO: namespace kubectl-454 deletion completed in 30.75982584s

• [SLOW TEST:33.272 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:31:55.845: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6126
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May  6 14:31:55.981: INFO: Pod name pod-release: Found 0 pods out of 1
May  6 14:32:00.984: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:32:01.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6126" for this suite.
May  6 14:32:08.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:32:10.760: INFO: namespace replication-controller-6126 deletion completed in 8.759831836s

• [SLOW TEST:14.915 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:32:10.760: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8947
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-9f030f15-bc3b-4532-84f4-713a0b46c202
STEP: Creating a pod to test consume secrets
May  6 14:32:10.902: INFO: Waiting up to 5m0s for pod "pod-secrets-f81f278f-6966-4d08-bde7-a67588089a39" in namespace "secrets-8947" to be "success or failure"
May  6 14:32:10.904: INFO: Pod "pod-secrets-f81f278f-6966-4d08-bde7-a67588089a39": Phase="Pending", Reason="", readiness=false. Elapsed: 1.688169ms
May  6 14:32:12.906: INFO: Pod "pod-secrets-f81f278f-6966-4d08-bde7-a67588089a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00431671s
May  6 14:32:14.909: INFO: Pod "pod-secrets-f81f278f-6966-4d08-bde7-a67588089a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007105116s
STEP: Saw pod success
May  6 14:32:14.909: INFO: Pod "pod-secrets-f81f278f-6966-4d08-bde7-a67588089a39" satisfied condition "success or failure"
May  6 14:32:14.911: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-secrets-f81f278f-6966-4d08-bde7-a67588089a39 container secret-env-test: <nil>
STEP: delete the pod
May  6 14:32:14.922: INFO: Waiting for pod pod-secrets-f81f278f-6966-4d08-bde7-a67588089a39 to disappear
May  6 14:32:14.924: INFO: Pod pod-secrets-f81f278f-6966-4d08-bde7-a67588089a39 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:32:14.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8947" for this suite.
May  6 14:32:20.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:32:23.692: INFO: namespace secrets-8947 deletion completed in 8.765142924s

• [SLOW TEST:12.931 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:32:23.692: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3450
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3450
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-3450
I0506 14:32:23.843390      25 runners.go:184] Created replication controller with name: externalname-service, namespace: services-3450, replica count: 2
May  6 14:32:26.893: INFO: Creating new exec pod
I0506 14:32:26.893665      25 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  6 14:32:29.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=services-3450 execpodzqvzl -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May  6 14:32:30.179: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May  6 14:32:30.180: INFO: stdout: ""
May  6 14:32:30.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=services-3450 execpodzqvzl -- /bin/sh -x -c nc -zv -t -w 2 10.0.11.194 80'
May  6 14:32:30.452: INFO: stderr: "+ nc -zv -t -w 2 10.0.11.194 80\nConnection to 10.0.11.194 80 port [tcp/http] succeeded!\n"
May  6 14:32:30.452: INFO: stdout: ""
May  6 14:32:30.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=services-3450 execpodzqvzl -- /bin/sh -x -c nc -zv -t -w 2 10.0.129.1 32252'
May  6 14:32:30.709: INFO: stderr: "+ nc -zv -t -w 2 10.0.129.1 32252\nConnection to 10.0.129.1 32252 port [tcp/32252] succeeded!\n"
May  6 14:32:30.709: INFO: stdout: ""
May  6 14:32:30.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=services-3450 execpodzqvzl -- /bin/sh -x -c nc -zv -t -w 2 10.0.129.108 32252'
May  6 14:32:30.970: INFO: stderr: "+ nc -zv -t -w 2 10.0.129.108 32252\nConnection to 10.0.129.108 32252 port [tcp/32252] succeeded!\n"
May  6 14:32:30.970: INFO: stdout: ""
May  6 14:32:30.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=services-3450 execpodzqvzl -- /bin/sh -x -c nc -zv -t -w 2 34.211.120.98 32252'
May  6 14:32:31.237: INFO: stderr: "+ nc -zv -t -w 2 34.211.120.98 32252\nConnection to 34.211.120.98 32252 port [tcp/32252] succeeded!\n"
May  6 14:32:31.237: INFO: stdout: ""
May  6 14:32:31.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=services-3450 execpodzqvzl -- /bin/sh -x -c nc -zv -t -w 2 54.148.156.50 32252'
May  6 14:32:31.500: INFO: stderr: "+ nc -zv -t -w 2 54.148.156.50 32252\nConnection to 54.148.156.50 32252 port [tcp/32252] succeeded!\n"
May  6 14:32:31.500: INFO: stdout: ""
May  6 14:32:31.500: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:32:31.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3450" for this suite.
May  6 14:32:37.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:32:40.280: INFO: namespace services-3450 deletion completed in 8.760269513s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:16.589 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:32:40.280: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9771
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-1631e557-d50e-49a5-8566-43f3671d77aa
STEP: Creating a pod to test consume secrets
May  6 14:32:40.420: INFO: Waiting up to 5m0s for pod "pod-secrets-538ed536-3907-42d9-95c7-d860d7154733" in namespace "secrets-9771" to be "success or failure"
May  6 14:32:40.422: INFO: Pod "pod-secrets-538ed536-3907-42d9-95c7-d860d7154733": Phase="Pending", Reason="", readiness=false. Elapsed: 1.685893ms
May  6 14:32:42.425: INFO: Pod "pod-secrets-538ed536-3907-42d9-95c7-d860d7154733": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004636738s
STEP: Saw pod success
May  6 14:32:42.425: INFO: Pod "pod-secrets-538ed536-3907-42d9-95c7-d860d7154733" satisfied condition "success or failure"
May  6 14:32:42.428: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-secrets-538ed536-3907-42d9-95c7-d860d7154733 container secret-volume-test: <nil>
STEP: delete the pod
May  6 14:32:42.441: INFO: Waiting for pod pod-secrets-538ed536-3907-42d9-95c7-d860d7154733 to disappear
May  6 14:32:42.446: INFO: Pod pod-secrets-538ed536-3907-42d9-95c7-d860d7154733 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:32:42.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9771" for this suite.
May  6 14:32:48.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:32:51.212: INFO: namespace secrets-9771 deletion completed in 8.761555464s

• [SLOW TEST:10.931 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:32:51.212: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5483
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 14:32:51.351: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cb223ab5-915b-4ec7-b2ac-f29520c0e25c" in namespace "downward-api-5483" to be "success or failure"
May  6 14:32:51.353: INFO: Pod "downwardapi-volume-cb223ab5-915b-4ec7-b2ac-f29520c0e25c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.9934ms
May  6 14:32:53.355: INFO: Pod "downwardapi-volume-cb223ab5-915b-4ec7-b2ac-f29520c0e25c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004480802s
STEP: Saw pod success
May  6 14:32:53.355: INFO: Pod "downwardapi-volume-cb223ab5-915b-4ec7-b2ac-f29520c0e25c" satisfied condition "success or failure"
May  6 14:32:53.357: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod downwardapi-volume-cb223ab5-915b-4ec7-b2ac-f29520c0e25c container client-container: <nil>
STEP: delete the pod
May  6 14:32:53.368: INFO: Waiting for pod downwardapi-volume-cb223ab5-915b-4ec7-b2ac-f29520c0e25c to disappear
May  6 14:32:53.370: INFO: Pod downwardapi-volume-cb223ab5-915b-4ec7-b2ac-f29520c0e25c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:32:53.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5483" for this suite.
May  6 14:32:59.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:33:02.144: INFO: namespace downward-api-5483 deletion completed in 8.771647833s

• [SLOW TEST:10.933 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:33:02.145: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3260
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
May  6 14:33:02.283: INFO: Waiting up to 5m0s for pod "pod-5c07add8-e917-4e28-bbc8-6e55d8fcf415" in namespace "emptydir-3260" to be "success or failure"
May  6 14:33:02.285: INFO: Pod "pod-5c07add8-e917-4e28-bbc8-6e55d8fcf415": Phase="Pending", Reason="", readiness=false. Elapsed: 1.729002ms
May  6 14:33:04.288: INFO: Pod "pod-5c07add8-e917-4e28-bbc8-6e55d8fcf415": Phase="Running", Reason="", readiness=true. Elapsed: 2.004502216s
May  6 14:33:06.291: INFO: Pod "pod-5c07add8-e917-4e28-bbc8-6e55d8fcf415": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007219944s
STEP: Saw pod success
May  6 14:33:06.291: INFO: Pod "pod-5c07add8-e917-4e28-bbc8-6e55d8fcf415" satisfied condition "success or failure"
May  6 14:33:06.293: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-5c07add8-e917-4e28-bbc8-6e55d8fcf415 container test-container: <nil>
STEP: delete the pod
May  6 14:33:06.304: INFO: Waiting for pod pod-5c07add8-e917-4e28-bbc8-6e55d8fcf415 to disappear
May  6 14:33:06.306: INFO: Pod pod-5c07add8-e917-4e28-bbc8-6e55d8fcf415 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:33:06.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3260" for this suite.
May  6 14:33:12.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:33:15.068: INFO: namespace emptydir-3260 deletion completed in 8.759306099s

• [SLOW TEST:12.923 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:33:15.068: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9350
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
May  6 14:33:15.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 api-versions'
May  6 14:33:15.256: INFO: stderr: ""
May  6 14:33:15.256: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncertmanager.k8s.io/v1alpha1\nconfig.gatekeeper.sh/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncore.kubefed.io/v1alpha1\ncore.kubefed.io/v1beta1\ncrd.projectcalico.org/v1\ncustom.metrics.k8s.io/v1beta1\ndex.coreos.com/v1\ndex.mesosphere.io/v1alpha1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nkommander.mesosphere.io/v1beta1\nkubeaddons.mesosphere.io/v1beta1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmulticlusterdns.kubefed.io/v1alpha1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nscheduling.kubefed.io/v1alpha1\nsnapshot.storage.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplates.gatekeeper.sh/v1alpha1\ntemplates.gatekeeper.sh/v1beta1\ntypes.kubefed.io/v1beta1\nv1\nvelero.io/v1\nwebhook.certmanager.k8s.io/v1beta1\nwebhook.kommander.mesosphere.io/v1beta1\nwebhook.workspaces.kommander.mesosphere.io/v1alpha1\nworkspaces.kommander.mesosphere.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:33:15.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9350" for this suite.
May  6 14:33:21.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:33:24.019: INFO: namespace kubectl-9350 deletion completed in 8.760237717s

• [SLOW TEST:8.951 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:33:24.019: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2841
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May  6 14:33:29.178: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:33:30.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2841" for this suite.
May  6 14:33:58.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:34:00.954: INFO: namespace replicaset-2841 deletion completed in 30.759838187s

• [SLOW TEST:36.934 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:34:00.954: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1467
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May  6 14:34:01.105: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1467 /api/v1/namespaces/watch-1467/configmaps/e2e-watch-test-label-changed c4b77c7d-f247-45a3-b013-6c181de75dca 99999 0 2020-05-06 14:34:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May  6 14:34:01.105: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1467 /api/v1/namespaces/watch-1467/configmaps/e2e-watch-test-label-changed c4b77c7d-f247-45a3-b013-6c181de75dca 100002 0 2020-05-06 14:34:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May  6 14:34:01.105: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1467 /api/v1/namespaces/watch-1467/configmaps/e2e-watch-test-label-changed c4b77c7d-f247-45a3-b013-6c181de75dca 100003 0 2020-05-06 14:34:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May  6 14:34:11.124: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1467 /api/v1/namespaces/watch-1467/configmaps/e2e-watch-test-label-changed c4b77c7d-f247-45a3-b013-6c181de75dca 100046 0 2020-05-06 14:34:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May  6 14:34:11.124: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1467 /api/v1/namespaces/watch-1467/configmaps/e2e-watch-test-label-changed c4b77c7d-f247-45a3-b013-6c181de75dca 100047 0 2020-05-06 14:34:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
May  6 14:34:11.124: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1467 /api/v1/namespaces/watch-1467/configmaps/e2e-watch-test-label-changed c4b77c7d-f247-45a3-b013-6c181de75dca 100048 0 2020-05-06 14:34:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:34:11.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1467" for this suite.
May  6 14:34:17.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:34:19.887: INFO: namespace watch-1467 deletion completed in 8.759541052s

• [SLOW TEST:18.933 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:34:19.887: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8166
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May  6 14:34:20.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-8166'
May  6 14:34:20.274: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May  6 14:34:20.274: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
May  6 14:34:20.277: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
May  6 14:34:20.283: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
May  6 14:34:20.295: INFO: scanned /root for discovery docs: <nil>
May  6 14:34:20.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-8166'
May  6 14:34:31.240: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May  6 14:34:31.240: INFO: stdout: "Created e2e-test-httpd-rc-8ee5dc8a05c17361392270326c94e3c1\nScaling up e2e-test-httpd-rc-8ee5dc8a05c17361392270326c94e3c1 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-8ee5dc8a05c17361392270326c94e3c1 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-8ee5dc8a05c17361392270326c94e3c1 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
May  6 14:34:31.240: INFO: stdout: "Created e2e-test-httpd-rc-8ee5dc8a05c17361392270326c94e3c1\nScaling up e2e-test-httpd-rc-8ee5dc8a05c17361392270326c94e3c1 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-8ee5dc8a05c17361392270326c94e3c1 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-8ee5dc8a05c17361392270326c94e3c1 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
May  6 14:34:31.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-8166'
May  6 14:34:31.348: INFO: stderr: ""
May  6 14:34:31.349: INFO: stdout: "e2e-test-httpd-rc-55rb6 e2e-test-httpd-rc-8ee5dc8a05c17361392270326c94e3c1-vvd4w "
STEP: Replicas for run=e2e-test-httpd-rc: expected=1 actual=2
May  6 14:34:36.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-8166'
May  6 14:34:36.454: INFO: stderr: ""
May  6 14:34:36.454: INFO: stdout: "e2e-test-httpd-rc-55rb6 e2e-test-httpd-rc-8ee5dc8a05c17361392270326c94e3c1-vvd4w "
STEP: Replicas for run=e2e-test-httpd-rc: expected=1 actual=2
May  6 14:34:41.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-8166'
May  6 14:34:41.556: INFO: stderr: ""
May  6 14:34:41.556: INFO: stdout: "e2e-test-httpd-rc-8ee5dc8a05c17361392270326c94e3c1-vvd4w "
May  6 14:34:41.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods e2e-test-httpd-rc-8ee5dc8a05c17361392270326c94e3c1-vvd4w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8166'
May  6 14:34:41.658: INFO: stderr: ""
May  6 14:34:41.658: INFO: stdout: "true"
May  6 14:34:41.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods e2e-test-httpd-rc-8ee5dc8a05c17361392270326c94e3c1-vvd4w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8166'
May  6 14:34:41.759: INFO: stderr: ""
May  6 14:34:41.759: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
May  6 14:34:41.759: INFO: e2e-test-httpd-rc-8ee5dc8a05c17361392270326c94e3c1-vvd4w is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
May  6 14:34:41.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 delete rc e2e-test-httpd-rc --namespace=kubectl-8166'
May  6 14:34:41.866: INFO: stderr: ""
May  6 14:34:41.866: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:34:41.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8166" for this suite.
May  6 14:34:47.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:34:50.629: INFO: namespace kubectl-8166 deletion completed in 8.760603871s

• [SLOW TEST:30.742 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:34:50.630: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8034
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May  6 14:34:50.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-8034'
May  6 14:34:50.837: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May  6 14:34:50.837: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
May  6 14:34:52.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 delete deployment e2e-test-httpd-deployment --namespace=kubectl-8034'
May  6 14:34:52.954: INFO: stderr: ""
May  6 14:34:52.954: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:34:52.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8034" for this suite.
May  6 14:36:24.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:36:27.718: INFO: namespace kubectl-8034 deletion completed in 1m34.761306279s

• [SLOW TEST:97.089 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:36:27.718: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9177
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
May  6 14:36:27.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 cluster-info'
May  6 14:36:27.950: INFO: stderr: ""
May  6 14:36:27.950: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:36:27.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9177" for this suite.
May  6 14:36:33.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:36:36.715: INFO: namespace kubectl-9177 deletion completed in 8.761584835s

• [SLOW TEST:8.997 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:36:36.715: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6659
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
May  6 14:36:36.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 create -f - --namespace=kubectl-6659'
May  6 14:36:37.109: INFO: stderr: ""
May  6 14:36:37.109: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  6 14:36:37.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6659'
May  6 14:36:37.213: INFO: stderr: ""
May  6 14:36:37.213: INFO: stdout: "update-demo-nautilus-l2hnm update-demo-nautilus-l84sd "
May  6 14:36:37.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-nautilus-l2hnm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6659'
May  6 14:36:37.315: INFO: stderr: ""
May  6 14:36:37.315: INFO: stdout: ""
May  6 14:36:37.315: INFO: update-demo-nautilus-l2hnm is created but not running
May  6 14:36:42.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6659'
May  6 14:36:42.419: INFO: stderr: ""
May  6 14:36:42.419: INFO: stdout: "update-demo-nautilus-l2hnm update-demo-nautilus-l84sd "
May  6 14:36:42.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-nautilus-l2hnm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6659'
May  6 14:36:42.519: INFO: stderr: ""
May  6 14:36:42.519: INFO: stdout: "true"
May  6 14:36:42.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-nautilus-l2hnm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6659'
May  6 14:36:42.619: INFO: stderr: ""
May  6 14:36:42.619: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  6 14:36:42.619: INFO: validating pod update-demo-nautilus-l2hnm
May  6 14:36:42.623: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 14:36:42.623: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 14:36:42.623: INFO: update-demo-nautilus-l2hnm is verified up and running
May  6 14:36:42.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-nautilus-l84sd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6659'
May  6 14:36:42.724: INFO: stderr: ""
May  6 14:36:42.724: INFO: stdout: "true"
May  6 14:36:42.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-nautilus-l84sd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6659'
May  6 14:36:42.827: INFO: stderr: ""
May  6 14:36:42.827: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  6 14:36:42.827: INFO: validating pod update-demo-nautilus-l84sd
May  6 14:36:42.830: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 14:36:42.830: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 14:36:42.830: INFO: update-demo-nautilus-l84sd is verified up and running
STEP: rolling-update to new replication controller
May  6 14:36:42.832: INFO: scanned /root for discovery docs: <nil>
May  6 14:36:42.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-6659'
May  6 14:36:57.441: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May  6 14:36:57.441: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  6 14:36:57.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6659'
May  6 14:36:57.553: INFO: stderr: ""
May  6 14:36:57.553: INFO: stdout: "update-demo-kitten-k8zqs update-demo-kitten-lk9qc "
May  6 14:36:57.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-kitten-k8zqs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6659'
May  6 14:36:57.677: INFO: stderr: ""
May  6 14:36:57.677: INFO: stdout: "true"
May  6 14:36:57.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-kitten-k8zqs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6659'
May  6 14:36:57.779: INFO: stderr: ""
May  6 14:36:57.779: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May  6 14:36:57.779: INFO: validating pod update-demo-kitten-k8zqs
May  6 14:36:57.782: INFO: got data: {
  "image": "kitten.jpg"
}

May  6 14:36:57.782: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May  6 14:36:57.782: INFO: update-demo-kitten-k8zqs is verified up and running
May  6 14:36:57.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-kitten-lk9qc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6659'
May  6 14:36:57.887: INFO: stderr: ""
May  6 14:36:57.887: INFO: stdout: "true"
May  6 14:36:57.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-kitten-lk9qc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6659'
May  6 14:36:57.990: INFO: stderr: ""
May  6 14:36:57.990: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May  6 14:36:57.990: INFO: validating pod update-demo-kitten-lk9qc
May  6 14:36:57.994: INFO: got data: {
  "image": "kitten.jpg"
}

May  6 14:36:57.994: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May  6 14:36:57.994: INFO: update-demo-kitten-lk9qc is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:36:57.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6659" for this suite.
May  6 14:37:10.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:37:12.756: INFO: namespace kubectl-6659 deletion completed in 14.75949698s

• [SLOW TEST:36.041 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:37:12.756: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8150
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
May  6 14:37:12.900: INFO: Waiting up to 5m0s for pod "pod-bb70cc2b-6423-4261-a3cc-dd0f214e2aec" in namespace "emptydir-8150" to be "success or failure"
May  6 14:37:12.902: INFO: Pod "pod-bb70cc2b-6423-4261-a3cc-dd0f214e2aec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.897155ms
May  6 14:37:14.904: INFO: Pod "pod-bb70cc2b-6423-4261-a3cc-dd0f214e2aec": Phase="Running", Reason="", readiness=true. Elapsed: 2.00436584s
May  6 14:37:16.908: INFO: Pod "pod-bb70cc2b-6423-4261-a3cc-dd0f214e2aec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007942661s
STEP: Saw pod success
May  6 14:37:16.908: INFO: Pod "pod-bb70cc2b-6423-4261-a3cc-dd0f214e2aec" satisfied condition "success or failure"
May  6 14:37:16.910: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-bb70cc2b-6423-4261-a3cc-dd0f214e2aec container test-container: <nil>
STEP: delete the pod
May  6 14:37:16.933: INFO: Waiting for pod pod-bb70cc2b-6423-4261-a3cc-dd0f214e2aec to disappear
May  6 14:37:16.935: INFO: Pod pod-bb70cc2b-6423-4261-a3cc-dd0f214e2aec no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:37:16.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8150" for this suite.
May  6 14:37:22.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:37:25.699: INFO: namespace emptydir-8150 deletion completed in 8.760337058s

• [SLOW TEST:12.943 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:37:25.699: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1959
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
May  6 14:37:25.830: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:37:49.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1959" for this suite.
May  6 14:37:55.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:37:58.385: INFO: namespace crd-publish-openapi-1959 deletion completed in 8.760527202s

• [SLOW TEST:32.686 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:37:58.385: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6513
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May  6 14:37:58.516: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  6 14:37:58.525: INFO: Waiting for terminating namespaces to be deleted...
May  6 14:37:58.526: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-129-1.us-west-2.compute.internal before test
May  6 14:37:58.539: INFO: kube-proxy-v9c2p from kube-system started at 2020-05-06 10:55:20 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.539: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 14:37:58.539: INFO: calico-node-wpbh7 from kube-system started at 2020-05-06 10:55:38 +0000 UTC (2 container statuses recorded)
May  6 14:37:58.539: INFO: 	Container bird-metrics ready: true, restart count 0
May  6 14:37:58.539: INFO: 	Container calico-node ready: true, restart count 0
May  6 14:37:58.539: INFO: cert-manager-kubeaddons-webhook-77fbc6d59b-fbbjd from cert-manager started at 2020-05-06 10:56:59 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.539: INFO: 	Container cert-manager ready: true, restart count 1
May  6 14:37:58.539: INFO: cert-manager-kubeaddons-7d7f98fbc6-qpl75 from cert-manager started at 2020-05-06 10:56:59 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.539: INFO: 	Container cert-manager ready: true, restart count 0
May  6 14:37:58.539: INFO: ebs-csi-node-f2cbn from kube-system started at 2020-05-06 10:57:49 +0000 UTC (3 container statuses recorded)
May  6 14:37:58.539: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 14:37:58.539: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 14:37:58.539: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  6 14:37:58.539: INFO: traefik-kubeaddons-6f97669977-2sxh8 from kubeaddons started at 2020-05-06 10:58:20 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.539: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
May  6 14:37:58.539: INFO: prometheus-kubeaddons-prometheus-node-exporter-b6z8v from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.539: INFO: 	Container node-exporter ready: true, restart count 0
May  6 14:37:58.539: INFO: kcl-cm-6588c896f9-4b2gk from kommander started at 2020-05-06 11:00:36 +0000 UTC (2 container statuses recorded)
May  6 14:37:58.539: INFO: 	Container controller-manager ready: true, restart count 0
May  6 14:37:58.539: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  6 14:37:58.539: INFO: elasticsearch-kubeaddons-master-1 from kubeaddons started at 2020-05-06 10:59:19 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.539: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 14:37:58.539: INFO: coredns-5644d7b6d9-5jf9p from kube-system started at 2020-05-06 10:56:00 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.539: INFO: 	Container coredns ready: true, restart count 0
May  6 14:37:58.539: INFO: kommander-kubeaddons-kubeaddons-catalog-6654f856df-ccmkl from kommander started at 2020-05-06 11:00:36 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.539: INFO: 	Container kubeaddons-catalog ready: true, restart count 0
May  6 14:37:58.539: INFO: kcl-utility-apiserver-5d448f665b-gf6gv from kommander started at 2020-05-06 11:00:36 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.539: INFO: 	Container server ready: true, restart count 0
May  6 14:37:58.539: INFO: elasticsearch-kubeaddons-data-1 from kubeaddons started at 2020-05-06 11:01:19 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.539: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 14:37:58.539: INFO: reloader-kubeaddons-reloader-7d4bd64cfb-xnxz8 from kubeaddons started at 2020-05-06 10:56:23 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.539: INFO: 	Container reloader-kubeaddons-reloader ready: true, restart count 0
May  6 14:37:58.539: INFO: kubefed-controller-manager-55fd5b4dff-bvsl6 from kommander started at 2020-05-06 11:00:46 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.539: INFO: 	Container controller-manager ready: true, restart count 0
May  6 14:37:58.539: INFO: prometheus-kubeaddons-grafana-99b49bd54-8h7z8 from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (2 container statuses recorded)
May  6 14:37:58.539: INFO: 	Container grafana ready: true, restart count 0
May  6 14:37:58.539: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
May  6 14:37:58.539: INFO: minio-3 from velero started at 2020-05-06 10:59:47 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.539: INFO: 	Container minio ready: true, restart count 0
May  6 14:37:58.539: INFO: sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-9wkcg from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 14:37:58.539: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 14:37:58.539: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 14:37:58.539: INFO: fluentbit-kubeaddons-fluent-bit-5rbrr from kubeaddons started at 2020-05-06 11:02:11 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.539: INFO: 	Container fluent-bit ready: true, restart count 0
May  6 14:37:58.539: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-129-108.us-west-2.compute.internal before test
May  6 14:37:58.553: INFO: minio-2 from velero started at 2020-05-06 10:59:44 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container minio ready: true, restart count 0
May  6 14:37:58.553: INFO: traefik-kubeaddons-6f97669977-9fwr9 from kubeaddons started at 2020-05-06 10:57:49 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
May  6 14:37:58.553: INFO: elasticsearch-kubeaddons-data-0 from kubeaddons started at 2020-05-06 10:58:15 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 14:37:58.553: INFO: calico-kube-controllers-84f666455-5t7xm from kube-system started at 2020-05-06 10:55:49 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  6 14:37:58.553: INFO: elasticsearchexporter-kubeaddons-elasticsearch-exporter-845tdq8 from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container elasticsearch-exporter ready: true, restart count 0
May  6 14:37:58.553: INFO: coredns-5644d7b6d9-hmtfs from kube-system started at 2020-05-06 10:56:00 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container coredns ready: true, restart count 0
May  6 14:37:58.553: INFO: sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-zqwx9 from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 14:37:58.553: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 14:37:58.553: INFO: kcl-tfcb-9c88c7bc9-lbgz6 from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container server ready: true, restart count 0
May  6 14:37:58.553: INFO: kcl-webhook-fb4dfc7d7-vv757 from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container webhook ready: true, restart count 0
May  6 14:37:58.553: INFO: kube-proxy-4lqz5 from kube-system started at 2020-05-06 10:55:19 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 14:37:58.553: INFO: kibana-kubeaddons-649897648c-mzc2h from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (2 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container initialize-kibana-index ready: true, restart count 0
May  6 14:37:58.553: INFO: 	Container kibana ready: true, restart count 0
May  6 14:37:58.553: INFO: kubernetes-dashboard-549989bcdf-2d767 from kubeaddons started at 2020-05-06 10:56:34 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
May  6 14:37:58.553: INFO: ebs-csi-node-vvbxt from kube-system started at 2020-05-06 10:57:49 +0000 UTC (3 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 14:37:58.553: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 14:37:58.553: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  6 14:37:58.553: INFO: cert-manager-kubeaddons-cainjector-6dcd94769b-ng6xl from cert-manager started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container cainjector ready: true, restart count 0
May  6 14:37:58.553: INFO: kubeaddons-controller-manager-77cf76b857-k89ml from kubeaddons started at 2020-05-06 10:55:49 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container manager ready: true, restart count 0
May  6 14:37:58.553: INFO: elasticsearch-kubeaddons-master-2 from kubeaddons started at 2020-05-06 11:00:07 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 14:37:58.553: INFO: kubefed-admission-webhook-5fb847574f-6mjm2 from kommander started at 2020-05-06 11:00:41 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container admission-webhook ready: true, restart count 0
May  6 14:37:58.553: INFO: fluentbit-kubeaddons-fluent-bit-t6xpz from kubeaddons started at 2020-05-06 11:02:11 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container fluent-bit ready: true, restart count 0
May  6 14:37:58.553: INFO: elasticsearch-kubeaddons-client-d4f5db58d-7qrn8 from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 14:37:58.553: INFO: kommander-kubeaddons-grafana-645f957f8f-96sf5 from kommander started at 2020-05-06 11:00:36 +0000 UTC (2 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container grafana ready: true, restart count 0
May  6 14:37:58.553: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
May  6 14:37:58.553: INFO: prometheus-kubeaddons-prometheus-node-exporter-2wrg9 from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container node-exporter ready: true, restart count 0
May  6 14:37:58.553: INFO: kommander-kubeaddons-karma-7688944fcf-hpnzl from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container karma ready: true, restart count 0
May  6 14:37:58.553: INFO: external-dns-kubeaddons-6c65ff8d88-sm9p2 from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container external-dns ready: true, restart count 0
May  6 14:37:58.553: INFO: calico-node-z679x from kube-system started at 2020-05-06 10:55:38 +0000 UTC (2 container statuses recorded)
May  6 14:37:58.553: INFO: 	Container bird-metrics ready: true, restart count 0
May  6 14:37:58.553: INFO: 	Container calico-node ready: true, restart count 0
May  6 14:37:58.553: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-131-234.us-west-2.compute.internal before test
May  6 14:37:58.568: INFO: calico-node-gm4h2 from kube-system started at 2020-05-06 10:55:38 +0000 UTC (2 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container bird-metrics ready: true, restart count 0
May  6 14:37:58.568: INFO: 	Container calico-node ready: true, restart count 0
May  6 14:37:58.568: INFO: elasticsearch-kubeaddons-client-d4f5db58d-7g4sd from kubeaddons started at 2020-05-06 10:57:52 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 14:37:58.568: INFO: minio-0 from velero started at 2020-05-06 10:59:44 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container minio ready: true, restart count 0
May  6 14:37:58.568: INFO: kube-proxy-ptcmp from kube-system started at 2020-05-06 10:55:19 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 14:37:58.568: INFO: sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-hqgql from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 14:37:58.568: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 14:37:58.568: INFO: gatekeeper-kubeaddons-fdc87db85-5rj6d from kubeaddons started at 2020-05-06 10:57:40 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container manager ready: true, restart count 0
May  6 14:37:58.568: INFO: kubefed-controller-manager-55fd5b4dff-g9m6m from kommander started at 2020-05-06 11:00:41 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container controller-manager ready: true, restart count 0
May  6 14:37:58.568: INFO: dex-kubeaddons-dex-controller-6b6c9fbd7f-pxz8h from kubeaddons started at 2020-05-06 10:58:46 +0000 UTC (2 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  6 14:37:58.568: INFO: 	Container manager ready: true, restart count 0
May  6 14:37:58.568: INFO: prometheus-kubeaddons-kube-state-metrics-78c65cc7bc-66zk8 from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container kube-state-metrics ready: true, restart count 0
May  6 14:37:58.568: INFO: kube-oidc-proxy-kubeaddons-6fbd5c8fc4-xtmsg from kubeaddons started at 2020-05-06 10:59:18 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container kube-oidc-proxy ready: true, restart count 0
May  6 14:37:58.568: INFO: traefik-forward-auth-kubeaddons-8bdddfcd-wdrjp from kubeaddons started at 2020-05-06 10:59:51 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container traefik-forward-auth ready: true, restart count 0
May  6 14:37:58.568: INFO: traefik-kubeaddons-1.72.17-5pttd from kubeaddons started at 2020-05-06 10:57:38 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container traefik ready: false, restart count 0
May  6 14:37:58.568: INFO: tiller-deploy-969865475-m2ph8 from kube-system started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container tiller ready: true, restart count 0
May  6 14:37:58.568: INFO: alertmanager-prometheus-kubeaddons-prom-alertmanager-0 from kubeaddons started at 2020-05-06 10:59:03 +0000 UTC (2 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container alertmanager ready: true, restart count 0
May  6 14:37:58.568: INFO: 	Container config-reloader ready: true, restart count 0
May  6 14:37:58.568: INFO: prometheusadapter-kubeaddons-prometheus-adapter-6b8975fc48pwzsr from kubeaddons started at 2020-05-06 10:59:55 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container prometheus-adapter ready: true, restart count 0
May  6 14:37:58.568: INFO: opsportal-kubeaddons-kommander-ui-679db5d9cb-62psh from kubeaddons started at 2020-05-06 10:56:45 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container opsportal-kubeaddons-kommander-ui ready: true, restart count 0
May  6 14:37:58.568: INFO: dex-kubeaddons-79b77778bc-84p4b from kubeaddons started at 2020-05-06 10:59:20 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container main ready: true, restart count 1
May  6 14:37:58.568: INFO: dstorageclass-controller-manager-5c966c767f-4bf6x from kubeaddons started at 2020-05-06 10:57:33 +0000 UTC (2 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  6 14:37:58.568: INFO: 	Container manager ready: true, restart count 0
May  6 14:37:58.568: INFO: prometheus-kubeaddons-prom-operator-cc56c6c-m8kfx from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (2 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container prometheus-operator ready: true, restart count 0
May  6 14:37:58.568: INFO: 	Container tls-proxy ready: true, restart count 0
May  6 14:37:58.568: INFO: prometheus-kubeaddons-prometheus-node-exporter-4jtfq from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container node-exporter ready: true, restart count 0
May  6 14:37:58.568: INFO: velero-kubeaddons-578b4667ff-6x2nw from velero started at 2020-05-06 10:59:40 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container velero ready: true, restart count 4
May  6 14:37:58.568: INFO: dex-k8s-authenticator-kubeaddons-6697855d4c-xbqks from kubeaddons started at 2020-05-06 10:59:29 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container dex-k8s-authenticator ready: true, restart count 0
May  6 14:37:58.568: INFO: fluentbit-kubeaddons-fluent-bit-wnqvd from kubeaddons started at 2020-05-06 11:02:11 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container fluent-bit ready: true, restart count 0
May  6 14:37:58.568: INFO: opsportal-landing-6f6865b688-vb67f from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container opsportal-landing ready: true, restart count 0
May  6 14:37:58.568: INFO: kommander-kubeaddons-kommander-ui-7b9b585d95-rgnz6 from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container kommander-kubeaddons-kommander-ui ready: true, restart count 0
May  6 14:37:58.568: INFO: ebs-csi-node-ffszp from kube-system started at 2020-05-06 10:57:49 +0000 UTC (3 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 14:37:58.568: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 14:37:58.568: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  6 14:37:58.568: INFO: kommander-kubeaddons-thanos-query-7597bf4957-tmdgd from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.568: INFO: 	Container thanos-query ready: true, restart count 0
May  6 14:37:58.568: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-131-80.us-west-2.compute.internal before test
May  6 14:37:58.587: INFO: ebs-csi-node-722lq from kube-system started at 2020-05-06 10:57:49 +0000 UTC (3 container statuses recorded)
May  6 14:37:58.587: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 14:37:58.587: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 14:37:58.587: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  6 14:37:58.587: INFO: kube-proxy-vmxhd from kube-system started at 2020-05-06 10:55:19 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.587: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 14:37:58.587: INFO: elasticsearch-kubeaddons-master-0 from kubeaddons started at 2020-05-06 13:31:19 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.587: INFO: 	Container elasticsearch ready: false, restart count 0
May  6 14:37:58.587: INFO: fluentbit-kubeaddons-fluent-bit-w966k from kubeaddons started at 2020-05-06 13:31:20 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.587: INFO: 	Container fluent-bit ready: true, restart count 0
May  6 14:37:58.587: INFO: ebs-csi-controller-0 from kube-system started at 2020-05-06 13:31:16 +0000 UTC (5 container statuses recorded)
May  6 14:37:58.587: INFO: 	Container csi-attacher ready: true, restart count 0
May  6 14:37:58.587: INFO: 	Container csi-provisioner ready: true, restart count 0
May  6 14:37:58.587: INFO: 	Container csi-snapshotter ready: true, restart count 0
May  6 14:37:58.587: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 14:37:58.587: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 14:37:58.587: INFO: minio-1 from velero started at 2020-05-06 13:31:21 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.587: INFO: 	Container minio ready: false, restart count 0
May  6 14:37:58.587: INFO: prometheus-kubeaddons-prometheus-node-exporter-2qwk2 from kubeaddons started at 2020-05-06 13:32:37 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.587: INFO: 	Container node-exporter ready: false, restart count 0
May  6 14:37:58.587: INFO: sonobuoy from sonobuoy started at 2020-05-06 13:56:26 +0000 UTC (1 container statuses recorded)
May  6 14:37:58.587: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  6 14:37:58.587: INFO: sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-cvh6w from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 14:37:58.587: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 14:37:58.587: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 14:37:58.587: INFO: calico-node-mrkdh from kube-system started at 2020-05-06 10:55:38 +0000 UTC (2 container statuses recorded)
May  6 14:37:58.587: INFO: 	Container bird-metrics ready: true, restart count 0
May  6 14:37:58.587: INFO: 	Container calico-node ready: true, restart count 0
May  6 14:37:58.587: INFO: sonobuoy-e2e-job-7496ec37786a4f0c from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 14:37:58.587: INFO: 	Container e2e ready: true, restart count 0
May  6 14:37:58.587: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 14:37:58.587: INFO: prometheus-prometheus-kubeaddons-prom-prometheus-0 from kubeaddons started at 2020-05-06 13:31:17 +0000 UTC (4 container statuses recorded)
May  6 14:37:58.587: INFO: 	Container prometheus ready: false, restart count 0
May  6 14:37:58.587: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May  6 14:37:58.587: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May  6 14:37:58.587: INFO: 	Container thanos-sidecar ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-3cdfc295-11ec-4afc-8890-2197a11e4aa1 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-3cdfc295-11ec-4afc-8890-2197a11e4aa1 off the node ip-10-0-131-80.us-west-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-3cdfc295-11ec-4afc-8890-2197a11e4aa1
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:43:02.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6513" for this suite.
May  6 14:43:18.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:43:21.416: INFO: namespace sched-pred-6513 deletion completed in 18.760050191s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:323.031 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:43:21.416: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6483
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
May  6 14:43:26.070: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6483 pod-service-account-4acfb14e-2abf-475a-91fc-2f14d5f33608 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May  6 14:43:26.335: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6483 pod-service-account-4acfb14e-2abf-475a-91fc-2f14d5f33608 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May  6 14:43:26.598: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6483 pod-service-account-4acfb14e-2abf-475a-91fc-2f14d5f33608 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:43:26.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6483" for this suite.
May  6 14:43:32.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:43:35.617: INFO: namespace svcaccounts-6483 deletion completed in 8.760438363s

• [SLOW TEST:14.201 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:43:35.617: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2546
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:43:51.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2546" for this suite.
May  6 14:43:57.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:44:00.578: INFO: namespace resourcequota-2546 deletion completed in 8.760005758s

• [SLOW TEST:24.961 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:44:00.578: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1553
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:44:11.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1553" for this suite.
May  6 14:44:17.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:44:20.509: INFO: namespace resourcequota-1553 deletion completed in 8.772259663s

• [SLOW TEST:19.930 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:44:20.509: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6207
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
May  6 14:44:25.175: INFO: Successfully updated pod "labelsupdate3b93d972-6ee9-400f-81ed-e6b83fadbb28"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:44:27.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6207" for this suite.
May  6 14:44:39.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:44:41.947: INFO: namespace projected-6207 deletion completed in 14.759545157s

• [SLOW TEST:21.439 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:44:41.948: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-1696
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
May  6 14:44:42.085: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-1696" to be "success or failure"
May  6 14:44:42.087: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 1.724741ms
May  6 14:44:44.089: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00438049s
STEP: Saw pod success
May  6 14:44:44.089: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
May  6 14:44:44.091: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
May  6 14:44:44.102: INFO: Waiting for pod pod-host-path-test to disappear
May  6 14:44:44.104: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:44:44.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-1696" for this suite.
May  6 14:44:50.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:44:52.868: INFO: namespace hostpath-1696 deletion completed in 8.760851366s

• [SLOW TEST:10.921 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:44:52.868: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8966
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
May  6 14:44:52.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 create -f - --namespace=kubectl-8966'
May  6 14:44:53.407: INFO: stderr: ""
May  6 14:44:53.408: INFO: stdout: "pod/pause created\n"
May  6 14:44:53.408: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May  6 14:44:53.408: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8966" to be "running and ready"
May  6 14:44:53.410: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.235916ms
May  6 14:44:55.412: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.004757837s
May  6 14:44:55.412: INFO: Pod "pause" satisfied condition "running and ready"
May  6 14:44:55.412: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
May  6 14:44:55.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 label pods pause testing-label=testing-label-value --namespace=kubectl-8966'
May  6 14:44:55.522: INFO: stderr: ""
May  6 14:44:55.522: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May  6 14:44:55.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pod pause -L testing-label --namespace=kubectl-8966'
May  6 14:44:55.625: INFO: stderr: ""
May  6 14:44:55.625: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May  6 14:44:55.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 label pods pause testing-label- --namespace=kubectl-8966'
May  6 14:44:55.737: INFO: stderr: ""
May  6 14:44:55.737: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May  6 14:44:55.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pod pause -L testing-label --namespace=kubectl-8966'
May  6 14:44:55.840: INFO: stderr: ""
May  6 14:44:55.840: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
May  6 14:44:55.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 delete --grace-period=0 --force -f - --namespace=kubectl-8966'
May  6 14:44:55.923: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 14:44:55.923: INFO: stdout: "pod \"pause\" force deleted\n"
May  6 14:44:55.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get rc,svc -l name=pause --no-headers --namespace=kubectl-8966'
May  6 14:44:56.068: INFO: stderr: "No resources found in kubectl-8966 namespace.\n"
May  6 14:44:56.068: INFO: stdout: ""
May  6 14:44:56.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods -l name=pause --namespace=kubectl-8966 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  6 14:44:56.179: INFO: stderr: ""
May  6 14:44:56.179: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:44:56.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8966" for this suite.
May  6 14:45:02.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:45:04.943: INFO: namespace kubectl-8966 deletion completed in 8.760798058s

• [SLOW TEST:12.074 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:45:04.943: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3732
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 14:45:05.092: INFO: Create a RollingUpdate DaemonSet
May  6 14:45:05.095: INFO: Check that daemon pods launch on every node of the cluster
May  6 14:45:05.098: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:45:05.098: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:45:05.098: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:45:05.100: INFO: Number of nodes with available pods: 0
May  6 14:45:05.101: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 14:45:06.104: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:45:06.105: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:45:06.105: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:45:06.107: INFO: Number of nodes with available pods: 0
May  6 14:45:06.107: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 14:45:07.104: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:45:07.104: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:45:07.104: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:45:07.107: INFO: Number of nodes with available pods: 2
May  6 14:45:07.107: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 14:45:08.106: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:45:08.106: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:45:08.106: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:45:08.108: INFO: Number of nodes with available pods: 4
May  6 14:45:08.108: INFO: Number of running nodes: 4, number of available pods: 4
May  6 14:45:08.108: INFO: Update the DaemonSet to trigger a rollout
May  6 14:45:08.115: INFO: Updating DaemonSet daemon-set
May  6 14:45:18.128: INFO: Roll back the DaemonSet before rollout is complete
May  6 14:45:18.134: INFO: Updating DaemonSet daemon-set
May  6 14:45:18.134: INFO: Make sure DaemonSet rollback is complete
May  6 14:45:18.136: INFO: Wrong image for pod: daemon-set-tbr48. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May  6 14:45:18.136: INFO: Pod daemon-set-tbr48 is not available
May  6 14:45:18.141: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:45:18.141: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:45:18.141: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:45:19.144: INFO: Wrong image for pod: daemon-set-tbr48. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May  6 14:45:19.144: INFO: Pod daemon-set-tbr48 is not available
May  6 14:45:19.147: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:45:19.147: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:45:19.147: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:45:20.144: INFO: Pod daemon-set-r68hj is not available
May  6 14:45:20.147: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:45:20.147: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 14:45:20.147: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3732, will wait for the garbage collector to delete the pods
May  6 14:45:20.208: INFO: Deleting DaemonSet.extensions daemon-set took: 5.436053ms
May  6 14:45:21.609: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.400155467s
May  6 14:46:37.211: INFO: Number of nodes with available pods: 0
May  6 14:46:37.211: INFO: Number of running nodes: 0, number of available pods: 0
May  6 14:46:37.213: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3732/daemonsets","resourceVersion":"104305"},"items":null}

May  6 14:46:37.215: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3732/pods","resourceVersion":"104305"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:46:37.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3732" for this suite.
May  6 14:46:43.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:46:45.988: INFO: namespace daemonsets-3732 deletion completed in 8.760470646s

• [SLOW TEST:101.045 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:46:45.988: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8177
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
May  6 14:46:46.119: INFO: PodSpec: initContainers in spec.initContainers
May  6 14:47:36.836: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-2d9d376a-63f8-4361-ae33-ca028deaf97a", GenerateName:"", Namespace:"init-container-8177", SelfLink:"/api/v1/namespaces/init-container-8177/pods/pod-init-2d9d376a-63f8-4361-ae33-ca028deaf97a", UID:"998b35fc-f101-434d-adcf-5cf87a40fc78", ResourceVersion:"104664", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63724373206, loc:(*time.Location)(0x789e8e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"119335123"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"192.168.85.113/32", "cni.projectcalico.org/podIPs":"192.168.85.113/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-xxv9k", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00288c840), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xxv9k", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xxv9k", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xxv9k", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0004b25b0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-131-80.us-west-2.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0025469c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0004b26a0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0004b2830)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0004b2838), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0004b283c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724373206, loc:(*time.Location)(0x789e8e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724373206, loc:(*time.Location)(0x789e8e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724373206, loc:(*time.Location)(0x789e8e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724373206, loc:(*time.Location)(0x789e8e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.131.80", PodIP:"192.168.85.113", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.85.113"}}, StartTime:(*v1.Time)(0xc0034aa600), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0008be070)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0008be150)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://427236f800fab11efb7be35dffc427514561d572a1a994fe77a0ba3c58e738c5", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0034aa660), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0034aa620), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc0004b2acf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:47:36.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8177" for this suite.
May  6 14:47:48.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:47:51.610: INFO: namespace init-container-8177 deletion completed in 14.770393758s

• [SLOW TEST:65.622 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:47:51.610: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-903
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May  6 14:47:51.749: INFO: Waiting up to 5m0s for pod "downward-api-d3968924-6692-414c-a2a9-45dcd2c5b387" in namespace "downward-api-903" to be "success or failure"
May  6 14:47:51.751: INFO: Pod "downward-api-d3968924-6692-414c-a2a9-45dcd2c5b387": Phase="Pending", Reason="", readiness=false. Elapsed: 1.832547ms
May  6 14:47:53.754: INFO: Pod "downward-api-d3968924-6692-414c-a2a9-45dcd2c5b387": Phase="Running", Reason="", readiness=true. Elapsed: 2.004375374s
May  6 14:47:55.757: INFO: Pod "downward-api-d3968924-6692-414c-a2a9-45dcd2c5b387": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007143825s
STEP: Saw pod success
May  6 14:47:55.757: INFO: Pod "downward-api-d3968924-6692-414c-a2a9-45dcd2c5b387" satisfied condition "success or failure"
May  6 14:47:55.759: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod downward-api-d3968924-6692-414c-a2a9-45dcd2c5b387 container dapi-container: <nil>
STEP: delete the pod
May  6 14:47:55.777: INFO: Waiting for pod downward-api-d3968924-6692-414c-a2a9-45dcd2c5b387 to disappear
May  6 14:47:55.779: INFO: Pod downward-api-d3968924-6692-414c-a2a9-45dcd2c5b387 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:47:55.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-903" for this suite.
May  6 14:48:01.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:48:04.544: INFO: namespace downward-api-903 deletion completed in 8.762470807s

• [SLOW TEST:12.934 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:48:04.544: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2453
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
May  6 14:48:04.679: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
May  6 14:48:20.069: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 14:48:23.892: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:48:40.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2453" for this suite.
May  6 14:48:46.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:48:49.268: INFO: namespace crd-publish-openapi-2453 deletion completed in 8.759783785s

• [SLOW TEST:44.724 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:48:49.268: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5442
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May  6 14:48:51.418: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:48:51.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5442" for this suite.
May  6 14:48:57.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:49:00.191: INFO: namespace container-runtime-5442 deletion completed in 8.759688718s

• [SLOW TEST:10.922 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:49:00.191: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3331
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May  6 14:49:04.852: INFO: Successfully updated pod "pod-update-activedeadlineseconds-dc489eba-3edb-408e-a34b-ad99bd451c73"
May  6 14:49:04.852: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-dc489eba-3edb-408e-a34b-ad99bd451c73" in namespace "pods-3331" to be "terminated due to deadline exceeded"
May  6 14:49:04.855: INFO: Pod "pod-update-activedeadlineseconds-dc489eba-3edb-408e-a34b-ad99bd451c73": Phase="Running", Reason="", readiness=true. Elapsed: 2.78353ms
May  6 14:49:06.860: INFO: Pod "pod-update-activedeadlineseconds-dc489eba-3edb-408e-a34b-ad99bd451c73": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.00766794s
May  6 14:49:06.860: INFO: Pod "pod-update-activedeadlineseconds-dc489eba-3edb-408e-a34b-ad99bd451c73" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:49:06.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3331" for this suite.
May  6 14:49:12.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:49:15.625: INFO: namespace pods-3331 deletion completed in 8.761801661s

• [SLOW TEST:15.434 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:49:15.625: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1756
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-1c0d4ae6-70db-414c-8c0c-583fbd697b46
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:49:15.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1756" for this suite.
May  6 14:49:21.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:49:24.531: INFO: namespace configmap-1756 deletion completed in 8.77013412s

• [SLOW TEST:8.906 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:49:24.531: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9088
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
May  6 14:49:24.667: INFO: Waiting up to 5m0s for pod "client-containers-1c50a1b3-5c15-4114-8d0c-8837144eeec1" in namespace "containers-9088" to be "success or failure"
May  6 14:49:24.669: INFO: Pod "client-containers-1c50a1b3-5c15-4114-8d0c-8837144eeec1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.738049ms
May  6 14:49:26.673: INFO: Pod "client-containers-1c50a1b3-5c15-4114-8d0c-8837144eeec1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005778329s
STEP: Saw pod success
May  6 14:49:26.673: INFO: Pod "client-containers-1c50a1b3-5c15-4114-8d0c-8837144eeec1" satisfied condition "success or failure"
May  6 14:49:26.675: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod client-containers-1c50a1b3-5c15-4114-8d0c-8837144eeec1 container test-container: <nil>
STEP: delete the pod
May  6 14:49:26.695: INFO: Waiting for pod client-containers-1c50a1b3-5c15-4114-8d0c-8837144eeec1 to disappear
May  6 14:49:26.697: INFO: Pod client-containers-1c50a1b3-5c15-4114-8d0c-8837144eeec1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:49:26.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9088" for this suite.
May  6 14:49:32.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:49:35.459: INFO: namespace containers-9088 deletion completed in 8.759159633s

• [SLOW TEST:10.928 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:49:35.459: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4879
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4879
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
May  6 14:49:35.597: INFO: Found 0 stateful pods, waiting for 3
May  6 14:49:45.600: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  6 14:49:45.600: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  6 14:49:45.600: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May  6 14:49:45.621: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May  6 14:49:55.647: INFO: Updating stateful set ss2
May  6 14:49:55.650: INFO: Waiting for Pod statefulset-4879/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
May  6 14:50:05.678: INFO: Found 1 stateful pods, waiting for 3
May  6 14:50:15.682: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  6 14:50:15.682: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  6 14:50:15.682: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May  6 14:50:15.701: INFO: Updating stateful set ss2
May  6 14:50:15.705: INFO: Waiting for Pod statefulset-4879/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May  6 14:50:25.725: INFO: Updating stateful set ss2
May  6 14:50:25.729: INFO: Waiting for StatefulSet statefulset-4879/ss2 to complete update
May  6 14:50:25.729: INFO: Waiting for Pod statefulset-4879/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May  6 14:50:35.734: INFO: Deleting all statefulset in ns statefulset-4879
May  6 14:50:35.735: INFO: Scaling statefulset ss2 to 0
May  6 14:50:55.745: INFO: Waiting for statefulset status.replicas updated to 0
May  6 14:50:55.747: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:50:55.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4879" for this suite.
May  6 14:51:01.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:51:04.518: INFO: namespace statefulset-4879 deletion completed in 8.759383186s

• [SLOW TEST:89.059 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:51:04.518: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6440
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-7d7ec179-23af-4d4a-8d17-7e5f89dec6a9
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-7d7ec179-23af-4d4a-8d17-7e5f89dec6a9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:51:08.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6440" for this suite.
May  6 14:51:36.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:51:39.455: INFO: namespace projected-6440 deletion completed in 30.760199747s

• [SLOW TEST:34.937 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:51:39.455: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7811
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7811
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7811
STEP: Creating statefulset with conflicting port in namespace statefulset-7811
STEP: Waiting until pod test-pod will start running in namespace statefulset-7811
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7811
May  6 14:51:43.606: INFO: Observed stateful pod in namespace: statefulset-7811, name: ss-0, uid: 0bfb007f-5e65-483d-9a84-2abda61f6b09, status phase: Failed. Waiting for statefulset controller to delete.
May  6 14:51:43.609: INFO: Observed stateful pod in namespace: statefulset-7811, name: ss-0, uid: 0bfb007f-5e65-483d-9a84-2abda61f6b09, status phase: Failed. Waiting for statefulset controller to delete.
May  6 14:51:43.612: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7811
STEP: Removing pod with conflicting port in namespace statefulset-7811
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7811 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May  6 14:51:45.625: INFO: Deleting all statefulset in ns statefulset-7811
May  6 14:51:45.627: INFO: Scaling statefulset ss to 0
May  6 14:52:05.637: INFO: Waiting for statefulset status.replicas updated to 0
May  6 14:52:05.639: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:52:05.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7811" for this suite.
May  6 14:52:11.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:52:14.410: INFO: namespace statefulset-7811 deletion completed in 8.759228901s

• [SLOW TEST:34.955 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:52:14.410: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-333
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 14:52:14.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 version'
May  6 14:52:14.592: INFO: stderr: ""
May  6 14:52:14.592: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.8\", GitCommit:\"ec6eb119b81be488b030e849b9e64fda4caaf33c\", GitTreeState:\"clean\", BuildDate:\"2020-03-12T21:00:06Z\", GoVersion:\"go1.13.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.8\", GitCommit:\"ec6eb119b81be488b030e849b9e64fda4caaf33c\", GitTreeState:\"clean\", BuildDate:\"2020-03-12T20:52:22Z\", GoVersion:\"go1.13.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:52:14.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-333" for this suite.
May  6 14:52:20.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:52:23.355: INFO: namespace kubectl-333 deletion completed in 8.759528181s

• [SLOW TEST:8.945 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:52:23.355: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7365
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 14:52:23.485: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May  6 14:52:27.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 --namespace=crd-publish-openapi-7365 create -f -'
May  6 14:52:28.274: INFO: stderr: ""
May  6 14:52:28.275: INFO: stdout: "e2e-test-crd-publish-openapi-3442-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May  6 14:52:28.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 --namespace=crd-publish-openapi-7365 delete e2e-test-crd-publish-openapi-3442-crds test-cr'
May  6 14:52:28.405: INFO: stderr: ""
May  6 14:52:28.405: INFO: stdout: "e2e-test-crd-publish-openapi-3442-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May  6 14:52:28.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 --namespace=crd-publish-openapi-7365 apply -f -'
May  6 14:52:28.588: INFO: stderr: ""
May  6 14:52:28.588: INFO: stdout: "e2e-test-crd-publish-openapi-3442-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May  6 14:52:28.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 --namespace=crd-publish-openapi-7365 delete e2e-test-crd-publish-openapi-3442-crds test-cr'
May  6 14:52:28.721: INFO: stderr: ""
May  6 14:52:28.721: INFO: stdout: "e2e-test-crd-publish-openapi-3442-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May  6 14:52:28.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 explain e2e-test-crd-publish-openapi-3442-crds'
May  6 14:52:28.937: INFO: stderr: ""
May  6 14:52:28.937: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3442-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:52:32.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7365" for this suite.
May  6 14:52:38.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:52:41.513: INFO: namespace crd-publish-openapi-7365 deletion completed in 8.759309155s

• [SLOW TEST:18.158 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:52:41.514: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8538
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-8538/configmap-test-cc6c4ef1-5da3-4cb1-bfe5-efd8e8bc1fa0
STEP: Creating a pod to test consume configMaps
May  6 14:52:41.654: INFO: Waiting up to 5m0s for pod "pod-configmaps-b793ed5c-2aa2-4c64-a152-238112fe58b6" in namespace "configmap-8538" to be "success or failure"
May  6 14:52:41.657: INFO: Pod "pod-configmaps-b793ed5c-2aa2-4c64-a152-238112fe58b6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.602222ms
May  6 14:52:43.660: INFO: Pod "pod-configmaps-b793ed5c-2aa2-4c64-a152-238112fe58b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006425855s
May  6 14:52:45.663: INFO: Pod "pod-configmaps-b793ed5c-2aa2-4c64-a152-238112fe58b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009192029s
STEP: Saw pod success
May  6 14:52:45.663: INFO: Pod "pod-configmaps-b793ed5c-2aa2-4c64-a152-238112fe58b6" satisfied condition "success or failure"
May  6 14:52:45.665: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-configmaps-b793ed5c-2aa2-4c64-a152-238112fe58b6 container env-test: <nil>
STEP: delete the pod
May  6 14:52:45.681: INFO: Waiting for pod pod-configmaps-b793ed5c-2aa2-4c64-a152-238112fe58b6 to disappear
May  6 14:52:45.683: INFO: Pod pod-configmaps-b793ed5c-2aa2-4c64-a152-238112fe58b6 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:52:45.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8538" for this suite.
May  6 14:52:51.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:52:54.445: INFO: namespace configmap-8538 deletion completed in 8.759038817s

• [SLOW TEST:12.931 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:52:54.445: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3632
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 14:52:54.590: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May  6 14:52:54.595: INFO: Number of nodes with available pods: 0
May  6 14:52:54.595: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May  6 14:52:54.608: INFO: Number of nodes with available pods: 0
May  6 14:52:54.608: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 14:52:55.611: INFO: Number of nodes with available pods: 0
May  6 14:52:55.611: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 14:52:56.611: INFO: Number of nodes with available pods: 0
May  6 14:52:56.611: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 14:52:57.611: INFO: Number of nodes with available pods: 1
May  6 14:52:57.611: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May  6 14:52:57.623: INFO: Number of nodes with available pods: 1
May  6 14:52:57.623: INFO: Number of running nodes: 0, number of available pods: 1
May  6 14:52:58.625: INFO: Number of nodes with available pods: 0
May  6 14:52:58.625: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May  6 14:52:58.630: INFO: Number of nodes with available pods: 0
May  6 14:52:58.630: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 14:52:59.633: INFO: Number of nodes with available pods: 0
May  6 14:52:59.633: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 14:53:00.633: INFO: Number of nodes with available pods: 0
May  6 14:53:00.633: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 14:53:01.633: INFO: Number of nodes with available pods: 0
May  6 14:53:01.633: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 14:53:02.633: INFO: Number of nodes with available pods: 0
May  6 14:53:02.633: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 14:53:03.633: INFO: Number of nodes with available pods: 0
May  6 14:53:03.633: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 14:53:04.633: INFO: Number of nodes with available pods: 0
May  6 14:53:04.633: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 14:53:05.633: INFO: Number of nodes with available pods: 0
May  6 14:53:05.633: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 14:53:06.633: INFO: Number of nodes with available pods: 0
May  6 14:53:06.633: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 14:53:07.633: INFO: Number of nodes with available pods: 0
May  6 14:53:07.633: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 14:53:08.633: INFO: Number of nodes with available pods: 0
May  6 14:53:08.633: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 14:53:09.633: INFO: Number of nodes with available pods: 1
May  6 14:53:09.634: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3632, will wait for the garbage collector to delete the pods
May  6 14:53:09.694: INFO: Deleting DaemonSet.extensions daemon-set took: 5.332427ms
May  6 14:53:11.094: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.400161682s
May  6 14:53:17.397: INFO: Number of nodes with available pods: 0
May  6 14:53:17.397: INFO: Number of running nodes: 0, number of available pods: 0
May  6 14:53:17.399: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3632/daemonsets","resourceVersion":"107204"},"items":null}

May  6 14:53:17.401: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3632/pods","resourceVersion":"107204"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:53:17.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3632" for this suite.
May  6 14:53:23.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:53:26.179: INFO: namespace daemonsets-3632 deletion completed in 8.759166513s

• [SLOW TEST:31.734 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:53:26.179: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2021
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 14:53:26.899: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  6 14:53:28.906: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724373606, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724373606, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724373606, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724373606, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 14:53:31.916: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 14:53:31.919: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2634-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:53:33.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2021" for this suite.
May  6 14:53:39.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:53:41.826: INFO: namespace webhook-2021 deletion completed in 8.76126757s
STEP: Destroying namespace "webhook-2021-markers" for this suite.
May  6 14:53:47.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:53:50.586: INFO: namespace webhook-2021-markers deletion completed in 8.759572809s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:24.417 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:53:50.596: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-900
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-739b7f29-e5bc-4067-8dee-8a5627fa0518
STEP: Creating a pod to test consume secrets
May  6 14:53:50.736: INFO: Waiting up to 5m0s for pod "pod-secrets-7ee6f512-3cd3-4170-a1cf-37af310be3d8" in namespace "secrets-900" to be "success or failure"
May  6 14:53:50.738: INFO: Pod "pod-secrets-7ee6f512-3cd3-4170-a1cf-37af310be3d8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.74786ms
May  6 14:53:52.740: INFO: Pod "pod-secrets-7ee6f512-3cd3-4170-a1cf-37af310be3d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004433957s
STEP: Saw pod success
May  6 14:53:52.740: INFO: Pod "pod-secrets-7ee6f512-3cd3-4170-a1cf-37af310be3d8" satisfied condition "success or failure"
May  6 14:53:52.742: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-secrets-7ee6f512-3cd3-4170-a1cf-37af310be3d8 container secret-volume-test: <nil>
STEP: delete the pod
May  6 14:53:52.754: INFO: Waiting for pod pod-secrets-7ee6f512-3cd3-4170-a1cf-37af310be3d8 to disappear
May  6 14:53:52.755: INFO: Pod pod-secrets-7ee6f512-3cd3-4170-a1cf-37af310be3d8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:53:52.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-900" for this suite.
May  6 14:53:58.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:54:01.519: INFO: namespace secrets-900 deletion completed in 8.761153325s

• [SLOW TEST:10.923 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:54:01.520: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8596
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
May  6 14:54:01.659: INFO: Waiting up to 5m0s for pod "var-expansion-759d2d97-3978-44be-b776-60c590bbb098" in namespace "var-expansion-8596" to be "success or failure"
May  6 14:54:01.661: INFO: Pod "var-expansion-759d2d97-3978-44be-b776-60c590bbb098": Phase="Pending", Reason="", readiness=false. Elapsed: 1.815601ms
May  6 14:54:03.664: INFO: Pod "var-expansion-759d2d97-3978-44be-b776-60c590bbb098": Phase="Running", Reason="", readiness=true. Elapsed: 2.004597923s
May  6 14:54:05.667: INFO: Pod "var-expansion-759d2d97-3978-44be-b776-60c590bbb098": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007391783s
STEP: Saw pod success
May  6 14:54:05.667: INFO: Pod "var-expansion-759d2d97-3978-44be-b776-60c590bbb098" satisfied condition "success or failure"
May  6 14:54:05.669: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod var-expansion-759d2d97-3978-44be-b776-60c590bbb098 container dapi-container: <nil>
STEP: delete the pod
May  6 14:54:05.680: INFO: Waiting for pod var-expansion-759d2d97-3978-44be-b776-60c590bbb098 to disappear
May  6 14:54:05.682: INFO: Pod var-expansion-759d2d97-3978-44be-b776-60c590bbb098 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:54:05.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8596" for this suite.
May  6 14:54:11.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:54:14.444: INFO: namespace var-expansion-8596 deletion completed in 8.759016715s

• [SLOW TEST:12.924 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:54:14.444: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2341
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
May  6 14:54:14.574: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-926700516 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:54:14.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2341" for this suite.
May  6 14:54:20.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:54:23.386: INFO: namespace kubectl-2341 deletion completed in 8.759950282s

• [SLOW TEST:8.942 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:54:23.386: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7234
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May  6 14:54:23.524: INFO: Waiting up to 5m0s for pod "downward-api-42bbbf83-f5ee-4ad3-81d3-3e0f0ef4de29" in namespace "downward-api-7234" to be "success or failure"
May  6 14:54:23.526: INFO: Pod "downward-api-42bbbf83-f5ee-4ad3-81d3-3e0f0ef4de29": Phase="Pending", Reason="", readiness=false. Elapsed: 1.800157ms
May  6 14:54:25.528: INFO: Pod "downward-api-42bbbf83-f5ee-4ad3-81d3-3e0f0ef4de29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004257609s
May  6 14:54:27.531: INFO: Pod "downward-api-42bbbf83-f5ee-4ad3-81d3-3e0f0ef4de29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00674715s
STEP: Saw pod success
May  6 14:54:27.531: INFO: Pod "downward-api-42bbbf83-f5ee-4ad3-81d3-3e0f0ef4de29" satisfied condition "success or failure"
May  6 14:54:27.533: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod downward-api-42bbbf83-f5ee-4ad3-81d3-3e0f0ef4de29 container dapi-container: <nil>
STEP: delete the pod
May  6 14:54:27.546: INFO: Waiting for pod downward-api-42bbbf83-f5ee-4ad3-81d3-3e0f0ef4de29 to disappear
May  6 14:54:27.548: INFO: Pod downward-api-42bbbf83-f5ee-4ad3-81d3-3e0f0ef4de29 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:54:27.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7234" for this suite.
May  6 14:54:33.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:54:36.312: INFO: namespace downward-api-7234 deletion completed in 8.761208869s

• [SLOW TEST:12.926 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:54:36.312: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6900
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May  6 14:54:40.969: INFO: Successfully updated pod "pod-update-659e2b0d-eb23-43fd-b2d5-e0e7c9d60b15"
STEP: verifying the updated pod is in kubernetes
May  6 14:54:40.974: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:54:40.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6900" for this suite.
May  6 14:54:52.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:54:55.736: INFO: namespace pods-6900 deletion completed in 14.759237214s

• [SLOW TEST:19.424 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:54:55.736: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6590
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 14:54:55.871: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
May  6 14:55:00.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 --namespace=crd-publish-openapi-6590 create -f -'
May  6 14:55:01.155: INFO: stderr: ""
May  6 14:55:01.155: INFO: stdout: "e2e-test-crd-publish-openapi-6977-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May  6 14:55:01.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 --namespace=crd-publish-openapi-6590 delete e2e-test-crd-publish-openapi-6977-crds test-foo'
May  6 14:55:01.287: INFO: stderr: ""
May  6 14:55:01.287: INFO: stdout: "e2e-test-crd-publish-openapi-6977-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May  6 14:55:01.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 --namespace=crd-publish-openapi-6590 apply -f -'
May  6 14:55:01.575: INFO: stderr: ""
May  6 14:55:01.575: INFO: stdout: "e2e-test-crd-publish-openapi-6977-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May  6 14:55:01.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 --namespace=crd-publish-openapi-6590 delete e2e-test-crd-publish-openapi-6977-crds test-foo'
May  6 14:55:01.687: INFO: stderr: ""
May  6 14:55:01.687: INFO: stdout: "e2e-test-crd-publish-openapi-6977-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
May  6 14:55:01.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 --namespace=crd-publish-openapi-6590 create -f -'
May  6 14:55:01.855: INFO: rc: 1
May  6 14:55:01.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 --namespace=crd-publish-openapi-6590 apply -f -'
May  6 14:55:02.027: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
May  6 14:55:02.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 --namespace=crd-publish-openapi-6590 create -f -'
May  6 14:55:02.268: INFO: rc: 1
May  6 14:55:02.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 --namespace=crd-publish-openapi-6590 apply -f -'
May  6 14:55:02.440: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
May  6 14:55:02.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 explain e2e-test-crd-publish-openapi-6977-crds'
May  6 14:55:02.730: INFO: stderr: ""
May  6 14:55:02.730: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6977-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
May  6 14:55:02.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 explain e2e-test-crd-publish-openapi-6977-crds.metadata'
May  6 14:55:03.022: INFO: stderr: ""
May  6 14:55:03.022: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6977-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May  6 14:55:03.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 explain e2e-test-crd-publish-openapi-6977-crds.spec'
May  6 14:55:03.302: INFO: stderr: ""
May  6 14:55:03.302: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6977-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May  6 14:55:03.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 explain e2e-test-crd-publish-openapi-6977-crds.spec.bars'
May  6 14:55:03.514: INFO: stderr: ""
May  6 14:55:03.514: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6977-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
May  6 14:55:03.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 explain e2e-test-crd-publish-openapi-6977-crds.spec.bars2'
May  6 14:55:03.803: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:55:08.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6590" for this suite.
May  6 14:55:14.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:55:16.916: INFO: namespace crd-publish-openapi-6590 deletion completed in 8.761912602s

• [SLOW TEST:21.180 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:55:16.917: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9271
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 14:55:17.060: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
May  6 14:55:19.083: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:55:20.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9271" for this suite.
May  6 14:55:26.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:55:28.850: INFO: namespace replication-controller-9271 deletion completed in 8.759463823s

• [SLOW TEST:11.933 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:55:28.850: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3285
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-084fe7aa-4fcb-41ed-bc32-48ac93511da2
STEP: Creating a pod to test consume configMaps
May  6 14:55:28.992: INFO: Waiting up to 5m0s for pod "pod-configmaps-364e04fe-edce-4d7c-b542-7add14377493" in namespace "configmap-3285" to be "success or failure"
May  6 14:55:28.994: INFO: Pod "pod-configmaps-364e04fe-edce-4d7c-b542-7add14377493": Phase="Pending", Reason="", readiness=false. Elapsed: 1.917419ms
May  6 14:55:30.996: INFO: Pod "pod-configmaps-364e04fe-edce-4d7c-b542-7add14377493": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004420605s
STEP: Saw pod success
May  6 14:55:30.996: INFO: Pod "pod-configmaps-364e04fe-edce-4d7c-b542-7add14377493" satisfied condition "success or failure"
May  6 14:55:30.998: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-configmaps-364e04fe-edce-4d7c-b542-7add14377493 container configmap-volume-test: <nil>
STEP: delete the pod
May  6 14:55:31.012: INFO: Waiting for pod pod-configmaps-364e04fe-edce-4d7c-b542-7add14377493 to disappear
May  6 14:55:31.014: INFO: Pod pod-configmaps-364e04fe-edce-4d7c-b542-7add14377493 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:55:31.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3285" for this suite.
May  6 14:55:37.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:55:39.785: INFO: namespace configmap-3285 deletion completed in 8.767824216s

• [SLOW TEST:10.935 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:55:39.785: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5720
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:55:39.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5720" for this suite.
May  6 14:55:45.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:55:48.681: INFO: namespace custom-resource-definition-5720 deletion completed in 8.759563251s

• [SLOW TEST:8.896 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:55:48.681: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3155
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 14:55:48.823: INFO: Waiting up to 5m0s for pod "downwardapi-volume-896f2931-cdeb-43fa-b568-9e816af1cb63" in namespace "projected-3155" to be "success or failure"
May  6 14:55:48.825: INFO: Pod "downwardapi-volume-896f2931-cdeb-43fa-b568-9e816af1cb63": Phase="Pending", Reason="", readiness=false. Elapsed: 1.970779ms
May  6 14:55:50.827: INFO: Pod "downwardapi-volume-896f2931-cdeb-43fa-b568-9e816af1cb63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004775407s
STEP: Saw pod success
May  6 14:55:50.827: INFO: Pod "downwardapi-volume-896f2931-cdeb-43fa-b568-9e816af1cb63" satisfied condition "success or failure"
May  6 14:55:50.829: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod downwardapi-volume-896f2931-cdeb-43fa-b568-9e816af1cb63 container client-container: <nil>
STEP: delete the pod
May  6 14:55:50.841: INFO: Waiting for pod downwardapi-volume-896f2931-cdeb-43fa-b568-9e816af1cb63 to disappear
May  6 14:55:50.842: INFO: Pod downwardapi-volume-896f2931-cdeb-43fa-b568-9e816af1cb63 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:55:50.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3155" for this suite.
May  6 14:55:56.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:55:59.607: INFO: namespace projected-3155 deletion completed in 8.761582595s

• [SLOW TEST:10.926 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:55:59.607: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5698
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-41918af9-f617-4c2b-9861-c25c6b674c3c
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-41918af9-f617-4c2b-9861-c25c6b674c3c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:56:03.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5698" for this suite.
May  6 14:56:15.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:56:18.536: INFO: namespace configmap-5698 deletion completed in 14.759566964s

• [SLOW TEST:18.929 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:56:18.536: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3964
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-3964
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May  6 14:56:18.668: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May  6 14:56:40.751: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.121.32:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3964 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 14:56:40.751: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 14:56:40.921: INFO: Found all expected endpoints: [netserver-0]
May  6 14:56:40.923: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.80.131:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3964 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 14:56:40.923: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 14:56:41.088: INFO: Found all expected endpoints: [netserver-1]
May  6 14:56:41.091: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.85.65:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3964 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 14:56:41.091: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 14:56:41.266: INFO: Found all expected endpoints: [netserver-2]
May  6 14:56:41.269: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.187.57:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3964 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 14:56:41.269: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 14:56:41.443: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:56:41.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3964" for this suite.
May  6 14:56:53.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:56:56.209: INFO: namespace pod-network-test-3964 deletion completed in 14.762651556s

• [SLOW TEST:37.673 seconds]
[sig-network] Networking
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:56:56.210: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-415
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-d09b6f71-b903-4838-a3d2-b8a3871f7dcd
STEP: Creating a pod to test consume configMaps
May  6 14:56:56.350: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6e250493-5fb1-4c1f-9cd2-2d655b122b8e" in namespace "projected-415" to be "success or failure"
May  6 14:56:56.354: INFO: Pod "pod-projected-configmaps-6e250493-5fb1-4c1f-9cd2-2d655b122b8e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.506093ms
May  6 14:56:58.357: INFO: Pod "pod-projected-configmaps-6e250493-5fb1-4c1f-9cd2-2d655b122b8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006083123s
STEP: Saw pod success
May  6 14:56:58.357: INFO: Pod "pod-projected-configmaps-6e250493-5fb1-4c1f-9cd2-2d655b122b8e" satisfied condition "success or failure"
May  6 14:56:58.358: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-projected-configmaps-6e250493-5fb1-4c1f-9cd2-2d655b122b8e container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  6 14:56:58.380: INFO: Waiting for pod pod-projected-configmaps-6e250493-5fb1-4c1f-9cd2-2d655b122b8e to disappear
May  6 14:56:58.382: INFO: Pod pod-projected-configmaps-6e250493-5fb1-4c1f-9cd2-2d655b122b8e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:56:58.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-415" for this suite.
May  6 14:57:04.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:57:07.147: INFO: namespace projected-415 deletion completed in 8.761923792s

• [SLOW TEST:10.937 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:57:07.147: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9153
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-2720c917-d38e-4abe-bb5f-d3334dcaeae6
STEP: Creating a pod to test consume secrets
May  6 14:57:07.291: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7b7f018d-e97f-4c71-b2f4-d3aa6267f528" in namespace "projected-9153" to be "success or failure"
May  6 14:57:07.292: INFO: Pod "pod-projected-secrets-7b7f018d-e97f-4c71-b2f4-d3aa6267f528": Phase="Pending", Reason="", readiness=false. Elapsed: 1.766516ms
May  6 14:57:09.295: INFO: Pod "pod-projected-secrets-7b7f018d-e97f-4c71-b2f4-d3aa6267f528": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004434198s
STEP: Saw pod success
May  6 14:57:09.295: INFO: Pod "pod-projected-secrets-7b7f018d-e97f-4c71-b2f4-d3aa6267f528" satisfied condition "success or failure"
May  6 14:57:09.297: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-projected-secrets-7b7f018d-e97f-4c71-b2f4-d3aa6267f528 container projected-secret-volume-test: <nil>
STEP: delete the pod
May  6 14:57:09.309: INFO: Waiting for pod pod-projected-secrets-7b7f018d-e97f-4c71-b2f4-d3aa6267f528 to disappear
May  6 14:57:09.310: INFO: Pod pod-projected-secrets-7b7f018d-e97f-4c71-b2f4-d3aa6267f528 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:57:09.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9153" for this suite.
May  6 14:57:15.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:57:18.073: INFO: namespace projected-9153 deletion completed in 8.759626778s

• [SLOW TEST:10.926 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:57:18.073: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-433
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 14:57:18.513: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 14:57:21.525: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:57:21.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-433" for this suite.
May  6 14:57:27.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:57:30.293: INFO: namespace webhook-433 deletion completed in 8.759502641s
STEP: Destroying namespace "webhook-433-markers" for this suite.
May  6 14:57:36.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:57:39.056: INFO: namespace webhook-433-markers deletion completed in 8.763065975s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:20.993 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:57:39.066: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6207
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-451ec582-9146-4c8d-9b22-a2a28e3ccf62
STEP: Creating a pod to test consume configMaps
May  6 14:57:39.205: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-715acb70-f24f-4a9d-91bd-5b5b53266df8" in namespace "projected-6207" to be "success or failure"
May  6 14:57:39.207: INFO: Pod "pod-projected-configmaps-715acb70-f24f-4a9d-91bd-5b5b53266df8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.743257ms
May  6 14:57:41.209: INFO: Pod "pod-projected-configmaps-715acb70-f24f-4a9d-91bd-5b5b53266df8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004049886s
STEP: Saw pod success
May  6 14:57:41.209: INFO: Pod "pod-projected-configmaps-715acb70-f24f-4a9d-91bd-5b5b53266df8" satisfied condition "success or failure"
May  6 14:57:41.211: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-projected-configmaps-715acb70-f24f-4a9d-91bd-5b5b53266df8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  6 14:57:41.226: INFO: Waiting for pod pod-projected-configmaps-715acb70-f24f-4a9d-91bd-5b5b53266df8 to disappear
May  6 14:57:41.228: INFO: Pod pod-projected-configmaps-715acb70-f24f-4a9d-91bd-5b5b53266df8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 14:57:41.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6207" for this suite.
May  6 14:57:47.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 14:57:49.993: INFO: namespace projected-6207 deletion completed in 8.761728532s

• [SLOW TEST:10.927 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 14:57:49.993: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3958
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-05351dab-2d00-4fef-ab92-a7d5398d14cd in namespace container-probe-3958
May  6 14:57:52.138: INFO: Started pod busybox-05351dab-2d00-4fef-ab92-a7d5398d14cd in namespace container-probe-3958
STEP: checking the pod's current state and verifying that restartCount is present
May  6 14:57:52.139: INFO: Initial restart count of pod busybox-05351dab-2d00-4fef-ab92-a7d5398d14cd is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:01:52.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3958" for this suite.
May  6 15:01:58.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:02:01.238: INFO: namespace container-probe-3958 deletion completed in 8.760819035s

• [SLOW TEST:251.245 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:02:01.238: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4628
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May  6 15:02:01.382: INFO: Waiting up to 5m0s for pod "downward-api-d7a9ebcc-96a7-43c3-8fea-da5f23b1aa3b" in namespace "downward-api-4628" to be "success or failure"
May  6 15:02:01.384: INFO: Pod "downward-api-d7a9ebcc-96a7-43c3-8fea-da5f23b1aa3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.963791ms
May  6 15:02:03.387: INFO: Pod "downward-api-d7a9ebcc-96a7-43c3-8fea-da5f23b1aa3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004831038s
May  6 15:02:05.390: INFO: Pod "downward-api-d7a9ebcc-96a7-43c3-8fea-da5f23b1aa3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007844242s
STEP: Saw pod success
May  6 15:02:05.390: INFO: Pod "downward-api-d7a9ebcc-96a7-43c3-8fea-da5f23b1aa3b" satisfied condition "success or failure"
May  6 15:02:05.392: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod downward-api-d7a9ebcc-96a7-43c3-8fea-da5f23b1aa3b container dapi-container: <nil>
STEP: delete the pod
May  6 15:02:05.411: INFO: Waiting for pod downward-api-d7a9ebcc-96a7-43c3-8fea-da5f23b1aa3b to disappear
May  6 15:02:05.413: INFO: Pod downward-api-d7a9ebcc-96a7-43c3-8fea-da5f23b1aa3b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:02:05.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4628" for this suite.
May  6 15:02:11.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:02:14.175: INFO: namespace downward-api-4628 deletion completed in 8.759273104s

• [SLOW TEST:12.937 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:02:14.176: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2302
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
May  6 15:02:14.315: INFO: Waiting up to 5m0s for pod "pod-4ea3b565-f609-4ae4-9d58-43bf7709289b" in namespace "emptydir-2302" to be "success or failure"
May  6 15:02:14.317: INFO: Pod "pod-4ea3b565-f609-4ae4-9d58-43bf7709289b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.897186ms
May  6 15:02:16.320: INFO: Pod "pod-4ea3b565-f609-4ae4-9d58-43bf7709289b": Phase="Running", Reason="", readiness=true. Elapsed: 2.004853902s
May  6 15:02:18.322: INFO: Pod "pod-4ea3b565-f609-4ae4-9d58-43bf7709289b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007544191s
STEP: Saw pod success
May  6 15:02:18.322: INFO: Pod "pod-4ea3b565-f609-4ae4-9d58-43bf7709289b" satisfied condition "success or failure"
May  6 15:02:18.324: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-4ea3b565-f609-4ae4-9d58-43bf7709289b container test-container: <nil>
STEP: delete the pod
May  6 15:02:18.336: INFO: Waiting for pod pod-4ea3b565-f609-4ae4-9d58-43bf7709289b to disappear
May  6 15:02:18.340: INFO: Pod pod-4ea3b565-f609-4ae4-9d58-43bf7709289b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:02:18.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2302" for this suite.
May  6 15:02:24.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:02:27.102: INFO: namespace emptydir-2302 deletion completed in 8.759400125s

• [SLOW TEST:12.926 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:02:27.102: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1581
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 15:02:27.698: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May  6 15:02:29.705: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724374147, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724374147, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724374147, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724374147, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 15:02:32.714: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
May  6 15:02:32.728: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:02:32.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1581" for this suite.
May  6 15:02:38.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:02:41.498: INFO: namespace webhook-1581 deletion completed in 8.759260014s
STEP: Destroying namespace "webhook-1581-markers" for this suite.
May  6 15:02:47.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:02:50.257: INFO: namespace webhook-1581-markers deletion completed in 8.759366844s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:23.166 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:02:50.269: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2190
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 15:02:50.407: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b080c67c-f9cb-4209-9d1d-9c8f2eb937cf" in namespace "downward-api-2190" to be "success or failure"
May  6 15:02:50.409: INFO: Pod "downwardapi-volume-b080c67c-f9cb-4209-9d1d-9c8f2eb937cf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.769586ms
May  6 15:02:52.412: INFO: Pod "downwardapi-volume-b080c67c-f9cb-4209-9d1d-9c8f2eb937cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004576005s
STEP: Saw pod success
May  6 15:02:52.412: INFO: Pod "downwardapi-volume-b080c67c-f9cb-4209-9d1d-9c8f2eb937cf" satisfied condition "success or failure"
May  6 15:02:52.414: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod downwardapi-volume-b080c67c-f9cb-4209-9d1d-9c8f2eb937cf container client-container: <nil>
STEP: delete the pod
May  6 15:02:52.426: INFO: Waiting for pod downwardapi-volume-b080c67c-f9cb-4209-9d1d-9c8f2eb937cf to disappear
May  6 15:02:52.428: INFO: Pod downwardapi-volume-b080c67c-f9cb-4209-9d1d-9c8f2eb937cf no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:02:52.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2190" for this suite.
May  6 15:02:58.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:03:01.193: INFO: namespace downward-api-2190 deletion completed in 8.762033418s

• [SLOW TEST:10.924 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:03:01.193: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7937
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7937.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7937.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7937.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7937.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7937.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7937.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  6 15:03:05.360: INFO: DNS probes using dns-7937/dns-test-1b31beec-f2e9-4ba9-b58b-3eeb0e099397 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:03:05.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7937" for this suite.
May  6 15:03:11.391: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:03:14.143: INFO: namespace dns-7937 deletion completed in 8.759445831s

• [SLOW TEST:12.950 seconds]
[sig-network] DNS
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:03:14.143: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3427
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 15:03:14.283: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-35140168-fa6e-491c-9697-5f8e299908a1" in namespace "security-context-test-3427" to be "success or failure"
May  6 15:03:14.285: INFO: Pod "alpine-nnp-false-35140168-fa6e-491c-9697-5f8e299908a1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.801062ms
May  6 15:03:16.287: INFO: Pod "alpine-nnp-false-35140168-fa6e-491c-9697-5f8e299908a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004373106s
May  6 15:03:16.287: INFO: Pod "alpine-nnp-false-35140168-fa6e-491c-9697-5f8e299908a1" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:03:16.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3427" for this suite.
May  6 15:03:22.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:03:25.054: INFO: namespace security-context-test-3427 deletion completed in 8.759616503s

• [SLOW TEST:10.911 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:03:25.054: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5945
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:03:29.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5945" for this suite.
May  6 15:03:35.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:03:37.978: INFO: namespace kubelet-test-5945 deletion completed in 8.759792468s

• [SLOW TEST:12.924 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:03:37.978: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4848
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 15:03:38.414: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  6 15:03:40.421: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724374218, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724374218, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724374218, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724374218, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 15:03:43.430: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:03:43.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4848" for this suite.
May  6 15:03:49.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:03:52.235: INFO: namespace webhook-4848 deletion completed in 8.759760038s
STEP: Destroying namespace "webhook-4848-markers" for this suite.
May  6 15:03:58.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:04:00.995: INFO: namespace webhook-4848-markers deletion completed in 8.760098983s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:23.028 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:04:01.006: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6928
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 15:04:01.139: INFO: Creating deployment "test-recreate-deployment"
May  6 15:04:01.143: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May  6 15:04:01.147: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May  6 15:04:03.152: INFO: Waiting deployment "test-recreate-deployment" to complete
May  6 15:04:03.154: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724374241, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724374241, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724374241, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724374241, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 15:04:05.157: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May  6 15:04:05.162: INFO: Updating deployment test-recreate-deployment
May  6 15:04:05.162: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May  6 15:04:05.214: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6928 /apis/apps/v1/namespaces/deployment-6928/deployments/test-recreate-deployment 6d2579e4-edc1-4076-b265-68def666b2c7 111490 2 2020-05-06 15:04:01 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004622fa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-05-06 15:04:05 +0000 UTC,LastTransitionTime:2020-05-06 15:04:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-05-06 15:04:05 +0000 UTC,LastTransitionTime:2020-05-06 15:04:01 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May  6 15:04:05.216: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-6928 /apis/apps/v1/namespaces/deployment-6928/replicasets/test-recreate-deployment-5f94c574ff a104bd01-28dd-46f2-8d6f-f64fd031cc37 111488 1 2020-05-06 15:04:05 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 6d2579e4-edc1-4076-b265-68def666b2c7 0xc004623387 0xc004623388}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0046233e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  6 15:04:05.216: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May  6 15:04:05.216: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-6928 /apis/apps/v1/namespaces/deployment-6928/replicasets/test-recreate-deployment-68fc85c7bb e50ce19d-3dec-42f9-ad68-41b3fd82ddca 111478 2 2020-05-06 15:04:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 6d2579e4-edc1-4076-b265-68def666b2c7 0xc004623457 0xc004623458}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0046234b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  6 15:04:05.219: INFO: Pod "test-recreate-deployment-5f94c574ff-hzh8q" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-hzh8q test-recreate-deployment-5f94c574ff- deployment-6928 /api/v1/namespaces/deployment-6928/pods/test-recreate-deployment-5f94c574ff-hzh8q c88291bc-ac4b-428e-8222-48da86bcfa61 111489 0 2020-05-06 15:04:05 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff a104bd01-28dd-46f2-8d6f-f64fd031cc37 0xc004623937 0xc004623938}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zxx2j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zxx2j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zxx2j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-80.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 15:04:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 15:04:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 15:04:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 15:04:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.80,PodIP:,StartTime:2020-05-06 15:04:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:04:05.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6928" for this suite.
May  6 15:04:11.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:04:13.981: INFO: namespace deployment-6928 deletion completed in 8.759737667s

• [SLOW TEST:12.975 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:04:13.982: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7060
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 15:04:14.119: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fb8b4feb-c499-47a7-b1e5-933c91650386" in namespace "projected-7060" to be "success or failure"
May  6 15:04:14.121: INFO: Pod "downwardapi-volume-fb8b4feb-c499-47a7-b1e5-933c91650386": Phase="Pending", Reason="", readiness=false. Elapsed: 1.81666ms
May  6 15:04:16.124: INFO: Pod "downwardapi-volume-fb8b4feb-c499-47a7-b1e5-933c91650386": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00467568s
STEP: Saw pod success
May  6 15:04:16.124: INFO: Pod "downwardapi-volume-fb8b4feb-c499-47a7-b1e5-933c91650386" satisfied condition "success or failure"
May  6 15:04:16.126: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod downwardapi-volume-fb8b4feb-c499-47a7-b1e5-933c91650386 container client-container: <nil>
STEP: delete the pod
May  6 15:04:16.137: INFO: Waiting for pod downwardapi-volume-fb8b4feb-c499-47a7-b1e5-933c91650386 to disappear
May  6 15:04:16.139: INFO: Pod downwardapi-volume-fb8b4feb-c499-47a7-b1e5-933c91650386 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:04:16.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7060" for this suite.
May  6 15:04:22.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:04:24.905: INFO: namespace projected-7060 deletion completed in 8.761484361s

• [SLOW TEST:10.923 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:04:24.905: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9241
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:04:25.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9241" for this suite.
May  6 15:04:31.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:04:33.828: INFO: namespace kubelet-test-9241 deletion completed in 8.772484038s

• [SLOW TEST:8.923 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:04:33.829: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2753
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
May  6 15:04:33.965: INFO: Waiting up to 5m0s for pod "client-containers-6e964940-8fe0-4cf3-b372-4e4773167973" in namespace "containers-2753" to be "success or failure"
May  6 15:04:33.967: INFO: Pod "client-containers-6e964940-8fe0-4cf3-b372-4e4773167973": Phase="Pending", Reason="", readiness=false. Elapsed: 1.755858ms
May  6 15:04:35.969: INFO: Pod "client-containers-6e964940-8fe0-4cf3-b372-4e4773167973": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003830512s
STEP: Saw pod success
May  6 15:04:35.969: INFO: Pod "client-containers-6e964940-8fe0-4cf3-b372-4e4773167973" satisfied condition "success or failure"
May  6 15:04:35.971: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod client-containers-6e964940-8fe0-4cf3-b372-4e4773167973 container test-container: <nil>
STEP: delete the pod
May  6 15:04:35.982: INFO: Waiting for pod client-containers-6e964940-8fe0-4cf3-b372-4e4773167973 to disappear
May  6 15:04:35.984: INFO: Pod client-containers-6e964940-8fe0-4cf3-b372-4e4773167973 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:04:35.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2753" for this suite.
May  6 15:04:41.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:04:44.746: INFO: namespace containers-2753 deletion completed in 8.759403831s

• [SLOW TEST:10.918 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:04:44.746: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6722
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 15:04:44.883: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6a43378a-bad5-44e4-908e-4f6144819799" in namespace "security-context-test-6722" to be "success or failure"
May  6 15:04:44.885: INFO: Pod "busybox-privileged-false-6a43378a-bad5-44e4-908e-4f6144819799": Phase="Pending", Reason="", readiness=false. Elapsed: 1.864846ms
May  6 15:04:46.888: INFO: Pod "busybox-privileged-false-6a43378a-bad5-44e4-908e-4f6144819799": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004710473s
May  6 15:04:48.891: INFO: Pod "busybox-privileged-false-6a43378a-bad5-44e4-908e-4f6144819799": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00765993s
May  6 15:04:48.891: INFO: Pod "busybox-privileged-false-6a43378a-bad5-44e4-908e-4f6144819799" satisfied condition "success or failure"
May  6 15:04:48.895: INFO: Got logs for pod "busybox-privileged-false-6a43378a-bad5-44e4-908e-4f6144819799": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:04:48.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6722" for this suite.
May  6 15:04:54.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:04:57.657: INFO: namespace security-context-test-6722 deletion completed in 8.759328643s

• [SLOW TEST:12.911 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:04:57.658: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6623
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May  6 15:04:57.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-6623'
May  6 15:04:57.874: INFO: stderr: ""
May  6 15:04:57.874: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
May  6 15:04:57.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 delete pods e2e-test-httpd-pod --namespace=kubectl-6623'
May  6 15:05:07.183: INFO: stderr: ""
May  6 15:05:07.183: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:05:07.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6623" for this suite.
May  6 15:05:13.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:05:15.946: INFO: namespace kubectl-6623 deletion completed in 8.759830346s

• [SLOW TEST:18.289 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:05:15.947: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2864
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May  6 15:05:20.108: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  6 15:05:20.112: INFO: Pod pod-with-prestop-exec-hook still exists
May  6 15:05:22.112: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  6 15:05:22.115: INFO: Pod pod-with-prestop-exec-hook still exists
May  6 15:05:24.112: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  6 15:05:24.115: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:05:24.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2864" for this suite.
May  6 15:05:36.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:05:38.881: INFO: namespace container-lifecycle-hook-2864 deletion completed in 14.759391484s

• [SLOW TEST:22.935 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:05:38.881: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7177
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 15:05:39.022: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ee084e92-4f22-49c0-ae15-010fc10fe705" in namespace "projected-7177" to be "success or failure"
May  6 15:05:39.024: INFO: Pod "downwardapi-volume-ee084e92-4f22-49c0-ae15-010fc10fe705": Phase="Pending", Reason="", readiness=false. Elapsed: 1.872582ms
May  6 15:05:41.026: INFO: Pod "downwardapi-volume-ee084e92-4f22-49c0-ae15-010fc10fe705": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004600325s
STEP: Saw pod success
May  6 15:05:41.027: INFO: Pod "downwardapi-volume-ee084e92-4f22-49c0-ae15-010fc10fe705" satisfied condition "success or failure"
May  6 15:05:41.028: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod downwardapi-volume-ee084e92-4f22-49c0-ae15-010fc10fe705 container client-container: <nil>
STEP: delete the pod
May  6 15:05:41.041: INFO: Waiting for pod downwardapi-volume-ee084e92-4f22-49c0-ae15-010fc10fe705 to disappear
May  6 15:05:41.043: INFO: Pod downwardapi-volume-ee084e92-4f22-49c0-ae15-010fc10fe705 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:05:41.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7177" for this suite.
May  6 15:05:47.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:05:49.809: INFO: namespace projected-7177 deletion completed in 8.763309378s

• [SLOW TEST:10.928 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:05:49.809: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2073
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5728
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4876
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:05:56.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2073" for this suite.
May  6 15:06:02.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:06:04.978: INFO: namespace namespaces-2073 deletion completed in 8.759707169s
STEP: Destroying namespace "nsdeletetest-5728" for this suite.
May  6 15:06:04.980: INFO: Namespace nsdeletetest-5728 was already deleted
STEP: Destroying namespace "nsdeletetest-4876" for this suite.
May  6 15:06:10.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:06:13.740: INFO: namespace nsdeletetest-4876 deletion completed in 8.759777567s

• [SLOW TEST:23.931 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:06:13.740: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9150
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9150
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-9150
I0506 15:06:13.887321      25 runners.go:184] Created replication controller with name: externalname-service, namespace: services-9150, replica count: 2
May  6 15:06:16.937: INFO: Creating new exec pod
I0506 15:06:16.937588      25 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  6 15:06:19.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=services-9150 execpod26r7m -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May  6 15:06:20.396: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May  6 15:06:20.396: INFO: stdout: ""
May  6 15:06:20.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=services-9150 execpod26r7m -- /bin/sh -x -c nc -zv -t -w 2 10.0.57.90 80'
May  6 15:06:20.656: INFO: stderr: "+ nc -zv -t -w 2 10.0.57.90 80\nConnection to 10.0.57.90 80 port [tcp/http] succeeded!\n"
May  6 15:06:20.656: INFO: stdout: ""
May  6 15:06:20.656: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:06:20.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9150" for this suite.
May  6 15:06:26.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:06:29.434: INFO: namespace services-9150 deletion completed in 8.760617289s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:15.694 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:06:29.435: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5586
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
May  6 15:06:29.574: INFO: Waiting up to 5m0s for pod "pod-058315b5-e9e9-41b3-9c47-ccea7e2f480f" in namespace "emptydir-5586" to be "success or failure"
May  6 15:06:29.576: INFO: Pod "pod-058315b5-e9e9-41b3-9c47-ccea7e2f480f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.65915ms
May  6 15:06:31.579: INFO: Pod "pod-058315b5-e9e9-41b3-9c47-ccea7e2f480f": Phase="Running", Reason="", readiness=true. Elapsed: 2.004218471s
May  6 15:06:33.581: INFO: Pod "pod-058315b5-e9e9-41b3-9c47-ccea7e2f480f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006609387s
STEP: Saw pod success
May  6 15:06:33.581: INFO: Pod "pod-058315b5-e9e9-41b3-9c47-ccea7e2f480f" satisfied condition "success or failure"
May  6 15:06:33.583: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-058315b5-e9e9-41b3-9c47-ccea7e2f480f container test-container: <nil>
STEP: delete the pod
May  6 15:06:33.600: INFO: Waiting for pod pod-058315b5-e9e9-41b3-9c47-ccea7e2f480f to disappear
May  6 15:06:33.603: INFO: Pod pod-058315b5-e9e9-41b3-9c47-ccea7e2f480f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:06:33.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5586" for this suite.
May  6 15:06:39.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:06:42.364: INFO: namespace emptydir-5586 deletion completed in 8.758966669s

• [SLOW TEST:12.929 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:06:42.364: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-379
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May  6 15:06:45.516: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:06:45.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-379" for this suite.
May  6 15:06:51.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:06:54.288: INFO: namespace container-runtime-379 deletion completed in 8.759268638s

• [SLOW TEST:11.923 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:06:54.288: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5230
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
May  6 15:06:54.424: INFO: Waiting up to 5m0s for pod "pod-3090ffec-97f9-495c-8281-b4dad0233e0b" in namespace "emptydir-5230" to be "success or failure"
May  6 15:06:54.427: INFO: Pod "pod-3090ffec-97f9-495c-8281-b4dad0233e0b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.742024ms
May  6 15:06:56.430: INFO: Pod "pod-3090ffec-97f9-495c-8281-b4dad0233e0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006397599s
STEP: Saw pod success
May  6 15:06:56.430: INFO: Pod "pod-3090ffec-97f9-495c-8281-b4dad0233e0b" satisfied condition "success or failure"
May  6 15:06:56.432: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-3090ffec-97f9-495c-8281-b4dad0233e0b container test-container: <nil>
STEP: delete the pod
May  6 15:06:56.444: INFO: Waiting for pod pod-3090ffec-97f9-495c-8281-b4dad0233e0b to disappear
May  6 15:06:56.445: INFO: Pod pod-3090ffec-97f9-495c-8281-b4dad0233e0b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:06:56.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5230" for this suite.
May  6 15:07:02.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:07:05.208: INFO: namespace emptydir-5230 deletion completed in 8.759237591s

• [SLOW TEST:10.920 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:07:05.208: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2384
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 15:07:05.643: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 15:07:08.657: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 15:07:08.659: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9277-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:07:09.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2384" for this suite.
May  6 15:07:15.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:07:18.499: INFO: namespace webhook-2384 deletion completed in 8.774506437s
STEP: Destroying namespace "webhook-2384-markers" for this suite.
May  6 15:07:24.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:07:27.259: INFO: namespace webhook-2384-markers deletion completed in 8.760021686s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:22.062 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:07:27.270: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1355
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
May  6 15:07:27.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 create -f - --namespace=kubectl-1355'
May  6 15:07:27.665: INFO: stderr: ""
May  6 15:07:27.665: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  6 15:07:27.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1355'
May  6 15:07:27.774: INFO: stderr: ""
May  6 15:07:27.774: INFO: stdout: "update-demo-nautilus-lp9c4 update-demo-nautilus-mkvwz "
May  6 15:07:27.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-nautilus-lp9c4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1355'
May  6 15:07:27.878: INFO: stderr: ""
May  6 15:07:27.878: INFO: stdout: ""
May  6 15:07:27.878: INFO: update-demo-nautilus-lp9c4 is created but not running
May  6 15:07:32.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1355'
May  6 15:07:32.983: INFO: stderr: ""
May  6 15:07:32.983: INFO: stdout: "update-demo-nautilus-lp9c4 update-demo-nautilus-mkvwz "
May  6 15:07:32.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-nautilus-lp9c4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1355'
May  6 15:07:33.089: INFO: stderr: ""
May  6 15:07:33.089: INFO: stdout: "true"
May  6 15:07:33.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-nautilus-lp9c4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1355'
May  6 15:07:33.194: INFO: stderr: ""
May  6 15:07:33.194: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  6 15:07:33.194: INFO: validating pod update-demo-nautilus-lp9c4
May  6 15:07:33.197: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 15:07:33.197: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 15:07:33.197: INFO: update-demo-nautilus-lp9c4 is verified up and running
May  6 15:07:33.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-nautilus-mkvwz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1355'
May  6 15:07:33.297: INFO: stderr: ""
May  6 15:07:33.297: INFO: stdout: "true"
May  6 15:07:33.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-nautilus-mkvwz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1355'
May  6 15:07:33.399: INFO: stderr: ""
May  6 15:07:33.399: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  6 15:07:33.399: INFO: validating pod update-demo-nautilus-mkvwz
May  6 15:07:33.402: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 15:07:33.402: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 15:07:33.402: INFO: update-demo-nautilus-mkvwz is verified up and running
STEP: using delete to clean up resources
May  6 15:07:33.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 delete --grace-period=0 --force -f - --namespace=kubectl-1355'
May  6 15:07:33.476: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 15:07:33.476: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May  6 15:07:33.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1355'
May  6 15:07:33.612: INFO: stderr: "No resources found in kubectl-1355 namespace.\n"
May  6 15:07:33.612: INFO: stdout: ""
May  6 15:07:33.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods -l name=update-demo --namespace=kubectl-1355 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  6 15:07:33.713: INFO: stderr: ""
May  6 15:07:33.713: INFO: stdout: "update-demo-nautilus-lp9c4\nupdate-demo-nautilus-mkvwz\n"
May  6 15:07:34.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1355'
May  6 15:07:34.350: INFO: stderr: "No resources found in kubectl-1355 namespace.\n"
May  6 15:07:34.350: INFO: stdout: ""
May  6 15:07:34.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods -l name=update-demo --namespace=kubectl-1355 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  6 15:07:34.452: INFO: stderr: ""
May  6 15:07:34.452: INFO: stdout: "update-demo-nautilus-lp9c4\nupdate-demo-nautilus-mkvwz\n"
May  6 15:07:34.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1355'
May  6 15:07:34.850: INFO: stderr: "No resources found in kubectl-1355 namespace.\n"
May  6 15:07:34.850: INFO: stdout: ""
May  6 15:07:34.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods -l name=update-demo --namespace=kubectl-1355 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  6 15:07:34.964: INFO: stderr: ""
May  6 15:07:34.964: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:07:34.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1355" for this suite.
May  6 15:08:02.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:08:05.727: INFO: namespace kubectl-1355 deletion completed in 30.759509944s

• [SLOW TEST:38.458 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:08:05.727: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6566
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 15:08:05.866: INFO: Waiting up to 5m0s for pod "downwardapi-volume-69ac501d-5d98-4945-a374-38da1a7a1419" in namespace "downward-api-6566" to be "success or failure"
May  6 15:08:05.868: INFO: Pod "downwardapi-volume-69ac501d-5d98-4945-a374-38da1a7a1419": Phase="Pending", Reason="", readiness=false. Elapsed: 1.718533ms
May  6 15:08:07.871: INFO: Pod "downwardapi-volume-69ac501d-5d98-4945-a374-38da1a7a1419": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004778006s
STEP: Saw pod success
May  6 15:08:07.871: INFO: Pod "downwardapi-volume-69ac501d-5d98-4945-a374-38da1a7a1419" satisfied condition "success or failure"
May  6 15:08:07.873: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod downwardapi-volume-69ac501d-5d98-4945-a374-38da1a7a1419 container client-container: <nil>
STEP: delete the pod
May  6 15:08:07.888: INFO: Waiting for pod downwardapi-volume-69ac501d-5d98-4945-a374-38da1a7a1419 to disappear
May  6 15:08:07.890: INFO: Pod downwardapi-volume-69ac501d-5d98-4945-a374-38da1a7a1419 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:08:07.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6566" for this suite.
May  6 15:08:13.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:08:16.663: INFO: namespace downward-api-6566 deletion completed in 8.769878338s

• [SLOW TEST:10.936 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:08:16.663: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-312
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May  6 15:08:16.821: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 15:08:16.821: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 15:08:16.821: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 15:08:16.823: INFO: Number of nodes with available pods: 0
May  6 15:08:16.823: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 15:08:17.826: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 15:08:17.826: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 15:08:17.826: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 15:08:17.829: INFO: Number of nodes with available pods: 0
May  6 15:08:17.829: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 15:08:18.826: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 15:08:18.826: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 15:08:18.826: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 15:08:18.829: INFO: Number of nodes with available pods: 1
May  6 15:08:18.829: INFO: Node ip-10-0-129-1.us-west-2.compute.internal is running more than one daemon pod
May  6 15:08:19.826: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 15:08:19.826: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 15:08:19.826: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 15:08:19.829: INFO: Number of nodes with available pods: 4
May  6 15:08:19.829: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May  6 15:08:19.839: INFO: DaemonSet pods can't tolerate node ip-10-0-192-36.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 15:08:19.839: INFO: DaemonSet pods can't tolerate node ip-10-0-194-233.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 15:08:19.839: INFO: DaemonSet pods can't tolerate node ip-10-0-195-144.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  6 15:08:19.841: INFO: Number of nodes with available pods: 4
May  6 15:08:19.841: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-312, will wait for the garbage collector to delete the pods
May  6 15:08:20.906: INFO: Deleting DaemonSet.extensions daemon-set took: 5.479163ms
May  6 15:08:22.306: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.400166687s
May  6 15:08:27.208: INFO: Number of nodes with available pods: 0
May  6 15:08:27.208: INFO: Number of running nodes: 0, number of available pods: 0
May  6 15:08:27.210: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-312/daemonsets","resourceVersion":"113536"},"items":null}

May  6 15:08:27.212: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-312/pods","resourceVersion":"113536"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:08:27.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-312" for this suite.
May  6 15:08:33.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:08:35.983: INFO: namespace daemonsets-312 deletion completed in 8.759394986s

• [SLOW TEST:19.320 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:08:35.983: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1553
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 15:08:36.114: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May  6 15:08:40.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 --namespace=crd-publish-openapi-1553 create -f -'
May  6 15:08:40.932: INFO: stderr: ""
May  6 15:08:40.932: INFO: stdout: "e2e-test-crd-publish-openapi-3701-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May  6 15:08:40.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 --namespace=crd-publish-openapi-1553 delete e2e-test-crd-publish-openapi-3701-crds test-cr'
May  6 15:08:41.041: INFO: stderr: ""
May  6 15:08:41.041: INFO: stdout: "e2e-test-crd-publish-openapi-3701-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May  6 15:08:41.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 --namespace=crd-publish-openapi-1553 apply -f -'
May  6 15:08:41.334: INFO: stderr: ""
May  6 15:08:41.334: INFO: stdout: "e2e-test-crd-publish-openapi-3701-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May  6 15:08:41.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 --namespace=crd-publish-openapi-1553 delete e2e-test-crd-publish-openapi-3701-crds test-cr'
May  6 15:08:41.444: INFO: stderr: ""
May  6 15:08:41.444: INFO: stdout: "e2e-test-crd-publish-openapi-3701-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
May  6 15:08:41.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 explain e2e-test-crd-publish-openapi-3701-crds'
May  6 15:08:41.736: INFO: stderr: ""
May  6 15:08:41.736: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3701-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:08:46.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1553" for this suite.
May  6 15:08:52.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:08:54.813: INFO: namespace crd-publish-openapi-1553 deletion completed in 8.760110913s

• [SLOW TEST:18.829 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:08:54.813: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9058
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 15:08:54.951: INFO: Waiting up to 5m0s for pod "downwardapi-volume-57f5f396-646c-4098-a9da-c0b9d0a768c5" in namespace "projected-9058" to be "success or failure"
May  6 15:08:54.953: INFO: Pod "downwardapi-volume-57f5f396-646c-4098-a9da-c0b9d0a768c5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.89914ms
May  6 15:08:56.962: INFO: Pod "downwardapi-volume-57f5f396-646c-4098-a9da-c0b9d0a768c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010534861s
STEP: Saw pod success
May  6 15:08:56.962: INFO: Pod "downwardapi-volume-57f5f396-646c-4098-a9da-c0b9d0a768c5" satisfied condition "success or failure"
May  6 15:08:56.964: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod downwardapi-volume-57f5f396-646c-4098-a9da-c0b9d0a768c5 container client-container: <nil>
STEP: delete the pod
May  6 15:08:56.985: INFO: Waiting for pod downwardapi-volume-57f5f396-646c-4098-a9da-c0b9d0a768c5 to disappear
May  6 15:08:56.986: INFO: Pod downwardapi-volume-57f5f396-646c-4098-a9da-c0b9d0a768c5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:08:56.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9058" for this suite.
May  6 15:09:02.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:09:05.748: INFO: namespace projected-9058 deletion completed in 8.759150794s

• [SLOW TEST:10.936 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:09:05.749: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7938
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:09:18.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7938" for this suite.
May  6 15:09:24.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:09:27.700: INFO: namespace resourcequota-7938 deletion completed in 8.770289149s

• [SLOW TEST:21.952 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:09:27.701: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7403
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7403
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
May  6 15:09:27.840: INFO: Found 0 stateful pods, waiting for 3
May  6 15:09:37.843: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  6 15:09:37.843: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  6 15:09:37.843: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May  6 15:09:37.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=statefulset-7403 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 15:09:38.125: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 15:09:38.125: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 15:09:38.125: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May  6 15:09:48.155: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May  6 15:09:58.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=statefulset-7403 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 15:09:58.434: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  6 15:09:58.434: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 15:09:58.435: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 15:10:08.448: INFO: Waiting for StatefulSet statefulset-7403/ss2 to complete update
May  6 15:10:08.448: INFO: Waiting for Pod statefulset-7403/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May  6 15:10:18.453: INFO: Waiting for StatefulSet statefulset-7403/ss2 to complete update
STEP: Rolling back to a previous revision
May  6 15:10:28.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=statefulset-7403 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 15:10:28.723: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 15:10:28.723: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 15:10:28.723: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 15:10:38.752: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May  6 15:10:48.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=statefulset-7403 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 15:10:49.038: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  6 15:10:49.038: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 15:10:49.038: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 15:10:59.052: INFO: Waiting for StatefulSet statefulset-7403/ss2 to complete update
May  6 15:10:59.052: INFO: Waiting for Pod statefulset-7403/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May  6 15:11:09.058: INFO: Waiting for StatefulSet statefulset-7403/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May  6 15:11:19.058: INFO: Deleting all statefulset in ns statefulset-7403
May  6 15:11:19.059: INFO: Scaling statefulset ss2 to 0
May  6 15:11:39.069: INFO: Waiting for statefulset status.replicas updated to 0
May  6 15:11:39.071: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:11:39.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7403" for this suite.
May  6 15:11:45.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:11:47.843: INFO: namespace statefulset-7403 deletion completed in 8.75962669s

• [SLOW TEST:140.143 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:11:47.844: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-222
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 15:11:47.978: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:11:48.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-222" for this suite.
May  6 15:11:54.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:11:57.272: INFO: namespace custom-resource-definition-222 deletion completed in 8.761707459s

• [SLOW TEST:9.428 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:11:57.272: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6099
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-be8988d3-8f43-44e0-973c-3a07fedfcee2
STEP: Creating a pod to test consume secrets
May  6 15:11:57.421: INFO: Waiting up to 5m0s for pod "pod-secrets-0efd2ba0-9439-46ee-9b86-b167417a5212" in namespace "secrets-6099" to be "success or failure"
May  6 15:11:57.423: INFO: Pod "pod-secrets-0efd2ba0-9439-46ee-9b86-b167417a5212": Phase="Pending", Reason="", readiness=false. Elapsed: 2.131341ms
May  6 15:11:59.426: INFO: Pod "pod-secrets-0efd2ba0-9439-46ee-9b86-b167417a5212": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004689197s
STEP: Saw pod success
May  6 15:11:59.426: INFO: Pod "pod-secrets-0efd2ba0-9439-46ee-9b86-b167417a5212" satisfied condition "success or failure"
May  6 15:11:59.428: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-secrets-0efd2ba0-9439-46ee-9b86-b167417a5212 container secret-volume-test: <nil>
STEP: delete the pod
May  6 15:11:59.447: INFO: Waiting for pod pod-secrets-0efd2ba0-9439-46ee-9b86-b167417a5212 to disappear
May  6 15:11:59.448: INFO: Pod pod-secrets-0efd2ba0-9439-46ee-9b86-b167417a5212 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:11:59.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6099" for this suite.
May  6 15:12:05.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:12:08.211: INFO: namespace secrets-6099 deletion completed in 8.7597373s

• [SLOW TEST:10.939 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:12:08.211: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-27
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May  6 15:12:08.344: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:12:17.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-27" for this suite.
May  6 15:12:23.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:12:26.062: INFO: namespace pods-27 deletion completed in 8.791234549s

• [SLOW TEST:17.851 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:12:26.062: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6122
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
May  6 15:12:36.252: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0506 15:12:36.252806      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:12:36.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6122" for this suite.
May  6 15:12:42.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:12:45.018: INFO: namespace gc-6122 deletion completed in 8.762784937s

• [SLOW TEST:18.956 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:12:45.018: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-187
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May  6 15:12:49.180: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  6 15:12:49.186: INFO: Pod pod-with-prestop-http-hook still exists
May  6 15:12:51.186: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  6 15:12:51.189: INFO: Pod pod-with-prestop-http-hook still exists
May  6 15:12:53.187: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  6 15:12:53.189: INFO: Pod pod-with-prestop-http-hook still exists
May  6 15:12:55.187: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  6 15:12:55.189: INFO: Pod pod-with-prestop-http-hook still exists
May  6 15:12:57.187: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  6 15:12:57.189: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:12:57.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-187" for this suite.
May  6 15:13:09.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:13:11.955: INFO: namespace container-lifecycle-hook-187 deletion completed in 14.759355285s

• [SLOW TEST:26.937 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:13:11.956: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9386
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 15:13:12.456: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 15:13:15.468: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:13:15.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9386" for this suite.
May  6 15:13:27.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:13:30.276: INFO: namespace webhook-9386 deletion completed in 14.773014342s
STEP: Destroying namespace "webhook-9386-markers" for this suite.
May  6 15:13:36.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:13:39.036: INFO: namespace webhook-9386-markers deletion completed in 8.759958535s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.090 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:13:39.046: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5300
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:13:55.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5300" for this suite.
May  6 15:14:01.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:14:03.968: INFO: namespace resourcequota-5300 deletion completed in 8.762311205s

• [SLOW TEST:24.922 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:14:03.968: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-669
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 15:14:04.113: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May  6 15:14:08.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 --namespace=crd-publish-openapi-669 create -f -'
May  6 15:14:08.922: INFO: stderr: ""
May  6 15:14:08.922: INFO: stdout: "e2e-test-crd-publish-openapi-7142-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May  6 15:14:08.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 --namespace=crd-publish-openapi-669 delete e2e-test-crd-publish-openapi-7142-crds test-cr'
May  6 15:14:09.054: INFO: stderr: ""
May  6 15:14:09.054: INFO: stdout: "e2e-test-crd-publish-openapi-7142-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May  6 15:14:09.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 --namespace=crd-publish-openapi-669 apply -f -'
May  6 15:14:09.343: INFO: stderr: ""
May  6 15:14:09.343: INFO: stdout: "e2e-test-crd-publish-openapi-7142-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May  6 15:14:09.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 --namespace=crd-publish-openapi-669 delete e2e-test-crd-publish-openapi-7142-crds test-cr'
May  6 15:14:09.452: INFO: stderr: ""
May  6 15:14:09.452: INFO: stdout: "e2e-test-crd-publish-openapi-7142-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May  6 15:14:09.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 explain e2e-test-crd-publish-openapi-7142-crds'
May  6 15:14:09.675: INFO: stderr: ""
May  6 15:14:09.675: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7142-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:14:13.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-669" for this suite.
May  6 15:14:19.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:14:22.747: INFO: namespace crd-publish-openapi-669 deletion completed in 8.759759347s

• [SLOW TEST:18.779 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:14:22.747: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6759
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6759.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6759.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6759.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6759.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6759.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6759.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  6 15:14:26.919: INFO: DNS probes using dns-6759/dns-test-fcd900c2-cf1e-4a69-a2de-992c0ff80d69 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:14:26.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6759" for this suite.
May  6 15:14:32.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:14:35.706: INFO: namespace dns-6759 deletion completed in 8.773484286s

• [SLOW TEST:12.959 seconds]
[sig-network] DNS
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:14:35.706: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6352
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
May  6 15:14:35.845: INFO: Waiting up to 5m0s for pod "pod-020a1bb0-c3d5-4b5c-8ca5-905c4efa5b7f" in namespace "emptydir-6352" to be "success or failure"
May  6 15:14:35.847: INFO: Pod "pod-020a1bb0-c3d5-4b5c-8ca5-905c4efa5b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.921707ms
May  6 15:14:37.850: INFO: Pod "pod-020a1bb0-c3d5-4b5c-8ca5-905c4efa5b7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004555872s
STEP: Saw pod success
May  6 15:14:37.850: INFO: Pod "pod-020a1bb0-c3d5-4b5c-8ca5-905c4efa5b7f" satisfied condition "success or failure"
May  6 15:14:37.852: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-020a1bb0-c3d5-4b5c-8ca5-905c4efa5b7f container test-container: <nil>
STEP: delete the pod
May  6 15:14:37.871: INFO: Waiting for pod pod-020a1bb0-c3d5-4b5c-8ca5-905c4efa5b7f to disappear
May  6 15:14:37.873: INFO: Pod pod-020a1bb0-c3d5-4b5c-8ca5-905c4efa5b7f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:14:37.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6352" for this suite.
May  6 15:14:43.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:14:46.635: INFO: namespace emptydir-6352 deletion completed in 8.759510717s

• [SLOW TEST:10.929 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:14:46.635: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-156
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
May  6 15:14:46.765: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:14:51.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-156" for this suite.
May  6 15:14:57.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:14:59.934: INFO: namespace init-container-156 deletion completed in 8.760980462s

• [SLOW TEST:13.299 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:14:59.934: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7069
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
May  6 15:15:00.065: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:15:21.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7069" for this suite.
May  6 15:15:27.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:15:29.890: INFO: namespace crd-publish-openapi-7069 deletion completed in 8.760064586s

• [SLOW TEST:29.956 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:15:29.890: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1285
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:15:41.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1285" for this suite.
May  6 15:15:47.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:15:49.819: INFO: namespace resourcequota-1285 deletion completed in 8.75987779s

• [SLOW TEST:19.928 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:15:49.819: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3495
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
May  6 15:15:50.469: INFO: created pod pod-service-account-defaultsa
May  6 15:15:50.469: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May  6 15:15:50.474: INFO: created pod pod-service-account-mountsa
May  6 15:15:50.474: INFO: pod pod-service-account-mountsa service account token volume mount: true
May  6 15:15:50.482: INFO: created pod pod-service-account-nomountsa
May  6 15:15:50.482: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May  6 15:15:50.488: INFO: created pod pod-service-account-defaultsa-mountspec
May  6 15:15:50.488: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May  6 15:15:50.495: INFO: created pod pod-service-account-mountsa-mountspec
May  6 15:15:50.495: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May  6 15:15:50.502: INFO: created pod pod-service-account-nomountsa-mountspec
May  6 15:15:50.502: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May  6 15:15:50.508: INFO: created pod pod-service-account-defaultsa-nomountspec
May  6 15:15:50.508: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May  6 15:15:50.513: INFO: created pod pod-service-account-mountsa-nomountspec
May  6 15:15:50.513: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May  6 15:15:50.519: INFO: created pod pod-service-account-nomountsa-nomountspec
May  6 15:15:50.519: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:15:50.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3495" for this suite.
May  6 15:15:56.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:15:59.285: INFO: namespace svcaccounts-3495 deletion completed in 8.760084013s

• [SLOW TEST:9.466 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:15:59.285: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 15:15:59.443: INFO: Waiting up to 5m0s for pod "downwardapi-volume-082c70e3-1ff2-4b10-8e2b-a095311d5eea" in namespace "projected-8112" to be "success or failure"
May  6 15:15:59.445: INFO: Pod "downwardapi-volume-082c70e3-1ff2-4b10-8e2b-a095311d5eea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.350356ms
May  6 15:16:01.448: INFO: Pod "downwardapi-volume-082c70e3-1ff2-4b10-8e2b-a095311d5eea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00504376s
STEP: Saw pod success
May  6 15:16:01.448: INFO: Pod "downwardapi-volume-082c70e3-1ff2-4b10-8e2b-a095311d5eea" satisfied condition "success or failure"
May  6 15:16:01.450: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod downwardapi-volume-082c70e3-1ff2-4b10-8e2b-a095311d5eea container client-container: <nil>
STEP: delete the pod
May  6 15:16:01.464: INFO: Waiting for pod downwardapi-volume-082c70e3-1ff2-4b10-8e2b-a095311d5eea to disappear
May  6 15:16:01.470: INFO: Pod downwardapi-volume-082c70e3-1ff2-4b10-8e2b-a095311d5eea no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:16:01.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8112" for this suite.
May  6 15:16:07.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:16:10.233: INFO: namespace projected-8112 deletion completed in 8.760028189s

• [SLOW TEST:10.948 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:16:10.233: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5053
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-e64c72f2-5560-4c79-ba78-dc2787cd23e0 in namespace container-probe-5053
May  6 15:16:12.377: INFO: Started pod liveness-e64c72f2-5560-4c79-ba78-dc2787cd23e0 in namespace container-probe-5053
STEP: checking the pod's current state and verifying that restartCount is present
May  6 15:16:12.378: INFO: Initial restart count of pod liveness-e64c72f2-5560-4c79-ba78-dc2787cd23e0 is 0
May  6 15:16:24.396: INFO: Restart count of pod container-probe-5053/liveness-e64c72f2-5560-4c79-ba78-dc2787cd23e0 is now 1 (12.017944616s elapsed)
May  6 15:16:44.424: INFO: Restart count of pod container-probe-5053/liveness-e64c72f2-5560-4c79-ba78-dc2787cd23e0 is now 2 (32.045225776s elapsed)
May  6 15:17:04.450: INFO: Restart count of pod container-probe-5053/liveness-e64c72f2-5560-4c79-ba78-dc2787cd23e0 is now 3 (52.071647183s elapsed)
May  6 15:17:22.474: INFO: Restart count of pod container-probe-5053/liveness-e64c72f2-5560-4c79-ba78-dc2787cd23e0 is now 4 (1m10.09597918s elapsed)
May  6 15:18:34.574: INFO: Restart count of pod container-probe-5053/liveness-e64c72f2-5560-4c79-ba78-dc2787cd23e0 is now 5 (2m22.195313141s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:18:34.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5053" for this suite.
May  6 15:18:40.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:18:43.344: INFO: namespace container-probe-5053 deletion completed in 8.759473852s

• [SLOW TEST:153.111 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:18:43.344: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7355
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:18:48.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7355" for this suite.
May  6 15:19:00.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:19:03.285: INFO: namespace replication-controller-7355 deletion completed in 14.77158078s

• [SLOW TEST:19.940 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:19:03.285: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8662
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-8662
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-8662
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8662
May  6 15:19:03.428: INFO: Found 0 stateful pods, waiting for 1
May  6 15:19:13.431: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May  6 15:19:13.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=statefulset-8662 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 15:19:13.693: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 15:19:13.693: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 15:19:13.693: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 15:19:13.696: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May  6 15:19:23.699: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  6 15:19:23.699: INFO: Waiting for statefulset status.replicas updated to 0
May  6 15:19:23.708: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
May  6 15:19:23.708: INFO: ss-0  ip-10-0-131-80.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:03 +0000 UTC  }]
May  6 15:19:23.708: INFO: 
May  6 15:19:23.708: INFO: StatefulSet ss has not reached scale 3, at 1
May  6 15:19:24.710: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99787037s
May  6 15:19:25.713: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994976199s
May  6 15:19:26.717: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.992005011s
May  6 15:19:27.719: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.988871247s
May  6 15:19:28.723: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.98598018s
May  6 15:19:29.726: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.981964305s
May  6 15:19:30.730: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.979004121s
May  6 15:19:31.733: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.975901435s
May  6 15:19:32.736: INFO: Verifying statefulset ss doesn't scale past 3 for another 972.837677ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8662
May  6 15:19:33.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=statefulset-8662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 15:19:34.000: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  6 15:19:34.000: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 15:19:34.000: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 15:19:34.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=statefulset-8662 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 15:19:34.267: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May  6 15:19:34.267: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 15:19:34.267: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 15:19:34.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=statefulset-8662 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 15:19:34.545: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May  6 15:19:34.545: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 15:19:34.545: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 15:19:34.548: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
May  6 15:19:44.551: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  6 15:19:44.551: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May  6 15:19:44.551: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May  6 15:19:44.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=statefulset-8662 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 15:19:44.812: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 15:19:44.812: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 15:19:44.812: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 15:19:44.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=statefulset-8662 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 15:19:45.083: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 15:19:45.083: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 15:19:45.083: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 15:19:45.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=statefulset-8662 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 15:19:45.385: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 15:19:45.385: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 15:19:45.385: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 15:19:45.385: INFO: Waiting for statefulset status.replicas updated to 0
May  6 15:19:45.388: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
May  6 15:19:55.393: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  6 15:19:55.393: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May  6 15:19:55.393: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May  6 15:19:55.400: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
May  6 15:19:55.400: INFO: ss-0  ip-10-0-131-80.us-west-2.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:03 +0000 UTC  }]
May  6 15:19:55.400: INFO: ss-1  ip-10-0-131-234.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:23 +0000 UTC  }]
May  6 15:19:55.400: INFO: ss-2  ip-10-0-129-108.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:23 +0000 UTC  }]
May  6 15:19:55.400: INFO: 
May  6 15:19:55.400: INFO: StatefulSet ss has not reached scale 0, at 3
May  6 15:19:56.403: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
May  6 15:19:56.403: INFO: ss-0  ip-10-0-131-80.us-west-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:03 +0000 UTC  }]
May  6 15:19:56.403: INFO: ss-1  ip-10-0-131-234.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:23 +0000 UTC  }]
May  6 15:19:56.403: INFO: ss-2  ip-10-0-129-108.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:23 +0000 UTC  }]
May  6 15:19:56.403: INFO: 
May  6 15:19:56.403: INFO: StatefulSet ss has not reached scale 0, at 3
May  6 15:19:57.406: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
May  6 15:19:57.406: INFO: ss-1  ip-10-0-131-234.us-west-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:23 +0000 UTC  }]
May  6 15:19:57.406: INFO: ss-2  ip-10-0-129-108.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 15:19:23 +0000 UTC  }]
May  6 15:19:57.406: INFO: 
May  6 15:19:57.406: INFO: StatefulSet ss has not reached scale 0, at 2
May  6 15:19:58.408: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.99177136s
May  6 15:19:59.411: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.989006946s
May  6 15:20:00.414: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.986146799s
May  6 15:20:01.417: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.983387456s
May  6 15:20:02.420: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.980512196s
May  6 15:20:03.422: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.977818948s
May  6 15:20:04.425: INFO: Verifying statefulset ss doesn't scale past 0 for another 975.331335ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8662
May  6 15:20:05.428: INFO: Scaling statefulset ss to 0
May  6 15:20:05.434: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May  6 15:20:05.436: INFO: Deleting all statefulset in ns statefulset-8662
May  6 15:20:05.437: INFO: Scaling statefulset ss to 0
May  6 15:20:05.443: INFO: Waiting for statefulset status.replicas updated to 0
May  6 15:20:05.444: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:20:05.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8662" for this suite.
May  6 15:20:11.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:20:14.215: INFO: namespace statefulset-8662 deletion completed in 8.759146339s

• [SLOW TEST:70.930 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:20:14.215: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3021
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 15:20:14.362: INFO: (0) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 12.981689ms)
May  6 15:20:14.365: INFO: (1) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 3.07018ms)
May  6 15:20:14.368: INFO: (2) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.695975ms)
May  6 15:20:14.370: INFO: (3) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.3311ms)
May  6 15:20:14.373: INFO: (4) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.341598ms)
May  6 15:20:14.375: INFO: (5) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.34764ms)
May  6 15:20:14.377: INFO: (6) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.228053ms)
May  6 15:20:14.380: INFO: (7) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.626019ms)
May  6 15:20:14.382: INFO: (8) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.270977ms)
May  6 15:20:14.385: INFO: (9) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.515911ms)
May  6 15:20:14.387: INFO: (10) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.36967ms)
May  6 15:20:14.389: INFO: (11) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.301741ms)
May  6 15:20:14.392: INFO: (12) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.32021ms)
May  6 15:20:14.394: INFO: (13) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.296088ms)
May  6 15:20:14.396: INFO: (14) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.259021ms)
May  6 15:20:14.399: INFO: (15) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.44458ms)
May  6 15:20:14.401: INFO: (16) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.381633ms)
May  6 15:20:14.404: INFO: (17) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.542302ms)
May  6 15:20:14.406: INFO: (18) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.649609ms)
May  6 15:20:14.409: INFO: (19) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.500198ms)
[AfterEach] version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:20:14.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3021" for this suite.
May  6 15:20:20.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:20:20.595: INFO: namespace proxy-3021 deletion completed in 6.182590101s

• [SLOW TEST:6.380 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:20:20.595: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5693
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May  6 15:20:20.733: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  6 15:20:20.742: INFO: Waiting for terminating namespaces to be deleted...
May  6 15:20:20.743: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-129-1.us-west-2.compute.internal before test
May  6 15:20:20.751: INFO: coredns-5644d7b6d9-5jf9p from kube-system started at 2020-05-06 10:56:00 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.751: INFO: 	Container coredns ready: true, restart count 0
May  6 15:20:20.751: INFO: kommander-kubeaddons-kubeaddons-catalog-6654f856df-ccmkl from kommander started at 2020-05-06 11:00:36 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.751: INFO: 	Container kubeaddons-catalog ready: true, restart count 0
May  6 15:20:20.751: INFO: kcl-utility-apiserver-5d448f665b-gf6gv from kommander started at 2020-05-06 11:00:36 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.751: INFO: 	Container server ready: true, restart count 0
May  6 15:20:20.751: INFO: elasticsearch-kubeaddons-data-1 from kubeaddons started at 2020-05-06 11:01:19 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.751: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 15:20:20.751: INFO: reloader-kubeaddons-reloader-7d4bd64cfb-xnxz8 from kubeaddons started at 2020-05-06 10:56:23 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.751: INFO: 	Container reloader-kubeaddons-reloader ready: true, restart count 0
May  6 15:20:20.751: INFO: kubefed-controller-manager-55fd5b4dff-bvsl6 from kommander started at 2020-05-06 11:00:46 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.751: INFO: 	Container controller-manager ready: true, restart count 0
May  6 15:20:20.751: INFO: prometheus-kubeaddons-grafana-99b49bd54-8h7z8 from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (2 container statuses recorded)
May  6 15:20:20.751: INFO: 	Container grafana ready: true, restart count 0
May  6 15:20:20.751: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
May  6 15:20:20.751: INFO: minio-3 from velero started at 2020-05-06 10:59:47 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.751: INFO: 	Container minio ready: true, restart count 0
May  6 15:20:20.751: INFO: sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-9wkcg from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 15:20:20.751: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May  6 15:20:20.751: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 15:20:20.751: INFO: fluentbit-kubeaddons-fluent-bit-5rbrr from kubeaddons started at 2020-05-06 11:02:11 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.751: INFO: 	Container fluent-bit ready: true, restart count 0
May  6 15:20:20.751: INFO: kcl-cm-6588c896f9-4b2gk from kommander started at 2020-05-06 11:00:36 +0000 UTC (2 container statuses recorded)
May  6 15:20:20.751: INFO: 	Container controller-manager ready: true, restart count 0
May  6 15:20:20.751: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  6 15:20:20.751: INFO: kube-proxy-v9c2p from kube-system started at 2020-05-06 10:55:20 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.751: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 15:20:20.751: INFO: calico-node-wpbh7 from kube-system started at 2020-05-06 10:55:38 +0000 UTC (2 container statuses recorded)
May  6 15:20:20.751: INFO: 	Container bird-metrics ready: true, restart count 0
May  6 15:20:20.751: INFO: 	Container calico-node ready: true, restart count 0
May  6 15:20:20.751: INFO: cert-manager-kubeaddons-webhook-77fbc6d59b-fbbjd from cert-manager started at 2020-05-06 10:56:59 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.751: INFO: 	Container cert-manager ready: true, restart count 1
May  6 15:20:20.751: INFO: cert-manager-kubeaddons-7d7f98fbc6-qpl75 from cert-manager started at 2020-05-06 10:56:59 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.751: INFO: 	Container cert-manager ready: true, restart count 0
May  6 15:20:20.751: INFO: ebs-csi-node-f2cbn from kube-system started at 2020-05-06 10:57:49 +0000 UTC (3 container statuses recorded)
May  6 15:20:20.751: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 15:20:20.751: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 15:20:20.751: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  6 15:20:20.751: INFO: traefik-kubeaddons-6f97669977-2sxh8 from kubeaddons started at 2020-05-06 10:58:20 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.752: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
May  6 15:20:20.752: INFO: prometheus-kubeaddons-prometheus-node-exporter-b6z8v from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.752: INFO: 	Container node-exporter ready: true, restart count 0
May  6 15:20:20.752: INFO: elasticsearch-kubeaddons-master-1 from kubeaddons started at 2020-05-06 10:59:19 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.752: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 15:20:20.752: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-129-108.us-west-2.compute.internal before test
May  6 15:20:20.767: INFO: kubeaddons-controller-manager-77cf76b857-k89ml from kubeaddons started at 2020-05-06 10:55:49 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container manager ready: true, restart count 0
May  6 15:20:20.767: INFO: ebs-csi-node-vvbxt from kube-system started at 2020-05-06 10:57:49 +0000 UTC (3 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 15:20:20.767: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 15:20:20.767: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  6 15:20:20.767: INFO: cert-manager-kubeaddons-cainjector-6dcd94769b-ng6xl from cert-manager started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container cainjector ready: true, restart count 0
May  6 15:20:20.767: INFO: elasticsearch-kubeaddons-master-2 from kubeaddons started at 2020-05-06 11:00:07 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 15:20:20.767: INFO: kommander-kubeaddons-grafana-645f957f8f-96sf5 from kommander started at 2020-05-06 11:00:36 +0000 UTC (2 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container grafana ready: true, restart count 0
May  6 15:20:20.767: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
May  6 15:20:20.767: INFO: kubefed-admission-webhook-5fb847574f-6mjm2 from kommander started at 2020-05-06 11:00:41 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container admission-webhook ready: true, restart count 0
May  6 15:20:20.767: INFO: fluentbit-kubeaddons-fluent-bit-t6xpz from kubeaddons started at 2020-05-06 11:02:11 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container fluent-bit ready: true, restart count 0
May  6 15:20:20.767: INFO: elasticsearch-kubeaddons-client-d4f5db58d-7qrn8 from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 15:20:20.767: INFO: calico-node-z679x from kube-system started at 2020-05-06 10:55:38 +0000 UTC (2 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container bird-metrics ready: true, restart count 0
May  6 15:20:20.767: INFO: 	Container calico-node ready: true, restart count 0
May  6 15:20:20.767: INFO: prometheus-kubeaddons-prometheus-node-exporter-2wrg9 from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container node-exporter ready: true, restart count 0
May  6 15:20:20.767: INFO: kommander-kubeaddons-karma-7688944fcf-hpnzl from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container karma ready: true, restart count 0
May  6 15:20:20.767: INFO: external-dns-kubeaddons-6c65ff8d88-sm9p2 from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container external-dns ready: true, restart count 0
May  6 15:20:20.767: INFO: calico-kube-controllers-84f666455-5t7xm from kube-system started at 2020-05-06 10:55:49 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  6 15:20:20.767: INFO: minio-2 from velero started at 2020-05-06 10:59:44 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container minio ready: true, restart count 0
May  6 15:20:20.767: INFO: traefik-kubeaddons-6f97669977-9fwr9 from kubeaddons started at 2020-05-06 10:57:49 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
May  6 15:20:20.767: INFO: elasticsearch-kubeaddons-data-0 from kubeaddons started at 2020-05-06 10:58:15 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 15:20:20.767: INFO: elasticsearchexporter-kubeaddons-elasticsearch-exporter-845tdq8 from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container elasticsearch-exporter ready: true, restart count 0
May  6 15:20:20.767: INFO: kcl-webhook-fb4dfc7d7-vv757 from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container webhook ready: true, restart count 0
May  6 15:20:20.767: INFO: kube-proxy-4lqz5 from kube-system started at 2020-05-06 10:55:19 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 15:20:20.767: INFO: coredns-5644d7b6d9-hmtfs from kube-system started at 2020-05-06 10:56:00 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container coredns ready: true, restart count 0
May  6 15:20:20.767: INFO: sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-zqwx9 from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May  6 15:20:20.767: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 15:20:20.767: INFO: kcl-tfcb-9c88c7bc9-lbgz6 from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container server ready: true, restart count 0
May  6 15:20:20.767: INFO: kubernetes-dashboard-549989bcdf-2d767 from kubeaddons started at 2020-05-06 10:56:34 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
May  6 15:20:20.767: INFO: kibana-kubeaddons-649897648c-mzc2h from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (2 container statuses recorded)
May  6 15:20:20.767: INFO: 	Container initialize-kibana-index ready: true, restart count 0
May  6 15:20:20.767: INFO: 	Container kibana ready: true, restart count 0
May  6 15:20:20.767: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-131-234.us-west-2.compute.internal before test
May  6 15:20:20.781: INFO: traefik-kubeaddons-1.72.17-5pttd from kubeaddons started at 2020-05-06 10:57:38 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container traefik ready: false, restart count 0
May  6 15:20:20.781: INFO: prometheus-kubeaddons-kube-state-metrics-78c65cc7bc-66zk8 from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container kube-state-metrics ready: true, restart count 0
May  6 15:20:20.781: INFO: kube-oidc-proxy-kubeaddons-6fbd5c8fc4-xtmsg from kubeaddons started at 2020-05-06 10:59:18 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container kube-oidc-proxy ready: true, restart count 0
May  6 15:20:20.781: INFO: traefik-forward-auth-kubeaddons-8bdddfcd-wdrjp from kubeaddons started at 2020-05-06 10:59:51 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container traefik-forward-auth ready: true, restart count 0
May  6 15:20:20.781: INFO: alertmanager-prometheus-kubeaddons-prom-alertmanager-0 from kubeaddons started at 2020-05-06 10:59:03 +0000 UTC (2 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container alertmanager ready: true, restart count 0
May  6 15:20:20.781: INFO: 	Container config-reloader ready: true, restart count 0
May  6 15:20:20.781: INFO: tiller-deploy-969865475-m2ph8 from kube-system started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container tiller ready: true, restart count 0
May  6 15:20:20.781: INFO: prometheusadapter-kubeaddons-prometheus-adapter-6b8975fc48pwzsr from kubeaddons started at 2020-05-06 10:59:55 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container prometheus-adapter ready: true, restart count 0
May  6 15:20:20.781: INFO: dstorageclass-controller-manager-5c966c767f-4bf6x from kubeaddons started at 2020-05-06 10:57:33 +0000 UTC (2 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  6 15:20:20.781: INFO: 	Container manager ready: true, restart count 0
May  6 15:20:20.781: INFO: opsportal-kubeaddons-kommander-ui-679db5d9cb-62psh from kubeaddons started at 2020-05-06 10:56:45 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container opsportal-kubeaddons-kommander-ui ready: true, restart count 0
May  6 15:20:20.781: INFO: dex-kubeaddons-79b77778bc-84p4b from kubeaddons started at 2020-05-06 10:59:20 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container main ready: true, restart count 1
May  6 15:20:20.781: INFO: dex-k8s-authenticator-kubeaddons-6697855d4c-xbqks from kubeaddons started at 2020-05-06 10:59:29 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container dex-k8s-authenticator ready: true, restart count 0
May  6 15:20:20.781: INFO: fluentbit-kubeaddons-fluent-bit-wnqvd from kubeaddons started at 2020-05-06 11:02:11 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container fluent-bit ready: true, restart count 0
May  6 15:20:20.781: INFO: opsportal-landing-6f6865b688-vb67f from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container opsportal-landing ready: true, restart count 0
May  6 15:20:20.781: INFO: kommander-kubeaddons-kommander-ui-7b9b585d95-rgnz6 from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container kommander-kubeaddons-kommander-ui ready: true, restart count 0
May  6 15:20:20.781: INFO: ebs-csi-node-ffszp from kube-system started at 2020-05-06 10:57:49 +0000 UTC (3 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 15:20:20.781: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 15:20:20.781: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  6 15:20:20.781: INFO: prometheus-kubeaddons-prom-operator-cc56c6c-m8kfx from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (2 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container prometheus-operator ready: true, restart count 0
May  6 15:20:20.781: INFO: 	Container tls-proxy ready: true, restart count 0
May  6 15:20:20.781: INFO: prometheus-kubeaddons-prometheus-node-exporter-4jtfq from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container node-exporter ready: true, restart count 0
May  6 15:20:20.781: INFO: velero-kubeaddons-578b4667ff-6x2nw from velero started at 2020-05-06 10:59:40 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container velero ready: true, restart count 4
May  6 15:20:20.781: INFO: kommander-kubeaddons-thanos-query-7597bf4957-tmdgd from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container thanos-query ready: true, restart count 0
May  6 15:20:20.781: INFO: kube-proxy-ptcmp from kube-system started at 2020-05-06 10:55:19 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 15:20:20.781: INFO: calico-node-gm4h2 from kube-system started at 2020-05-06 10:55:38 +0000 UTC (2 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container bird-metrics ready: true, restart count 0
May  6 15:20:20.781: INFO: 	Container calico-node ready: true, restart count 0
May  6 15:20:20.781: INFO: elasticsearch-kubeaddons-client-d4f5db58d-7g4sd from kubeaddons started at 2020-05-06 10:57:52 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 15:20:20.781: INFO: minio-0 from velero started at 2020-05-06 10:59:44 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container minio ready: true, restart count 0
May  6 15:20:20.781: INFO: gatekeeper-kubeaddons-fdc87db85-5rj6d from kubeaddons started at 2020-05-06 10:57:40 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container manager ready: true, restart count 0
May  6 15:20:20.781: INFO: sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-hqgql from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May  6 15:20:20.781: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 15:20:20.781: INFO: dex-kubeaddons-dex-controller-6b6c9fbd7f-pxz8h from kubeaddons started at 2020-05-06 10:58:46 +0000 UTC (2 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  6 15:20:20.781: INFO: 	Container manager ready: true, restart count 0
May  6 15:20:20.781: INFO: kubefed-controller-manager-55fd5b4dff-g9m6m from kommander started at 2020-05-06 11:00:41 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.781: INFO: 	Container controller-manager ready: true, restart count 0
May  6 15:20:20.781: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-131-80.us-west-2.compute.internal before test
May  6 15:20:20.793: INFO: minio-1 from velero started at 2020-05-06 13:31:21 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.793: INFO: 	Container minio ready: false, restart count 0
May  6 15:20:20.793: INFO: prometheus-kubeaddons-prometheus-node-exporter-2qwk2 from kubeaddons started at 2020-05-06 13:32:37 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.793: INFO: 	Container node-exporter ready: false, restart count 0
May  6 15:20:20.793: INFO: sonobuoy from sonobuoy started at 2020-05-06 13:56:26 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.793: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  6 15:20:20.793: INFO: sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-cvh6w from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 15:20:20.793: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May  6 15:20:20.793: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 15:20:20.793: INFO: calico-node-mrkdh from kube-system started at 2020-05-06 10:55:38 +0000 UTC (2 container statuses recorded)
May  6 15:20:20.793: INFO: 	Container bird-metrics ready: true, restart count 0
May  6 15:20:20.793: INFO: 	Container calico-node ready: true, restart count 0
May  6 15:20:20.793: INFO: sonobuoy-e2e-job-7496ec37786a4f0c from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 15:20:20.793: INFO: 	Container e2e ready: true, restart count 0
May  6 15:20:20.793: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 15:20:20.793: INFO: prometheus-prometheus-kubeaddons-prom-prometheus-0 from kubeaddons started at 2020-05-06 13:31:17 +0000 UTC (4 container statuses recorded)
May  6 15:20:20.793: INFO: 	Container prometheus ready: false, restart count 0
May  6 15:20:20.793: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May  6 15:20:20.793: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May  6 15:20:20.793: INFO: 	Container thanos-sidecar ready: true, restart count 0
May  6 15:20:20.793: INFO: ebs-csi-node-722lq from kube-system started at 2020-05-06 10:57:49 +0000 UTC (3 container statuses recorded)
May  6 15:20:20.793: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 15:20:20.793: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 15:20:20.793: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  6 15:20:20.793: INFO: kube-proxy-vmxhd from kube-system started at 2020-05-06 10:55:19 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.793: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 15:20:20.793: INFO: elasticsearch-kubeaddons-master-0 from kubeaddons started at 2020-05-06 13:31:19 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.793: INFO: 	Container elasticsearch ready: false, restart count 0
May  6 15:20:20.793: INFO: fluentbit-kubeaddons-fluent-bit-w966k from kubeaddons started at 2020-05-06 13:31:20 +0000 UTC (1 container statuses recorded)
May  6 15:20:20.793: INFO: 	Container fluent-bit ready: true, restart count 0
May  6 15:20:20.793: INFO: ebs-csi-controller-0 from kube-system started at 2020-05-06 13:31:16 +0000 UTC (5 container statuses recorded)
May  6 15:20:20.793: INFO: 	Container csi-attacher ready: true, restart count 0
May  6 15:20:20.793: INFO: 	Container csi-provisioner ready: true, restart count 0
May  6 15:20:20.793: INFO: 	Container csi-snapshotter ready: true, restart count 0
May  6 15:20:20.793: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 15:20:20.793: INFO: 	Container liveness-probe ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-4574097c-9e34-47b4-a6e4-d7253b568e6c 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-4574097c-9e34-47b4-a6e4-d7253b568e6c off the node ip-10-0-131-80.us-west-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-4574097c-9e34-47b4-a6e4-d7253b568e6c
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:20:28.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5693" for this suite.
May  6 15:20:50.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:20:53.629: INFO: namespace sched-pred-5693 deletion completed in 24.759457245s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:33.035 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:20:53.629: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-507
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 15:20:53.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 create -f - --namespace=kubectl-507'
May  6 15:20:54.023: INFO: stderr: ""
May  6 15:20:54.023: INFO: stdout: "replicationcontroller/redis-master created\n"
May  6 15:20:54.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 create -f - --namespace=kubectl-507'
May  6 15:20:54.286: INFO: stderr: ""
May  6 15:20:54.286: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
May  6 15:20:55.289: INFO: Selector matched 1 pods for map[app:redis]
May  6 15:20:55.289: INFO: Found 0 / 1
May  6 15:20:56.289: INFO: Selector matched 1 pods for map[app:redis]
May  6 15:20:56.289: INFO: Found 1 / 1
May  6 15:20:56.289: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May  6 15:20:56.292: INFO: Selector matched 1 pods for map[app:redis]
May  6 15:20:56.292: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  6 15:20:56.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 describe pod redis-master-p6tkh --namespace=kubectl-507'
May  6 15:20:56.406: INFO: stderr: ""
May  6 15:20:56.406: INFO: stdout: "Name:         redis-master-p6tkh\nNamespace:    kubectl-507\nPriority:     0\nNode:         ip-10-0-131-80.us-west-2.compute.internal/10.0.131.80\nStart Time:   Wed, 06 May 2020 15:20:54 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 192.168.85.77/32\n              cni.projectcalico.org/podIPs: 192.168.85.77/32\nStatus:       Running\nIP:           192.168.85.77\nIPs:\n  IP:           192.168.85.77\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   containerd://4ce0f38fc5564cc6558a91a6684dbde9a22213b3413d70ad52fead49cb08730c\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 06 May 2020 15:20:55 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-42gmb (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-42gmb:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-42gmb\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                                                Message\n  ----    ------     ----       ----                                                -------\n  Normal  Scheduled  <unknown>  default-scheduler                                   Successfully assigned kubectl-507/redis-master-p6tkh to ip-10-0-131-80.us-west-2.compute.internal\n  Normal  Pulling    2s         kubelet, ip-10-0-131-80.us-west-2.compute.internal  Pulling image \"docker.io/library/redis:5.0.5-alpine\"\n  Normal  Pulled     1s         kubelet, ip-10-0-131-80.us-west-2.compute.internal  Successfully pulled image \"docker.io/library/redis:5.0.5-alpine\"\n  Normal  Created    1s         kubelet, ip-10-0-131-80.us-west-2.compute.internal  Created container redis-master\n  Normal  Started    1s         kubelet, ip-10-0-131-80.us-west-2.compute.internal  Started container redis-master\n"
May  6 15:20:56.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 describe rc redis-master --namespace=kubectl-507'
May  6 15:20:56.521: INFO: stderr: ""
May  6 15:20:56.521: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-507\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-p6tkh\n"
May  6 15:20:56.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 describe service redis-master --namespace=kubectl-507'
May  6 15:20:56.633: INFO: stderr: ""
May  6 15:20:56.633: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-507\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.0.62.26\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         192.168.85.77:6379\nSession Affinity:  None\nEvents:            <none>\n"
May  6 15:20:56.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 describe node ip-10-0-129-1.us-west-2.compute.internal'
May  6 15:20:56.759: INFO: stderr: ""
May  6 15:20:56.759: INFO: stdout: "Name:               ip-10-0-129-1.us-west-2.compute.internal\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m5.2xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-west-2\n                    failure-domain.beta.kubernetes.io/zone=us-west-2c\n                    konvoy.mesosphere.com/inventory_hostname=10.0.129.1\n                    konvoy.mesosphere.com/node_pool=worker\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-129-1.us-west-2.compute.internal\n                    kubernetes.io/os=linux\n                    topology.ebs.csi.aws.com/zone=us-west-2c\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-0acfc24a40e935a24\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.129.1/32\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.121.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 06 May 2020 10:55:20 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 06 May 2020 10:55:53 +0000   Wed, 06 May 2020 10:55:53 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 06 May 2020 15:20:18 +0000   Wed, 06 May 2020 10:55:20 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 06 May 2020 15:20:18 +0000   Wed, 06 May 2020 10:55:20 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 06 May 2020 15:20:18 +0000   Wed, 06 May 2020 10:55:20 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 06 May 2020 15:20:18 +0000   Wed, 06 May 2020 10:55:50 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.0.129.1\n  ExternalIP:   34.211.120.98\n  Hostname:     ip-10-0-129-1.us-west-2.compute.internal\n  InternalDNS:  ip-10-0-129-1.us-west-2.compute.internal\n  ExternalDNS:  ec2-34-211-120-98.us-west-2.compute.amazonaws.com\nCapacity:\n attachable-volumes-aws-ebs:  25\n cpu:                         8\n ephemeral-storage:           83874796Ki\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      31960748Ki\n pods:                        110\nAllocatable:\n attachable-volumes-aws-ebs:  25\n cpu:                         8\n ephemeral-storage:           77299011866\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      31858348Ki\n pods:                        110\nSystem Info:\n Machine ID:                 05cb8c7b39fe0f70e3ce97e5beab809d\n System UUID:                EC2A9D86-F48C-0805-E63D-46B960D50CD8\n Boot ID:                    1eb2abe1-74f6-4f4d-bded-d453784247f7\n Kernel Version:             3.10.0-957.1.3.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  containerd://1.2.6\n Kubelet Version:            v1.16.8\n Kube-Proxy Version:         v1.16.8\nProviderID:                  aws:///us-west-2c/i-0acfc24a40e935a24\nNon-terminated Pods:         (19 in total)\n  Namespace                  Name                                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                        ------------  ----------  ---------------  -------------  ---\n  cert-manager               cert-manager-kubeaddons-7d7f98fbc6-qpl75                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h23m\n  cert-manager               cert-manager-kubeaddons-webhook-77fbc6d59b-fbbjd            0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h23m\n  kommander                  kcl-cm-6588c896f9-4b2gk                                     500m (6%)     500m (6%)   128Mi (0%)       256Mi (0%)     4h20m\n  kommander                  kcl-utility-apiserver-5d448f665b-gf6gv                      100m (1%)     200m (2%)   100Mi (0%)       200Mi (0%)     4h20m\n  kommander                  kommander-kubeaddons-kubeaddons-catalog-6654f856df-ccmkl    100m (1%)     400m (5%)   20Mi (0%)        500Mi (1%)     4h20m\n  kommander                  kubefed-controller-manager-55fd5b4dff-bvsl6                 100m (1%)     500m (6%)   64Mi (0%)        128Mi (0%)     4h20m\n  kube-system                calico-node-wpbh7                                           300m (3%)     0 (0%)      32M (0%)         0 (0%)         4h25m\n  kube-system                coredns-5644d7b6d9-5jf9p                                    100m (1%)     0 (0%)      70Mi (0%)        170Mi (0%)     4h28m\n  kube-system                ebs-csi-node-f2cbn                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h23m\n  kube-system                kube-proxy-v9c2p                                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h25m\n  kubeaddons                 elasticsearch-kubeaddons-data-1                             1 (12%)       2 (25%)     4608Mi (14%)     8Gi (26%)      4h19m\n  kubeaddons                 elasticsearch-kubeaddons-master-1                           500m (6%)     1 (12%)     1536Mi (4%)      2Gi (6%)       4h21m\n  kubeaddons                 fluentbit-kubeaddons-fluent-bit-5rbrr                       200m (2%)     0 (0%)      200Mi (0%)       750Mi (2%)     4h18m\n  kubeaddons                 prometheus-kubeaddons-grafana-99b49bd54-8h7z8               200m (2%)     300m (3%)   100Mi (0%)       100Mi (0%)     4h22m\n  kubeaddons                 prometheus-kubeaddons-prometheus-node-exporter-b6z8v        0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h22m\n  kubeaddons                 reloader-kubeaddons-reloader-7d4bd64cfb-xnxz8               100m (1%)     100m (1%)   128Mi (0%)       512Mi (1%)     4h24m\n  kubeaddons                 traefik-kubeaddons-6f97669977-2sxh8                         500m (6%)     1 (12%)     0 (0%)           0 (0%)         4h22m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-9wkcg     0 (0%)        0 (0%)      0 (0%)           0 (0%)         84m\n  velero                     minio-3                                                     250m (3%)     750m (9%)   256Mi (0%)       512Mi (1%)     4h21m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests         Limits\n  --------                    --------         ------\n  cpu                         3950m (49%)      6750m (84%)\n  memory                      7414290Ki (23%)  13368Mi (42%)\n  ephemeral-storage           0 (0%)           0 (0%)\n  attachable-volumes-aws-ebs  0                0\nEvents:                       <none>\n"
May  6 15:20:56.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 describe namespace kubectl-507'
May  6 15:20:56.869: INFO: stderr: ""
May  6 15:20:56.869: INFO: stdout: "Name:         kubectl-507\nLabels:       e2e-framework=kubectl\n              e2e-run=60d39153-42a8-47ca-8924-4d01d74f713f\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:20:56.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-507" for this suite.
May  6 15:21:08.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:21:11.633: INFO: namespace kubectl-507 deletion completed in 14.761125135s

• [SLOW TEST:18.004 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:21:11.633: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4067
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May  6 15:21:14.791: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:21:14.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4067" for this suite.
May  6 15:21:20.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:21:23.563: INFO: namespace container-runtime-4067 deletion completed in 8.759528121s

• [SLOW TEST:11.930 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:21:23.563: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4455
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-975f762e-dc0b-41c5-b0f2-cd19800a7ee9
STEP: Creating a pod to test consume secrets
May  6 15:21:23.703: INFO: Waiting up to 5m0s for pod "pod-secrets-4b85081d-cb02-4b29-9176-3988cb4ae577" in namespace "secrets-4455" to be "success or failure"
May  6 15:21:23.705: INFO: Pod "pod-secrets-4b85081d-cb02-4b29-9176-3988cb4ae577": Phase="Pending", Reason="", readiness=false. Elapsed: 1.670478ms
May  6 15:21:25.708: INFO: Pod "pod-secrets-4b85081d-cb02-4b29-9176-3988cb4ae577": Phase="Running", Reason="", readiness=true. Elapsed: 2.004322626s
May  6 15:21:27.710: INFO: Pod "pod-secrets-4b85081d-cb02-4b29-9176-3988cb4ae577": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006555945s
STEP: Saw pod success
May  6 15:21:27.710: INFO: Pod "pod-secrets-4b85081d-cb02-4b29-9176-3988cb4ae577" satisfied condition "success or failure"
May  6 15:21:27.712: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-secrets-4b85081d-cb02-4b29-9176-3988cb4ae577 container secret-volume-test: <nil>
STEP: delete the pod
May  6 15:21:27.724: INFO: Waiting for pod pod-secrets-4b85081d-cb02-4b29-9176-3988cb4ae577 to disappear
May  6 15:21:27.726: INFO: Pod pod-secrets-4b85081d-cb02-4b29-9176-3988cb4ae577 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:21:27.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4455" for this suite.
May  6 15:21:33.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:21:36.487: INFO: namespace secrets-4455 deletion completed in 8.758984337s

• [SLOW TEST:12.924 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:21:36.487: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7480
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:21:36.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7480" for this suite.
May  6 15:21:48.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:21:51.393: INFO: namespace pods-7480 deletion completed in 14.760509936s

• [SLOW TEST:14.906 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:21:51.393: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9782
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 15:21:53.549: INFO: Waiting up to 5m0s for pod "client-envvars-f361c24a-b19b-4af9-a8ca-f2b755fee955" in namespace "pods-9782" to be "success or failure"
May  6 15:21:53.551: INFO: Pod "client-envvars-f361c24a-b19b-4af9-a8ca-f2b755fee955": Phase="Pending", Reason="", readiness=false. Elapsed: 1.989897ms
May  6 15:21:55.554: INFO: Pod "client-envvars-f361c24a-b19b-4af9-a8ca-f2b755fee955": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004484682s
STEP: Saw pod success
May  6 15:21:55.554: INFO: Pod "client-envvars-f361c24a-b19b-4af9-a8ca-f2b755fee955" satisfied condition "success or failure"
May  6 15:21:55.556: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod client-envvars-f361c24a-b19b-4af9-a8ca-f2b755fee955 container env3cont: <nil>
STEP: delete the pod
May  6 15:21:55.568: INFO: Waiting for pod client-envvars-f361c24a-b19b-4af9-a8ca-f2b755fee955 to disappear
May  6 15:21:55.570: INFO: Pod client-envvars-f361c24a-b19b-4af9-a8ca-f2b755fee955 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:21:55.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9782" for this suite.
May  6 15:22:23.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:22:26.333: INFO: namespace pods-9782 deletion completed in 30.759768045s

• [SLOW TEST:34.940 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:22:26.333: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8151
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
May  6 15:22:26.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 create -f - --namespace=kubectl-8151'
May  6 15:22:26.730: INFO: stderr: ""
May  6 15:22:26.730: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  6 15:22:26.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8151'
May  6 15:22:26.833: INFO: stderr: ""
May  6 15:22:26.833: INFO: stdout: "update-demo-nautilus-bm256 update-demo-nautilus-wngr6 "
May  6 15:22:26.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-nautilus-bm256 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8151'
May  6 15:22:26.938: INFO: stderr: ""
May  6 15:22:26.938: INFO: stdout: ""
May  6 15:22:26.938: INFO: update-demo-nautilus-bm256 is created but not running
May  6 15:22:31.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8151'
May  6 15:22:32.046: INFO: stderr: ""
May  6 15:22:32.046: INFO: stdout: "update-demo-nautilus-bm256 update-demo-nautilus-wngr6 "
May  6 15:22:32.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-nautilus-bm256 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8151'
May  6 15:22:32.148: INFO: stderr: ""
May  6 15:22:32.148: INFO: stdout: "true"
May  6 15:22:32.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-nautilus-bm256 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8151'
May  6 15:22:32.251: INFO: stderr: ""
May  6 15:22:32.251: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  6 15:22:32.251: INFO: validating pod update-demo-nautilus-bm256
May  6 15:22:32.254: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 15:22:32.254: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 15:22:32.254: INFO: update-demo-nautilus-bm256 is verified up and running
May  6 15:22:32.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-nautilus-wngr6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8151'
May  6 15:22:32.358: INFO: stderr: ""
May  6 15:22:32.358: INFO: stdout: "true"
May  6 15:22:32.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-nautilus-wngr6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8151'
May  6 15:22:32.464: INFO: stderr: ""
May  6 15:22:32.464: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  6 15:22:32.464: INFO: validating pod update-demo-nautilus-wngr6
May  6 15:22:32.467: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 15:22:32.467: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 15:22:32.467: INFO: update-demo-nautilus-wngr6 is verified up and running
STEP: scaling down the replication controller
May  6 15:22:32.469: INFO: scanned /root for discovery docs: <nil>
May  6 15:22:32.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-8151'
May  6 15:22:32.624: INFO: stderr: ""
May  6 15:22:32.624: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  6 15:22:32.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8151'
May  6 15:22:32.734: INFO: stderr: ""
May  6 15:22:32.734: INFO: stdout: "update-demo-nautilus-bm256 update-demo-nautilus-wngr6 "
STEP: Replicas for name=update-demo: expected=1 actual=2
May  6 15:22:37.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8151'
May  6 15:22:37.840: INFO: stderr: ""
May  6 15:22:37.840: INFO: stdout: "update-demo-nautilus-wngr6 "
May  6 15:22:37.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-nautilus-wngr6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8151'
May  6 15:22:37.943: INFO: stderr: ""
May  6 15:22:37.943: INFO: stdout: "true"
May  6 15:22:37.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-nautilus-wngr6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8151'
May  6 15:22:38.047: INFO: stderr: ""
May  6 15:22:38.047: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  6 15:22:38.047: INFO: validating pod update-demo-nautilus-wngr6
May  6 15:22:38.050: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 15:22:38.050: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 15:22:38.050: INFO: update-demo-nautilus-wngr6 is verified up and running
STEP: scaling up the replication controller
May  6 15:22:38.054: INFO: scanned /root for discovery docs: <nil>
May  6 15:22:38.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-8151'
May  6 15:22:38.205: INFO: stderr: ""
May  6 15:22:38.205: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  6 15:22:38.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8151'
May  6 15:22:38.310: INFO: stderr: ""
May  6 15:22:38.310: INFO: stdout: "update-demo-nautilus-56gvs update-demo-nautilus-wngr6 "
May  6 15:22:38.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-nautilus-56gvs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8151'
May  6 15:22:38.414: INFO: stderr: ""
May  6 15:22:38.414: INFO: stdout: ""
May  6 15:22:38.414: INFO: update-demo-nautilus-56gvs is created but not running
May  6 15:22:43.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8151'
May  6 15:22:43.520: INFO: stderr: ""
May  6 15:22:43.520: INFO: stdout: "update-demo-nautilus-56gvs update-demo-nautilus-wngr6 "
May  6 15:22:43.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-nautilus-56gvs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8151'
May  6 15:22:43.621: INFO: stderr: ""
May  6 15:22:43.621: INFO: stdout: "true"
May  6 15:22:43.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-nautilus-56gvs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8151'
May  6 15:22:43.725: INFO: stderr: ""
May  6 15:22:43.725: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  6 15:22:43.725: INFO: validating pod update-demo-nautilus-56gvs
May  6 15:22:43.729: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 15:22:43.729: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 15:22:43.729: INFO: update-demo-nautilus-56gvs is verified up and running
May  6 15:22:43.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-nautilus-wngr6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8151'
May  6 15:22:43.832: INFO: stderr: ""
May  6 15:22:43.832: INFO: stdout: "true"
May  6 15:22:43.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods update-demo-nautilus-wngr6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8151'
May  6 15:22:43.936: INFO: stderr: ""
May  6 15:22:43.936: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  6 15:22:43.936: INFO: validating pod update-demo-nautilus-wngr6
May  6 15:22:43.939: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 15:22:43.939: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 15:22:43.939: INFO: update-demo-nautilus-wngr6 is verified up and running
STEP: using delete to clean up resources
May  6 15:22:43.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 delete --grace-period=0 --force -f - --namespace=kubectl-8151'
May  6 15:22:44.014: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 15:22:44.014: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May  6 15:22:44.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8151'
May  6 15:22:44.154: INFO: stderr: "No resources found in kubectl-8151 namespace.\n"
May  6 15:22:44.154: INFO: stdout: ""
May  6 15:22:44.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods -l name=update-demo --namespace=kubectl-8151 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  6 15:22:44.258: INFO: stderr: ""
May  6 15:22:44.258: INFO: stdout: "update-demo-nautilus-56gvs\nupdate-demo-nautilus-wngr6\n"
May  6 15:22:44.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8151'
May  6 15:22:44.897: INFO: stderr: "No resources found in kubectl-8151 namespace.\n"
May  6 15:22:44.897: INFO: stdout: ""
May  6 15:22:44.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods -l name=update-demo --namespace=kubectl-8151 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  6 15:22:45.003: INFO: stderr: ""
May  6 15:22:45.003: INFO: stdout: "update-demo-nautilus-56gvs\nupdate-demo-nautilus-wngr6\n"
May  6 15:22:45.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8151'
May  6 15:22:45.401: INFO: stderr: "No resources found in kubectl-8151 namespace.\n"
May  6 15:22:45.401: INFO: stdout: ""
May  6 15:22:45.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 get pods -l name=update-demo --namespace=kubectl-8151 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  6 15:22:45.511: INFO: stderr: ""
May  6 15:22:45.511: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:22:45.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8151" for this suite.
May  6 15:22:51.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:22:54.287: INFO: namespace kubectl-8151 deletion completed in 8.772953945s

• [SLOW TEST:27.954 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:22:54.288: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2749
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-vpq6
STEP: Creating a pod to test atomic-volume-subpath
May  6 15:22:54.430: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-vpq6" in namespace "subpath-2749" to be "success or failure"
May  6 15:22:54.432: INFO: Pod "pod-subpath-test-secret-vpq6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.723926ms
May  6 15:22:56.435: INFO: Pod "pod-subpath-test-secret-vpq6": Phase="Running", Reason="", readiness=true. Elapsed: 2.004443869s
May  6 15:22:58.438: INFO: Pod "pod-subpath-test-secret-vpq6": Phase="Running", Reason="", readiness=true. Elapsed: 4.007246811s
May  6 15:23:00.440: INFO: Pod "pod-subpath-test-secret-vpq6": Phase="Running", Reason="", readiness=true. Elapsed: 6.010098208s
May  6 15:23:02.443: INFO: Pod "pod-subpath-test-secret-vpq6": Phase="Running", Reason="", readiness=true. Elapsed: 8.012846323s
May  6 15:23:04.446: INFO: Pod "pod-subpath-test-secret-vpq6": Phase="Running", Reason="", readiness=true. Elapsed: 10.015395385s
May  6 15:23:06.448: INFO: Pod "pod-subpath-test-secret-vpq6": Phase="Running", Reason="", readiness=true. Elapsed: 12.018062684s
May  6 15:23:08.451: INFO: Pod "pod-subpath-test-secret-vpq6": Phase="Running", Reason="", readiness=true. Elapsed: 14.020787157s
May  6 15:23:10.454: INFO: Pod "pod-subpath-test-secret-vpq6": Phase="Running", Reason="", readiness=true. Elapsed: 16.023688969s
May  6 15:23:12.457: INFO: Pod "pod-subpath-test-secret-vpq6": Phase="Running", Reason="", readiness=true. Elapsed: 18.026381105s
May  6 15:23:14.459: INFO: Pod "pod-subpath-test-secret-vpq6": Phase="Running", Reason="", readiness=true. Elapsed: 20.02905491s
May  6 15:23:16.462: INFO: Pod "pod-subpath-test-secret-vpq6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.031851696s
STEP: Saw pod success
May  6 15:23:16.462: INFO: Pod "pod-subpath-test-secret-vpq6" satisfied condition "success or failure"
May  6 15:23:16.464: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-subpath-test-secret-vpq6 container test-container-subpath-secret-vpq6: <nil>
STEP: delete the pod
May  6 15:23:16.476: INFO: Waiting for pod pod-subpath-test-secret-vpq6 to disappear
May  6 15:23:16.477: INFO: Pod pod-subpath-test-secret-vpq6 no longer exists
STEP: Deleting pod pod-subpath-test-secret-vpq6
May  6 15:23:16.477: INFO: Deleting pod "pod-subpath-test-secret-vpq6" in namespace "subpath-2749"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:23:16.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2749" for this suite.
May  6 15:23:22.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:23:25.241: INFO: namespace subpath-2749 deletion completed in 8.759313212s

• [SLOW TEST:30.954 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:23:25.241: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-1957
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May  6 15:23:25.604: INFO: Pod name wrapped-volume-race-f34857ea-f0a9-426b-bd10-e64bb9c5f969: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f34857ea-f0a9-426b-bd10-e64bb9c5f969 in namespace emptydir-wrapper-1957, will wait for the garbage collector to delete the pods
May  6 15:23:31.714: INFO: Deleting ReplicationController wrapped-volume-race-f34857ea-f0a9-426b-bd10-e64bb9c5f969 took: 5.717007ms
May  6 15:23:33.114: INFO: Terminating ReplicationController wrapped-volume-race-f34857ea-f0a9-426b-bd10-e64bb9c5f969 pods took: 1.400145682s
STEP: Creating RC which spawns configmap-volume pods
May  6 15:24:17.427: INFO: Pod name wrapped-volume-race-84dd472d-db93-435e-8b46-87067fcde6df: Found 0 pods out of 5
May  6 15:24:22.430: INFO: Pod name wrapped-volume-race-84dd472d-db93-435e-8b46-87067fcde6df: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-84dd472d-db93-435e-8b46-87067fcde6df in namespace emptydir-wrapper-1957, will wait for the garbage collector to delete the pods
May  6 15:24:24.502: INFO: Deleting ReplicationController wrapped-volume-race-84dd472d-db93-435e-8b46-87067fcde6df took: 6.439126ms
May  6 15:24:25.902: INFO: Terminating ReplicationController wrapped-volume-race-84dd472d-db93-435e-8b46-87067fcde6df pods took: 1.400144811s
STEP: Creating RC which spawns configmap-volume pods
May  6 15:25:07.515: INFO: Pod name wrapped-volume-race-3b7481e9-162e-4d36-918a-b35275a8bd0e: Found 0 pods out of 5
May  6 15:25:12.519: INFO: Pod name wrapped-volume-race-3b7481e9-162e-4d36-918a-b35275a8bd0e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3b7481e9-162e-4d36-918a-b35275a8bd0e in namespace emptydir-wrapper-1957, will wait for the garbage collector to delete the pods
May  6 15:25:16.591: INFO: Deleting ReplicationController wrapped-volume-race-3b7481e9-162e-4d36-918a-b35275a8bd0e took: 6.060413ms
May  6 15:25:17.992: INFO: Terminating ReplicationController wrapped-volume-race-3b7481e9-162e-4d36-918a-b35275a8bd0e pods took: 1.40014314s
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:25:57.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1957" for this suite.
May  6 15:26:05.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:26:08.474: INFO: namespace emptydir-wrapper-1957 deletion completed in 10.759663476s

• [SLOW TEST:163.233 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:26:08.474: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-4350
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4350
I0506 15:26:08.609445      25 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4350, replica count: 1
I0506 15:26:09.659713      25 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0506 15:26:10.659855      25 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  6 15:26:10.767: INFO: Created: latency-svc-vtlhw
May  6 15:26:10.771: INFO: Got endpoints: latency-svc-vtlhw [11.077604ms]
May  6 15:26:10.779: INFO: Created: latency-svc-qtvr2
May  6 15:26:10.783: INFO: Created: latency-svc-z9jgx
May  6 15:26:10.783: INFO: Got endpoints: latency-svc-qtvr2 [12.376391ms]
May  6 15:26:10.787: INFO: Got endpoints: latency-svc-z9jgx [16.810869ms]
May  6 15:26:10.789: INFO: Created: latency-svc-h99hs
May  6 15:26:10.794: INFO: Got endpoints: latency-svc-h99hs [23.560155ms]
May  6 15:26:10.794: INFO: Created: latency-svc-d92h4
May  6 15:26:10.798: INFO: Got endpoints: latency-svc-d92h4 [27.43836ms]
May  6 15:26:10.799: INFO: Created: latency-svc-d4d7j
May  6 15:26:10.803: INFO: Got endpoints: latency-svc-d4d7j [32.106428ms]
May  6 15:26:10.804: INFO: Created: latency-svc-wt8sx
May  6 15:26:10.808: INFO: Got endpoints: latency-svc-wt8sx [36.815083ms]
May  6 15:26:10.809: INFO: Created: latency-svc-p4dgt
May  6 15:26:10.812: INFO: Got endpoints: latency-svc-p4dgt [41.107646ms]
May  6 15:26:10.814: INFO: Created: latency-svc-2ckpr
May  6 15:26:10.818: INFO: Created: latency-svc-slmnj
May  6 15:26:10.820: INFO: Got endpoints: latency-svc-2ckpr [49.363686ms]
May  6 15:26:10.821: INFO: Got endpoints: latency-svc-slmnj [50.559057ms]
May  6 15:26:10.825: INFO: Created: latency-svc-m2c6m
May  6 15:26:10.828: INFO: Got endpoints: latency-svc-m2c6m [57.346783ms]
May  6 15:26:10.829: INFO: Created: latency-svc-dp4nj
May  6 15:26:10.833: INFO: Got endpoints: latency-svc-dp4nj [61.776737ms]
May  6 15:26:10.834: INFO: Created: latency-svc-h25t9
May  6 15:26:10.837: INFO: Got endpoints: latency-svc-h25t9 [66.639575ms]
May  6 15:26:10.839: INFO: Created: latency-svc-g87h8
May  6 15:26:10.843: INFO: Got endpoints: latency-svc-g87h8 [71.944797ms]
May  6 15:26:10.844: INFO: Created: latency-svc-r87g7
May  6 15:26:10.850: INFO: Got endpoints: latency-svc-r87g7 [78.672709ms]
May  6 15:26:10.850: INFO: Created: latency-svc-s9cfq
May  6 15:26:10.854: INFO: Got endpoints: latency-svc-s9cfq [82.915051ms]
May  6 15:26:10.856: INFO: Created: latency-svc-nrpjb
May  6 15:26:10.859: INFO: Got endpoints: latency-svc-nrpjb [75.655147ms]
May  6 15:26:10.860: INFO: Created: latency-svc-6ngm2
May  6 15:26:10.863: INFO: Got endpoints: latency-svc-6ngm2 [75.987392ms]
May  6 15:26:10.865: INFO: Created: latency-svc-px757
May  6 15:26:10.869: INFO: Got endpoints: latency-svc-px757 [74.954141ms]
May  6 15:26:10.872: INFO: Created: latency-svc-zhktv
May  6 15:26:10.877: INFO: Got endpoints: latency-svc-zhktv [78.328249ms]
May  6 15:26:10.879: INFO: Created: latency-svc-ctdxp
May  6 15:26:10.882: INFO: Got endpoints: latency-svc-ctdxp [78.625982ms]
May  6 15:26:10.884: INFO: Created: latency-svc-497zz
May  6 15:26:10.887: INFO: Got endpoints: latency-svc-497zz [79.007211ms]
May  6 15:26:10.890: INFO: Created: latency-svc-gxflf
May  6 15:26:10.894: INFO: Created: latency-svc-vwkx4
May  6 15:26:10.897: INFO: Got endpoints: latency-svc-gxflf [84.667476ms]
May  6 15:26:10.898: INFO: Got endpoints: latency-svc-vwkx4 [78.025255ms]
May  6 15:26:10.899: INFO: Created: latency-svc-zsth9
May  6 15:26:10.903: INFO: Got endpoints: latency-svc-zsth9 [81.812843ms]
May  6 15:26:10.905: INFO: Created: latency-svc-cctt2
May  6 15:26:10.908: INFO: Got endpoints: latency-svc-cctt2 [79.852716ms]
May  6 15:26:10.911: INFO: Created: latency-svc-t4s6j
May  6 15:26:10.914: INFO: Got endpoints: latency-svc-t4s6j [81.288415ms]
May  6 15:26:10.916: INFO: Created: latency-svc-pgwpx
May  6 15:26:10.923: INFO: Got endpoints: latency-svc-pgwpx [86.007799ms]
May  6 15:26:10.925: INFO: Created: latency-svc-rg5mt
May  6 15:26:10.933: INFO: Got endpoints: latency-svc-rg5mt [90.499386ms]
May  6 15:26:10.939: INFO: Created: latency-svc-t4tg6
May  6 15:26:10.943: INFO: Got endpoints: latency-svc-t4tg6 [93.153832ms]
May  6 15:26:10.945: INFO: Created: latency-svc-6rjtp
May  6 15:26:10.950: INFO: Got endpoints: latency-svc-6rjtp [96.20097ms]
May  6 15:26:10.953: INFO: Created: latency-svc-64hzl
May  6 15:26:10.958: INFO: Got endpoints: latency-svc-64hzl [99.488575ms]
May  6 15:26:10.959: INFO: Created: latency-svc-qvm8t
May  6 15:26:10.965: INFO: Got endpoints: latency-svc-qvm8t [101.078127ms]
May  6 15:26:10.965: INFO: Created: latency-svc-r5bg6
May  6 15:26:10.969: INFO: Created: latency-svc-7km5h
May  6 15:26:10.970: INFO: Got endpoints: latency-svc-r5bg6 [101.154549ms]
May  6 15:26:10.974: INFO: Created: latency-svc-7cxx4
May  6 15:26:10.982: INFO: Created: latency-svc-fx4ff
May  6 15:26:10.991: INFO: Created: latency-svc-vgtlf
May  6 15:26:11.002: INFO: Created: latency-svc-jw5mr
May  6 15:26:11.008: INFO: Created: latency-svc-vkgj4
May  6 15:26:11.014: INFO: Created: latency-svc-zm5hm
May  6 15:26:11.022: INFO: Created: latency-svc-s2cmq
May  6 15:26:11.022: INFO: Got endpoints: latency-svc-7km5h [145.365687ms]
May  6 15:26:11.028: INFO: Created: latency-svc-kfk4d
May  6 15:26:11.039: INFO: Created: latency-svc-7thh4
May  6 15:26:11.044: INFO: Created: latency-svc-qsmjb
May  6 15:26:11.049: INFO: Created: latency-svc-w7gqp
May  6 15:26:11.056: INFO: Created: latency-svc-fnpkx
May  6 15:26:11.061: INFO: Created: latency-svc-x6bpx
May  6 15:26:11.065: INFO: Created: latency-svc-k2gkq
May  6 15:26:11.070: INFO: Created: latency-svc-6fbnx
May  6 15:26:11.070: INFO: Got endpoints: latency-svc-7cxx4 [188.820635ms]
May  6 15:26:11.077: INFO: Created: latency-svc-zfgbn
May  6 15:26:11.121: INFO: Got endpoints: latency-svc-fx4ff [234.358883ms]
May  6 15:26:11.128: INFO: Created: latency-svc-fnfrc
May  6 15:26:11.171: INFO: Got endpoints: latency-svc-vgtlf [274.174975ms]
May  6 15:26:11.177: INFO: Created: latency-svc-dzm8d
May  6 15:26:11.221: INFO: Got endpoints: latency-svc-jw5mr [322.709658ms]
May  6 15:26:11.229: INFO: Created: latency-svc-2mg6n
May  6 15:26:11.271: INFO: Got endpoints: latency-svc-vkgj4 [367.45076ms]
May  6 15:26:11.279: INFO: Created: latency-svc-z6qdk
May  6 15:26:11.322: INFO: Got endpoints: latency-svc-zm5hm [413.97694ms]
May  6 15:26:11.328: INFO: Created: latency-svc-dn94k
May  6 15:26:11.371: INFO: Got endpoints: latency-svc-s2cmq [456.702576ms]
May  6 15:26:11.378: INFO: Created: latency-svc-jxw2b
May  6 15:26:11.421: INFO: Got endpoints: latency-svc-kfk4d [497.392501ms]
May  6 15:26:11.427: INFO: Created: latency-svc-9df7n
May  6 15:26:11.470: INFO: Got endpoints: latency-svc-7thh4 [536.815779ms]
May  6 15:26:11.477: INFO: Created: latency-svc-dzwfq
May  6 15:26:11.521: INFO: Got endpoints: latency-svc-qsmjb [577.897158ms]
May  6 15:26:11.527: INFO: Created: latency-svc-lvm7g
May  6 15:26:11.571: INFO: Got endpoints: latency-svc-w7gqp [620.571335ms]
May  6 15:26:11.577: INFO: Created: latency-svc-jrxqb
May  6 15:26:11.621: INFO: Got endpoints: latency-svc-fnpkx [662.984148ms]
May  6 15:26:11.628: INFO: Created: latency-svc-cgddg
May  6 15:26:11.671: INFO: Got endpoints: latency-svc-x6bpx [706.024388ms]
May  6 15:26:11.677: INFO: Created: latency-svc-srsvq
May  6 15:26:11.721: INFO: Got endpoints: latency-svc-k2gkq [750.495007ms]
May  6 15:26:11.728: INFO: Created: latency-svc-9wltd
May  6 15:26:11.771: INFO: Got endpoints: latency-svc-6fbnx [749.117528ms]
May  6 15:26:11.779: INFO: Created: latency-svc-6t4mr
May  6 15:26:11.821: INFO: Got endpoints: latency-svc-zfgbn [750.619872ms]
May  6 15:26:11.827: INFO: Created: latency-svc-c7v7k
May  6 15:26:11.871: INFO: Got endpoints: latency-svc-fnfrc [749.683149ms]
May  6 15:26:11.877: INFO: Created: latency-svc-w4d22
May  6 15:26:11.921: INFO: Got endpoints: latency-svc-dzm8d [750.081312ms]
May  6 15:26:11.927: INFO: Created: latency-svc-hkblb
May  6 15:26:11.971: INFO: Got endpoints: latency-svc-2mg6n [749.805324ms]
May  6 15:26:11.979: INFO: Created: latency-svc-7gxr4
May  6 15:26:12.021: INFO: Got endpoints: latency-svc-z6qdk [750.515569ms]
May  6 15:26:12.028: INFO: Created: latency-svc-nkz6h
May  6 15:26:12.072: INFO: Got endpoints: latency-svc-dn94k [750.041519ms]
May  6 15:26:12.079: INFO: Created: latency-svc-q9rbb
May  6 15:26:12.121: INFO: Got endpoints: latency-svc-jxw2b [750.644301ms]
May  6 15:26:12.128: INFO: Created: latency-svc-gktzq
May  6 15:26:12.171: INFO: Got endpoints: latency-svc-9df7n [749.952087ms]
May  6 15:26:12.178: INFO: Created: latency-svc-lrch8
May  6 15:26:12.222: INFO: Got endpoints: latency-svc-dzwfq [751.40027ms]
May  6 15:26:12.228: INFO: Created: latency-svc-9qmqd
May  6 15:26:12.271: INFO: Got endpoints: latency-svc-lvm7g [750.14972ms]
May  6 15:26:12.278: INFO: Created: latency-svc-9wmww
May  6 15:26:12.321: INFO: Got endpoints: latency-svc-jrxqb [750.236058ms]
May  6 15:26:12.328: INFO: Created: latency-svc-qrl2h
May  6 15:26:12.371: INFO: Got endpoints: latency-svc-cgddg [749.467754ms]
May  6 15:26:12.378: INFO: Created: latency-svc-5b79z
May  6 15:26:12.424: INFO: Got endpoints: latency-svc-srsvq [753.384797ms]
May  6 15:26:12.430: INFO: Created: latency-svc-rppd4
May  6 15:26:12.471: INFO: Got endpoints: latency-svc-9wltd [749.902283ms]
May  6 15:26:12.477: INFO: Created: latency-svc-v6khg
May  6 15:26:12.521: INFO: Got endpoints: latency-svc-6t4mr [749.82498ms]
May  6 15:26:12.529: INFO: Created: latency-svc-gc6dl
May  6 15:26:12.571: INFO: Got endpoints: latency-svc-c7v7k [750.107416ms]
May  6 15:26:12.578: INFO: Created: latency-svc-hfc4g
May  6 15:26:12.621: INFO: Got endpoints: latency-svc-w4d22 [750.335028ms]
May  6 15:26:12.628: INFO: Created: latency-svc-xrh5t
May  6 15:26:12.672: INFO: Got endpoints: latency-svc-hkblb [750.794617ms]
May  6 15:26:12.679: INFO: Created: latency-svc-fj8zz
May  6 15:26:12.721: INFO: Got endpoints: latency-svc-7gxr4 [750.532023ms]
May  6 15:26:12.729: INFO: Created: latency-svc-gprgj
May  6 15:26:12.771: INFO: Got endpoints: latency-svc-nkz6h [749.64955ms]
May  6 15:26:12.778: INFO: Created: latency-svc-v5pg2
May  6 15:26:12.821: INFO: Got endpoints: latency-svc-q9rbb [748.79082ms]
May  6 15:26:12.827: INFO: Created: latency-svc-lx79w
May  6 15:26:12.871: INFO: Got endpoints: latency-svc-gktzq [750.114244ms]
May  6 15:26:12.878: INFO: Created: latency-svc-c8gfn
May  6 15:26:12.921: INFO: Got endpoints: latency-svc-lrch8 [750.038364ms]
May  6 15:26:12.927: INFO: Created: latency-svc-kgvqn
May  6 15:26:12.971: INFO: Got endpoints: latency-svc-9qmqd [749.344145ms]
May  6 15:26:12.978: INFO: Created: latency-svc-mt8xq
May  6 15:26:13.021: INFO: Got endpoints: latency-svc-9wmww [749.894303ms]
May  6 15:26:13.028: INFO: Created: latency-svc-h65md
May  6 15:26:13.071: INFO: Got endpoints: latency-svc-qrl2h [749.886648ms]
May  6 15:26:13.078: INFO: Created: latency-svc-pcdwg
May  6 15:26:13.121: INFO: Got endpoints: latency-svc-5b79z [750.331706ms]
May  6 15:26:13.132: INFO: Created: latency-svc-lbcr4
May  6 15:26:13.171: INFO: Got endpoints: latency-svc-rppd4 [747.021017ms]
May  6 15:26:13.177: INFO: Created: latency-svc-m29lw
May  6 15:26:13.221: INFO: Got endpoints: latency-svc-v6khg [750.286848ms]
May  6 15:26:13.228: INFO: Created: latency-svc-h4m6h
May  6 15:26:13.271: INFO: Got endpoints: latency-svc-gc6dl [749.998602ms]
May  6 15:26:13.279: INFO: Created: latency-svc-l5s2l
May  6 15:26:13.321: INFO: Got endpoints: latency-svc-hfc4g [750.116801ms]
May  6 15:26:13.328: INFO: Created: latency-svc-csc44
May  6 15:26:13.371: INFO: Got endpoints: latency-svc-xrh5t [749.786501ms]
May  6 15:26:13.378: INFO: Created: latency-svc-n62dh
May  6 15:26:13.421: INFO: Got endpoints: latency-svc-fj8zz [749.03495ms]
May  6 15:26:13.428: INFO: Created: latency-svc-fgncw
May  6 15:26:13.471: INFO: Got endpoints: latency-svc-gprgj [749.820453ms]
May  6 15:26:13.478: INFO: Created: latency-svc-w676m
May  6 15:26:13.520: INFO: Got endpoints: latency-svc-v5pg2 [749.437785ms]
May  6 15:26:13.527: INFO: Created: latency-svc-4cnmf
May  6 15:26:13.571: INFO: Got endpoints: latency-svc-lx79w [750.224717ms]
May  6 15:26:13.578: INFO: Created: latency-svc-b5slh
May  6 15:26:13.622: INFO: Got endpoints: latency-svc-c8gfn [750.297013ms]
May  6 15:26:13.629: INFO: Created: latency-svc-8xf5h
May  6 15:26:13.671: INFO: Got endpoints: latency-svc-kgvqn [749.816587ms]
May  6 15:26:13.679: INFO: Created: latency-svc-gftrn
May  6 15:26:13.721: INFO: Got endpoints: latency-svc-mt8xq [749.778634ms]
May  6 15:26:13.732: INFO: Created: latency-svc-vpxn6
May  6 15:26:13.771: INFO: Got endpoints: latency-svc-h65md [750.276685ms]
May  6 15:26:13.779: INFO: Created: latency-svc-thnv7
May  6 15:26:13.821: INFO: Got endpoints: latency-svc-pcdwg [750.184407ms]
May  6 15:26:13.833: INFO: Created: latency-svc-zkwql
May  6 15:26:13.871: INFO: Got endpoints: latency-svc-lbcr4 [749.505167ms]
May  6 15:26:13.877: INFO: Created: latency-svc-q72ss
May  6 15:26:13.922: INFO: Got endpoints: latency-svc-m29lw [750.621663ms]
May  6 15:26:13.928: INFO: Created: latency-svc-kfqbc
May  6 15:26:13.971: INFO: Got endpoints: latency-svc-h4m6h [750.024524ms]
May  6 15:26:13.978: INFO: Created: latency-svc-xzwhm
May  6 15:26:14.021: INFO: Got endpoints: latency-svc-l5s2l [749.879248ms]
May  6 15:26:14.028: INFO: Created: latency-svc-2q88b
May  6 15:26:14.071: INFO: Got endpoints: latency-svc-csc44 [749.515462ms]
May  6 15:26:14.080: INFO: Created: latency-svc-8bp48
May  6 15:26:14.121: INFO: Got endpoints: latency-svc-n62dh [749.858449ms]
May  6 15:26:14.128: INFO: Created: latency-svc-9dqhd
May  6 15:26:14.171: INFO: Got endpoints: latency-svc-fgncw [750.500412ms]
May  6 15:26:14.178: INFO: Created: latency-svc-vqrgg
May  6 15:26:14.221: INFO: Got endpoints: latency-svc-w676m [749.358394ms]
May  6 15:26:14.228: INFO: Created: latency-svc-t76mw
May  6 15:26:14.271: INFO: Got endpoints: latency-svc-4cnmf [750.359343ms]
May  6 15:26:14.278: INFO: Created: latency-svc-mdv6z
May  6 15:26:14.321: INFO: Got endpoints: latency-svc-b5slh [750.093988ms]
May  6 15:26:14.344: INFO: Created: latency-svc-p97l6
May  6 15:26:14.371: INFO: Got endpoints: latency-svc-8xf5h [749.122427ms]
May  6 15:26:14.378: INFO: Created: latency-svc-w25hr
May  6 15:26:14.421: INFO: Got endpoints: latency-svc-gftrn [749.974578ms]
May  6 15:26:14.428: INFO: Created: latency-svc-7lwsw
May  6 15:26:14.471: INFO: Got endpoints: latency-svc-vpxn6 [750.159708ms]
May  6 15:26:14.478: INFO: Created: latency-svc-z4cfg
May  6 15:26:14.521: INFO: Got endpoints: latency-svc-thnv7 [750.059272ms]
May  6 15:26:14.529: INFO: Created: latency-svc-tpc24
May  6 15:26:14.571: INFO: Got endpoints: latency-svc-zkwql [749.787045ms]
May  6 15:26:14.577: INFO: Created: latency-svc-4xczv
May  6 15:26:14.622: INFO: Got endpoints: latency-svc-q72ss [750.987609ms]
May  6 15:26:14.628: INFO: Created: latency-svc-twlb9
May  6 15:26:14.673: INFO: Got endpoints: latency-svc-kfqbc [751.094623ms]
May  6 15:26:14.679: INFO: Created: latency-svc-hpqvl
May  6 15:26:14.722: INFO: Got endpoints: latency-svc-xzwhm [750.512993ms]
May  6 15:26:14.728: INFO: Created: latency-svc-5zng9
May  6 15:26:14.771: INFO: Got endpoints: latency-svc-2q88b [750.144193ms]
May  6 15:26:14.778: INFO: Created: latency-svc-5xw8n
May  6 15:26:14.821: INFO: Got endpoints: latency-svc-8bp48 [749.864281ms]
May  6 15:26:14.828: INFO: Created: latency-svc-z7pt2
May  6 15:26:14.874: INFO: Got endpoints: latency-svc-9dqhd [752.827111ms]
May  6 15:26:14.880: INFO: Created: latency-svc-z76th
May  6 15:26:14.921: INFO: Got endpoints: latency-svc-vqrgg [749.346692ms]
May  6 15:26:14.927: INFO: Created: latency-svc-4sm8t
May  6 15:26:14.972: INFO: Got endpoints: latency-svc-t76mw [750.979123ms]
May  6 15:26:14.978: INFO: Created: latency-svc-hxn52
May  6 15:26:15.021: INFO: Got endpoints: latency-svc-mdv6z [750.216546ms]
May  6 15:26:15.029: INFO: Created: latency-svc-s9rp9
May  6 15:26:15.071: INFO: Got endpoints: latency-svc-p97l6 [749.509386ms]
May  6 15:26:15.078: INFO: Created: latency-svc-54c2r
May  6 15:26:15.121: INFO: Got endpoints: latency-svc-w25hr [749.557202ms]
May  6 15:26:15.127: INFO: Created: latency-svc-6hshc
May  6 15:26:15.170: INFO: Got endpoints: latency-svc-7lwsw [749.583782ms]
May  6 15:26:15.177: INFO: Created: latency-svc-wr6g8
May  6 15:26:15.225: INFO: Got endpoints: latency-svc-z4cfg [753.649168ms]
May  6 15:26:15.231: INFO: Created: latency-svc-rt7t2
May  6 15:26:15.271: INFO: Got endpoints: latency-svc-tpc24 [750.099318ms]
May  6 15:26:15.281: INFO: Created: latency-svc-mq6qm
May  6 15:26:15.321: INFO: Got endpoints: latency-svc-4xczv [749.853862ms]
May  6 15:26:15.328: INFO: Created: latency-svc-tfrmh
May  6 15:26:15.371: INFO: Got endpoints: latency-svc-twlb9 [749.230785ms]
May  6 15:26:15.378: INFO: Created: latency-svc-8p9tr
May  6 15:26:15.421: INFO: Got endpoints: latency-svc-hpqvl [747.753094ms]
May  6 15:26:15.427: INFO: Created: latency-svc-dftk9
May  6 15:26:15.471: INFO: Got endpoints: latency-svc-5zng9 [749.006183ms]
May  6 15:26:15.482: INFO: Created: latency-svc-n4xjf
May  6 15:26:15.521: INFO: Got endpoints: latency-svc-5xw8n [749.535117ms]
May  6 15:26:15.527: INFO: Created: latency-svc-8q5jd
May  6 15:26:15.570: INFO: Got endpoints: latency-svc-z7pt2 [749.404753ms]
May  6 15:26:15.578: INFO: Created: latency-svc-zvlk6
May  6 15:26:15.621: INFO: Got endpoints: latency-svc-z76th [746.88984ms]
May  6 15:26:15.627: INFO: Created: latency-svc-wdktr
May  6 15:26:15.671: INFO: Got endpoints: latency-svc-4sm8t [749.832823ms]
May  6 15:26:15.677: INFO: Created: latency-svc-n9gjh
May  6 15:26:15.721: INFO: Got endpoints: latency-svc-hxn52 [749.135409ms]
May  6 15:26:15.727: INFO: Created: latency-svc-jjhzz
May  6 15:26:15.771: INFO: Got endpoints: latency-svc-s9rp9 [749.629998ms]
May  6 15:26:15.777: INFO: Created: latency-svc-4b9qr
May  6 15:26:15.821: INFO: Got endpoints: latency-svc-54c2r [749.823061ms]
May  6 15:26:15.828: INFO: Created: latency-svc-pl5zz
May  6 15:26:15.871: INFO: Got endpoints: latency-svc-6hshc [750.0635ms]
May  6 15:26:15.878: INFO: Created: latency-svc-nbcwk
May  6 15:26:15.921: INFO: Got endpoints: latency-svc-wr6g8 [750.318037ms]
May  6 15:26:15.928: INFO: Created: latency-svc-9qrb4
May  6 15:26:15.972: INFO: Got endpoints: latency-svc-rt7t2 [747.173186ms]
May  6 15:26:15.978: INFO: Created: latency-svc-7zffx
May  6 15:26:16.021: INFO: Got endpoints: latency-svc-mq6qm [749.756383ms]
May  6 15:26:16.028: INFO: Created: latency-svc-rt8tj
May  6 15:26:16.071: INFO: Got endpoints: latency-svc-tfrmh [750.020828ms]
May  6 15:26:16.078: INFO: Created: latency-svc-w59mc
May  6 15:26:16.121: INFO: Got endpoints: latency-svc-8p9tr [749.844904ms]
May  6 15:26:16.128: INFO: Created: latency-svc-zbkws
May  6 15:26:16.171: INFO: Got endpoints: latency-svc-dftk9 [750.228978ms]
May  6 15:26:16.178: INFO: Created: latency-svc-jr9k4
May  6 15:26:16.221: INFO: Got endpoints: latency-svc-n4xjf [750.161738ms]
May  6 15:26:16.228: INFO: Created: latency-svc-bk2wd
May  6 15:26:16.271: INFO: Got endpoints: latency-svc-8q5jd [750.205833ms]
May  6 15:26:16.277: INFO: Created: latency-svc-4nv2z
May  6 15:26:16.321: INFO: Got endpoints: latency-svc-zvlk6 [750.88796ms]
May  6 15:26:16.327: INFO: Created: latency-svc-gz8z9
May  6 15:26:16.372: INFO: Got endpoints: latency-svc-wdktr [751.390443ms]
May  6 15:26:16.379: INFO: Created: latency-svc-52kqv
May  6 15:26:16.421: INFO: Got endpoints: latency-svc-n9gjh [750.510805ms]
May  6 15:26:16.428: INFO: Created: latency-svc-s6mg5
May  6 15:26:16.471: INFO: Got endpoints: latency-svc-jjhzz [750.066614ms]
May  6 15:26:16.478: INFO: Created: latency-svc-t25f2
May  6 15:26:16.521: INFO: Got endpoints: latency-svc-4b9qr [750.251922ms]
May  6 15:26:16.528: INFO: Created: latency-svc-kwchd
May  6 15:26:16.573: INFO: Got endpoints: latency-svc-pl5zz [751.980501ms]
May  6 15:26:16.580: INFO: Created: latency-svc-g6cdd
May  6 15:26:16.621: INFO: Got endpoints: latency-svc-nbcwk [749.882885ms]
May  6 15:26:16.627: INFO: Created: latency-svc-tdmdw
May  6 15:26:16.671: INFO: Got endpoints: latency-svc-9qrb4 [749.959615ms]
May  6 15:26:16.677: INFO: Created: latency-svc-z28nb
May  6 15:26:16.721: INFO: Got endpoints: latency-svc-7zffx [749.174662ms]
May  6 15:26:16.728: INFO: Created: latency-svc-m5r4z
May  6 15:26:16.775: INFO: Got endpoints: latency-svc-rt8tj [753.882822ms]
May  6 15:26:16.782: INFO: Created: latency-svc-5ztzk
May  6 15:26:16.821: INFO: Got endpoints: latency-svc-w59mc [750.066855ms]
May  6 15:26:16.829: INFO: Created: latency-svc-5gbwr
May  6 15:26:16.873: INFO: Got endpoints: latency-svc-zbkws [751.848273ms]
May  6 15:26:16.881: INFO: Created: latency-svc-5nsh9
May  6 15:26:16.921: INFO: Got endpoints: latency-svc-jr9k4 [749.891731ms]
May  6 15:26:16.938: INFO: Created: latency-svc-4m4qv
May  6 15:26:16.971: INFO: Got endpoints: latency-svc-bk2wd [749.887157ms]
May  6 15:26:16.984: INFO: Created: latency-svc-gj748
May  6 15:26:17.021: INFO: Got endpoints: latency-svc-4nv2z [750.576067ms]
May  6 15:26:17.028: INFO: Created: latency-svc-kvpcp
May  6 15:26:17.071: INFO: Got endpoints: latency-svc-gz8z9 [750.305986ms]
May  6 15:26:17.080: INFO: Created: latency-svc-nl557
May  6 15:26:17.121: INFO: Got endpoints: latency-svc-52kqv [749.044432ms]
May  6 15:26:17.133: INFO: Created: latency-svc-cvrws
May  6 15:26:17.171: INFO: Got endpoints: latency-svc-s6mg5 [749.700239ms]
May  6 15:26:17.179: INFO: Created: latency-svc-zm5nv
May  6 15:26:17.221: INFO: Got endpoints: latency-svc-t25f2 [749.930495ms]
May  6 15:26:17.228: INFO: Created: latency-svc-ncvdm
May  6 15:26:17.271: INFO: Got endpoints: latency-svc-kwchd [750.131294ms]
May  6 15:26:17.278: INFO: Created: latency-svc-frfgb
May  6 15:26:17.321: INFO: Got endpoints: latency-svc-g6cdd [748.284055ms]
May  6 15:26:17.327: INFO: Created: latency-svc-wfr55
May  6 15:26:17.371: INFO: Got endpoints: latency-svc-tdmdw [750.711354ms]
May  6 15:26:17.378: INFO: Created: latency-svc-q728v
May  6 15:26:17.421: INFO: Got endpoints: latency-svc-z28nb [750.453648ms]
May  6 15:26:17.428: INFO: Created: latency-svc-7kmvq
May  6 15:26:17.471: INFO: Got endpoints: latency-svc-m5r4z [750.35214ms]
May  6 15:26:17.479: INFO: Created: latency-svc-h5w26
May  6 15:26:17.521: INFO: Got endpoints: latency-svc-5ztzk [745.744617ms]
May  6 15:26:17.527: INFO: Created: latency-svc-426vl
May  6 15:26:17.571: INFO: Got endpoints: latency-svc-5gbwr [750.598634ms]
May  6 15:26:17.578: INFO: Created: latency-svc-rf9gd
May  6 15:26:17.621: INFO: Got endpoints: latency-svc-5nsh9 [748.497581ms]
May  6 15:26:17.629: INFO: Created: latency-svc-4ttfh
May  6 15:26:17.671: INFO: Got endpoints: latency-svc-4m4qv [750.605824ms]
May  6 15:26:17.679: INFO: Created: latency-svc-lgskz
May  6 15:26:17.721: INFO: Got endpoints: latency-svc-gj748 [750.559861ms]
May  6 15:26:17.729: INFO: Created: latency-svc-6mtxl
May  6 15:26:17.771: INFO: Got endpoints: latency-svc-kvpcp [749.754844ms]
May  6 15:26:17.779: INFO: Created: latency-svc-2dl7g
May  6 15:26:17.821: INFO: Got endpoints: latency-svc-nl557 [749.575962ms]
May  6 15:26:17.828: INFO: Created: latency-svc-rzrpm
May  6 15:26:17.871: INFO: Got endpoints: latency-svc-cvrws [749.557851ms]
May  6 15:26:17.878: INFO: Created: latency-svc-7dld7
May  6 15:26:17.921: INFO: Got endpoints: latency-svc-zm5nv [750.123776ms]
May  6 15:26:17.928: INFO: Created: latency-svc-4c7dq
May  6 15:26:17.973: INFO: Got endpoints: latency-svc-ncvdm [752.245781ms]
May  6 15:26:17.980: INFO: Created: latency-svc-hnxqr
May  6 15:26:18.022: INFO: Got endpoints: latency-svc-frfgb [750.784022ms]
May  6 15:26:18.032: INFO: Created: latency-svc-rgvj7
May  6 15:26:18.072: INFO: Got endpoints: latency-svc-wfr55 [751.263092ms]
May  6 15:26:18.085: INFO: Created: latency-svc-7ghnr
May  6 15:26:18.123: INFO: Got endpoints: latency-svc-q728v [751.272175ms]
May  6 15:26:18.132: INFO: Created: latency-svc-cc2ql
May  6 15:26:18.171: INFO: Got endpoints: latency-svc-7kmvq [749.438078ms]
May  6 15:26:18.182: INFO: Created: latency-svc-jd6j5
May  6 15:26:18.221: INFO: Got endpoints: latency-svc-h5w26 [749.887461ms]
May  6 15:26:18.229: INFO: Created: latency-svc-v6nl9
May  6 15:26:18.271: INFO: Got endpoints: latency-svc-426vl [750.336264ms]
May  6 15:26:18.278: INFO: Created: latency-svc-slgc7
May  6 15:26:18.321: INFO: Got endpoints: latency-svc-rf9gd [749.403226ms]
May  6 15:26:18.327: INFO: Created: latency-svc-4k7mm
May  6 15:26:18.371: INFO: Got endpoints: latency-svc-4ttfh [749.563477ms]
May  6 15:26:18.378: INFO: Created: latency-svc-c68kj
May  6 15:26:18.421: INFO: Got endpoints: latency-svc-lgskz [749.046642ms]
May  6 15:26:18.427: INFO: Created: latency-svc-tlfzc
May  6 15:26:18.471: INFO: Got endpoints: latency-svc-6mtxl [749.726485ms]
May  6 15:26:18.478: INFO: Created: latency-svc-999jd
May  6 15:26:18.523: INFO: Got endpoints: latency-svc-2dl7g [751.667358ms]
May  6 15:26:18.550: INFO: Created: latency-svc-qk9pz
May  6 15:26:18.571: INFO: Got endpoints: latency-svc-rzrpm [750.258555ms]
May  6 15:26:18.578: INFO: Created: latency-svc-nmt6h
May  6 15:26:18.622: INFO: Got endpoints: latency-svc-7dld7 [750.850483ms]
May  6 15:26:18.676: INFO: Got endpoints: latency-svc-4c7dq [755.044746ms]
May  6 15:26:18.721: INFO: Got endpoints: latency-svc-hnxqr [747.997107ms]
May  6 15:26:18.771: INFO: Got endpoints: latency-svc-rgvj7 [749.378711ms]
May  6 15:26:18.821: INFO: Got endpoints: latency-svc-7ghnr [748.387035ms]
May  6 15:26:18.871: INFO: Got endpoints: latency-svc-cc2ql [747.987916ms]
May  6 15:26:18.921: INFO: Got endpoints: latency-svc-jd6j5 [750.369887ms]
May  6 15:26:18.971: INFO: Got endpoints: latency-svc-v6nl9 [749.469607ms]
May  6 15:26:19.021: INFO: Got endpoints: latency-svc-slgc7 [749.622072ms]
May  6 15:26:19.072: INFO: Got endpoints: latency-svc-4k7mm [750.75039ms]
May  6 15:26:19.121: INFO: Got endpoints: latency-svc-c68kj [749.947263ms]
May  6 15:26:19.171: INFO: Got endpoints: latency-svc-tlfzc [750.46504ms]
May  6 15:26:19.221: INFO: Got endpoints: latency-svc-999jd [749.523143ms]
May  6 15:26:19.271: INFO: Got endpoints: latency-svc-qk9pz [748.269003ms]
May  6 15:26:19.321: INFO: Got endpoints: latency-svc-nmt6h [749.583969ms]
May  6 15:26:19.321: INFO: Latencies: [12.376391ms 16.810869ms 23.560155ms 27.43836ms 32.106428ms 36.815083ms 41.107646ms 49.363686ms 50.559057ms 57.346783ms 61.776737ms 66.639575ms 71.944797ms 74.954141ms 75.655147ms 75.987392ms 78.025255ms 78.328249ms 78.625982ms 78.672709ms 79.007211ms 79.852716ms 81.288415ms 81.812843ms 82.915051ms 84.667476ms 86.007799ms 90.499386ms 93.153832ms 96.20097ms 99.488575ms 101.078127ms 101.154549ms 145.365687ms 188.820635ms 234.358883ms 274.174975ms 322.709658ms 367.45076ms 413.97694ms 456.702576ms 497.392501ms 536.815779ms 577.897158ms 620.571335ms 662.984148ms 706.024388ms 745.744617ms 746.88984ms 747.021017ms 747.173186ms 747.753094ms 747.987916ms 747.997107ms 748.269003ms 748.284055ms 748.387035ms 748.497581ms 748.79082ms 749.006183ms 749.03495ms 749.044432ms 749.046642ms 749.117528ms 749.122427ms 749.135409ms 749.174662ms 749.230785ms 749.344145ms 749.346692ms 749.358394ms 749.378711ms 749.403226ms 749.404753ms 749.437785ms 749.438078ms 749.467754ms 749.469607ms 749.505167ms 749.509386ms 749.515462ms 749.523143ms 749.535117ms 749.557202ms 749.557851ms 749.563477ms 749.575962ms 749.583782ms 749.583969ms 749.622072ms 749.629998ms 749.64955ms 749.683149ms 749.700239ms 749.726485ms 749.754844ms 749.756383ms 749.778634ms 749.786501ms 749.787045ms 749.805324ms 749.816587ms 749.820453ms 749.823061ms 749.82498ms 749.832823ms 749.844904ms 749.853862ms 749.858449ms 749.864281ms 749.879248ms 749.882885ms 749.886648ms 749.887157ms 749.887461ms 749.891731ms 749.894303ms 749.902283ms 749.930495ms 749.947263ms 749.952087ms 749.959615ms 749.974578ms 749.998602ms 750.020828ms 750.024524ms 750.038364ms 750.041519ms 750.059272ms 750.0635ms 750.066614ms 750.066855ms 750.081312ms 750.093988ms 750.099318ms 750.107416ms 750.114244ms 750.116801ms 750.123776ms 750.131294ms 750.144193ms 750.14972ms 750.159708ms 750.161738ms 750.184407ms 750.205833ms 750.216546ms 750.224717ms 750.228978ms 750.236058ms 750.251922ms 750.258555ms 750.276685ms 750.286848ms 750.297013ms 750.305986ms 750.318037ms 750.331706ms 750.335028ms 750.336264ms 750.35214ms 750.359343ms 750.369887ms 750.453648ms 750.46504ms 750.495007ms 750.500412ms 750.510805ms 750.512993ms 750.515569ms 750.532023ms 750.559861ms 750.576067ms 750.598634ms 750.605824ms 750.619872ms 750.621663ms 750.644301ms 750.711354ms 750.75039ms 750.784022ms 750.794617ms 750.850483ms 750.88796ms 750.979123ms 750.987609ms 751.094623ms 751.263092ms 751.272175ms 751.390443ms 751.40027ms 751.667358ms 751.848273ms 751.980501ms 752.245781ms 752.827111ms 753.384797ms 753.649168ms 753.882822ms 755.044746ms]
May  6 15:26:19.321: INFO: 50 %ile: 749.805324ms
May  6 15:26:19.321: INFO: 90 %ile: 750.784022ms
May  6 15:26:19.321: INFO: 99 %ile: 753.882822ms
May  6 15:26:19.321: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:26:19.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4350" for this suite.
May  6 15:26:39.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:26:42.088: INFO: namespace svc-latency-4350 deletion completed in 22.762794192s

• [SLOW TEST:33.615 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:26:42.089: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-203
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:27:09.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-203" for this suite.
May  6 15:27:15.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:27:18.134: INFO: namespace container-runtime-203 deletion completed in 8.760032612s

• [SLOW TEST:36.046 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:27:18.134: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5116
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-a654d5d3-1f09-4522-899a-7a769c2c5e7e
STEP: Creating a pod to test consume secrets
May  6 15:27:18.284: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-258ff09f-a8c3-4c2c-a5d1-ec61f392556d" in namespace "projected-5116" to be "success or failure"
May  6 15:27:18.286: INFO: Pod "pod-projected-secrets-258ff09f-a8c3-4c2c-a5d1-ec61f392556d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.689504ms
May  6 15:27:20.289: INFO: Pod "pod-projected-secrets-258ff09f-a8c3-4c2c-a5d1-ec61f392556d": Phase="Running", Reason="", readiness=true. Elapsed: 2.004537884s
May  6 15:27:22.292: INFO: Pod "pod-projected-secrets-258ff09f-a8c3-4c2c-a5d1-ec61f392556d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00746575s
STEP: Saw pod success
May  6 15:27:22.292: INFO: Pod "pod-projected-secrets-258ff09f-a8c3-4c2c-a5d1-ec61f392556d" satisfied condition "success or failure"
May  6 15:27:22.293: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-projected-secrets-258ff09f-a8c3-4c2c-a5d1-ec61f392556d container projected-secret-volume-test: <nil>
STEP: delete the pod
May  6 15:27:22.312: INFO: Waiting for pod pod-projected-secrets-258ff09f-a8c3-4c2c-a5d1-ec61f392556d to disappear
May  6 15:27:22.314: INFO: Pod pod-projected-secrets-258ff09f-a8c3-4c2c-a5d1-ec61f392556d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:27:22.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5116" for this suite.
May  6 15:27:28.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:27:31.090: INFO: namespace projected-5116 deletion completed in 8.772828051s

• [SLOW TEST:12.955 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:27:31.090: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6790
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 15:27:31.563: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 15:27:34.575: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:27:34.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6790" for this suite.
May  6 15:27:40.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:27:43.381: INFO: namespace webhook-6790 deletion completed in 8.759769878s
STEP: Destroying namespace "webhook-6790-markers" for this suite.
May  6 15:27:49.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:27:52.141: INFO: namespace webhook-6790-markers deletion completed in 8.760024805s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.060 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:27:52.151: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6167
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May  6 15:27:52.289: INFO: Waiting up to 5m0s for pod "downward-api-4f113901-06fb-40e8-90c1-aa35788d2db4" in namespace "downward-api-6167" to be "success or failure"
May  6 15:27:52.293: INFO: Pod "downward-api-4f113901-06fb-40e8-90c1-aa35788d2db4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.4908ms
May  6 15:27:54.296: INFO: Pod "downward-api-4f113901-06fb-40e8-90c1-aa35788d2db4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007251906s
May  6 15:27:56.299: INFO: Pod "downward-api-4f113901-06fb-40e8-90c1-aa35788d2db4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01026205s
STEP: Saw pod success
May  6 15:27:56.299: INFO: Pod "downward-api-4f113901-06fb-40e8-90c1-aa35788d2db4" satisfied condition "success or failure"
May  6 15:27:56.301: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod downward-api-4f113901-06fb-40e8-90c1-aa35788d2db4 container dapi-container: <nil>
STEP: delete the pod
May  6 15:27:56.313: INFO: Waiting for pod downward-api-4f113901-06fb-40e8-90c1-aa35788d2db4 to disappear
May  6 15:27:56.315: INFO: Pod downward-api-4f113901-06fb-40e8-90c1-aa35788d2db4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:27:56.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6167" for this suite.
May  6 15:28:02.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:28:05.078: INFO: namespace downward-api-6167 deletion completed in 8.760044021s

• [SLOW TEST:12.927 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:28:05.078: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6716
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 15:28:05.601: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 15:28:08.615: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:28:08.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6716" for this suite.
May  6 15:28:14.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:28:17.402: INFO: namespace webhook-6716 deletion completed in 8.759511336s
STEP: Destroying namespace "webhook-6716-markers" for this suite.
May  6 15:28:23.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:28:26.162: INFO: namespace webhook-6716-markers deletion completed in 8.760001439s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.094 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:28:26.172: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2616
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 15:28:26.310: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cba9127b-caa5-4c8a-976f-13db04d7cb42" in namespace "projected-2616" to be "success or failure"
May  6 15:28:26.312: INFO: Pod "downwardapi-volume-cba9127b-caa5-4c8a-976f-13db04d7cb42": Phase="Pending", Reason="", readiness=false. Elapsed: 1.870617ms
May  6 15:28:28.315: INFO: Pod "downwardapi-volume-cba9127b-caa5-4c8a-976f-13db04d7cb42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004607722s
STEP: Saw pod success
May  6 15:28:28.315: INFO: Pod "downwardapi-volume-cba9127b-caa5-4c8a-976f-13db04d7cb42" satisfied condition "success or failure"
May  6 15:28:28.317: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod downwardapi-volume-cba9127b-caa5-4c8a-976f-13db04d7cb42 container client-container: <nil>
STEP: delete the pod
May  6 15:28:28.328: INFO: Waiting for pod downwardapi-volume-cba9127b-caa5-4c8a-976f-13db04d7cb42 to disappear
May  6 15:28:28.330: INFO: Pod downwardapi-volume-cba9127b-caa5-4c8a-976f-13db04d7cb42 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:28:28.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2616" for this suite.
May  6 15:28:34.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:28:37.094: INFO: namespace projected-2616 deletion completed in 8.760085786s

• [SLOW TEST:10.923 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:28:37.095: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9213
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
May  6 15:28:37.231: INFO: Waiting up to 5m0s for pod "pod-a4f9b1ad-f382-4701-9333-8f41b535b050" in namespace "emptydir-9213" to be "success or failure"
May  6 15:28:37.233: INFO: Pod "pod-a4f9b1ad-f382-4701-9333-8f41b535b050": Phase="Pending", Reason="", readiness=false. Elapsed: 1.768414ms
May  6 15:28:39.236: INFO: Pod "pod-a4f9b1ad-f382-4701-9333-8f41b535b050": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004347966s
STEP: Saw pod success
May  6 15:28:39.236: INFO: Pod "pod-a4f9b1ad-f382-4701-9333-8f41b535b050" satisfied condition "success or failure"
May  6 15:28:39.238: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-a4f9b1ad-f382-4701-9333-8f41b535b050 container test-container: <nil>
STEP: delete the pod
May  6 15:28:39.251: INFO: Waiting for pod pod-a4f9b1ad-f382-4701-9333-8f41b535b050 to disappear
May  6 15:28:39.252: INFO: Pod pod-a4f9b1ad-f382-4701-9333-8f41b535b050 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:28:39.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9213" for this suite.
May  6 15:28:45.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:28:48.015: INFO: namespace emptydir-9213 deletion completed in 8.759269628s

• [SLOW TEST:10.920 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:28:48.015: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8208
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
May  6 15:28:50.678: INFO: Successfully updated pod "labelsupdateed34235f-efee-4373-91ec-bdf2b6a0c6c1"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:28:52.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8208" for this suite.
May  6 15:29:20.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:29:23.449: INFO: namespace downward-api-8208 deletion completed in 30.759295323s

• [SLOW TEST:35.434 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:29:23.449: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8984
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0506 15:30:03.602117      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May  6 15:30:03.602: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:30:03.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8984" for this suite.
May  6 15:30:09.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:30:12.379: INFO: namespace gc-8984 deletion completed in 8.774337969s

• [SLOW TEST:48.930 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:30:12.379: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8823
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-6399416d-c1dc-4f37-a180-4d16bce35ed7
STEP: Creating configMap with name cm-test-opt-upd-86fbd800-ea33-4085-86e1-b2dcd8a6a5de
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-6399416d-c1dc-4f37-a180-4d16bce35ed7
STEP: Updating configmap cm-test-opt-upd-86fbd800-ea33-4085-86e1-b2dcd8a6a5de
STEP: Creating configMap with name cm-test-opt-create-839502bf-14ed-444f-b098-c0814d1464e0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:31:24.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8823" for this suite.
May  6 15:31:38.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:31:41.499: INFO: namespace configmap-8823 deletion completed in 16.763438514s

• [SLOW TEST:89.120 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:31:41.499: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1391
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-5f364aa7-04a6-4a97-94ef-1dc355643aad
May  6 15:31:41.635: INFO: Pod name my-hostname-basic-5f364aa7-04a6-4a97-94ef-1dc355643aad: Found 0 pods out of 1
May  6 15:31:46.638: INFO: Pod name my-hostname-basic-5f364aa7-04a6-4a97-94ef-1dc355643aad: Found 1 pods out of 1
May  6 15:31:46.638: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-5f364aa7-04a6-4a97-94ef-1dc355643aad" are running
May  6 15:31:46.640: INFO: Pod "my-hostname-basic-5f364aa7-04a6-4a97-94ef-1dc355643aad-xbplv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-06 15:31:41 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-06 15:31:43 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-06 15:31:43 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-06 15:31:41 +0000 UTC Reason: Message:}])
May  6 15:31:46.640: INFO: Trying to dial the pod
May  6 15:31:51.648: INFO: Controller my-hostname-basic-5f364aa7-04a6-4a97-94ef-1dc355643aad: Got expected result from replica 1 [my-hostname-basic-5f364aa7-04a6-4a97-94ef-1dc355643aad-xbplv]: "my-hostname-basic-5f364aa7-04a6-4a97-94ef-1dc355643aad-xbplv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:31:51.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1391" for this suite.
May  6 15:31:57.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:32:00.411: INFO: namespace replication-controller-1391 deletion completed in 8.760060915s

• [SLOW TEST:18.912 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:32:00.411: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1676
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 15:32:01.042: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 15:32:04.055: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:32:04.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1676" for this suite.
May  6 15:32:10.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:32:12.854: INFO: namespace webhook-1676 deletion completed in 8.759467125s
STEP: Destroying namespace "webhook-1676-markers" for this suite.
May  6 15:32:18.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:32:21.617: INFO: namespace webhook-1676-markers deletion completed in 8.762290717s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.216 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:32:21.627: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1725
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May  6 15:32:21.766: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1725 /api/v1/namespaces/watch-1725/configmaps/e2e-watch-test-configmap-a 276434c5-5ff5-4681-869a-35a36e34e44c 125383 0 2020-05-06 15:32:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May  6 15:32:21.766: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1725 /api/v1/namespaces/watch-1725/configmaps/e2e-watch-test-configmap-a 276434c5-5ff5-4681-869a-35a36e34e44c 125383 0 2020-05-06 15:32:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May  6 15:32:31.772: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1725 /api/v1/namespaces/watch-1725/configmaps/e2e-watch-test-configmap-a 276434c5-5ff5-4681-869a-35a36e34e44c 125429 0 2020-05-06 15:32:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May  6 15:32:31.772: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1725 /api/v1/namespaces/watch-1725/configmaps/e2e-watch-test-configmap-a 276434c5-5ff5-4681-869a-35a36e34e44c 125429 0 2020-05-06 15:32:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May  6 15:32:41.777: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1725 /api/v1/namespaces/watch-1725/configmaps/e2e-watch-test-configmap-a 276434c5-5ff5-4681-869a-35a36e34e44c 125473 0 2020-05-06 15:32:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May  6 15:32:41.778: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1725 /api/v1/namespaces/watch-1725/configmaps/e2e-watch-test-configmap-a 276434c5-5ff5-4681-869a-35a36e34e44c 125473 0 2020-05-06 15:32:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May  6 15:32:51.783: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1725 /api/v1/namespaces/watch-1725/configmaps/e2e-watch-test-configmap-a 276434c5-5ff5-4681-869a-35a36e34e44c 125520 0 2020-05-06 15:32:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May  6 15:32:51.783: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1725 /api/v1/namespaces/watch-1725/configmaps/e2e-watch-test-configmap-a 276434c5-5ff5-4681-869a-35a36e34e44c 125520 0 2020-05-06 15:32:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May  6 15:33:01.789: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1725 /api/v1/namespaces/watch-1725/configmaps/e2e-watch-test-configmap-b 09db6957-c5ca-4d28-8582-a7281af9c2b4 125566 0 2020-05-06 15:33:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May  6 15:33:01.789: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1725 /api/v1/namespaces/watch-1725/configmaps/e2e-watch-test-configmap-b 09db6957-c5ca-4d28-8582-a7281af9c2b4 125566 0 2020-05-06 15:33:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May  6 15:33:11.796: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1725 /api/v1/namespaces/watch-1725/configmaps/e2e-watch-test-configmap-b 09db6957-c5ca-4d28-8582-a7281af9c2b4 125610 0 2020-05-06 15:33:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May  6 15:33:11.796: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1725 /api/v1/namespaces/watch-1725/configmaps/e2e-watch-test-configmap-b 09db6957-c5ca-4d28-8582-a7281af9c2b4 125610 0 2020-05-06 15:33:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:33:21.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1725" for this suite.
May  6 15:33:27.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:33:30.560: INFO: namespace watch-1725 deletion completed in 8.760115268s

• [SLOW TEST:68.933 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:33:30.560: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4305
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-4b94be2c-d736-4cde-94ee-566d4d004fc3 in namespace container-probe-4305
May  6 15:33:32.707: INFO: Started pod test-webserver-4b94be2c-d736-4cde-94ee-566d4d004fc3 in namespace container-probe-4305
STEP: checking the pod's current state and verifying that restartCount is present
May  6 15:33:32.709: INFO: Initial restart count of pod test-webserver-4b94be2c-d736-4cde-94ee-566d4d004fc3 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:37:33.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4305" for this suite.
May  6 15:37:39.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:37:41.819: INFO: namespace container-probe-4305 deletion completed in 8.76060749s

• [SLOW TEST:251.259 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:37:41.819: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-2177
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
May  6 15:37:41.952: INFO: Waiting up to 1m0s for all nodes to be ready
May  6 15:38:41.997: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 15:38:41.999: INFO: Starting informer...
STEP: Starting pods...
May  6 15:38:42.214: INFO: Pod1 is running on ip-10-0-131-80.us-west-2.compute.internal. Tainting Node
May  6 15:38:44.428: INFO: Pod2 is running on ip-10-0-131-80.us-west-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
May  6 15:38:51.880: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May  6 15:39:17.181: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:39:17.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-2177" for this suite.
May  6 15:39:23.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:39:25.976: INFO: namespace taint-multiple-pods-2177 deletion completed in 8.780300281s

• [SLOW TEST:104.156 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:39:25.976: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6411
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-6411
STEP: creating replication controller nodeport-test in namespace services-6411
I0506 15:39:26.118290      25 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-6411, replica count: 2
I0506 15:39:29.168550      25 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  6 15:39:29.168: INFO: Creating new exec pod
May  6 15:39:32.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=services-6411 execpodcbkgw -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
May  6 15:39:32.598: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May  6 15:39:32.598: INFO: stdout: ""
May  6 15:39:32.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=services-6411 execpodcbkgw -- /bin/sh -x -c nc -zv -t -w 2 10.0.1.162 80'
May  6 15:39:32.860: INFO: stderr: "+ nc -zv -t -w 2 10.0.1.162 80\nConnection to 10.0.1.162 80 port [tcp/http] succeeded!\n"
May  6 15:39:32.860: INFO: stdout: ""
May  6 15:39:32.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=services-6411 execpodcbkgw -- /bin/sh -x -c nc -zv -t -w 2 10.0.129.1 32543'
May  6 15:39:33.145: INFO: stderr: "+ nc -zv -t -w 2 10.0.129.1 32543\nConnection to 10.0.129.1 32543 port [tcp/32543] succeeded!\n"
May  6 15:39:33.145: INFO: stdout: ""
May  6 15:39:33.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=services-6411 execpodcbkgw -- /bin/sh -x -c nc -zv -t -w 2 10.0.129.108 32543'
May  6 15:39:33.403: INFO: stderr: "+ nc -zv -t -w 2 10.0.129.108 32543\nConnection to 10.0.129.108 32543 port [tcp/32543] succeeded!\n"
May  6 15:39:33.403: INFO: stdout: ""
May  6 15:39:33.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=services-6411 execpodcbkgw -- /bin/sh -x -c nc -zv -t -w 2 34.211.120.98 32543'
May  6 15:39:33.660: INFO: stderr: "+ nc -zv -t -w 2 34.211.120.98 32543\nConnection to 34.211.120.98 32543 port [tcp/32543] succeeded!\n"
May  6 15:39:33.660: INFO: stdout: ""
May  6 15:39:33.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=services-6411 execpodcbkgw -- /bin/sh -x -c nc -zv -t -w 2 54.148.156.50 32543'
May  6 15:39:33.916: INFO: stderr: "+ nc -zv -t -w 2 54.148.156.50 32543\nConnection to 54.148.156.50 32543 port [tcp/32543] succeeded!\n"
May  6 15:39:33.916: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:39:33.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6411" for this suite.
May  6 15:39:39.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:39:42.679: INFO: namespace services-6411 deletion completed in 8.759697091s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:16.703 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:39:42.679: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6828
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
May  6 15:39:42.809: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 15:39:47.649: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:40:04.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6828" for this suite.
May  6 15:40:10.704: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:40:13.456: INFO: namespace crd-publish-openapi-6828 deletion completed in 8.759491682s

• [SLOW TEST:30.777 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:40:13.456: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2778
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
May  6 15:40:13.595: INFO: Waiting up to 5m0s for pod "pod-c68f20d8-0c45-431b-a5df-969018b7ac10" in namespace "emptydir-2778" to be "success or failure"
May  6 15:40:13.597: INFO: Pod "pod-c68f20d8-0c45-431b-a5df-969018b7ac10": Phase="Pending", Reason="", readiness=false. Elapsed: 1.805014ms
May  6 15:40:15.599: INFO: Pod "pod-c68f20d8-0c45-431b-a5df-969018b7ac10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004568974s
STEP: Saw pod success
May  6 15:40:15.599: INFO: Pod "pod-c68f20d8-0c45-431b-a5df-969018b7ac10" satisfied condition "success or failure"
May  6 15:40:15.602: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-c68f20d8-0c45-431b-a5df-969018b7ac10 container test-container: <nil>
STEP: delete the pod
May  6 15:40:15.620: INFO: Waiting for pod pod-c68f20d8-0c45-431b-a5df-969018b7ac10 to disappear
May  6 15:40:15.622: INFO: Pod pod-c68f20d8-0c45-431b-a5df-969018b7ac10 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:40:15.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2778" for this suite.
May  6 15:40:21.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:40:24.385: INFO: namespace emptydir-2778 deletion completed in 8.759496772s

• [SLOW TEST:10.929 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:40:24.385: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8769
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-ff591c15-d005-4ae9-85b7-884c22bc45c1
STEP: Creating secret with name s-test-opt-upd-6ea51deb-23cc-4ff4-82af-12b3edd2aad4
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ff591c15-d005-4ae9-85b7-884c22bc45c1
STEP: Updating secret s-test-opt-upd-6ea51deb-23cc-4ff4-82af-12b3edd2aad4
STEP: Creating secret with name s-test-opt-create-961c158c-9b80-400b-b2d3-968dd8040f91
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:41:38.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8769" for this suite.
May  6 15:41:50.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:41:53.506: INFO: namespace secrets-8769 deletion completed in 14.759315604s

• [SLOW TEST:89.121 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:41:53.506: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5527
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
May  6 15:41:53.637: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

May  6 15:41:53.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 create -f - --namespace=kubectl-5527'
May  6 15:41:53.897: INFO: stderr: ""
May  6 15:41:53.897: INFO: stdout: "service/redis-slave created\n"
May  6 15:41:53.897: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

May  6 15:41:53.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 create -f - --namespace=kubectl-5527'
May  6 15:41:54.087: INFO: stderr: ""
May  6 15:41:54.087: INFO: stdout: "service/redis-master created\n"
May  6 15:41:54.088: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May  6 15:41:54.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 create -f - --namespace=kubectl-5527'
May  6 15:41:54.278: INFO: stderr: ""
May  6 15:41:54.278: INFO: stdout: "service/frontend created\n"
May  6 15:41:54.278: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

May  6 15:41:54.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 create -f - --namespace=kubectl-5527'
May  6 15:41:54.538: INFO: stderr: ""
May  6 15:41:54.538: INFO: stdout: "deployment.apps/frontend created\n"
May  6 15:41:54.539: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May  6 15:41:54.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 create -f - --namespace=kubectl-5527'
May  6 15:41:54.724: INFO: stderr: ""
May  6 15:41:54.724: INFO: stdout: "deployment.apps/redis-master created\n"
May  6 15:41:54.724: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

May  6 15:41:54.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 create -f - --namespace=kubectl-5527'
May  6 15:41:54.910: INFO: stderr: ""
May  6 15:41:54.910: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
May  6 15:41:54.910: INFO: Waiting for all frontend pods to be Running.
May  6 15:41:59.960: INFO: Waiting for frontend to serve content.
May  6 15:41:59.971: INFO: Trying to add a new entry to the guestbook.
May  6 15:41:59.981: INFO: Verifying that added entry can be retrieved.
May  6 15:41:59.988: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
May  6 15:42:04.998: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
May  6 15:42:10.008: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
May  6 15:42:15.020: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
May  6 15:42:20.030: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
May  6 15:42:25.040: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
May  6 15:42:30.050: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
May  6 15:42:35.060: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
May  6 15:42:40.070: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
May  6 15:42:45.080: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
May  6 15:42:50.090: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
May  6 15:42:55.099: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
May  6 15:43:00.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 delete --grace-period=0 --force -f - --namespace=kubectl-5527'
May  6 15:43:00.195: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 15:43:00.196: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
May  6 15:43:00.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 delete --grace-period=0 --force -f - --namespace=kubectl-5527'
May  6 15:43:00.296: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 15:43:00.296: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May  6 15:43:00.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 delete --grace-period=0 --force -f - --namespace=kubectl-5527'
May  6 15:43:00.386: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 15:43:00.386: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May  6 15:43:00.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 delete --grace-period=0 --force -f - --namespace=kubectl-5527'
May  6 15:43:00.462: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 15:43:00.462: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May  6 15:43:00.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 delete --grace-period=0 --force -f - --namespace=kubectl-5527'
May  6 15:43:00.538: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 15:43:00.538: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May  6 15:43:00.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 delete --grace-period=0 --force -f - --namespace=kubectl-5527'
May  6 15:43:00.613: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 15:43:00.613: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:43:00.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5527" for this suite.
May  6 15:43:12.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:43:15.375: INFO: namespace kubectl-5527 deletion completed in 14.759152766s

• [SLOW TEST:81.869 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:43:15.376: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9421
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
May  6 15:43:15.506: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-926700516 proxy --unix-socket=/tmp/kubectl-proxy-unix323903887/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:43:15.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9421" for this suite.
May  6 15:43:21.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:43:24.314: INFO: namespace kubectl-9421 deletion completed in 8.760579058s

• [SLOW TEST:8.938 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:43:24.314: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5604
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
May  6 15:43:24.458: INFO: Waiting up to 5m0s for pod "pod-f3962013-e544-4615-b8bd-7e013e6b78cb" in namespace "emptydir-5604" to be "success or failure"
May  6 15:43:24.460: INFO: Pod "pod-f3962013-e544-4615-b8bd-7e013e6b78cb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.791625ms
May  6 15:43:26.463: INFO: Pod "pod-f3962013-e544-4615-b8bd-7e013e6b78cb": Phase="Running", Reason="", readiness=true. Elapsed: 2.004637159s
May  6 15:43:28.466: INFO: Pod "pod-f3962013-e544-4615-b8bd-7e013e6b78cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007644547s
STEP: Saw pod success
May  6 15:43:28.466: INFO: Pod "pod-f3962013-e544-4615-b8bd-7e013e6b78cb" satisfied condition "success or failure"
May  6 15:43:28.468: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-f3962013-e544-4615-b8bd-7e013e6b78cb container test-container: <nil>
STEP: delete the pod
May  6 15:43:28.485: INFO: Waiting for pod pod-f3962013-e544-4615-b8bd-7e013e6b78cb to disappear
May  6 15:43:28.487: INFO: Pod pod-f3962013-e544-4615-b8bd-7e013e6b78cb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:43:28.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5604" for this suite.
May  6 15:43:34.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:43:37.250: INFO: namespace emptydir-5604 deletion completed in 8.760693033s

• [SLOW TEST:12.936 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:43:37.250: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2344
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 15:43:38.007: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  6 15:43:40.014: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724376618, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724376618, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724376618, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724376618, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 15:43:43.023: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:43:55.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2344" for this suite.
May  6 15:44:01.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:44:03.871: INFO: namespace webhook-2344 deletion completed in 8.763094885s
STEP: Destroying namespace "webhook-2344-markers" for this suite.
May  6 15:44:09.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:44:12.639: INFO: namespace webhook-2344-markers deletion completed in 8.768212987s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:35.399 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:44:12.649: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-1621
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:44:12.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1621" for this suite.
May  6 15:44:18.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:44:21.544: INFO: namespace tables-1621 deletion completed in 8.761556625s

• [SLOW TEST:8.895 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:44:21.544: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-938
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-938/secret-test-0497909e-bd70-4966-8827-13fc000e9497
STEP: Creating a pod to test consume secrets
May  6 15:44:21.684: INFO: Waiting up to 5m0s for pod "pod-configmaps-a96eb062-eafa-46f4-9f45-6f33d0db8bb8" in namespace "secrets-938" to be "success or failure"
May  6 15:44:21.689: INFO: Pod "pod-configmaps-a96eb062-eafa-46f4-9f45-6f33d0db8bb8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.352289ms
May  6 15:44:23.692: INFO: Pod "pod-configmaps-a96eb062-eafa-46f4-9f45-6f33d0db8bb8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007833888s
May  6 15:44:25.695: INFO: Pod "pod-configmaps-a96eb062-eafa-46f4-9f45-6f33d0db8bb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010581425s
STEP: Saw pod success
May  6 15:44:25.695: INFO: Pod "pod-configmaps-a96eb062-eafa-46f4-9f45-6f33d0db8bb8" satisfied condition "success or failure"
May  6 15:44:25.696: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-configmaps-a96eb062-eafa-46f4-9f45-6f33d0db8bb8 container env-test: <nil>
STEP: delete the pod
May  6 15:44:25.708: INFO: Waiting for pod pod-configmaps-a96eb062-eafa-46f4-9f45-6f33d0db8bb8 to disappear
May  6 15:44:25.709: INFO: Pod pod-configmaps-a96eb062-eafa-46f4-9f45-6f33d0db8bb8 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:44:25.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-938" for this suite.
May  6 15:44:31.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:44:34.471: INFO: namespace secrets-938 deletion completed in 8.759173508s

• [SLOW TEST:12.927 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:44:34.472: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9369
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:44:36.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9369" for this suite.
May  6 15:44:50.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:44:53.384: INFO: namespace containers-9369 deletion completed in 16.759178886s

• [SLOW TEST:18.912 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:44:53.384: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-8114
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May  6 15:44:55.530: INFO: &Pod{ObjectMeta:{send-events-1a971b63-604b-491a-aedb-c408ba19acd6  events-8114 /api/v1/namespaces/events-8114/pods/send-events-1a971b63-604b-491a-aedb-c408ba19acd6 5aa316c4-a760-4e35-bbc5-f0d9c7184ec5 129600 0 2020-05-06 15:44:53 +0000 UTC <nil> <nil> map[name:foo time:514065580] map[cni.projectcalico.org/podIP:192.168.85.122/32 cni.projectcalico.org/podIPs:192.168.85.122/32] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-x5pwx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-x5pwx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-x5pwx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-80.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 15:44:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 15:44:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 15:44:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 15:44:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.80,PodIP:192.168.85.122,StartTime:2020-05-06 15:44:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 15:44:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:containerd://b3643010b50f736cd87e64119338bea5fb7834023d7ca75b98fed5637a3c8a79,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.85.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
May  6 15:44:57.533: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May  6 15:44:59.535: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:44:59.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8114" for this suite.
May  6 15:45:45.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:45:48.304: INFO: namespace events-8114 deletion completed in 48.761235089s

• [SLOW TEST:54.920 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:45:48.304: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5935
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
May  6 15:45:48.441: INFO: Waiting up to 5m0s for pod "var-expansion-e1584496-2e3c-4e1b-9e6b-c11a4d2b6463" in namespace "var-expansion-5935" to be "success or failure"
May  6 15:45:48.443: INFO: Pod "var-expansion-e1584496-2e3c-4e1b-9e6b-c11a4d2b6463": Phase="Pending", Reason="", readiness=false. Elapsed: 1.836002ms
May  6 15:45:50.446: INFO: Pod "var-expansion-e1584496-2e3c-4e1b-9e6b-c11a4d2b6463": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004390914s
May  6 15:45:52.448: INFO: Pod "var-expansion-e1584496-2e3c-4e1b-9e6b-c11a4d2b6463": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00693227s
STEP: Saw pod success
May  6 15:45:52.448: INFO: Pod "var-expansion-e1584496-2e3c-4e1b-9e6b-c11a4d2b6463" satisfied condition "success or failure"
May  6 15:45:52.450: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod var-expansion-e1584496-2e3c-4e1b-9e6b-c11a4d2b6463 container dapi-container: <nil>
STEP: delete the pod
May  6 15:45:52.465: INFO: Waiting for pod var-expansion-e1584496-2e3c-4e1b-9e6b-c11a4d2b6463 to disappear
May  6 15:45:52.466: INFO: Pod var-expansion-e1584496-2e3c-4e1b-9e6b-c11a4d2b6463 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:45:52.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5935" for this suite.
May  6 15:45:58.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:46:01.231: INFO: namespace var-expansion-5935 deletion completed in 8.761873449s

• [SLOW TEST:12.927 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:46:01.231: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4974
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-4974
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May  6 15:46:01.362: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May  6 15:46:23.440: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.80.156 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4974 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 15:46:23.440: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 15:46:24.595: INFO: Found all expected endpoints: [netserver-0]
May  6 15:46:24.597: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.187.23 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4974 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 15:46:24.597: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 15:46:25.750: INFO: Found all expected endpoints: [netserver-1]
May  6 15:46:25.753: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.85.72 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4974 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 15:46:25.753: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 15:46:26.905: INFO: Found all expected endpoints: [netserver-2]
May  6 15:46:26.908: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.121.54 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4974 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 15:46:26.908: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 15:46:28.060: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:46:28.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4974" for this suite.
May  6 15:46:40.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:46:42.844: INFO: namespace pod-network-test-4974 deletion completed in 14.777414287s

• [SLOW TEST:41.613 seconds]
[sig-network] Networking
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:46:42.844: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6599
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 15:46:42.975: INFO: Creating ReplicaSet my-hostname-basic-40097527-c26e-4862-9fab-6764bc280df3
May  6 15:46:42.981: INFO: Pod name my-hostname-basic-40097527-c26e-4862-9fab-6764bc280df3: Found 0 pods out of 1
May  6 15:46:47.984: INFO: Pod name my-hostname-basic-40097527-c26e-4862-9fab-6764bc280df3: Found 1 pods out of 1
May  6 15:46:47.984: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-40097527-c26e-4862-9fab-6764bc280df3" is running
May  6 15:46:47.985: INFO: Pod "my-hostname-basic-40097527-c26e-4862-9fab-6764bc280df3-vj26j" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-06 15:46:42 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-06 15:46:44 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-06 15:46:44 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-06 15:46:42 +0000 UTC Reason: Message:}])
May  6 15:46:47.986: INFO: Trying to dial the pod
May  6 15:46:52.993: INFO: Controller my-hostname-basic-40097527-c26e-4862-9fab-6764bc280df3: Got expected result from replica 1 [my-hostname-basic-40097527-c26e-4862-9fab-6764bc280df3-vj26j]: "my-hostname-basic-40097527-c26e-4862-9fab-6764bc280df3-vj26j", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:46:52.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6599" for this suite.
May  6 15:46:59.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:47:01.758: INFO: namespace replicaset-6599 deletion completed in 8.761427589s

• [SLOW TEST:18.913 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:47:01.758: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4387
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 15:47:02.379: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  6 15:47:04.386: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724376822, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724376822, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724376822, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724376822, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 15:47:07.395: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
May  6 15:47:09.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 attach --namespace=webhook-4387 to-be-attached-pod -i -c=container1'
May  6 15:47:09.522: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:47:09.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4387" for this suite.
May  6 15:47:21.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:47:24.290: INFO: namespace webhook-4387 deletion completed in 14.759521195s
STEP: Destroying namespace "webhook-4387-markers" for this suite.
May  6 15:47:30.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:47:33.059: INFO: namespace webhook-4387-markers deletion completed in 8.768401961s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:31.311 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:47:33.069: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3384
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 15:47:33.206: INFO: Waiting up to 5m0s for pod "downwardapi-volume-af30fc50-42ea-492f-800a-f71cde3c8374" in namespace "projected-3384" to be "success or failure"
May  6 15:47:33.211: INFO: Pod "downwardapi-volume-af30fc50-42ea-492f-800a-f71cde3c8374": Phase="Pending", Reason="", readiness=false. Elapsed: 5.625615ms
May  6 15:47:35.214: INFO: Pod "downwardapi-volume-af30fc50-42ea-492f-800a-f71cde3c8374": Phase="Running", Reason="", readiness=true. Elapsed: 2.008280563s
May  6 15:47:37.217: INFO: Pod "downwardapi-volume-af30fc50-42ea-492f-800a-f71cde3c8374": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011137436s
STEP: Saw pod success
May  6 15:47:37.217: INFO: Pod "downwardapi-volume-af30fc50-42ea-492f-800a-f71cde3c8374" satisfied condition "success or failure"
May  6 15:47:37.219: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod downwardapi-volume-af30fc50-42ea-492f-800a-f71cde3c8374 container client-container: <nil>
STEP: delete the pod
May  6 15:47:37.235: INFO: Waiting for pod downwardapi-volume-af30fc50-42ea-492f-800a-f71cde3c8374 to disappear
May  6 15:47:37.237: INFO: Pod downwardapi-volume-af30fc50-42ea-492f-800a-f71cde3c8374 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:47:37.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3384" for this suite.
May  6 15:47:43.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:47:45.999: INFO: namespace projected-3384 deletion completed in 8.75938939s

• [SLOW TEST:12.930 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:47:45.999: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7781
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 15:47:46.473: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  6 15:47:48.479: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724376866, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724376866, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724376866, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724376866, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 15:47:51.488: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:48:01.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7781" for this suite.
May  6 15:48:07.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:48:10.318: INFO: namespace webhook-7781 deletion completed in 8.759611674s
STEP: Destroying namespace "webhook-7781-markers" for this suite.
May  6 15:48:16.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:48:19.078: INFO: namespace webhook-7781-markers deletion completed in 8.759896858s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:33.089 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:48:19.088: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3891
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-78d5d4da-c225-4b60-b99e-51221532dfd3
STEP: Creating a pod to test consume configMaps
May  6 15:48:19.227: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-090b3d21-717b-4200-89d8-f9021ad967f3" in namespace "projected-3891" to be "success or failure"
May  6 15:48:19.232: INFO: Pod "pod-projected-configmaps-090b3d21-717b-4200-89d8-f9021ad967f3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.678819ms
May  6 15:48:21.235: INFO: Pod "pod-projected-configmaps-090b3d21-717b-4200-89d8-f9021ad967f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007495024s
STEP: Saw pod success
May  6 15:48:21.235: INFO: Pod "pod-projected-configmaps-090b3d21-717b-4200-89d8-f9021ad967f3" satisfied condition "success or failure"
May  6 15:48:21.237: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-projected-configmaps-090b3d21-717b-4200-89d8-f9021ad967f3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  6 15:48:21.249: INFO: Waiting for pod pod-projected-configmaps-090b3d21-717b-4200-89d8-f9021ad967f3 to disappear
May  6 15:48:21.251: INFO: Pod pod-projected-configmaps-090b3d21-717b-4200-89d8-f9021ad967f3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:48:21.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3891" for this suite.
May  6 15:48:27.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:48:30.013: INFO: namespace projected-3891 deletion completed in 8.759511354s

• [SLOW TEST:10.925 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:48:30.013: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3042
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 15:48:30.543: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 15:48:33.556: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:48:33.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3042" for this suite.
May  6 15:48:39.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:48:42.367: INFO: namespace webhook-3042 deletion completed in 8.758937389s
STEP: Destroying namespace "webhook-3042-markers" for this suite.
May  6 15:48:48.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:48:51.128: INFO: namespace webhook-3042-markers deletion completed in 8.760448962s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.125 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:48:51.138: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6285
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:48:59.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6285" for this suite.
May  6 15:49:05.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:49:08.041: INFO: namespace job-6285 deletion completed in 8.760980428s

• [SLOW TEST:16.903 seconds]
[sig-apps] Job
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:49:08.041: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9348
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-46e564e5-cff6-43bc-a847-c2b88fbb261d
STEP: Creating secret with name s-test-opt-upd-a51f7e2a-485a-4004-93e5-81c6ba45688e
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-46e564e5-cff6-43bc-a847-c2b88fbb261d
STEP: Updating secret s-test-opt-upd-a51f7e2a-485a-4004-93e5-81c6ba45688e
STEP: Creating secret with name s-test-opt-create-fd1caeb4-55e8-42ad-b602-9e8160b91f3e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:50:38.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9348" for this suite.
May  6 15:50:50.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:50:53.221: INFO: namespace projected-9348 deletion completed in 14.759878749s

• [SLOW TEST:105.180 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:50:53.221: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4250
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
May  6 15:50:53.362: INFO: Waiting up to 5m0s for pod "pod-40bda688-d812-4a4e-a400-4389bd29c2f6" in namespace "emptydir-4250" to be "success or failure"
May  6 15:50:53.364: INFO: Pod "pod-40bda688-d812-4a4e-a400-4389bd29c2f6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.809001ms
May  6 15:50:55.366: INFO: Pod "pod-40bda688-d812-4a4e-a400-4389bd29c2f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004501201s
STEP: Saw pod success
May  6 15:50:55.367: INFO: Pod "pod-40bda688-d812-4a4e-a400-4389bd29c2f6" satisfied condition "success or failure"
May  6 15:50:55.368: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-40bda688-d812-4a4e-a400-4389bd29c2f6 container test-container: <nil>
STEP: delete the pod
May  6 15:50:55.382: INFO: Waiting for pod pod-40bda688-d812-4a4e-a400-4389bd29c2f6 to disappear
May  6 15:50:55.384: INFO: Pod pod-40bda688-d812-4a4e-a400-4389bd29c2f6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:50:55.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4250" for this suite.
May  6 15:51:01.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:51:04.146: INFO: namespace emptydir-4250 deletion completed in 8.759648321s

• [SLOW TEST:10.925 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:51:04.146: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5363
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5363.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5363.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  6 15:51:08.310: INFO: DNS probes using dns-5363/dns-test-82652ffa-18f3-4a3b-adf6-e7125ef80c63 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:51:08.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5363" for this suite.
May  6 15:51:14.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:51:17.090: INFO: namespace dns-5363 deletion completed in 8.762930338s

• [SLOW TEST:12.944 seconds]
[sig-network] DNS
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:51:17.090: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-467
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
May  6 15:51:17.222: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:51:21.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-467" for this suite.
May  6 15:51:27.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:51:30.014: INFO: namespace init-container-467 deletion completed in 8.761851886s

• [SLOW TEST:12.924 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:51:30.015: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4410
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-e0274356-0278-423a-82c3-4bf672f3fa25
STEP: Creating a pod to test consume configMaps
May  6 15:51:30.155: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b5065422-1ca4-4a21-8241-100694c956c0" in namespace "projected-4410" to be "success or failure"
May  6 15:51:30.157: INFO: Pod "pod-projected-configmaps-b5065422-1ca4-4a21-8241-100694c956c0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.785907ms
May  6 15:51:32.160: INFO: Pod "pod-projected-configmaps-b5065422-1ca4-4a21-8241-100694c956c0": Phase="Running", Reason="", readiness=true. Elapsed: 2.004583415s
May  6 15:51:34.163: INFO: Pod "pod-projected-configmaps-b5065422-1ca4-4a21-8241-100694c956c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007118184s
STEP: Saw pod success
May  6 15:51:34.163: INFO: Pod "pod-projected-configmaps-b5065422-1ca4-4a21-8241-100694c956c0" satisfied condition "success or failure"
May  6 15:51:34.165: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-projected-configmaps-b5065422-1ca4-4a21-8241-100694c956c0 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  6 15:51:34.176: INFO: Waiting for pod pod-projected-configmaps-b5065422-1ca4-4a21-8241-100694c956c0 to disappear
May  6 15:51:34.178: INFO: Pod pod-projected-configmaps-b5065422-1ca4-4a21-8241-100694c956c0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:51:34.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4410" for this suite.
May  6 15:51:40.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:51:42.940: INFO: namespace projected-4410 deletion completed in 8.759310341s

• [SLOW TEST:12.926 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:51:42.940: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4215
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
May  6 15:51:43.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-4215 -- logs-generator --log-lines-total 100 --run-duration 20s'
May  6 15:51:43.311: INFO: stderr: ""
May  6 15:51:43.311: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
May  6 15:51:43.311: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May  6 15:51:43.311: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4215" to be "running and ready, or succeeded"
May  6 15:51:43.314: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.564039ms
May  6 15:51:45.316: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.005100599s
May  6 15:51:45.316: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May  6 15:51:45.316: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
May  6 15:51:45.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 logs logs-generator logs-generator --namespace=kubectl-4215'
May  6 15:51:45.419: INFO: stderr: ""
May  6 15:51:45.419: INFO: stdout: "I0506 15:51:44.425005       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/8vnj 259\nI0506 15:51:44.625119       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/9nwd 485\nI0506 15:51:44.825120       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/j8h7 483\nI0506 15:51:45.025114       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/hvj 228\nI0506 15:51:45.225134       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/t6zm 384\n"
STEP: limiting log lines
May  6 15:51:45.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 logs logs-generator logs-generator --namespace=kubectl-4215 --tail=1'
May  6 15:51:45.521: INFO: stderr: ""
May  6 15:51:45.521: INFO: stdout: "I0506 15:51:45.425115       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/fj86 295\n"
STEP: limiting log bytes
May  6 15:51:45.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 logs logs-generator logs-generator --namespace=kubectl-4215 --limit-bytes=1'
May  6 15:51:45.619: INFO: stderr: ""
May  6 15:51:45.619: INFO: stdout: "I"
STEP: exposing timestamps
May  6 15:51:45.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 logs logs-generator logs-generator --namespace=kubectl-4215 --tail=1 --timestamps'
May  6 15:51:45.716: INFO: stderr: ""
May  6 15:51:45.716: INFO: stdout: "2020-05-06T15:51:45.625223752Z I0506 15:51:45.625125       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/rt66 526\n"
STEP: restricting to a time range
May  6 15:51:48.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 logs logs-generator logs-generator --namespace=kubectl-4215 --since=1s'
May  6 15:51:48.318: INFO: stderr: ""
May  6 15:51:48.318: INFO: stdout: "I0506 15:51:47.425121       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/k7d 420\nI0506 15:51:47.625117       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/kdkf 355\nI0506 15:51:47.825131       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/s9s 387\nI0506 15:51:48.025131       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/bl7r 451\nI0506 15:51:48.225115       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/z4md 589\n"
May  6 15:51:48.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 logs logs-generator logs-generator --namespace=kubectl-4215 --since=24h'
May  6 15:51:48.413: INFO: stderr: ""
May  6 15:51:48.413: INFO: stdout: "I0506 15:51:44.425005       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/8vnj 259\nI0506 15:51:44.625119       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/9nwd 485\nI0506 15:51:44.825120       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/j8h7 483\nI0506 15:51:45.025114       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/hvj 228\nI0506 15:51:45.225134       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/t6zm 384\nI0506 15:51:45.425115       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/fj86 295\nI0506 15:51:45.625125       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/rt66 526\nI0506 15:51:45.825132       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/zgxn 441\nI0506 15:51:46.025133       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/j2b 253\nI0506 15:51:46.225131       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/dhb 222\nI0506 15:51:46.425121       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/9sn 484\nI0506 15:51:46.625124       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/klcm 228\nI0506 15:51:46.825118       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/zqh 503\nI0506 15:51:47.025112       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/nb5g 537\nI0506 15:51:47.225130       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/pc8 476\nI0506 15:51:47.425121       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/k7d 420\nI0506 15:51:47.625117       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/kdkf 355\nI0506 15:51:47.825131       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/s9s 387\nI0506 15:51:48.025131       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/bl7r 451\nI0506 15:51:48.225115       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/z4md 589\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
May  6 15:51:48.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 delete pod logs-generator --namespace=kubectl-4215'
May  6 15:51:57.187: INFO: stderr: ""
May  6 15:51:57.187: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:51:57.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4215" for this suite.
May  6 15:52:03.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:52:05.966: INFO: namespace kubectl-4215 deletion completed in 8.773634403s

• [SLOW TEST:23.026 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:52:05.966: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1948
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:52:17.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1948" for this suite.
May  6 15:52:23.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:52:25.886: INFO: namespace resourcequota-1948 deletion completed in 8.759533935s

• [SLOW TEST:19.920 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:52:25.886: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8275
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
May  6 15:52:26.024: INFO: Waiting up to 5m0s for pod "var-expansion-fb7b7267-1fe9-48f6-9a70-df2b2b129a14" in namespace "var-expansion-8275" to be "success or failure"
May  6 15:52:26.026: INFO: Pod "var-expansion-fb7b7267-1fe9-48f6-9a70-df2b2b129a14": Phase="Pending", Reason="", readiness=false. Elapsed: 1.900344ms
May  6 15:52:28.028: INFO: Pod "var-expansion-fb7b7267-1fe9-48f6-9a70-df2b2b129a14": Phase="Running", Reason="", readiness=true. Elapsed: 2.0048149s
May  6 15:52:30.031: INFO: Pod "var-expansion-fb7b7267-1fe9-48f6-9a70-df2b2b129a14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007608384s
STEP: Saw pod success
May  6 15:52:30.031: INFO: Pod "var-expansion-fb7b7267-1fe9-48f6-9a70-df2b2b129a14" satisfied condition "success or failure"
May  6 15:52:30.033: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod var-expansion-fb7b7267-1fe9-48f6-9a70-df2b2b129a14 container dapi-container: <nil>
STEP: delete the pod
May  6 15:52:30.044: INFO: Waiting for pod var-expansion-fb7b7267-1fe9-48f6-9a70-df2b2b129a14 to disappear
May  6 15:52:30.046: INFO: Pod var-expansion-fb7b7267-1fe9-48f6-9a70-df2b2b129a14 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:52:30.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8275" for this suite.
May  6 15:52:36.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:52:38.821: INFO: namespace var-expansion-8275 deletion completed in 8.772714424s

• [SLOW TEST:12.935 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:52:38.821: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1993
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 15:52:38.978: INFO: (0) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 14.379084ms)
May  6 15:52:38.981: INFO: (1) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.61532ms)
May  6 15:52:38.983: INFO: (2) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.459842ms)
May  6 15:52:38.985: INFO: (3) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.299659ms)
May  6 15:52:38.988: INFO: (4) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.271565ms)
May  6 15:52:38.990: INFO: (5) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.267981ms)
May  6 15:52:38.992: INFO: (6) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.25292ms)
May  6 15:52:38.994: INFO: (7) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.11927ms)
May  6 15:52:38.997: INFO: (8) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.202125ms)
May  6 15:52:38.999: INFO: (9) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.089317ms)
May  6 15:52:39.001: INFO: (10) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.238481ms)
May  6 15:52:39.003: INFO: (11) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.3891ms)
May  6 15:52:39.006: INFO: (12) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.407774ms)
May  6 15:52:39.008: INFO: (13) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.207423ms)
May  6 15:52:39.010: INFO: (14) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.414623ms)
May  6 15:52:39.013: INFO: (15) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.212752ms)
May  6 15:52:39.015: INFO: (16) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.207071ms)
May  6 15:52:39.017: INFO: (17) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.210214ms)
May  6 15:52:39.019: INFO: (18) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.102307ms)
May  6 15:52:39.021: INFO: (19) /api/v1/nodes/ip-10-0-129-1.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 2.179073ms)
[AfterEach] version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:52:39.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1993" for this suite.
May  6 15:52:45.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:52:45.198: INFO: namespace proxy-1993 deletion completed in 6.173445746s

• [SLOW TEST:6.376 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:52:45.198: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-117
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
May  6 15:52:45.327: INFO: namespace kubectl-117
May  6 15:52:45.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 create -f - --namespace=kubectl-117'
May  6 15:52:45.513: INFO: stderr: ""
May  6 15:52:45.513: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May  6 15:52:46.515: INFO: Selector matched 1 pods for map[app:redis]
May  6 15:52:46.515: INFO: Found 0 / 1
May  6 15:52:47.516: INFO: Selector matched 1 pods for map[app:redis]
May  6 15:52:47.516: INFO: Found 1 / 1
May  6 15:52:47.516: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May  6 15:52:47.518: INFO: Selector matched 1 pods for map[app:redis]
May  6 15:52:47.518: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  6 15:52:47.518: INFO: wait on redis-master startup in kubectl-117 
May  6 15:52:47.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 logs redis-master-8tqkq redis-master --namespace=kubectl-117'
May  6 15:52:47.624: INFO: stderr: ""
May  6 15:52:47.624: INFO: stdout: "1:C 06 May 2020 15:52:46.943 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 06 May 2020 15:52:46.943 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 06 May 2020 15:52:46.943 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 06 May 2020 15:52:46.944 * Running mode=standalone, port=6379.\n1:M 06 May 2020 15:52:46.945 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 May 2020 15:52:46.945 # Server initialized\n1:M 06 May 2020 15:52:46.945 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 May 2020 15:52:46.945 * Ready to accept connections\n"
STEP: exposing RC
May  6 15:52:47.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-117'
May  6 15:52:47.766: INFO: stderr: ""
May  6 15:52:47.766: INFO: stdout: "service/rm2 exposed\n"
May  6 15:52:47.769: INFO: Service rm2 in namespace kubectl-117 found.
STEP: exposing service
May  6 15:52:49.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-117'
May  6 15:52:49.901: INFO: stderr: ""
May  6 15:52:49.901: INFO: stdout: "service/rm3 exposed\n"
May  6 15:52:49.903: INFO: Service rm3 in namespace kubectl-117 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:52:51.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-117" for this suite.
May  6 15:53:03.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:53:06.675: INFO: namespace kubectl-117 deletion completed in 14.764288638s

• [SLOW TEST:21.478 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:53:06.676: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8892
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-340c4fb0-5709-4488-ab29-4fdbe65d221d
STEP: Creating a pod to test consume configMaps
May  6 15:53:06.819: INFO: Waiting up to 5m0s for pod "pod-configmaps-c8066d0d-df0c-4014-8b4b-522806d117fd" in namespace "configmap-8892" to be "success or failure"
May  6 15:53:06.822: INFO: Pod "pod-configmaps-c8066d0d-df0c-4014-8b4b-522806d117fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.616606ms
May  6 15:53:08.824: INFO: Pod "pod-configmaps-c8066d0d-df0c-4014-8b4b-522806d117fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005100739s
STEP: Saw pod success
May  6 15:53:08.824: INFO: Pod "pod-configmaps-c8066d0d-df0c-4014-8b4b-522806d117fd" satisfied condition "success or failure"
May  6 15:53:08.826: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-configmaps-c8066d0d-df0c-4014-8b4b-522806d117fd container configmap-volume-test: <nil>
STEP: delete the pod
May  6 15:53:08.839: INFO: Waiting for pod pod-configmaps-c8066d0d-df0c-4014-8b4b-522806d117fd to disappear
May  6 15:53:08.841: INFO: Pod pod-configmaps-c8066d0d-df0c-4014-8b4b-522806d117fd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:53:08.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8892" for this suite.
May  6 15:53:14.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:53:17.603: INFO: namespace configmap-8892 deletion completed in 8.759110415s

• [SLOW TEST:10.927 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:53:17.603: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5848
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 15:53:17.733: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:53:19.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5848" for this suite.
May  6 15:54:03.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:54:06.702: INFO: namespace pods-5848 deletion completed in 46.773446675s

• [SLOW TEST:49.099 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:54:06.702: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6802
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
May  6 15:54:06.841: INFO: Waiting up to 5m0s for pod "pod-e468213a-1d94-415b-8411-3c1bd4d7b2d4" in namespace "emptydir-6802" to be "success or failure"
May  6 15:54:06.843: INFO: Pod "pod-e468213a-1d94-415b-8411-3c1bd4d7b2d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.867578ms
May  6 15:54:08.846: INFO: Pod "pod-e468213a-1d94-415b-8411-3c1bd4d7b2d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004568185s
STEP: Saw pod success
May  6 15:54:08.846: INFO: Pod "pod-e468213a-1d94-415b-8411-3c1bd4d7b2d4" satisfied condition "success or failure"
May  6 15:54:08.848: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-e468213a-1d94-415b-8411-3c1bd4d7b2d4 container test-container: <nil>
STEP: delete the pod
May  6 15:54:08.860: INFO: Waiting for pod pod-e468213a-1d94-415b-8411-3c1bd4d7b2d4 to disappear
May  6 15:54:08.862: INFO: Pod pod-e468213a-1d94-415b-8411-3c1bd4d7b2d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:54:08.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6802" for this suite.
May  6 15:54:14.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:54:17.624: INFO: namespace emptydir-6802 deletion completed in 8.759624897s

• [SLOW TEST:10.922 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:54:17.625: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8019
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 15:54:17.765: INFO: Waiting up to 5m0s for pod "downwardapi-volume-66af6735-9bd0-4e40-a271-0a078f2452ff" in namespace "downward-api-8019" to be "success or failure"
May  6 15:54:17.767: INFO: Pod "downwardapi-volume-66af6735-9bd0-4e40-a271-0a078f2452ff": Phase="Pending", Reason="", readiness=false. Elapsed: 1.984446ms
May  6 15:54:19.770: INFO: Pod "downwardapi-volume-66af6735-9bd0-4e40-a271-0a078f2452ff": Phase="Running", Reason="", readiness=true. Elapsed: 2.004654457s
May  6 15:54:21.773: INFO: Pod "downwardapi-volume-66af6735-9bd0-4e40-a271-0a078f2452ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007794398s
STEP: Saw pod success
May  6 15:54:21.773: INFO: Pod "downwardapi-volume-66af6735-9bd0-4e40-a271-0a078f2452ff" satisfied condition "success or failure"
May  6 15:54:21.775: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod downwardapi-volume-66af6735-9bd0-4e40-a271-0a078f2452ff container client-container: <nil>
STEP: delete the pod
May  6 15:54:21.787: INFO: Waiting for pod downwardapi-volume-66af6735-9bd0-4e40-a271-0a078f2452ff to disappear
May  6 15:54:21.789: INFO: Pod downwardapi-volume-66af6735-9bd0-4e40-a271-0a078f2452ff no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:54:21.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8019" for this suite.
May  6 15:54:27.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:54:30.561: INFO: namespace downward-api-8019 deletion completed in 8.769107535s

• [SLOW TEST:12.936 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:54:30.561: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4882
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May  6 15:54:34.725: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  6 15:54:34.727: INFO: Pod pod-with-poststart-exec-hook still exists
May  6 15:54:36.727: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  6 15:54:36.730: INFO: Pod pod-with-poststart-exec-hook still exists
May  6 15:54:38.727: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  6 15:54:38.730: INFO: Pod pod-with-poststart-exec-hook still exists
May  6 15:54:40.727: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  6 15:54:40.730: INFO: Pod pod-with-poststart-exec-hook still exists
May  6 15:54:42.727: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  6 15:54:42.730: INFO: Pod pod-with-poststart-exec-hook still exists
May  6 15:54:44.727: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  6 15:54:44.730: INFO: Pod pod-with-poststart-exec-hook still exists
May  6 15:54:46.727: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  6 15:54:46.730: INFO: Pod pod-with-poststart-exec-hook still exists
May  6 15:54:48.727: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  6 15:54:48.731: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:54:48.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4882" for this suite.
May  6 15:55:16.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:55:19.494: INFO: namespace container-lifecycle-hook-4882 deletion completed in 30.76030481s

• [SLOW TEST:48.933 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:55:19.494: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-4701
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May  6 15:55:25.652: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4701 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 15:55:25.652: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 15:55:25.823: INFO: Exec stderr: ""
May  6 15:55:25.823: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4701 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 15:55:25.823: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 15:55:25.987: INFO: Exec stderr: ""
May  6 15:55:25.987: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4701 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 15:55:25.987: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 15:55:26.151: INFO: Exec stderr: ""
May  6 15:55:26.151: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4701 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 15:55:26.151: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 15:55:26.320: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May  6 15:55:26.320: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4701 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 15:55:26.320: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 15:55:26.486: INFO: Exec stderr: ""
May  6 15:55:26.486: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4701 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 15:55:26.486: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 15:55:26.653: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May  6 15:55:26.653: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4701 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 15:55:26.653: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 15:55:26.818: INFO: Exec stderr: ""
May  6 15:55:26.818: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4701 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 15:55:26.818: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 15:55:26.988: INFO: Exec stderr: ""
May  6 15:55:26.988: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4701 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 15:55:26.988: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 15:55:27.157: INFO: Exec stderr: ""
May  6 15:55:27.157: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4701 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 15:55:27.157: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
May  6 15:55:27.326: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:55:27.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4701" for this suite.
May  6 15:56:13.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:56:16.089: INFO: namespace e2e-kubelet-etc-hosts-4701 deletion completed in 48.75966629s

• [SLOW TEST:56.595 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:56:16.089: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-2296
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
May  6 15:56:20.738: INFO: Successfully updated pod "adopt-release-n6nbk"
STEP: Checking that the Job readopts the Pod
May  6 15:56:20.738: INFO: Waiting up to 15m0s for pod "adopt-release-n6nbk" in namespace "job-2296" to be "adopted"
May  6 15:56:20.740: INFO: Pod "adopt-release-n6nbk": Phase="Running", Reason="", readiness=true. Elapsed: 1.979324ms
May  6 15:56:22.742: INFO: Pod "adopt-release-n6nbk": Phase="Running", Reason="", readiness=true. Elapsed: 2.004874841s
May  6 15:56:22.743: INFO: Pod "adopt-release-n6nbk" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
May  6 15:56:23.251: INFO: Successfully updated pod "adopt-release-n6nbk"
STEP: Checking that the Job releases the Pod
May  6 15:56:23.251: INFO: Waiting up to 15m0s for pod "adopt-release-n6nbk" in namespace "job-2296" to be "released"
May  6 15:56:23.255: INFO: Pod "adopt-release-n6nbk": Phase="Running", Reason="", readiness=true. Elapsed: 4.23838ms
May  6 15:56:25.258: INFO: Pod "adopt-release-n6nbk": Phase="Running", Reason="", readiness=true. Elapsed: 2.006521826s
May  6 15:56:25.258: INFO: Pod "adopt-release-n6nbk" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:56:25.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2296" for this suite.
May  6 15:57:11.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:57:14.020: INFO: namespace job-2296 deletion completed in 48.759701614s

• [SLOW TEST:57.931 seconds]
[sig-apps] Job
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:57:14.020: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7585
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 15:57:14.151: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May  6 15:57:14.156: INFO: Pod name sample-pod: Found 0 pods out of 1
May  6 15:57:19.159: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May  6 15:57:19.159: INFO: Creating deployment "test-rolling-update-deployment"
May  6 15:57:19.163: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May  6 15:57:19.167: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May  6 15:57:21.172: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May  6 15:57:21.174: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May  6 15:57:21.179: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7585 /apis/apps/v1/namespaces/deployment-7585/deployments/test-rolling-update-deployment 5de8b066-050c-4d61-b2eb-cb963a93b713 134583 1 2020-05-06 15:57:19 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0039ec3b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-05-06 15:57:19 +0000 UTC,LastTransitionTime:2020-05-06 15:57:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-05-06 15:57:21 +0000 UTC,LastTransitionTime:2020-05-06 15:57:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  6 15:57:21.181: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-7585 /apis/apps/v1/namespaces/deployment-7585/replicasets/test-rolling-update-deployment-55d946486 d63299d5-cc5b-4956-9e7d-3453cf329627 134572 1 2020-05-06 15:57:19 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 5de8b066-050c-4d61-b2eb-cb963a93b713 0xc0039ec8b0 0xc0039ec8b1}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0039ec918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  6 15:57:21.181: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May  6 15:57:21.181: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7585 /apis/apps/v1/namespaces/deployment-7585/replicasets/test-rolling-update-controller 47420a3f-962d-4418-b3b8-947b77dc8c16 134582 2 2020-05-06 15:57:14 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 5de8b066-050c-4d61-b2eb-cb963a93b713 0xc0039ec7e7 0xc0039ec7e8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0039ec848 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  6 15:57:21.184: INFO: Pod "test-rolling-update-deployment-55d946486-clqrd" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-clqrd test-rolling-update-deployment-55d946486- deployment-7585 /api/v1/namespaces/deployment-7585/pods/test-rolling-update-deployment-55d946486-clqrd 1b629651-93ed-41f5-83c5-23b49fef6f77 134571 0 2020-05-06 15:57:19 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[cni.projectcalico.org/podIP:192.168.85.99/32 cni.projectcalico.org/podIPs:192.168.85.99/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 d63299d5-cc5b-4956-9e7d-3453cf329627 0xc0039ecdb0 0xc0039ecdb1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hhvfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hhvfb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hhvfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-80.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 15:57:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 15:57:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 15:57:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 15:57:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.80,PodIP:192.168.85.99,StartTime:2020-05-06 15:57:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 15:57:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:containerd://cf8cd3beb88ad69c105992140810ff0ed8582353a66d7f4ada164f139d56a3b2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.85.99,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:57:21.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7585" for this suite.
May  6 15:57:27.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:57:29.947: INFO: namespace deployment-7585 deletion completed in 8.759512419s

• [SLOW TEST:15.927 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:57:29.947: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9420
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
May  6 15:57:40.096: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0506 15:57:40.096683      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:57:40.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9420" for this suite.
May  6 15:57:46.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:57:48.858: INFO: namespace gc-9420 deletion completed in 8.759566792s

• [SLOW TEST:18.911 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:57:48.859: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-904
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
May  6 15:57:51.009: INFO: Pod pod-hostip-fd66f14d-3517-4a68-bec9-833e64bc82f7 has hostIP: 10.0.131.80
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:57:51.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-904" for this suite.
May  6 15:58:19.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:58:21.773: INFO: namespace pods-904 deletion completed in 30.760829063s

• [SLOW TEST:32.914 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:58:21.773: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1687
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0506 15:58:23.932249      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May  6 15:58:23.932: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:58:23.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1687" for this suite.
May  6 15:58:29.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:58:32.694: INFO: namespace gc-1687 deletion completed in 8.759322946s

• [SLOW TEST:10.921 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:58:32.694: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4810
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 15:58:32.826: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:58:38.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4810" for this suite.
May  6 15:58:44.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:58:47.696: INFO: namespace custom-resource-definition-4810 deletion completed in 8.761116906s

• [SLOW TEST:15.002 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:58:47.696: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5477
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May  6 15:58:47.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-5477'
May  6 15:58:47.909: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May  6 15:58:47.909: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
May  6 15:58:47.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 delete jobs e2e-test-httpd-job --namespace=kubectl-5477'
May  6 15:58:48.024: INFO: stderr: ""
May  6 15:58:48.024: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:58:48.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5477" for this suite.
May  6 15:59:00.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:59:02.790: INFO: namespace kubectl-5477 deletion completed in 14.762065502s

• [SLOW TEST:15.094 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:59:02.790: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6321
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May  6 15:59:02.929: INFO: Waiting up to 5m0s for pod "downward-api-077bb879-27f4-4fd2-895b-fac65afa9151" in namespace "downward-api-6321" to be "success or failure"
May  6 15:59:02.931: INFO: Pod "downward-api-077bb879-27f4-4fd2-895b-fac65afa9151": Phase="Pending", Reason="", readiness=false. Elapsed: 1.738699ms
May  6 15:59:04.933: INFO: Pod "downward-api-077bb879-27f4-4fd2-895b-fac65afa9151": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004232559s
STEP: Saw pod success
May  6 15:59:04.933: INFO: Pod "downward-api-077bb879-27f4-4fd2-895b-fac65afa9151" satisfied condition "success or failure"
May  6 15:59:04.935: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod downward-api-077bb879-27f4-4fd2-895b-fac65afa9151 container dapi-container: <nil>
STEP: delete the pod
May  6 15:59:04.955: INFO: Waiting for pod downward-api-077bb879-27f4-4fd2-895b-fac65afa9151 to disappear
May  6 15:59:04.957: INFO: Pod downward-api-077bb879-27f4-4fd2-895b-fac65afa9151 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:59:04.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6321" for this suite.
May  6 15:59:10.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:59:13.719: INFO: namespace downward-api-6321 deletion completed in 8.759180047s

• [SLOW TEST:10.929 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:59:13.719: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3854
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:59:29.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3854" for this suite.
May  6 15:59:35.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 15:59:38.675: INFO: namespace resourcequota-3854 deletion completed in 8.759726423s

• [SLOW TEST:24.956 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 15:59:38.675: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4210
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4210
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4210
STEP: creating replication controller externalsvc in namespace services-4210
I0506 15:59:38.845850      25 runners.go:184] Created replication controller with name: externalsvc, namespace: services-4210, replica count: 2
I0506 15:59:41.896162      25 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
May  6 15:59:41.907: INFO: Creating new exec pod
May  6 15:59:43.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec --namespace=services-4210 execpodbxvl8 -- /bin/sh -x -c nslookup clusterip-service'
May  6 15:59:44.181: INFO: stderr: "+ nslookup clusterip-service\n"
May  6 15:59:44.182: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nclusterip-service.services-4210.svc.cluster.local\tcanonical name = externalsvc.services-4210.svc.cluster.local.\nName:\texternalsvc.services-4210.svc.cluster.local\nAddress: 10.0.60.6\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4210, will wait for the garbage collector to delete the pods
May  6 15:59:44.240: INFO: Deleting ReplicationController externalsvc took: 5.891071ms
May  6 15:59:45.640: INFO: Terminating ReplicationController externalsvc pods took: 1.400162213s
May  6 15:59:57.255: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 15:59:57.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4210" for this suite.
May  6 16:00:05.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 16:00:08.043: INFO: namespace services-4210 deletion completed in 10.774143548s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:29.368 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 16:00:08.043: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7713
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May  6 16:00:12.198: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 16:00:12.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7713" for this suite.
May  6 16:00:18.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 16:00:20.979: INFO: namespace container-runtime-7713 deletion completed in 8.769130761s

• [SLOW TEST:12.936 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 16:00:20.979: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1338
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-707b2e3e-6dea-4f28-bab0-ce98830d1bcc
STEP: Creating a pod to test consume secrets
May  6 16:00:21.128: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c54a9535-cf3d-4da0-9d58-1d0dcd234613" in namespace "projected-1338" to be "success or failure"
May  6 16:00:21.130: INFO: Pod "pod-projected-secrets-c54a9535-cf3d-4da0-9d58-1d0dcd234613": Phase="Pending", Reason="", readiness=false. Elapsed: 1.946278ms
May  6 16:00:23.133: INFO: Pod "pod-projected-secrets-c54a9535-cf3d-4da0-9d58-1d0dcd234613": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004587045s
STEP: Saw pod success
May  6 16:00:23.133: INFO: Pod "pod-projected-secrets-c54a9535-cf3d-4da0-9d58-1d0dcd234613" satisfied condition "success or failure"
May  6 16:00:23.135: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-projected-secrets-c54a9535-cf3d-4da0-9d58-1d0dcd234613 container secret-volume-test: <nil>
STEP: delete the pod
May  6 16:00:23.147: INFO: Waiting for pod pod-projected-secrets-c54a9535-cf3d-4da0-9d58-1d0dcd234613 to disappear
May  6 16:00:23.149: INFO: Pod pod-projected-secrets-c54a9535-cf3d-4da0-9d58-1d0dcd234613 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 16:00:23.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1338" for this suite.
May  6 16:00:29.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 16:00:31.911: INFO: namespace projected-1338 deletion completed in 8.760040351s

• [SLOW TEST:10.932 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 16:00:31.912: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9691
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 16:00:32.457: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 16:00:35.470: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 16:00:35.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9691" for this suite.
May  6 16:00:41.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 16:00:44.335: INFO: namespace webhook-9691 deletion completed in 8.758969715s
STEP: Destroying namespace "webhook-9691-markers" for this suite.
May  6 16:00:50.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 16:00:53.095: INFO: namespace webhook-9691-markers deletion completed in 8.759725798s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.194 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 16:00:53.105: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3460
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
May  6 16:00:57.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-926700516 exec pod-sharedvolume-95a51db4-a210-426e-8172-7bf6a066ba01 -c busybox-main-container --namespace=emptydir-3460 -- cat /usr/share/volumeshare/shareddata.txt'
May  6 16:00:57.524: INFO: stderr: ""
May  6 16:00:57.524: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 16:00:57.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3460" for this suite.
May  6 16:01:03.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 16:01:06.287: INFO: namespace emptydir-3460 deletion completed in 8.759922932s

• [SLOW TEST:13.182 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 16:01:06.287: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-6608
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 16:01:06.418: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Creating first CR 
May  6 16:01:06.996: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-06T16:01:06Z generation:1 name:name1 resourceVersion:136352 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:39ffee2d-0a7e-4478-a1a8-9aca6a3478f1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
May  6 16:01:17.000: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-06T16:01:16Z generation:1 name:name2 resourceVersion:136398 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:3f051f9e-68a7-4e32-9d53-306b857eaf35] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
May  6 16:01:27.004: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-06T16:01:06Z generation:2 name:name1 resourceVersion:136447 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:39ffee2d-0a7e-4478-a1a8-9aca6a3478f1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
May  6 16:01:37.008: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-06T16:01:16Z generation:2 name:name2 resourceVersion:136494 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:3f051f9e-68a7-4e32-9d53-306b857eaf35] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
May  6 16:01:47.015: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-06T16:01:06Z generation:2 name:name1 resourceVersion:136539 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:39ffee2d-0a7e-4478-a1a8-9aca6a3478f1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
May  6 16:01:57.021: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-06T16:01:16Z generation:2 name:name2 resourceVersion:136587 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:3f051f9e-68a7-4e32-9d53-306b857eaf35] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 16:02:07.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6608" for this suite.
May  6 16:02:13.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 16:02:16.291: INFO: namespace crd-watch-6608 deletion completed in 8.759893876s

• [SLOW TEST:70.004 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 16:02:16.292: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8727
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May  6 16:02:16.423: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  6 16:02:16.432: INFO: Waiting for terminating namespaces to be deleted...
May  6 16:02:16.434: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-129-1.us-west-2.compute.internal before test
May  6 16:02:16.447: INFO: ebs-csi-node-f2cbn from kube-system started at 2020-05-06 10:57:49 +0000 UTC (3 container statuses recorded)
May  6 16:02:16.447: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 16:02:16.447: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 16:02:16.447: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  6 16:02:16.447: INFO: traefik-kubeaddons-6f97669977-2sxh8 from kubeaddons started at 2020-05-06 10:58:20 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.447: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
May  6 16:02:16.447: INFO: prometheus-kubeaddons-prometheus-node-exporter-b6z8v from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.447: INFO: 	Container node-exporter ready: true, restart count 0
May  6 16:02:16.447: INFO: kcl-cm-6588c896f9-4b2gk from kommander started at 2020-05-06 11:00:36 +0000 UTC (2 container statuses recorded)
May  6 16:02:16.447: INFO: 	Container controller-manager ready: true, restart count 0
May  6 16:02:16.447: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  6 16:02:16.447: INFO: kube-proxy-v9c2p from kube-system started at 2020-05-06 10:55:20 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.447: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 16:02:16.447: INFO: calico-node-wpbh7 from kube-system started at 2020-05-06 10:55:38 +0000 UTC (2 container statuses recorded)
May  6 16:02:16.447: INFO: 	Container bird-metrics ready: true, restart count 0
May  6 16:02:16.447: INFO: 	Container calico-node ready: true, restart count 0
May  6 16:02:16.447: INFO: cert-manager-kubeaddons-webhook-77fbc6d59b-fbbjd from cert-manager started at 2020-05-06 10:56:59 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.447: INFO: 	Container cert-manager ready: true, restart count 1
May  6 16:02:16.447: INFO: cert-manager-kubeaddons-7d7f98fbc6-qpl75 from cert-manager started at 2020-05-06 10:56:59 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.447: INFO: 	Container cert-manager ready: true, restart count 0
May  6 16:02:16.447: INFO: elasticsearch-kubeaddons-master-1 from kubeaddons started at 2020-05-06 10:59:19 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.447: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 16:02:16.447: INFO: coredns-5644d7b6d9-5jf9p from kube-system started at 2020-05-06 10:56:00 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.447: INFO: 	Container coredns ready: true, restart count 0
May  6 16:02:16.447: INFO: kommander-kubeaddons-kubeaddons-catalog-6654f856df-ccmkl from kommander started at 2020-05-06 11:00:36 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.447: INFO: 	Container kubeaddons-catalog ready: true, restart count 0
May  6 16:02:16.447: INFO: kcl-utility-apiserver-5d448f665b-gf6gv from kommander started at 2020-05-06 11:00:36 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.447: INFO: 	Container server ready: true, restart count 0
May  6 16:02:16.447: INFO: elasticsearch-kubeaddons-data-1 from kubeaddons started at 2020-05-06 11:01:19 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.447: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 16:02:16.447: INFO: sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-9wkcg from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 16:02:16.447: INFO: 	Container sonobuoy-worker ready: true, restart count 2
May  6 16:02:16.447: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 16:02:16.447: INFO: reloader-kubeaddons-reloader-7d4bd64cfb-xnxz8 from kubeaddons started at 2020-05-06 10:56:23 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.447: INFO: 	Container reloader-kubeaddons-reloader ready: true, restart count 0
May  6 16:02:16.447: INFO: kubefed-controller-manager-55fd5b4dff-bvsl6 from kommander started at 2020-05-06 11:00:46 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.447: INFO: 	Container controller-manager ready: true, restart count 0
May  6 16:02:16.447: INFO: prometheus-kubeaddons-grafana-99b49bd54-8h7z8 from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (2 container statuses recorded)
May  6 16:02:16.447: INFO: 	Container grafana ready: true, restart count 0
May  6 16:02:16.447: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
May  6 16:02:16.447: INFO: minio-3 from velero started at 2020-05-06 10:59:47 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.447: INFO: 	Container minio ready: true, restart count 0
May  6 16:02:16.447: INFO: fluentbit-kubeaddons-fluent-bit-5rbrr from kubeaddons started at 2020-05-06 11:02:11 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.447: INFO: 	Container fluent-bit ready: true, restart count 0
May  6 16:02:16.447: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-129-108.us-west-2.compute.internal before test
May  6 16:02:16.461: INFO: calico-node-z679x from kube-system started at 2020-05-06 10:55:38 +0000 UTC (2 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container bird-metrics ready: true, restart count 0
May  6 16:02:16.461: INFO: 	Container calico-node ready: true, restart count 0
May  6 16:02:16.461: INFO: prometheus-kubeaddons-prometheus-node-exporter-2wrg9 from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container node-exporter ready: true, restart count 0
May  6 16:02:16.461: INFO: kommander-kubeaddons-karma-7688944fcf-hpnzl from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container karma ready: true, restart count 0
May  6 16:02:16.461: INFO: external-dns-kubeaddons-6c65ff8d88-sm9p2 from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container external-dns ready: true, restart count 0
May  6 16:02:16.461: INFO: calico-kube-controllers-84f666455-5t7xm from kube-system started at 2020-05-06 10:55:49 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  6 16:02:16.461: INFO: minio-2 from velero started at 2020-05-06 10:59:44 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container minio ready: true, restart count 0
May  6 16:02:16.461: INFO: traefik-kubeaddons-6f97669977-9fwr9 from kubeaddons started at 2020-05-06 10:57:49 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
May  6 16:02:16.461: INFO: elasticsearch-kubeaddons-data-0 from kubeaddons started at 2020-05-06 10:58:15 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 16:02:16.461: INFO: elasticsearchexporter-kubeaddons-elasticsearch-exporter-845tdq8 from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container elasticsearch-exporter ready: true, restart count 0
May  6 16:02:16.461: INFO: kube-proxy-4lqz5 from kube-system started at 2020-05-06 10:55:19 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 16:02:16.461: INFO: coredns-5644d7b6d9-hmtfs from kube-system started at 2020-05-06 10:56:00 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container coredns ready: true, restart count 0
May  6 16:02:16.461: INFO: sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-zqwx9 from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container sonobuoy-worker ready: true, restart count 2
May  6 16:02:16.461: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 16:02:16.461: INFO: kcl-tfcb-9c88c7bc9-lbgz6 from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container server ready: true, restart count 0
May  6 16:02:16.461: INFO: kcl-webhook-fb4dfc7d7-vv757 from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container webhook ready: true, restart count 0
May  6 16:02:16.461: INFO: kubernetes-dashboard-549989bcdf-2d767 from kubeaddons started at 2020-05-06 10:56:34 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
May  6 16:02:16.461: INFO: kibana-kubeaddons-649897648c-mzc2h from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (2 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container initialize-kibana-index ready: true, restart count 0
May  6 16:02:16.461: INFO: 	Container kibana ready: true, restart count 0
May  6 16:02:16.461: INFO: kubeaddons-controller-manager-77cf76b857-k89ml from kubeaddons started at 2020-05-06 10:55:49 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container manager ready: true, restart count 0
May  6 16:02:16.461: INFO: ebs-csi-node-vvbxt from kube-system started at 2020-05-06 10:57:49 +0000 UTC (3 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 16:02:16.461: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 16:02:16.461: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  6 16:02:16.461: INFO: cert-manager-kubeaddons-cainjector-6dcd94769b-ng6xl from cert-manager started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container cainjector ready: true, restart count 0
May  6 16:02:16.461: INFO: elasticsearch-kubeaddons-master-2 from kubeaddons started at 2020-05-06 11:00:07 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 16:02:16.461: INFO: kommander-kubeaddons-grafana-645f957f8f-96sf5 from kommander started at 2020-05-06 11:00:36 +0000 UTC (2 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container grafana ready: true, restart count 0
May  6 16:02:16.461: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
May  6 16:02:16.461: INFO: kubefed-admission-webhook-5fb847574f-6mjm2 from kommander started at 2020-05-06 11:00:41 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container admission-webhook ready: true, restart count 0
May  6 16:02:16.461: INFO: fluentbit-kubeaddons-fluent-bit-t6xpz from kubeaddons started at 2020-05-06 11:02:11 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container fluent-bit ready: true, restart count 0
May  6 16:02:16.461: INFO: elasticsearch-kubeaddons-client-d4f5db58d-7qrn8 from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.461: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 16:02:16.461: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-131-234.us-west-2.compute.internal before test
May  6 16:02:16.475: INFO: traefik-kubeaddons-1.72.17-5pttd from kubeaddons started at 2020-05-06 10:57:38 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container traefik ready: false, restart count 0
May  6 16:02:16.475: INFO: prometheus-kubeaddons-kube-state-metrics-78c65cc7bc-66zk8 from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container kube-state-metrics ready: true, restart count 0
May  6 16:02:16.475: INFO: kube-oidc-proxy-kubeaddons-6fbd5c8fc4-xtmsg from kubeaddons started at 2020-05-06 10:59:18 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container kube-oidc-proxy ready: true, restart count 0
May  6 16:02:16.475: INFO: traefik-forward-auth-kubeaddons-8bdddfcd-wdrjp from kubeaddons started at 2020-05-06 10:59:51 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container traefik-forward-auth ready: true, restart count 0
May  6 16:02:16.475: INFO: alertmanager-prometheus-kubeaddons-prom-alertmanager-0 from kubeaddons started at 2020-05-06 10:59:03 +0000 UTC (2 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container alertmanager ready: true, restart count 0
May  6 16:02:16.475: INFO: 	Container config-reloader ready: true, restart count 0
May  6 16:02:16.475: INFO: tiller-deploy-969865475-m2ph8 from kube-system started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container tiller ready: true, restart count 0
May  6 16:02:16.475: INFO: prometheusadapter-kubeaddons-prometheus-adapter-6b8975fc48pwzsr from kubeaddons started at 2020-05-06 10:59:55 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container prometheus-adapter ready: true, restart count 0
May  6 16:02:16.475: INFO: dstorageclass-controller-manager-5c966c767f-4bf6x from kubeaddons started at 2020-05-06 10:57:33 +0000 UTC (2 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  6 16:02:16.475: INFO: 	Container manager ready: true, restart count 0
May  6 16:02:16.475: INFO: opsportal-kubeaddons-kommander-ui-679db5d9cb-62psh from kubeaddons started at 2020-05-06 10:56:45 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container opsportal-kubeaddons-kommander-ui ready: true, restart count 0
May  6 16:02:16.475: INFO: dex-kubeaddons-79b77778bc-84p4b from kubeaddons started at 2020-05-06 10:59:20 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container main ready: true, restart count 1
May  6 16:02:16.475: INFO: opsportal-landing-6f6865b688-vb67f from kubeaddons started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container opsportal-landing ready: true, restart count 0
May  6 16:02:16.475: INFO: kommander-kubeaddons-kommander-ui-7b9b585d95-rgnz6 from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container kommander-kubeaddons-kommander-ui ready: true, restart count 0
May  6 16:02:16.475: INFO: ebs-csi-node-ffszp from kube-system started at 2020-05-06 10:57:49 +0000 UTC (3 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 16:02:16.475: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 16:02:16.475: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  6 16:02:16.475: INFO: prometheus-kubeaddons-prom-operator-cc56c6c-m8kfx from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (2 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container prometheus-operator ready: true, restart count 0
May  6 16:02:16.475: INFO: 	Container tls-proxy ready: true, restart count 0
May  6 16:02:16.475: INFO: prometheus-kubeaddons-prometheus-node-exporter-4jtfq from kubeaddons started at 2020-05-06 10:58:52 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container node-exporter ready: true, restart count 0
May  6 16:02:16.475: INFO: velero-kubeaddons-578b4667ff-6x2nw from velero started at 2020-05-06 10:59:40 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container velero ready: true, restart count 4
May  6 16:02:16.475: INFO: dex-k8s-authenticator-kubeaddons-6697855d4c-xbqks from kubeaddons started at 2020-05-06 10:59:29 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container dex-k8s-authenticator ready: true, restart count 0
May  6 16:02:16.475: INFO: fluentbit-kubeaddons-fluent-bit-wnqvd from kubeaddons started at 2020-05-06 11:02:11 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container fluent-bit ready: true, restart count 0
May  6 16:02:16.475: INFO: kommander-kubeaddons-thanos-query-7597bf4957-tmdgd from kommander started at 2020-05-06 13:07:51 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container thanos-query ready: true, restart count 0
May  6 16:02:16.475: INFO: kube-proxy-ptcmp from kube-system started at 2020-05-06 10:55:19 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 16:02:16.475: INFO: calico-node-gm4h2 from kube-system started at 2020-05-06 10:55:38 +0000 UTC (2 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container bird-metrics ready: true, restart count 0
May  6 16:02:16.475: INFO: 	Container calico-node ready: true, restart count 0
May  6 16:02:16.475: INFO: elasticsearch-kubeaddons-client-d4f5db58d-7g4sd from kubeaddons started at 2020-05-06 10:57:52 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container elasticsearch ready: true, restart count 0
May  6 16:02:16.475: INFO: minio-0 from velero started at 2020-05-06 10:59:44 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container minio ready: true, restart count 0
May  6 16:02:16.475: INFO: gatekeeper-kubeaddons-fdc87db85-5rj6d from kubeaddons started at 2020-05-06 10:57:40 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container manager ready: true, restart count 0
May  6 16:02:16.475: INFO: sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-hqgql from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container sonobuoy-worker ready: true, restart count 2
May  6 16:02:16.475: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 16:02:16.475: INFO: dex-kubeaddons-dex-controller-6b6c9fbd7f-pxz8h from kubeaddons started at 2020-05-06 10:58:46 +0000 UTC (2 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  6 16:02:16.475: INFO: 	Container manager ready: true, restart count 0
May  6 16:02:16.475: INFO: kubefed-controller-manager-55fd5b4dff-g9m6m from kommander started at 2020-05-06 11:00:41 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.475: INFO: 	Container controller-manager ready: true, restart count 0
May  6 16:02:16.475: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-131-80.us-west-2.compute.internal before test
May  6 16:02:16.485: INFO: calico-node-mrkdh from kube-system started at 2020-05-06 10:55:38 +0000 UTC (2 container statuses recorded)
May  6 16:02:16.485: INFO: 	Container bird-metrics ready: true, restart count 0
May  6 16:02:16.485: INFO: 	Container calico-node ready: true, restart count 0
May  6 16:02:16.485: INFO: sonobuoy-e2e-job-7496ec37786a4f0c from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 16:02:16.485: INFO: 	Container e2e ready: true, restart count 0
May  6 16:02:16.485: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 16:02:16.485: INFO: prometheus-prometheus-kubeaddons-prom-prometheus-0 from kubeaddons started at 2020-05-06 13:31:17 +0000 UTC (4 container statuses recorded)
May  6 16:02:16.485: INFO: 	Container prometheus ready: false, restart count 0
May  6 16:02:16.485: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May  6 16:02:16.485: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May  6 16:02:16.485: INFO: 	Container thanos-sidecar ready: true, restart count 0
May  6 16:02:16.485: INFO: ebs-csi-node-722lq from kube-system started at 2020-05-06 10:57:49 +0000 UTC (3 container statuses recorded)
May  6 16:02:16.485: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 16:02:16.485: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 16:02:16.485: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  6 16:02:16.485: INFO: kube-proxy-vmxhd from kube-system started at 2020-05-06 10:55:19 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.485: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 16:02:16.485: INFO: elasticsearch-kubeaddons-master-0 from kubeaddons started at 2020-05-06 13:31:19 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.485: INFO: 	Container elasticsearch ready: false, restart count 0
May  6 16:02:16.485: INFO: fluentbit-kubeaddons-fluent-bit-w966k from kubeaddons started at 2020-05-06 13:31:20 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.485: INFO: 	Container fluent-bit ready: true, restart count 0
May  6 16:02:16.485: INFO: ebs-csi-controller-0 from kube-system started at 2020-05-06 13:31:16 +0000 UTC (5 container statuses recorded)
May  6 16:02:16.485: INFO: 	Container csi-attacher ready: true, restart count 0
May  6 16:02:16.485: INFO: 	Container csi-provisioner ready: true, restart count 0
May  6 16:02:16.485: INFO: 	Container csi-snapshotter ready: true, restart count 0
May  6 16:02:16.485: INFO: 	Container ebs-plugin ready: true, restart count 0
May  6 16:02:16.485: INFO: 	Container liveness-probe ready: true, restart count 0
May  6 16:02:16.485: INFO: minio-1 from velero started at 2020-05-06 13:31:21 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.485: INFO: 	Container minio ready: false, restart count 0
May  6 16:02:16.485: INFO: prometheus-kubeaddons-prometheus-node-exporter-2qwk2 from kubeaddons started at 2020-05-06 13:32:37 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.485: INFO: 	Container node-exporter ready: false, restart count 0
May  6 16:02:16.485: INFO: sonobuoy from sonobuoy started at 2020-05-06 13:56:26 +0000 UTC (1 container statuses recorded)
May  6 16:02:16.485: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  6 16:02:16.485: INFO: sonobuoy-systemd-logs-daemon-set-9d3c15d99df742b8-cvh6w from sonobuoy started at 2020-05-06 13:56:28 +0000 UTC (2 container statuses recorded)
May  6 16:02:16.485: INFO: 	Container sonobuoy-worker ready: true, restart count 2
May  6 16:02:16.485: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-76fdfbd1-cb66-4ab4-ad09-1dd186ca973f 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-76fdfbd1-cb66-4ab4-ad09-1dd186ca973f off the node ip-10-0-131-80.us-west-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-76fdfbd1-cb66-4ab4-ad09-1dd186ca973f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 16:02:20.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8727" for this suite.
May  6 16:02:28.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 16:02:31.301: INFO: namespace sched-pred-8727 deletion completed in 10.759328168s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:15.009 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 16:02:31.301: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-66
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-hns7
STEP: Creating a pod to test atomic-volume-subpath
May  6 16:02:31.445: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-hns7" in namespace "subpath-66" to be "success or failure"
May  6 16:02:31.447: INFO: Pod "pod-subpath-test-projected-hns7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.781317ms
May  6 16:02:33.449: INFO: Pod "pod-subpath-test-projected-hns7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00433187s
May  6 16:02:35.452: INFO: Pod "pod-subpath-test-projected-hns7": Phase="Running", Reason="", readiness=true. Elapsed: 4.006972541s
May  6 16:02:37.455: INFO: Pod "pod-subpath-test-projected-hns7": Phase="Running", Reason="", readiness=true. Elapsed: 6.009657971s
May  6 16:02:39.458: INFO: Pod "pod-subpath-test-projected-hns7": Phase="Running", Reason="", readiness=true. Elapsed: 8.012575151s
May  6 16:02:41.460: INFO: Pod "pod-subpath-test-projected-hns7": Phase="Running", Reason="", readiness=true. Elapsed: 10.015128581s
May  6 16:02:43.463: INFO: Pod "pod-subpath-test-projected-hns7": Phase="Running", Reason="", readiness=true. Elapsed: 12.017746152s
May  6 16:02:45.465: INFO: Pod "pod-subpath-test-projected-hns7": Phase="Running", Reason="", readiness=true. Elapsed: 14.020357923s
May  6 16:02:47.468: INFO: Pod "pod-subpath-test-projected-hns7": Phase="Running", Reason="", readiness=true. Elapsed: 16.02282548s
May  6 16:02:49.471: INFO: Pod "pod-subpath-test-projected-hns7": Phase="Running", Reason="", readiness=true. Elapsed: 18.025597676s
May  6 16:02:51.473: INFO: Pod "pod-subpath-test-projected-hns7": Phase="Running", Reason="", readiness=true. Elapsed: 20.028398918s
May  6 16:02:53.475: INFO: Pod "pod-subpath-test-projected-hns7": Phase="Running", Reason="", readiness=true. Elapsed: 22.030481592s
May  6 16:02:55.478: INFO: Pod "pod-subpath-test-projected-hns7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.033174529s
STEP: Saw pod success
May  6 16:02:55.478: INFO: Pod "pod-subpath-test-projected-hns7" satisfied condition "success or failure"
May  6 16:02:55.480: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-subpath-test-projected-hns7 container test-container-subpath-projected-hns7: <nil>
STEP: delete the pod
May  6 16:02:55.492: INFO: Waiting for pod pod-subpath-test-projected-hns7 to disappear
May  6 16:02:55.494: INFO: Pod pod-subpath-test-projected-hns7 no longer exists
STEP: Deleting pod pod-subpath-test-projected-hns7
May  6 16:02:55.494: INFO: Deleting pod "pod-subpath-test-projected-hns7" in namespace "subpath-66"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 16:02:55.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-66" for this suite.
May  6 16:03:01.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 16:03:04.260: INFO: namespace subpath-66 deletion completed in 8.761062783s

• [SLOW TEST:32.959 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 16:03:04.260: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5691
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 16:03:04.399: INFO: Waiting up to 5m0s for pod "downwardapi-volume-adf44b35-e0e6-4960-b365-e4a106928b77" in namespace "downward-api-5691" to be "success or failure"
May  6 16:03:04.405: INFO: Pod "downwardapi-volume-adf44b35-e0e6-4960-b365-e4a106928b77": Phase="Pending", Reason="", readiness=false. Elapsed: 6.505463ms
May  6 16:03:06.408: INFO: Pod "downwardapi-volume-adf44b35-e0e6-4960-b365-e4a106928b77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009133945s
STEP: Saw pod success
May  6 16:03:06.408: INFO: Pod "downwardapi-volume-adf44b35-e0e6-4960-b365-e4a106928b77" satisfied condition "success or failure"
May  6 16:03:06.410: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod downwardapi-volume-adf44b35-e0e6-4960-b365-e4a106928b77 container client-container: <nil>
STEP: delete the pod
May  6 16:03:06.422: INFO: Waiting for pod downwardapi-volume-adf44b35-e0e6-4960-b365-e4a106928b77 to disappear
May  6 16:03:06.423: INFO: Pod downwardapi-volume-adf44b35-e0e6-4960-b365-e4a106928b77 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 16:03:06.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5691" for this suite.
May  6 16:03:12.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 16:03:15.186: INFO: namespace downward-api-5691 deletion completed in 8.759950594s

• [SLOW TEST:10.926 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 16:03:15.186: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2957
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-a9e20edf-2869-454e-bc3b-36d30acd62bb
STEP: Creating a pod to test consume secrets
May  6 16:03:15.327: INFO: Waiting up to 5m0s for pod "pod-secrets-a038f4e3-cc57-4f86-ad64-8f760ba6c96d" in namespace "secrets-2957" to be "success or failure"
May  6 16:03:15.329: INFO: Pod "pod-secrets-a038f4e3-cc57-4f86-ad64-8f760ba6c96d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.731934ms
May  6 16:03:17.332: INFO: Pod "pod-secrets-a038f4e3-cc57-4f86-ad64-8f760ba6c96d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004515546s
STEP: Saw pod success
May  6 16:03:17.332: INFO: Pod "pod-secrets-a038f4e3-cc57-4f86-ad64-8f760ba6c96d" satisfied condition "success or failure"
May  6 16:03:17.334: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-secrets-a038f4e3-cc57-4f86-ad64-8f760ba6c96d container secret-volume-test: <nil>
STEP: delete the pod
May  6 16:03:17.345: INFO: Waiting for pod pod-secrets-a038f4e3-cc57-4f86-ad64-8f760ba6c96d to disappear
May  6 16:03:17.347: INFO: Pod pod-secrets-a038f4e3-cc57-4f86-ad64-8f760ba6c96d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 16:03:17.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2957" for this suite.
May  6 16:03:23.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 16:03:26.109: INFO: namespace secrets-2957 deletion completed in 8.759122506s

• [SLOW TEST:10.923 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 16:03:26.109: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2909
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-4517
STEP: Creating secret with name secret-test-2e887745-c3be-4acf-84e2-73c2489784d9
STEP: Creating a pod to test consume secrets
May  6 16:03:26.381: INFO: Waiting up to 5m0s for pod "pod-secrets-a3ad8d54-36e8-4bc0-b396-a159af1b13d1" in namespace "secrets-2909" to be "success or failure"
May  6 16:03:26.382: INFO: Pod "pod-secrets-a3ad8d54-36e8-4bc0-b396-a159af1b13d1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.656222ms
May  6 16:03:28.385: INFO: Pod "pod-secrets-a3ad8d54-36e8-4bc0-b396-a159af1b13d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004062731s
STEP: Saw pod success
May  6 16:03:28.385: INFO: Pod "pod-secrets-a3ad8d54-36e8-4bc0-b396-a159af1b13d1" satisfied condition "success or failure"
May  6 16:03:28.387: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-secrets-a3ad8d54-36e8-4bc0-b396-a159af1b13d1 container secret-volume-test: <nil>
STEP: delete the pod
May  6 16:03:28.398: INFO: Waiting for pod pod-secrets-a3ad8d54-36e8-4bc0-b396-a159af1b13d1 to disappear
May  6 16:03:28.400: INFO: Pod pod-secrets-a3ad8d54-36e8-4bc0-b396-a159af1b13d1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 16:03:28.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2909" for this suite.
May  6 16:03:34.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 16:03:37.164: INFO: namespace secrets-2909 deletion completed in 8.761008085s
STEP: Destroying namespace "secret-namespace-4517" for this suite.
May  6 16:03:43.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 16:03:45.924: INFO: namespace secret-namespace-4517 deletion completed in 8.76006343s

• [SLOW TEST:19.814 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 16:03:45.924: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9379
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-02db50a6-c257-4ad1-a1b6-696c40bed684
STEP: Creating a pod to test consume secrets
May  6 16:03:46.064: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fab3628e-6a8f-425b-8377-9bb058ebc138" in namespace "projected-9379" to be "success or failure"
May  6 16:03:46.066: INFO: Pod "pod-projected-secrets-fab3628e-6a8f-425b-8377-9bb058ebc138": Phase="Pending", Reason="", readiness=false. Elapsed: 1.766745ms
May  6 16:03:48.069: INFO: Pod "pod-projected-secrets-fab3628e-6a8f-425b-8377-9bb058ebc138": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004258208s
STEP: Saw pod success
May  6 16:03:48.069: INFO: Pod "pod-projected-secrets-fab3628e-6a8f-425b-8377-9bb058ebc138" satisfied condition "success or failure"
May  6 16:03:48.071: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-projected-secrets-fab3628e-6a8f-425b-8377-9bb058ebc138 container projected-secret-volume-test: <nil>
STEP: delete the pod
May  6 16:03:48.083: INFO: Waiting for pod pod-projected-secrets-fab3628e-6a8f-425b-8377-9bb058ebc138 to disappear
May  6 16:03:48.085: INFO: Pod pod-projected-secrets-fab3628e-6a8f-425b-8377-9bb058ebc138 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 16:03:48.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9379" for this suite.
May  6 16:03:54.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 16:03:56.848: INFO: namespace projected-9379 deletion completed in 8.759401084s

• [SLOW TEST:10.924 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 16:03:56.848: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1077
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-adb33772-d9f1-408d-88dd-b0e33c084f02
STEP: Creating a pod to test consume secrets
May  6 16:03:56.991: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-06002842-c3a8-4920-83c3-7a6cac7461a5" in namespace "projected-1077" to be "success or failure"
May  6 16:03:56.993: INFO: Pod "pod-projected-secrets-06002842-c3a8-4920-83c3-7a6cac7461a5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.821076ms
May  6 16:03:58.996: INFO: Pod "pod-projected-secrets-06002842-c3a8-4920-83c3-7a6cac7461a5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004510187s
May  6 16:04:00.999: INFO: Pod "pod-projected-secrets-06002842-c3a8-4920-83c3-7a6cac7461a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007531937s
STEP: Saw pod success
May  6 16:04:00.999: INFO: Pod "pod-projected-secrets-06002842-c3a8-4920-83c3-7a6cac7461a5" satisfied condition "success or failure"
May  6 16:04:01.001: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-projected-secrets-06002842-c3a8-4920-83c3-7a6cac7461a5 container projected-secret-volume-test: <nil>
STEP: delete the pod
May  6 16:04:01.017: INFO: Waiting for pod pod-projected-secrets-06002842-c3a8-4920-83c3-7a6cac7461a5 to disappear
May  6 16:04:01.020: INFO: Pod pod-projected-secrets-06002842-c3a8-4920-83c3-7a6cac7461a5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 16:04:01.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1077" for this suite.
May  6 16:04:07.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 16:04:09.782: INFO: namespace projected-1077 deletion completed in 8.759570855s

• [SLOW TEST:12.934 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 16:04:09.783: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6037
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May  6 16:04:09.931: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6037 /api/v1/namespaces/watch-6037/configmaps/e2e-watch-test-resource-version e5b7770f-fac2-4484-afc4-042e7df20118 137501 0 2020-05-06 16:04:09 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May  6 16:04:09.931: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6037 /api/v1/namespaces/watch-6037/configmaps/e2e-watch-test-resource-version e5b7770f-fac2-4484-afc4-042e7df20118 137502 0 2020-05-06 16:04:09 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 16:04:09.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6037" for this suite.
May  6 16:04:15.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 16:04:18.702: INFO: namespace watch-6037 deletion completed in 8.768844163s

• [SLOW TEST:8.920 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 16:04:18.702: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7768
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 16:04:18.851: INFO: Pod name rollover-pod: Found 0 pods out of 1
May  6 16:04:23.854: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May  6 16:04:23.854: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May  6 16:04:25.857: INFO: Creating deployment "test-rollover-deployment"
May  6 16:04:25.863: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May  6 16:04:27.867: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May  6 16:04:27.871: INFO: Ensure that both replica sets have 1 created replica
May  6 16:04:27.874: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May  6 16:04:27.879: INFO: Updating deployment test-rollover-deployment
May  6 16:04:27.879: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May  6 16:04:29.884: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May  6 16:04:29.888: INFO: Make sure deployment "test-rollover-deployment" is complete
May  6 16:04:29.891: INFO: all replica sets need to contain the pod-template-hash label
May  6 16:04:29.891: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724377865, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724377865, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724377869, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724377865, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 16:04:31.896: INFO: all replica sets need to contain the pod-template-hash label
May  6 16:04:31.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724377865, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724377865, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724377869, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724377865, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 16:04:33.896: INFO: all replica sets need to contain the pod-template-hash label
May  6 16:04:33.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724377865, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724377865, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724377869, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724377865, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 16:04:35.896: INFO: all replica sets need to contain the pod-template-hash label
May  6 16:04:35.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724377865, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724377865, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724377869, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724377865, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 16:04:37.897: INFO: all replica sets need to contain the pod-template-hash label
May  6 16:04:37.897: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724377865, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724377865, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724377869, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724377865, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 16:04:39.896: INFO: 
May  6 16:04:39.896: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May  6 16:04:39.902: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-7768 /apis/apps/v1/namespaces/deployment-7768/deployments/test-rollover-deployment 507f5bd8-83d7-45e8-b52e-c452c9ec2148 137734 2 2020-05-06 16:04:25 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00337d4c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-05-06 16:04:25 +0000 UTC,LastTransitionTime:2020-05-06 16:04:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-05-06 16:04:39 +0000 UTC,LastTransitionTime:2020-05-06 16:04:25 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  6 16:04:39.904: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-7768 /apis/apps/v1/namespaces/deployment-7768/replicasets/test-rollover-deployment-7d7dc6548c 539ec6de-2c4b-4524-887e-1a7a5eef88b3 137723 2 2020-05-06 16:04:27 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 507f5bd8-83d7-45e8-b52e-c452c9ec2148 0xc00337d987 0xc00337d988}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00337d9e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  6 16:04:39.904: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May  6 16:04:39.904: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7768 /apis/apps/v1/namespaces/deployment-7768/replicasets/test-rollover-controller 01c6a5de-8343-4a40-a492-7a70b385f9fd 137733 2 2020-05-06 16:04:18 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 507f5bd8-83d7-45e8-b52e-c452c9ec2148 0xc00337d8b7 0xc00337d8b8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00337d918 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  6 16:04:39.904: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-7768 /apis/apps/v1/namespaces/deployment-7768/replicasets/test-rollover-deployment-f6c94f66c 501fd2b5-5ff2-470b-8632-6860aea8bf96 137651 2 2020-05-06 16:04:25 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 507f5bd8-83d7-45e8-b52e-c452c9ec2148 0xc00337da50 0xc00337da51}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00337dac8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  6 16:04:39.906: INFO: Pod "test-rollover-deployment-7d7dc6548c-rhgqk" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-rhgqk test-rollover-deployment-7d7dc6548c- deployment-7768 /api/v1/namespaces/deployment-7768/pods/test-rollover-deployment-7d7dc6548c-rhgqk b973d613-4d35-47a1-a52e-167a1c04f167 137676 0 2020-05-06 16:04:27 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[cni.projectcalico.org/podIP:192.168.85.123/32 cni.projectcalico.org/podIPs:192.168.85.123/32] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c 539ec6de-2c4b-4524-887e-1a7a5eef88b3 0xc0004a8a27 0xc0004a8a28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sldgl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sldgl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sldgl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-80.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 16:04:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 16:04:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 16:04:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 16:04:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.80,PodIP:192.168.85.123,StartTime:2020-05-06 16:04:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 16:04:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:containerd://f1470d6b2a41d58107316f73e78c829cb220d72e5aae1fb43ed2df2b8dc46173,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.85.123,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 16:04:39.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7768" for this suite.
May  6 16:04:45.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 16:04:48.668: INFO: namespace deployment-7768 deletion completed in 8.75962768s

• [SLOW TEST:29.966 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 16:04:48.668: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5571
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-913a9e12-0190-40d6-8065-0a9405057e13
STEP: Creating a pod to test consume secrets
May  6 16:04:48.814: INFO: Waiting up to 5m0s for pod "pod-secrets-c4b07ffa-ea79-4341-81a1-26dd7e0145bd" in namespace "secrets-5571" to be "success or failure"
May  6 16:04:48.816: INFO: Pod "pod-secrets-c4b07ffa-ea79-4341-81a1-26dd7e0145bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.639811ms
May  6 16:04:50.818: INFO: Pod "pod-secrets-c4b07ffa-ea79-4341-81a1-26dd7e0145bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004133751s
STEP: Saw pod success
May  6 16:04:50.818: INFO: Pod "pod-secrets-c4b07ffa-ea79-4341-81a1-26dd7e0145bd" satisfied condition "success or failure"
May  6 16:04:50.820: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-secrets-c4b07ffa-ea79-4341-81a1-26dd7e0145bd container secret-volume-test: <nil>
STEP: delete the pod
May  6 16:04:50.834: INFO: Waiting for pod pod-secrets-c4b07ffa-ea79-4341-81a1-26dd7e0145bd to disappear
May  6 16:04:50.836: INFO: Pod pod-secrets-c4b07ffa-ea79-4341-81a1-26dd7e0145bd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 16:04:50.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5571" for this suite.
May  6 16:04:56.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 16:04:59.598: INFO: namespace secrets-5571 deletion completed in 8.759356808s

• [SLOW TEST:10.930 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 16:04:59.599: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3795
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
May  6 16:05:02.259: INFO: Successfully updated pod "annotationupdateb6d38974-15c2-49b8-a23f-8a6c85d4583f"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 16:05:04.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3795" for this suite.
May  6 16:05:16.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 16:05:19.032: INFO: namespace downward-api-3795 deletion completed in 14.761664213s

• [SLOW TEST:19.434 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 16:05:19.032: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6296
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
May  6 16:05:49.686: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0506 16:05:49.686441      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 16:05:49.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6296" for this suite.
May  6 16:05:55.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 16:05:58.448: INFO: namespace gc-6296 deletion completed in 8.759341899s

• [SLOW TEST:39.416 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 16:05:58.448: INFO: >>> kubeConfig: /tmp/kubeconfig-926700516
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5143
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-0df720ff-68b5-46ef-a13f-6790dfe7d06c
STEP: Creating a pod to test consume configMaps
May  6 16:05:58.591: INFO: Waiting up to 5m0s for pod "pod-configmaps-de9d2b06-04ea-41a4-9027-54c2f4e2f1ae" in namespace "configmap-5143" to be "success or failure"
May  6 16:05:58.593: INFO: Pod "pod-configmaps-de9d2b06-04ea-41a4-9027-54c2f4e2f1ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.164899ms
May  6 16:06:00.596: INFO: Pod "pod-configmaps-de9d2b06-04ea-41a4-9027-54c2f4e2f1ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005185142s
STEP: Saw pod success
May  6 16:06:00.596: INFO: Pod "pod-configmaps-de9d2b06-04ea-41a4-9027-54c2f4e2f1ae" satisfied condition "success or failure"
May  6 16:06:00.598: INFO: Trying to get logs from node ip-10-0-131-80.us-west-2.compute.internal pod pod-configmaps-de9d2b06-04ea-41a4-9027-54c2f4e2f1ae container configmap-volume-test: <nil>
STEP: delete the pod
May  6 16:06:00.609: INFO: Waiting for pod pod-configmaps-de9d2b06-04ea-41a4-9027-54c2f4e2f1ae to disappear
May  6 16:06:00.611: INFO: Pod pod-configmaps-de9d2b06-04ea-41a4-9027-54c2f4e2f1ae no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 16:06:00.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5143" for this suite.
May  6 16:06:06.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 16:06:09.373: INFO: namespace configmap-5143 deletion completed in 8.759396023s

• [SLOW TEST:10.925 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSMay  6 16:06:09.373: INFO: Running AfterSuite actions on all nodes
May  6 16:06:09.373: INFO: Running AfterSuite actions on node 1
May  6 16:06:09.373: INFO: Skipping dumping logs from cluster

Ran 276 of 4731 Specs in 7767.972 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4455 Skipped
PASS

Ginkgo ran 1 suite in 2h9m29.05072296s
Test Suite Passed
