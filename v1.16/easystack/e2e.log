I0312 07:11:43.343330      24 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-675793433
I0312 07:11:43.343598      24 e2e.go:92] Starting e2e run "89cb804f-94de-4d46-9e23-74bab84b7cf4" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1583997099 - Will randomize all specs
Will run 276 of 4731 specs

Mar 12 07:11:43.359: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 07:11:43.364: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar 12 07:11:43.389: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 12 07:11:43.456: INFO: The status of Pod kube-monitor-1583997000-qh8dq is Succeeded, skipping waiting
Mar 12 07:11:43.456: INFO: 23 / 24 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 12 07:11:43.456: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Mar 12 07:11:43.456: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 12 07:11:43.470: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'coredns' (0 seconds elapsed)
Mar 12 07:11:43.470: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'k8s-keystone-auth' (0 seconds elapsed)
Mar 12 07:11:43.470: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-flannel' (0 seconds elapsed)
Mar 12 07:11:43.470: INFO: e2e test version: v1.16.6
Mar 12 07:11:43.471: INFO: kube-apiserver version: v1.16.6-es
Mar 12 07:11:43.471: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 07:11:43.478: INFO: Cluster IP family: ipv4
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:11:43.478: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename downward-api
Mar 12 07:11:43.543: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 12 07:11:43.563: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4bd386b8-fc8f-4bc1-bca3-50440702befa" in namespace "downward-api-6379" to be "success or failure"
Mar 12 07:11:43.566: INFO: Pod "downwardapi-volume-4bd386b8-fc8f-4bc1-bca3-50440702befa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.325089ms
Mar 12 07:11:45.572: INFO: Pod "downwardapi-volume-4bd386b8-fc8f-4bc1-bca3-50440702befa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009341316s
Mar 12 07:11:47.579: INFO: Pod "downwardapi-volume-4bd386b8-fc8f-4bc1-bca3-50440702befa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015542424s
Mar 12 07:11:49.584: INFO: Pod "downwardapi-volume-4bd386b8-fc8f-4bc1-bca3-50440702befa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021198204s
Mar 12 07:11:51.590: INFO: Pod "downwardapi-volume-4bd386b8-fc8f-4bc1-bca3-50440702befa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026639599s
Mar 12 07:11:53.597: INFO: Pod "downwardapi-volume-4bd386b8-fc8f-4bc1-bca3-50440702befa": Phase="Pending", Reason="", readiness=false. Elapsed: 10.033955791s
Mar 12 07:11:55.603: INFO: Pod "downwardapi-volume-4bd386b8-fc8f-4bc1-bca3-50440702befa": Phase="Pending", Reason="", readiness=false. Elapsed: 12.039546884s
Mar 12 07:11:57.608: INFO: Pod "downwardapi-volume-4bd386b8-fc8f-4bc1-bca3-50440702befa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.045245821s
STEP: Saw pod success
Mar 12 07:11:57.608: INFO: Pod "downwardapi-volume-4bd386b8-fc8f-4bc1-bca3-50440702befa" satisfied condition "success or failure"
Mar 12 07:11:57.612: INFO: Trying to get logs from node node-3 pod downwardapi-volume-4bd386b8-fc8f-4bc1-bca3-50440702befa container client-container: <nil>
STEP: delete the pod
Mar 12 07:11:57.647: INFO: Waiting for pod downwardapi-volume-4bd386b8-fc8f-4bc1-bca3-50440702befa to disappear
Mar 12 07:11:57.650: INFO: Pod downwardapi-volume-4bd386b8-fc8f-4bc1-bca3-50440702befa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:11:57.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6379" for this suite.
Mar 12 07:12:03.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:12:03.801: INFO: namespace downward-api-6379 deletion completed in 6.146282565s

• [SLOW TEST:20.323 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:12:03.801: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4088
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-4088
Mar 12 07:12:03.888: INFO: Found 0 stateful pods, waiting for 1
Mar 12 07:12:13.894: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Mar 12 07:12:23.895: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar 12 07:12:23.928: INFO: Deleting all statefulset in ns statefulset-4088
Mar 12 07:12:23.934: INFO: Scaling statefulset ss to 0
Mar 12 07:12:43.979: INFO: Waiting for statefulset status.replicas updated to 0
Mar 12 07:12:43.983: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:12:43.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4088" for this suite.
Mar 12 07:12:50.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:12:50.120: INFO: namespace statefulset-4088 deletion completed in 6.116447144s

• [SLOW TEST:46.319 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:12:50.120: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 07:12:50.200: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-186675a4-ad55-41cb-a06d-717fddc6275c" in namespace "security-context-test-8031" to be "success or failure"
Mar 12 07:12:50.204: INFO: Pod "alpine-nnp-false-186675a4-ad55-41cb-a06d-717fddc6275c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.090221ms
Mar 12 07:12:52.209: INFO: Pod "alpine-nnp-false-186675a4-ad55-41cb-a06d-717fddc6275c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008712378s
Mar 12 07:12:54.214: INFO: Pod "alpine-nnp-false-186675a4-ad55-41cb-a06d-717fddc6275c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013883782s
Mar 12 07:12:56.220: INFO: Pod "alpine-nnp-false-186675a4-ad55-41cb-a06d-717fddc6275c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019501948s
Mar 12 07:12:58.225: INFO: Pod "alpine-nnp-false-186675a4-ad55-41cb-a06d-717fddc6275c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024916241s
Mar 12 07:13:00.232: INFO: Pod "alpine-nnp-false-186675a4-ad55-41cb-a06d-717fddc6275c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.031165169s
Mar 12 07:13:02.237: INFO: Pod "alpine-nnp-false-186675a4-ad55-41cb-a06d-717fddc6275c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.036969028s
Mar 12 07:13:04.244: INFO: Pod "alpine-nnp-false-186675a4-ad55-41cb-a06d-717fddc6275c": Phase="Running", Reason="", readiness=true. Elapsed: 14.043403732s
Mar 12 07:13:06.250: INFO: Pod "alpine-nnp-false-186675a4-ad55-41cb-a06d-717fddc6275c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.049035031s
Mar 12 07:13:06.250: INFO: Pod "alpine-nnp-false-186675a4-ad55-41cb-a06d-717fddc6275c" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:13:06.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8031" for this suite.
Mar 12 07:13:12.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:13:12.511: INFO: namespace security-context-test-8031 deletion completed in 6.246492972s

• [SLOW TEST:22.391 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:13:12.512: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 07:13:12.573: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Mar 12 07:13:20.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 --namespace=crd-publish-openapi-3796 create -f -'
Mar 12 07:13:24.080: INFO: stderr: ""
Mar 12 07:13:24.081: INFO: stdout: "e2e-test-crd-publish-openapi-4692-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 12 07:13:24.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 --namespace=crd-publish-openapi-3796 delete e2e-test-crd-publish-openapi-4692-crds test-foo'
Mar 12 07:13:24.248: INFO: stderr: ""
Mar 12 07:13:24.248: INFO: stdout: "e2e-test-crd-publish-openapi-4692-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar 12 07:13:24.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 --namespace=crd-publish-openapi-3796 apply -f -'
Mar 12 07:13:24.570: INFO: stderr: ""
Mar 12 07:13:24.570: INFO: stdout: "e2e-test-crd-publish-openapi-4692-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 12 07:13:24.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 --namespace=crd-publish-openapi-3796 delete e2e-test-crd-publish-openapi-4692-crds test-foo'
Mar 12 07:13:24.673: INFO: stderr: ""
Mar 12 07:13:24.674: INFO: stdout: "e2e-test-crd-publish-openapi-4692-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar 12 07:13:24.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 --namespace=crd-publish-openapi-3796 create -f -'
Mar 12 07:13:24.974: INFO: rc: 1
Mar 12 07:13:24.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 --namespace=crd-publish-openapi-3796 apply -f -'
Mar 12 07:13:25.586: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Mar 12 07:13:25.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 --namespace=crd-publish-openapi-3796 create -f -'
Mar 12 07:13:25.854: INFO: rc: 1
Mar 12 07:13:25.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 --namespace=crd-publish-openapi-3796 apply -f -'
Mar 12 07:13:26.634: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Mar 12 07:13:26.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 explain e2e-test-crd-publish-openapi-4692-crds'
Mar 12 07:13:26.905: INFO: stderr: ""
Mar 12 07:13:26.905: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4692-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Mar 12 07:13:26.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 explain e2e-test-crd-publish-openapi-4692-crds.metadata'
Mar 12 07:13:27.172: INFO: stderr: ""
Mar 12 07:13:27.172: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4692-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar 12 07:13:27.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 explain e2e-test-crd-publish-openapi-4692-crds.spec'
Mar 12 07:13:27.420: INFO: stderr: ""
Mar 12 07:13:27.420: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4692-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar 12 07:13:27.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 explain e2e-test-crd-publish-openapi-4692-crds.spec.bars'
Mar 12 07:13:27.659: INFO: stderr: ""
Mar 12 07:13:27.659: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4692-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Mar 12 07:13:27.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 explain e2e-test-crd-publish-openapi-4692-crds.spec.bars2'
Mar 12 07:13:28.019: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:13:31.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3796" for this suite.
Mar 12 07:13:37.543: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:13:37.646: INFO: namespace crd-publish-openapi-3796 deletion completed in 6.119816356s

• [SLOW TEST:25.134 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:13:37.647: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Mar 12 07:13:37.699: INFO: namespace kubectl-4707
Mar 12 07:13:37.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 create -f - --namespace=kubectl-4707'
Mar 12 07:13:38.016: INFO: stderr: ""
Mar 12 07:13:38.016: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar 12 07:13:39.021: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:13:39.021: INFO: Found 0 / 1
Mar 12 07:13:40.022: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:13:40.022: INFO: Found 0 / 1
Mar 12 07:13:41.023: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:13:41.023: INFO: Found 0 / 1
Mar 12 07:13:42.023: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:13:42.023: INFO: Found 0 / 1
Mar 12 07:13:43.022: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:13:43.022: INFO: Found 0 / 1
Mar 12 07:13:44.021: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:13:44.021: INFO: Found 0 / 1
Mar 12 07:13:45.022: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:13:45.022: INFO: Found 0 / 1
Mar 12 07:13:46.022: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:13:46.022: INFO: Found 0 / 1
Mar 12 07:13:47.022: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:13:47.022: INFO: Found 0 / 1
Mar 12 07:13:48.022: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:13:48.022: INFO: Found 0 / 1
Mar 12 07:13:49.021: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:13:49.021: INFO: Found 0 / 1
Mar 12 07:13:50.021: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:13:50.021: INFO: Found 0 / 1
Mar 12 07:13:51.022: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:13:51.022: INFO: Found 0 / 1
Mar 12 07:13:52.021: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:13:52.021: INFO: Found 0 / 1
Mar 12 07:13:53.026: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:13:53.026: INFO: Found 1 / 1
Mar 12 07:13:53.026: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 12 07:13:53.030: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:13:53.030: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 12 07:13:53.030: INFO: wait on redis-master startup in kubectl-4707 
Mar 12 07:13:53.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 logs redis-master-hvnw8 redis-master --namespace=kubectl-4707'
Mar 12 07:13:53.170: INFO: stderr: ""
Mar 12 07:13:53.170: INFO: stdout: "1:C 12 Mar 2020 07:13:52.753 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 12 Mar 2020 07:13:52.753 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 12 Mar 2020 07:13:52.753 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 12 Mar 2020 07:13:52.757 * Running mode=standalone, port=6379.\n1:M 12 Mar 2020 07:13:52.757 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 12 Mar 2020 07:13:52.757 # Server initialized\n1:M 12 Mar 2020 07:13:52.757 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 12 Mar 2020 07:13:52.757 * Ready to accept connections\n"
STEP: exposing RC
Mar 12 07:13:53.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-4707'
Mar 12 07:13:53.301: INFO: stderr: ""
Mar 12 07:13:53.301: INFO: stdout: "service/rm2 exposed\n"
Mar 12 07:13:53.310: INFO: Service rm2 in namespace kubectl-4707 found.
STEP: exposing service
Mar 12 07:13:55.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-4707'
Mar 12 07:13:55.451: INFO: stderr: ""
Mar 12 07:13:55.451: INFO: stdout: "service/rm3 exposed\n"
Mar 12 07:13:55.457: INFO: Service rm3 in namespace kubectl-4707 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:13:57.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4707" for this suite.
Mar 12 07:14:25.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:14:25.593: INFO: namespace kubectl-4707 deletion completed in 28.122500053s

• [SLOW TEST:47.947 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:14:25.595: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-893757c5-c7f9-45e3-840c-675b5f27425a
STEP: Creating a pod to test consume secrets
Mar 12 07:14:25.674: INFO: Waiting up to 5m0s for pod "pod-secrets-a89389e1-181f-4a1c-bc1c-7a6eeb752211" in namespace "secrets-1510" to be "success or failure"
Mar 12 07:14:25.677: INFO: Pod "pod-secrets-a89389e1-181f-4a1c-bc1c-7a6eeb752211": Phase="Pending", Reason="", readiness=false. Elapsed: 2.901623ms
Mar 12 07:14:27.683: INFO: Pod "pod-secrets-a89389e1-181f-4a1c-bc1c-7a6eeb752211": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009104606s
Mar 12 07:14:29.689: INFO: Pod "pod-secrets-a89389e1-181f-4a1c-bc1c-7a6eeb752211": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015055743s
Mar 12 07:14:31.694: INFO: Pod "pod-secrets-a89389e1-181f-4a1c-bc1c-7a6eeb752211": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020285665s
Mar 12 07:14:33.701: INFO: Pod "pod-secrets-a89389e1-181f-4a1c-bc1c-7a6eeb752211": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026939745s
Mar 12 07:14:35.706: INFO: Pod "pod-secrets-a89389e1-181f-4a1c-bc1c-7a6eeb752211": Phase="Pending", Reason="", readiness=false. Elapsed: 10.031981318s
Mar 12 07:14:37.711: INFO: Pod "pod-secrets-a89389e1-181f-4a1c-bc1c-7a6eeb752211": Phase="Pending", Reason="", readiness=false. Elapsed: 12.036850299s
Mar 12 07:14:39.717: INFO: Pod "pod-secrets-a89389e1-181f-4a1c-bc1c-7a6eeb752211": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.042876526s
STEP: Saw pod success
Mar 12 07:14:39.717: INFO: Pod "pod-secrets-a89389e1-181f-4a1c-bc1c-7a6eeb752211" satisfied condition "success or failure"
Mar 12 07:14:39.721: INFO: Trying to get logs from node node-3 pod pod-secrets-a89389e1-181f-4a1c-bc1c-7a6eeb752211 container secret-volume-test: <nil>
STEP: delete the pod
Mar 12 07:14:39.784: INFO: Waiting for pod pod-secrets-a89389e1-181f-4a1c-bc1c-7a6eeb752211 to disappear
Mar 12 07:14:39.788: INFO: Pod pod-secrets-a89389e1-181f-4a1c-bc1c-7a6eeb752211 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:14:39.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1510" for this suite.
Mar 12 07:14:47.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:14:47.905: INFO: namespace secrets-1510 deletion completed in 8.112126733s

• [SLOW TEST:22.310 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:14:47.906: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:15:19.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3226" for this suite.
Mar 12 07:15:26.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:15:26.100: INFO: namespace job-3226 deletion completed in 6.110515921s

• [SLOW TEST:38.195 seconds]
[sig-apps] Job
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:15:26.101: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 12 07:15:26.179: INFO: Waiting up to 5m0s for pod "pod-39c964cf-bf4d-4aa8-82ba-9082153b86e4" in namespace "emptydir-6146" to be "success or failure"
Mar 12 07:15:26.181: INFO: Pod "pod-39c964cf-bf4d-4aa8-82ba-9082153b86e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.642532ms
Mar 12 07:15:28.186: INFO: Pod "pod-39c964cf-bf4d-4aa8-82ba-9082153b86e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007672555s
Mar 12 07:15:30.191: INFO: Pod "pod-39c964cf-bf4d-4aa8-82ba-9082153b86e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012230396s
Mar 12 07:15:32.212: INFO: Pod "pod-39c964cf-bf4d-4aa8-82ba-9082153b86e4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033491912s
Mar 12 07:15:34.216: INFO: Pod "pod-39c964cf-bf4d-4aa8-82ba-9082153b86e4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.037382125s
Mar 12 07:15:36.221: INFO: Pod "pod-39c964cf-bf4d-4aa8-82ba-9082153b86e4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.042551751s
Mar 12 07:15:38.277: INFO: Pod "pod-39c964cf-bf4d-4aa8-82ba-9082153b86e4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.098430047s
Mar 12 07:15:40.283: INFO: Pod "pod-39c964cf-bf4d-4aa8-82ba-9082153b86e4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.104160211s
Mar 12 07:15:42.289: INFO: Pod "pod-39c964cf-bf4d-4aa8-82ba-9082153b86e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.110337746s
STEP: Saw pod success
Mar 12 07:15:42.289: INFO: Pod "pod-39c964cf-bf4d-4aa8-82ba-9082153b86e4" satisfied condition "success or failure"
Mar 12 07:15:42.293: INFO: Trying to get logs from node node-2 pod pod-39c964cf-bf4d-4aa8-82ba-9082153b86e4 container test-container: <nil>
STEP: delete the pod
Mar 12 07:15:42.336: INFO: Waiting for pod pod-39c964cf-bf4d-4aa8-82ba-9082153b86e4 to disappear
Mar 12 07:15:42.340: INFO: Pod pod-39c964cf-bf4d-4aa8-82ba-9082153b86e4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:15:42.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6146" for this suite.
Mar 12 07:15:48.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:15:48.466: INFO: namespace emptydir-6146 deletion completed in 6.120679154s

• [SLOW TEST:22.365 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:15:48.467: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 12 07:15:48.943: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 12 07:15:50.958: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:15:52.964: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:15:54.963: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:15:56.963: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:15:58.965: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:16:00.963: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:16:03.469: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594148, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 12 07:16:05.980: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Mar 12 07:16:22.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 attach --namespace=webhook-7267 to-be-attached-pod -i -c=container1'
Mar 12 07:16:22.194: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:16:22.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7267" for this suite.
Mar 12 07:16:34.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:16:34.338: INFO: namespace webhook-7267 deletion completed in 12.124454507s
STEP: Destroying namespace "webhook-7267-markers" for this suite.
Mar 12 07:16:40.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:16:40.458: INFO: namespace webhook-7267-markers deletion completed in 6.120336154s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:52.009 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:16:40.476: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7761.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7761.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7761.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7761.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 12 07:17:00.568: INFO: File jessie_udp@dns-test-service-3.dns-7761.svc.cluster.local from pod  dns-7761/dns-test-6a18e8d8-283f-474a-8af0-7325839e8bd0 contains '' instead of 'foo.example.com.'
Mar 12 07:17:00.568: INFO: Lookups using dns-7761/dns-test-6a18e8d8-283f-474a-8af0-7325839e8bd0 failed for: [jessie_udp@dns-test-service-3.dns-7761.svc.cluster.local]

Mar 12 07:17:05.578: INFO: DNS probes using dns-test-6a18e8d8-283f-474a-8af0-7325839e8bd0 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7761.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7761.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7761.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7761.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 12 07:17:25.704: INFO: File jessie_udp@dns-test-service-3.dns-7761.svc.cluster.local from pod  dns-7761/dns-test-411b234f-21a6-43e4-9812-82d48488ea5e contains '' instead of 'bar.example.com.'
Mar 12 07:17:25.704: INFO: Lookups using dns-7761/dns-test-411b234f-21a6-43e4-9812-82d48488ea5e failed for: [jessie_udp@dns-test-service-3.dns-7761.svc.cluster.local]

Mar 12 07:17:30.715: INFO: DNS probes using dns-test-411b234f-21a6-43e4-9812-82d48488ea5e succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7761.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7761.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7761.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7761.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 12 07:17:46.871: INFO: DNS probes using dns-test-ac2a5810-e470-4bd7-b3ca-90b75bdbcbed succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:17:46.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7761" for this suite.
Mar 12 07:17:52.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:17:53.046: INFO: namespace dns-7761 deletion completed in 6.123642793s

• [SLOW TEST:72.570 seconds]
[sig-network] DNS
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:17:53.046: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:18:06.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6475" for this suite.
Mar 12 07:18:12.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:18:12.424: INFO: namespace resourcequota-6475 deletion completed in 6.106993648s

• [SLOW TEST:19.378 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:18:12.424: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:18:26.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5138" for this suite.
Mar 12 07:19:10.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:19:10.662: INFO: namespace kubelet-test-5138 deletion completed in 44.116604387s

• [SLOW TEST:58.238 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:19:10.662: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-6458
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 12 07:19:10.716: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 12 07:19:56.848: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.57:8080/dial?request=hostName&protocol=udp&host=10.233.65.56&port=8081&tries=1'] Namespace:pod-network-test-6458 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 12 07:19:56.848: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 07:19:57.771: INFO: Waiting for endpoints: map[]
Mar 12 07:19:57.778: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.57:8080/dial?request=hostName&protocol=udp&host=10.233.64.248&port=8081&tries=1'] Namespace:pod-network-test-6458 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 12 07:19:57.778: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 07:19:57.879: INFO: Waiting for endpoints: map[]
Mar 12 07:19:57.883: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.57:8080/dial?request=hostName&protocol=udp&host=10.233.66.21&port=8081&tries=1'] Namespace:pod-network-test-6458 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 12 07:19:57.883: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 07:19:57.973: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:19:57.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6458" for this suite.
Mar 12 07:20:09.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:20:10.137: INFO: namespace pod-network-test-6458 deletion completed in 12.156101211s

• [SLOW TEST:59.475 seconds]
[sig-network] Networking
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:20:10.138: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:20:26.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2051" for this suite.
Mar 12 07:20:32.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:20:32.450: INFO: namespace resourcequota-2051 deletion completed in 6.116447583s

• [SLOW TEST:22.313 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:20:32.450: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar 12 07:20:47.055: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5863 pod-service-account-316e1b62-8547-411e-a030-a06234386cbc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar 12 07:20:47.286: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5863 pod-service-account-316e1b62-8547-411e-a030-a06234386cbc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar 12 07:20:47.454: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5863 pod-service-account-316e1b62-8547-411e-a030-a06234386cbc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:20:47.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5863" for this suite.
Mar 12 07:20:53.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:20:53.743: INFO: namespace svcaccounts-5863 deletion completed in 6.114203168s

• [SLOW TEST:21.292 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:20:53.743: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:21:00.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4062" for this suite.
Mar 12 07:21:06.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:21:06.952: INFO: namespace resourcequota-4062 deletion completed in 6.125781271s

• [SLOW TEST:13.209 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:21:06.953: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 12 07:21:07.030: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b8f85c36-3f2c-4a77-933e-de9e6c3d8619" in namespace "projected-3800" to be "success or failure"
Mar 12 07:21:07.034: INFO: Pod "downwardapi-volume-b8f85c36-3f2c-4a77-933e-de9e6c3d8619": Phase="Pending", Reason="", readiness=false. Elapsed: 3.861191ms
Mar 12 07:21:09.040: INFO: Pod "downwardapi-volume-b8f85c36-3f2c-4a77-933e-de9e6c3d8619": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009192995s
Mar 12 07:21:11.045: INFO: Pod "downwardapi-volume-b8f85c36-3f2c-4a77-933e-de9e6c3d8619": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014943653s
Mar 12 07:21:13.050: INFO: Pod "downwardapi-volume-b8f85c36-3f2c-4a77-933e-de9e6c3d8619": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019525504s
Mar 12 07:21:15.055: INFO: Pod "downwardapi-volume-b8f85c36-3f2c-4a77-933e-de9e6c3d8619": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024853945s
Mar 12 07:21:17.061: INFO: Pod "downwardapi-volume-b8f85c36-3f2c-4a77-933e-de9e6c3d8619": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030865171s
Mar 12 07:21:19.067: INFO: Pod "downwardapi-volume-b8f85c36-3f2c-4a77-933e-de9e6c3d8619": Phase="Pending", Reason="", readiness=false. Elapsed: 12.036985399s
Mar 12 07:21:21.073: INFO: Pod "downwardapi-volume-b8f85c36-3f2c-4a77-933e-de9e6c3d8619": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.042716883s
STEP: Saw pod success
Mar 12 07:21:21.073: INFO: Pod "downwardapi-volume-b8f85c36-3f2c-4a77-933e-de9e6c3d8619" satisfied condition "success or failure"
Mar 12 07:21:21.077: INFO: Trying to get logs from node node-1 pod downwardapi-volume-b8f85c36-3f2c-4a77-933e-de9e6c3d8619 container client-container: <nil>
STEP: delete the pod
Mar 12 07:21:21.133: INFO: Waiting for pod downwardapi-volume-b8f85c36-3f2c-4a77-933e-de9e6c3d8619 to disappear
Mar 12 07:21:21.138: INFO: Pod downwardapi-volume-b8f85c36-3f2c-4a77-933e-de9e6c3d8619 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:21:21.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3800" for this suite.
Mar 12 07:21:27.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:21:27.266: INFO: namespace projected-3800 deletion completed in 6.122508758s

• [SLOW TEST:20.314 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:21:27.266: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-dgqm
STEP: Creating a pod to test atomic-volume-subpath
Mar 12 07:21:27.359: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-dgqm" in namespace "subpath-8852" to be "success or failure"
Mar 12 07:21:27.363: INFO: Pod "pod-subpath-test-downwardapi-dgqm": Phase="Pending", Reason="", readiness=false. Elapsed: 3.625796ms
Mar 12 07:21:29.368: INFO: Pod "pod-subpath-test-downwardapi-dgqm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008724998s
Mar 12 07:21:31.374: INFO: Pod "pod-subpath-test-downwardapi-dgqm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014341624s
Mar 12 07:21:33.389: INFO: Pod "pod-subpath-test-downwardapi-dgqm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030179221s
Mar 12 07:21:35.395: INFO: Pod "pod-subpath-test-downwardapi-dgqm": Phase="Pending", Reason="", readiness=false. Elapsed: 8.035778782s
Mar 12 07:21:37.402: INFO: Pod "pod-subpath-test-downwardapi-dgqm": Phase="Pending", Reason="", readiness=false. Elapsed: 10.042379748s
Mar 12 07:21:39.408: INFO: Pod "pod-subpath-test-downwardapi-dgqm": Phase="Pending", Reason="", readiness=false. Elapsed: 12.048287198s
Mar 12 07:21:41.413: INFO: Pod "pod-subpath-test-downwardapi-dgqm": Phase="Running", Reason="", readiness=true. Elapsed: 14.054121052s
Mar 12 07:21:43.419: INFO: Pod "pod-subpath-test-downwardapi-dgqm": Phase="Running", Reason="", readiness=true. Elapsed: 16.059503031s
Mar 12 07:21:45.424: INFO: Pod "pod-subpath-test-downwardapi-dgqm": Phase="Running", Reason="", readiness=true. Elapsed: 18.064455543s
Mar 12 07:21:47.429: INFO: Pod "pod-subpath-test-downwardapi-dgqm": Phase="Running", Reason="", readiness=true. Elapsed: 20.070198829s
Mar 12 07:21:49.434: INFO: Pod "pod-subpath-test-downwardapi-dgqm": Phase="Running", Reason="", readiness=true. Elapsed: 22.074478013s
Mar 12 07:21:51.439: INFO: Pod "pod-subpath-test-downwardapi-dgqm": Phase="Running", Reason="", readiness=true. Elapsed: 24.079947684s
Mar 12 07:21:53.445: INFO: Pod "pod-subpath-test-downwardapi-dgqm": Phase="Running", Reason="", readiness=true. Elapsed: 26.08583614s
Mar 12 07:21:55.450: INFO: Pod "pod-subpath-test-downwardapi-dgqm": Phase="Running", Reason="", readiness=true. Elapsed: 28.09097596s
Mar 12 07:21:57.455: INFO: Pod "pod-subpath-test-downwardapi-dgqm": Phase="Running", Reason="", readiness=true. Elapsed: 30.096016197s
Mar 12 07:21:59.461: INFO: Pod "pod-subpath-test-downwardapi-dgqm": Phase="Running", Reason="", readiness=true. Elapsed: 32.101310611s
Mar 12 07:22:01.466: INFO: Pod "pod-subpath-test-downwardapi-dgqm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.106492421s
STEP: Saw pod success
Mar 12 07:22:01.466: INFO: Pod "pod-subpath-test-downwardapi-dgqm" satisfied condition "success or failure"
Mar 12 07:22:01.468: INFO: Trying to get logs from node node-2 pod pod-subpath-test-downwardapi-dgqm container test-container-subpath-downwardapi-dgqm: <nil>
STEP: delete the pod
Mar 12 07:22:01.501: INFO: Waiting for pod pod-subpath-test-downwardapi-dgqm to disappear
Mar 12 07:22:01.505: INFO: Pod pod-subpath-test-downwardapi-dgqm no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-dgqm
Mar 12 07:22:01.505: INFO: Deleting pod "pod-subpath-test-downwardapi-dgqm" in namespace "subpath-8852"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:22:01.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8852" for this suite.
Mar 12 07:22:07.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:22:07.623: INFO: namespace subpath-8852 deletion completed in 6.10943022s

• [SLOW TEST:40.356 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:22:07.623: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 07:22:07.686: INFO: (0) /api/v1/nodes/node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 5.493689ms)
Mar 12 07:22:07.692: INFO: (1) /api/v1/nodes/node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 6.23985ms)
Mar 12 07:22:07.697: INFO: (2) /api/v1/nodes/node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 4.92771ms)
Mar 12 07:22:07.702: INFO: (3) /api/v1/nodes/node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 5.043973ms)
Mar 12 07:22:07.706: INFO: (4) /api/v1/nodes/node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 4.185086ms)
Mar 12 07:22:07.712: INFO: (5) /api/v1/nodes/node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 5.459573ms)
Mar 12 07:22:07.719: INFO: (6) /api/v1/nodes/node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 7.728995ms)
Mar 12 07:22:07.724: INFO: (7) /api/v1/nodes/node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 4.538457ms)
Mar 12 07:22:07.728: INFO: (8) /api/v1/nodes/node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 3.94781ms)
Mar 12 07:22:07.732: INFO: (9) /api/v1/nodes/node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 4.321214ms)
Mar 12 07:22:07.738: INFO: (10) /api/v1/nodes/node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 5.619657ms)
Mar 12 07:22:07.742: INFO: (11) /api/v1/nodes/node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 4.121628ms)
Mar 12 07:22:07.746: INFO: (12) /api/v1/nodes/node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 4.042448ms)
Mar 12 07:22:07.750: INFO: (13) /api/v1/nodes/node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 3.857344ms)
Mar 12 07:22:07.754: INFO: (14) /api/v1/nodes/node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 3.770385ms)
Mar 12 07:22:07.758: INFO: (15) /api/v1/nodes/node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 4.082877ms)
Mar 12 07:22:07.762: INFO: (16) /api/v1/nodes/node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 3.869883ms)
Mar 12 07:22:07.766: INFO: (17) /api/v1/nodes/node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 3.815613ms)
Mar 12 07:22:07.771: INFO: (18) /api/v1/nodes/node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 5.37636ms)
Mar 12 07:22:07.775: INFO: (19) /api/v1/nodes/node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 4.127448ms)
[AfterEach] version v1
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:22:07.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5232" for this suite.
Mar 12 07:22:13.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:22:13.884: INFO: namespace proxy-5232 deletion completed in 6.104049253s

• [SLOW TEST:6.261 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:22:13.884: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-66df6b3f-e72c-4d5c-b9f0-424719a476af
STEP: Creating a pod to test consume secrets
Mar 12 07:22:13.962: INFO: Waiting up to 5m0s for pod "pod-secrets-2d7fd677-bcfb-42a4-9b17-e87c7853bbe4" in namespace "secrets-5321" to be "success or failure"
Mar 12 07:22:13.965: INFO: Pod "pod-secrets-2d7fd677-bcfb-42a4-9b17-e87c7853bbe4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.912681ms
Mar 12 07:22:15.971: INFO: Pod "pod-secrets-2d7fd677-bcfb-42a4-9b17-e87c7853bbe4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008528927s
Mar 12 07:22:17.977: INFO: Pod "pod-secrets-2d7fd677-bcfb-42a4-9b17-e87c7853bbe4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014679349s
Mar 12 07:22:19.982: INFO: Pod "pod-secrets-2d7fd677-bcfb-42a4-9b17-e87c7853bbe4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019912457s
Mar 12 07:22:21.988: INFO: Pod "pod-secrets-2d7fd677-bcfb-42a4-9b17-e87c7853bbe4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025605149s
Mar 12 07:22:23.993: INFO: Pod "pod-secrets-2d7fd677-bcfb-42a4-9b17-e87c7853bbe4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.031158269s
Mar 12 07:22:25.999: INFO: Pod "pod-secrets-2d7fd677-bcfb-42a4-9b17-e87c7853bbe4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.037042621s
Mar 12 07:22:28.005: INFO: Pod "pod-secrets-2d7fd677-bcfb-42a4-9b17-e87c7853bbe4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.042870825s
STEP: Saw pod success
Mar 12 07:22:28.005: INFO: Pod "pod-secrets-2d7fd677-bcfb-42a4-9b17-e87c7853bbe4" satisfied condition "success or failure"
Mar 12 07:22:28.009: INFO: Trying to get logs from node node-3 pod pod-secrets-2d7fd677-bcfb-42a4-9b17-e87c7853bbe4 container secret-volume-test: <nil>
STEP: delete the pod
Mar 12 07:22:28.059: INFO: Waiting for pod pod-secrets-2d7fd677-bcfb-42a4-9b17-e87c7853bbe4 to disappear
Mar 12 07:22:28.063: INFO: Pod pod-secrets-2d7fd677-bcfb-42a4-9b17-e87c7853bbe4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:22:28.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5321" for this suite.
Mar 12 07:22:34.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:22:34.196: INFO: namespace secrets-5321 deletion completed in 6.127935328s

• [SLOW TEST:20.312 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:22:34.196: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 12 07:22:34.261: INFO: Waiting up to 5m0s for pod "downwardapi-volume-673cb850-8d37-4dce-9065-904ab9c2309f" in namespace "downward-api-8657" to be "success or failure"
Mar 12 07:22:34.264: INFO: Pod "downwardapi-volume-673cb850-8d37-4dce-9065-904ab9c2309f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.377674ms
Mar 12 07:22:36.270: INFO: Pod "downwardapi-volume-673cb850-8d37-4dce-9065-904ab9c2309f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009122268s
Mar 12 07:22:38.276: INFO: Pod "downwardapi-volume-673cb850-8d37-4dce-9065-904ab9c2309f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015239643s
Mar 12 07:22:40.283: INFO: Pod "downwardapi-volume-673cb850-8d37-4dce-9065-904ab9c2309f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022186686s
Mar 12 07:22:42.289: INFO: Pod "downwardapi-volume-673cb850-8d37-4dce-9065-904ab9c2309f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02819133s
Mar 12 07:22:44.295: INFO: Pod "downwardapi-volume-673cb850-8d37-4dce-9065-904ab9c2309f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.034046716s
Mar 12 07:22:46.302: INFO: Pod "downwardapi-volume-673cb850-8d37-4dce-9065-904ab9c2309f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.041511888s
Mar 12 07:22:48.307: INFO: Pod "downwardapi-volume-673cb850-8d37-4dce-9065-904ab9c2309f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.046431085s
STEP: Saw pod success
Mar 12 07:22:48.307: INFO: Pod "downwardapi-volume-673cb850-8d37-4dce-9065-904ab9c2309f" satisfied condition "success or failure"
Mar 12 07:22:48.311: INFO: Trying to get logs from node node-2 pod downwardapi-volume-673cb850-8d37-4dce-9065-904ab9c2309f container client-container: <nil>
STEP: delete the pod
Mar 12 07:22:48.338: INFO: Waiting for pod downwardapi-volume-673cb850-8d37-4dce-9065-904ab9c2309f to disappear
Mar 12 07:22:48.341: INFO: Pod downwardapi-volume-673cb850-8d37-4dce-9065-904ab9c2309f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:22:48.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8657" for this suite.
Mar 12 07:22:54.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:22:54.464: INFO: namespace downward-api-8657 deletion completed in 6.117778855s

• [SLOW TEST:20.268 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:22:54.465: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar 12 07:22:54.522: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar 12 07:23:12.790: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 07:23:20.906: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:23:39.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9532" for this suite.
Mar 12 07:23:45.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:23:45.164: INFO: namespace crd-publish-openapi-9532 deletion completed in 6.113950883s

• [SLOW TEST:50.699 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:23:45.164: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:23:45.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4270" for this suite.
Mar 12 07:23:57.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:23:57.332: INFO: namespace pods-4270 deletion completed in 12.104140642s

• [SLOW TEST:12.168 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:23:57.332: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-6718f2cf-cd02-46d0-819d-3ca06da3cf7c
STEP: Creating a pod to test consume configMaps
Mar 12 07:23:57.389: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5a1419a5-0114-463b-80c9-f8150a4a4b82" in namespace "projected-2931" to be "success or failure"
Mar 12 07:23:57.392: INFO: Pod "pod-projected-configmaps-5a1419a5-0114-463b-80c9-f8150a4a4b82": Phase="Pending", Reason="", readiness=false. Elapsed: 3.002772ms
Mar 12 07:23:59.397: INFO: Pod "pod-projected-configmaps-5a1419a5-0114-463b-80c9-f8150a4a4b82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007578529s
Mar 12 07:24:01.402: INFO: Pod "pod-projected-configmaps-5a1419a5-0114-463b-80c9-f8150a4a4b82": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012931569s
Mar 12 07:24:03.407: INFO: Pod "pod-projected-configmaps-5a1419a5-0114-463b-80c9-f8150a4a4b82": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018195959s
Mar 12 07:24:05.412: INFO: Pod "pod-projected-configmaps-5a1419a5-0114-463b-80c9-f8150a4a4b82": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022636416s
Mar 12 07:24:07.417: INFO: Pod "pod-projected-configmaps-5a1419a5-0114-463b-80c9-f8150a4a4b82": Phase="Pending", Reason="", readiness=false. Elapsed: 10.027515479s
Mar 12 07:24:09.421: INFO: Pod "pod-projected-configmaps-5a1419a5-0114-463b-80c9-f8150a4a4b82": Phase="Pending", Reason="", readiness=false. Elapsed: 12.032242291s
Mar 12 07:24:11.427: INFO: Pod "pod-projected-configmaps-5a1419a5-0114-463b-80c9-f8150a4a4b82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.037348988s
STEP: Saw pod success
Mar 12 07:24:11.427: INFO: Pod "pod-projected-configmaps-5a1419a5-0114-463b-80c9-f8150a4a4b82" satisfied condition "success or failure"
Mar 12 07:24:11.429: INFO: Trying to get logs from node node-2 pod pod-projected-configmaps-5a1419a5-0114-463b-80c9-f8150a4a4b82 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 12 07:24:11.465: INFO: Waiting for pod pod-projected-configmaps-5a1419a5-0114-463b-80c9-f8150a4a4b82 to disappear
Mar 12 07:24:11.469: INFO: Pod pod-projected-configmaps-5a1419a5-0114-463b-80c9-f8150a4a4b82 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:24:11.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2931" for this suite.
Mar 12 07:24:17.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:24:17.570: INFO: namespace projected-2931 deletion completed in 6.095946567s

• [SLOW TEST:20.238 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:24:17.570: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1935
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-1935
I0312 07:24:17.646983      24 runners.go:184] Created replication controller with name: externalname-service, namespace: services-1935, replica count: 2
I0312 07:24:20.697543      24 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 07:24:23.697888      24 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 07:24:26.698151      24 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 07:24:29.698454      24 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 12 07:24:32.698: INFO: Creating new exec pod
I0312 07:24:32.698787      24 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 12 07:24:47.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=services-1935 execpodcsrfv -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 12 07:24:48.731: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 12 07:24:48.731: INFO: stdout: ""
Mar 12 07:24:48.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=services-1935 execpodcsrfv -- /bin/sh -x -c nc -zv -t -w 2 10.233.32.106 80'
Mar 12 07:24:48.917: INFO: stderr: "+ nc -zv -t -w 2 10.233.32.106 80\nConnection to 10.233.32.106 80 port [tcp/http] succeeded!\n"
Mar 12 07:24:48.917: INFO: stdout: ""
Mar 12 07:24:48.917: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:24:48.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1935" for this suite.
Mar 12 07:24:54.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:24:55.077: INFO: namespace services-1935 deletion completed in 6.116509309s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:37.507 seconds]
[sig-network] Services
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:24:55.077: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0312 07:24:56.204916      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 12 07:24:56.205: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:24:56.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5019" for this suite.
Mar 12 07:25:02.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:25:02.364: INFO: namespace gc-5019 deletion completed in 6.152086927s

• [SLOW TEST:7.287 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:25:02.364: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 12 07:25:02.427: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9a92f561-db24-4fe7-bdcd-6aa23c8d8a94" in namespace "projected-3114" to be "success or failure"
Mar 12 07:25:02.431: INFO: Pod "downwardapi-volume-9a92f561-db24-4fe7-bdcd-6aa23c8d8a94": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005489ms
Mar 12 07:25:04.438: INFO: Pod "downwardapi-volume-9a92f561-db24-4fe7-bdcd-6aa23c8d8a94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010793187s
Mar 12 07:25:06.443: INFO: Pod "downwardapi-volume-9a92f561-db24-4fe7-bdcd-6aa23c8d8a94": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015797188s
Mar 12 07:25:08.449: INFO: Pod "downwardapi-volume-9a92f561-db24-4fe7-bdcd-6aa23c8d8a94": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022309939s
Mar 12 07:25:10.456: INFO: Pod "downwardapi-volume-9a92f561-db24-4fe7-bdcd-6aa23c8d8a94": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02918684s
Mar 12 07:25:12.464: INFO: Pod "downwardapi-volume-9a92f561-db24-4fe7-bdcd-6aa23c8d8a94": Phase="Pending", Reason="", readiness=false. Elapsed: 10.036970583s
Mar 12 07:25:14.471: INFO: Pod "downwardapi-volume-9a92f561-db24-4fe7-bdcd-6aa23c8d8a94": Phase="Pending", Reason="", readiness=false. Elapsed: 12.043999012s
Mar 12 07:25:16.476: INFO: Pod "downwardapi-volume-9a92f561-db24-4fe7-bdcd-6aa23c8d8a94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.049132649s
STEP: Saw pod success
Mar 12 07:25:16.476: INFO: Pod "downwardapi-volume-9a92f561-db24-4fe7-bdcd-6aa23c8d8a94" satisfied condition "success or failure"
Mar 12 07:25:16.479: INFO: Trying to get logs from node node-2 pod downwardapi-volume-9a92f561-db24-4fe7-bdcd-6aa23c8d8a94 container client-container: <nil>
STEP: delete the pod
Mar 12 07:25:16.506: INFO: Waiting for pod downwardapi-volume-9a92f561-db24-4fe7-bdcd-6aa23c8d8a94 to disappear
Mar 12 07:25:16.508: INFO: Pod downwardapi-volume-9a92f561-db24-4fe7-bdcd-6aa23c8d8a94 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:25:16.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3114" for this suite.
Mar 12 07:25:22.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:25:22.633: INFO: namespace projected-3114 deletion completed in 6.121830149s

• [SLOW TEST:20.269 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:25:22.634: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 07:25:22.685: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 12 07:25:31.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 --namespace=crd-publish-openapi-249 create -f -'
Mar 12 07:25:32.571: INFO: stderr: ""
Mar 12 07:25:32.571: INFO: stdout: "e2e-test-crd-publish-openapi-6869-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 12 07:25:32.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 --namespace=crd-publish-openapi-249 delete e2e-test-crd-publish-openapi-6869-crds test-cr'
Mar 12 07:25:32.751: INFO: stderr: ""
Mar 12 07:25:32.751: INFO: stdout: "e2e-test-crd-publish-openapi-6869-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar 12 07:25:32.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 --namespace=crd-publish-openapi-249 apply -f -'
Mar 12 07:25:33.044: INFO: stderr: ""
Mar 12 07:25:33.044: INFO: stdout: "e2e-test-crd-publish-openapi-6869-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 12 07:25:33.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 --namespace=crd-publish-openapi-249 delete e2e-test-crd-publish-openapi-6869-crds test-cr'
Mar 12 07:25:33.184: INFO: stderr: ""
Mar 12 07:25:33.184: INFO: stdout: "e2e-test-crd-publish-openapi-6869-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 12 07:25:33.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 explain e2e-test-crd-publish-openapi-6869-crds'
Mar 12 07:25:33.410: INFO: stderr: ""
Mar 12 07:25:33.410: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6869-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:25:36.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-249" for this suite.
Mar 12 07:25:43.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:25:43.158: INFO: namespace crd-publish-openapi-249 deletion completed in 6.154093921s

• [SLOW TEST:20.524 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:25:43.158: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 07:25:43.247: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 12 07:25:48.253: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 12 07:25:56.263: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 12 07:25:58.268: INFO: Creating deployment "test-rollover-deployment"
Mar 12 07:25:58.283: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 12 07:26:00.292: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 12 07:26:00.301: INFO: Ensure that both replica sets have 1 created replica
Mar 12 07:26:00.308: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 12 07:26:00.322: INFO: Updating deployment test-rollover-deployment
Mar 12 07:26:00.323: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 12 07:26:02.331: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 12 07:26:02.338: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 12 07:26:02.345: INFO: all replica sets need to contain the pod-template-hash label
Mar 12 07:26:02.346: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594760, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:26:04.356: INFO: all replica sets need to contain the pod-template-hash label
Mar 12 07:26:04.356: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594760, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:26:06.355: INFO: all replica sets need to contain the pod-template-hash label
Mar 12 07:26:06.356: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594760, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:26:08.355: INFO: all replica sets need to contain the pod-template-hash label
Mar 12 07:26:08.355: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594760, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:26:10.357: INFO: all replica sets need to contain the pod-template-hash label
Mar 12 07:26:10.358: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594760, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:26:12.357: INFO: all replica sets need to contain the pod-template-hash label
Mar 12 07:26:12.357: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594760, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:26:14.355: INFO: all replica sets need to contain the pod-template-hash label
Mar 12 07:26:14.355: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594760, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:26:16.357: INFO: all replica sets need to contain the pod-template-hash label
Mar 12 07:26:16.357: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594774, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:26:18.355: INFO: all replica sets need to contain the pod-template-hash label
Mar 12 07:26:18.355: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594774, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:26:20.354: INFO: all replica sets need to contain the pod-template-hash label
Mar 12 07:26:20.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594774, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:26:22.354: INFO: all replica sets need to contain the pod-template-hash label
Mar 12 07:26:22.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594774, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:26:24.356: INFO: all replica sets need to contain the pod-template-hash label
Mar 12 07:26:24.356: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594774, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719594758, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:26:26.355: INFO: 
Mar 12 07:26:26.355: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar 12 07:26:26.364: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2684 /apis/apps/v1/namespaces/deployment-2684/deployments/test-rollover-deployment f8fba5fc-5729-40af-9ba9-242d812697bf 137710 2 2020-03-12 07:25:58 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002d52068 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-12 07:25:58 +0000 UTC,LastTransitionTime:2020-03-12 07:25:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-03-12 07:26:24 +0000 UTC,LastTransitionTime:2020-03-12 07:25:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 12 07:26:26.369: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-2684 /apis/apps/v1/namespaces/deployment-2684/replicasets/test-rollover-deployment-7d7dc6548c 9fec0390-d905-4eb9-9320-8b05c50f56c8 137699 2 2020-03-12 07:26:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment f8fba5fc-5729-40af-9ba9-242d812697bf 0xc002d52697 0xc002d52698}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002d52738 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 12 07:26:26.369: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 12 07:26:26.369: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2684 /apis/apps/v1/namespaces/deployment-2684/replicasets/test-rollover-controller de88f064-8be3-4a51-ab9a-29dcabc592ab 137709 2 2020-03-12 07:25:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment f8fba5fc-5729-40af-9ba9-242d812697bf 0xc002d52547 0xc002d52548}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002d52618 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 12 07:26:26.370: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-2684 /apis/apps/v1/namespaces/deployment-2684/replicasets/test-rollover-deployment-f6c94f66c 24ba39d9-c224-45b5-b6f7-c4163677795e 137642 2 2020-03-12 07:25:58 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment f8fba5fc-5729-40af-9ba9-242d812697bf 0xc002d527c0 0xc002d527c1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002d52858 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 12 07:26:26.374: INFO: Pod "test-rollover-deployment-7d7dc6548c-gjjqw" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-gjjqw test-rollover-deployment-7d7dc6548c- deployment-2684 /api/v1/namespaces/deployment-2684/pods/test-rollover-deployment-7d7dc6548c-gjjqw 961c204c-ce51-4dcd-ac89-a9533dfd4db4 137679 0 2020-03-12 07:26:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c 9fec0390-d905-4eb9-9320-8b05c50f56c8 0xc002e37fc7 0xc002e37fc8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4trl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4trl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4trl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 07:26:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 07:26:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 07:26:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 07:26:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.20.5,PodIP:10.233.65.65,StartTime:2020-03-12 07:26:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-12 07:26:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/redis:5.0.5-alpine,ImageID:docker-pullable://docker.io/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://8db1649711b9cf7ff2466502c23fac715220ef80badbe724dabc9c3c800d7f56,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.65,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:26:26.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2684" for this suite.
Mar 12 07:26:32.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:26:32.507: INFO: namespace deployment-2684 deletion completed in 6.128565146s

• [SLOW TEST:49.349 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:26:32.507: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-46917bef-b6ea-4aa1-8814-823a09416e7f
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-46917bef-b6ea-4aa1-8814-823a09416e7f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:28:13.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4384" for this suite.
Mar 12 07:28:25.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:28:25.358: INFO: namespace projected-4384 deletion completed in 12.134029402s

• [SLOW TEST:112.851 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:28:25.358: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-1112
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Mar 12 07:28:25.446: INFO: Found 0 stateful pods, waiting for 3
Mar 12 07:28:35.453: INFO: Found 1 stateful pods, waiting for 3
Mar 12 07:28:45.455: INFO: Found 2 stateful pods, waiting for 3
Mar 12 07:28:55.458: INFO: Found 2 stateful pods, waiting for 3
Mar 12 07:29:05.453: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 12 07:29:05.453: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 12 07:29:05.453: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar 12 07:29:15.453: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 12 07:29:15.453: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 12 07:29:15.453: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 12 07:29:15.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=statefulset-1112 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 12 07:29:15.695: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 12 07:29:15.695: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 12 07:29:15.696: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar 12 07:29:25.769: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar 12 07:29:35.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=statefulset-1112 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 12 07:29:35.999: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 12 07:29:35.999: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 12 07:29:35.999: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 12 07:29:46.026: INFO: Waiting for StatefulSet statefulset-1112/ss2 to complete update
Mar 12 07:29:46.026: INFO: Waiting for Pod statefulset-1112/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 12 07:29:46.026: INFO: Waiting for Pod statefulset-1112/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 12 07:29:56.036: INFO: Waiting for StatefulSet statefulset-1112/ss2 to complete update
Mar 12 07:29:56.036: INFO: Waiting for Pod statefulset-1112/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 12 07:29:56.036: INFO: Waiting for Pod statefulset-1112/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 12 07:30:06.038: INFO: Waiting for StatefulSet statefulset-1112/ss2 to complete update
Mar 12 07:30:06.038: INFO: Waiting for Pod statefulset-1112/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 12 07:30:06.038: INFO: Waiting for Pod statefulset-1112/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 12 07:30:16.037: INFO: Waiting for StatefulSet statefulset-1112/ss2 to complete update
Mar 12 07:30:16.037: INFO: Waiting for Pod statefulset-1112/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 12 07:30:26.038: INFO: Waiting for StatefulSet statefulset-1112/ss2 to complete update
Mar 12 07:30:26.038: INFO: Waiting for Pod statefulset-1112/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 12 07:30:36.038: INFO: Waiting for StatefulSet statefulset-1112/ss2 to complete update
Mar 12 07:30:46.038: INFO: Waiting for StatefulSet statefulset-1112/ss2 to complete update
STEP: Rolling back to a previous revision
Mar 12 07:30:56.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=statefulset-1112 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 12 07:30:56.228: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 12 07:30:56.228: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 12 07:30:56.228: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 12 07:30:56.266: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar 12 07:31:06.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=statefulset-1112 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 12 07:31:06.527: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 12 07:31:06.527: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 12 07:31:06.527: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 12 07:31:16.555: INFO: Waiting for StatefulSet statefulset-1112/ss2 to complete update
Mar 12 07:31:16.555: INFO: Waiting for Pod statefulset-1112/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 12 07:31:16.555: INFO: Waiting for Pod statefulset-1112/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 12 07:31:26.567: INFO: Waiting for StatefulSet statefulset-1112/ss2 to complete update
Mar 12 07:31:26.567: INFO: Waiting for Pod statefulset-1112/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 12 07:31:26.567: INFO: Waiting for Pod statefulset-1112/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 12 07:31:36.566: INFO: Waiting for StatefulSet statefulset-1112/ss2 to complete update
Mar 12 07:31:36.566: INFO: Waiting for Pod statefulset-1112/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 12 07:31:46.563: INFO: Waiting for StatefulSet statefulset-1112/ss2 to complete update
Mar 12 07:31:46.563: INFO: Waiting for Pod statefulset-1112/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 12 07:31:56.564: INFO: Waiting for StatefulSet statefulset-1112/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar 12 07:32:06.564: INFO: Deleting all statefulset in ns statefulset-1112
Mar 12 07:32:06.567: INFO: Scaling statefulset ss2 to 0
Mar 12 07:32:26.594: INFO: Waiting for statefulset status.replicas updated to 0
Mar 12 07:32:26.599: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:32:26.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1112" for this suite.
Mar 12 07:32:32.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:32:32.753: INFO: namespace statefulset-1112 deletion completed in 6.131985728s

• [SLOW TEST:247.395 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:32:32.754: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-802ba948-ec2e-41e5-8bf6-3586a94799d9
STEP: Creating a pod to test consume secrets
Mar 12 07:32:32.828: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-465d57f3-a68a-41d3-9adc-28f749ab887b" in namespace "projected-6554" to be "success or failure"
Mar 12 07:32:32.832: INFO: Pod "pod-projected-secrets-465d57f3-a68a-41d3-9adc-28f749ab887b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.439783ms
Mar 12 07:32:34.837: INFO: Pod "pod-projected-secrets-465d57f3-a68a-41d3-9adc-28f749ab887b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008403014s
Mar 12 07:32:36.848: INFO: Pod "pod-projected-secrets-465d57f3-a68a-41d3-9adc-28f749ab887b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020243419s
Mar 12 07:32:38.855: INFO: Pod "pod-projected-secrets-465d57f3-a68a-41d3-9adc-28f749ab887b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027031285s
Mar 12 07:32:40.860: INFO: Pod "pod-projected-secrets-465d57f3-a68a-41d3-9adc-28f749ab887b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032035948s
Mar 12 07:32:42.864: INFO: Pod "pod-projected-secrets-465d57f3-a68a-41d3-9adc-28f749ab887b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.035743537s
Mar 12 07:32:44.869: INFO: Pod "pod-projected-secrets-465d57f3-a68a-41d3-9adc-28f749ab887b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.040330771s
Mar 12 07:32:46.874: INFO: Pod "pod-projected-secrets-465d57f3-a68a-41d3-9adc-28f749ab887b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.046096982s
STEP: Saw pod success
Mar 12 07:32:46.874: INFO: Pod "pod-projected-secrets-465d57f3-a68a-41d3-9adc-28f749ab887b" satisfied condition "success or failure"
Mar 12 07:32:46.878: INFO: Trying to get logs from node node-3 pod pod-projected-secrets-465d57f3-a68a-41d3-9adc-28f749ab887b container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 12 07:32:46.912: INFO: Waiting for pod pod-projected-secrets-465d57f3-a68a-41d3-9adc-28f749ab887b to disappear
Mar 12 07:32:46.915: INFO: Pod pod-projected-secrets-465d57f3-a68a-41d3-9adc-28f749ab887b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:32:46.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6554" for this suite.
Mar 12 07:32:52.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:32:53.030: INFO: namespace projected-6554 deletion completed in 6.110525529s

• [SLOW TEST:20.276 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:32:53.030: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 12 07:33:06.177: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:33:06.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6049" for this suite.
Mar 12 07:33:12.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:33:12.314: INFO: namespace container-runtime-6049 deletion completed in 6.10786231s

• [SLOW TEST:19.284 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:33:12.314: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-b504b2f9-810f-412a-94cd-87b6187d976f
STEP: Creating secret with name secret-projected-all-test-volume-c96ce9a0-6c8d-4c40-9069-a750985cab7e
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar 12 07:33:12.397: INFO: Waiting up to 5m0s for pod "projected-volume-ae75f383-d857-4c62-ab9d-16d14d1f74ed" in namespace "projected-9262" to be "success or failure"
Mar 12 07:33:12.401: INFO: Pod "projected-volume-ae75f383-d857-4c62-ab9d-16d14d1f74ed": Phase="Pending", Reason="", readiness=false. Elapsed: 3.854626ms
Mar 12 07:33:14.406: INFO: Pod "projected-volume-ae75f383-d857-4c62-ab9d-16d14d1f74ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008726971s
Mar 12 07:33:16.412: INFO: Pod "projected-volume-ae75f383-d857-4c62-ab9d-16d14d1f74ed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014898393s
Mar 12 07:33:18.420: INFO: Pod "projected-volume-ae75f383-d857-4c62-ab9d-16d14d1f74ed": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02297212s
Mar 12 07:33:20.426: INFO: Pod "projected-volume-ae75f383-d857-4c62-ab9d-16d14d1f74ed": Phase="Pending", Reason="", readiness=false. Elapsed: 8.028354926s
Mar 12 07:33:22.431: INFO: Pod "projected-volume-ae75f383-d857-4c62-ab9d-16d14d1f74ed": Phase="Pending", Reason="", readiness=false. Elapsed: 10.033659071s
Mar 12 07:33:24.439: INFO: Pod "projected-volume-ae75f383-d857-4c62-ab9d-16d14d1f74ed": Phase="Pending", Reason="", readiness=false. Elapsed: 12.041874062s
Mar 12 07:33:26.444: INFO: Pod "projected-volume-ae75f383-d857-4c62-ab9d-16d14d1f74ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.046546898s
STEP: Saw pod success
Mar 12 07:33:26.444: INFO: Pod "projected-volume-ae75f383-d857-4c62-ab9d-16d14d1f74ed" satisfied condition "success or failure"
Mar 12 07:33:26.447: INFO: Trying to get logs from node node-2 pod projected-volume-ae75f383-d857-4c62-ab9d-16d14d1f74ed container projected-all-volume-test: <nil>
STEP: delete the pod
Mar 12 07:33:26.484: INFO: Waiting for pod projected-volume-ae75f383-d857-4c62-ab9d-16d14d1f74ed to disappear
Mar 12 07:33:26.488: INFO: Pod projected-volume-ae75f383-d857-4c62-ab9d-16d14d1f74ed no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:33:26.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9262" for this suite.
Mar 12 07:33:32.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:33:32.605: INFO: namespace projected-9262 deletion completed in 6.110001195s

• [SLOW TEST:20.291 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:33:32.606: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Mar 12 07:33:46.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec pod-sharedvolume-75cf6a79-bca5-49a8-a2b3-4399a236555f -c busybox-main-container --namespace=emptydir-5224 -- cat /usr/share/volumeshare/shareddata.txt'
Mar 12 07:33:47.154: INFO: stderr: ""
Mar 12 07:33:47.154: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:33:47.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5224" for this suite.
Mar 12 07:33:53.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:33:53.291: INFO: namespace emptydir-5224 deletion completed in 6.130722166s

• [SLOW TEST:20.685 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:33:53.292: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 12 07:33:53.840: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 12 07:33:55.854: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:33:57.858: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:33:59.859: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:34:01.858: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:34:03.859: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:34:05.858: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595233, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 12 07:34:08.884: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:34:09.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7606" for this suite.
Mar 12 07:34:15.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:34:15.255: INFO: namespace webhook-7606 deletion completed in 6.10876452s
STEP: Destroying namespace "webhook-7606-markers" for this suite.
Mar 12 07:34:21.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:34:21.358: INFO: namespace webhook-7606-markers deletion completed in 6.102011572s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:28.082 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:34:21.375: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Mar 12 07:34:21.412: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-675793433 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:34:21.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5627" for this suite.
Mar 12 07:34:27.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:34:27.610: INFO: namespace kubectl-5627 deletion completed in 6.115410217s

• [SLOW TEST:6.235 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:34:27.610: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Mar 12 07:34:27.655: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 12 07:34:27.674: INFO: Waiting for terminating namespaces to be deleted...
Mar 12 07:34:27.678: INFO: 
Logging pods the kubelet thinks is on node node-1 before test
Mar 12 07:34:27.697: INFO: sonobuoy-e2e-job-0ad0ccbfe03640eb from sonobuoy started at 2020-03-12 07:11:26 +0000 UTC (2 container statuses recorded)
Mar 12 07:34:27.697: INFO: 	Container e2e ready: true, restart count 0
Mar 12 07:34:27.697: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 12 07:34:27.697: INFO: kube-flannel-hgdc4 from kube-system started at 2020-03-11 16:40:11 +0000 UTC (2 container statuses recorded)
Mar 12 07:34:27.697: INFO: 	Container install-cni ready: true, restart count 0
Mar 12 07:34:27.697: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 12 07:34:27.697: INFO: sonobuoy from sonobuoy started at 2020-03-12 07:11:13 +0000 UTC (1 container statuses recorded)
Mar 12 07:34:27.697: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 12 07:34:27.697: INFO: k8s-keystone-auth-7n8pn from kube-system started at 2020-03-11 16:39:41 +0000 UTC (1 container statuses recorded)
Mar 12 07:34:27.697: INFO: 	Container k8s-keystone-auth ready: true, restart count 0
Mar 12 07:34:27.697: INFO: sonobuoy-systemd-logs-daemon-set-b6e3a2e4f0de4f41-fktq2 from sonobuoy started at 2020-03-12 07:11:26 +0000 UTC (2 container statuses recorded)
Mar 12 07:34:27.697: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 12 07:34:27.697: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 12 07:34:27.697: INFO: kube-proxy-node-1 from kube-system started at 2020-03-11 11:52:32 +0000 UTC (1 container statuses recorded)
Mar 12 07:34:27.697: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 12 07:34:27.697: INFO: kube-scheduler-node-1 from kube-system started at 2020-03-11 11:53:05 +0000 UTC (1 container statuses recorded)
Mar 12 07:34:27.697: INFO: 	Container kube-scheduler ready: true, restart count 3
Mar 12 07:34:27.697: INFO: kube-apiserver-node-1 from kube-system started at 2020-03-12 07:03:22 +0000 UTC (1 container statuses recorded)
Mar 12 07:34:27.697: INFO: 	Container kube-apiserver ready: true, restart count 1
Mar 12 07:34:27.697: INFO: kube-controller-manager-node-1 from kube-system started at 2020-03-11 13:01:20 +0000 UTC (1 container statuses recorded)
Mar 12 07:34:27.697: INFO: 	Container kube-controller-manager ready: true, restart count 4
Mar 12 07:34:27.697: INFO: coredns-p4prs from kube-system started at 2020-03-11 16:39:41 +0000 UTC (1 container statuses recorded)
Mar 12 07:34:27.697: INFO: 	Container coredns ready: true, restart count 0
Mar 12 07:34:27.697: INFO: 
Logging pods the kubelet thinks is on node node-2 before test
Mar 12 07:34:27.710: INFO: kube-controller-manager-node-2 from kube-system started at 2020-03-11 13:01:20 +0000 UTC (1 container statuses recorded)
Mar 12 07:34:27.710: INFO: 	Container kube-controller-manager ready: true, restart count 2
Mar 12 07:34:27.710: INFO: coredns-bs764 from kube-system started at 2020-03-11 13:04:04 +0000 UTC (1 container statuses recorded)
Mar 12 07:34:27.710: INFO: 	Container coredns ready: true, restart count 11
Mar 12 07:34:27.710: INFO: sonobuoy-systemd-logs-daemon-set-b6e3a2e4f0de4f41-2gk9x from sonobuoy started at 2020-03-12 07:11:26 +0000 UTC (2 container statuses recorded)
Mar 12 07:34:27.710: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 12 07:34:27.710: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 12 07:34:27.710: INFO: kube-proxy-node-2 from kube-system started at 2020-03-11 11:52:32 +0000 UTC (1 container statuses recorded)
Mar 12 07:34:27.710: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 12 07:34:27.710: INFO: kube-scheduler-node-2 from kube-system started at 2020-03-11 11:53:05 +0000 UTC (1 container statuses recorded)
Mar 12 07:34:27.710: INFO: 	Container kube-scheduler ready: true, restart count 2
Mar 12 07:34:27.710: INFO: kube-flannel-pj49q from kube-system started at 2020-03-11 13:02:04 +0000 UTC (2 container statuses recorded)
Mar 12 07:34:27.710: INFO: 	Container install-cni ready: true, restart count 0
Mar 12 07:34:27.710: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 12 07:34:27.710: INFO: kube-apiserver-node-2 from kube-system started at 2020-03-11 11:52:50 +0000 UTC (1 container statuses recorded)
Mar 12 07:34:27.710: INFO: 	Container kube-apiserver ready: true, restart count 0
Mar 12 07:34:27.710: INFO: heapster-659dbd5d99-p9ddb from kube-system started at 2020-03-11 13:42:58 +0000 UTC (1 container statuses recorded)
Mar 12 07:34:27.710: INFO: 	Container heapster ready: true, restart count 0
Mar 12 07:34:27.710: INFO: k8s-keystone-auth-rpjrg from kube-system started at 2020-03-11 13:42:59 +0000 UTC (1 container statuses recorded)
Mar 12 07:34:27.710: INFO: 	Container k8s-keystone-auth ready: true, restart count 0
Mar 12 07:34:27.710: INFO: 
Logging pods the kubelet thinks is on node node-3 before test
Mar 12 07:34:27.728: INFO: kube-flannel-h7vhj from kube-system started at 2020-03-11 13:02:04 +0000 UTC (2 container statuses recorded)
Mar 12 07:34:27.728: INFO: 	Container install-cni ready: true, restart count 0
Mar 12 07:34:27.728: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 12 07:34:27.728: INFO: tiller-deploy-7db6d94b4f-nqfpr from kube-system started at 2020-03-11 13:03:14 +0000 UTC (1 container statuses recorded)
Mar 12 07:34:27.728: INFO: 	Container tiller ready: true, restart count 0
Mar 12 07:34:27.728: INFO: k8s-keystone-auth-swhr6 from kube-system started at 2020-03-11 13:42:59 +0000 UTC (1 container statuses recorded)
Mar 12 07:34:27.728: INFO: 	Container k8s-keystone-auth ready: true, restart count 0
Mar 12 07:34:27.728: INFO: kube-apiserver-node-3 from kube-system started at 2020-03-11 11:52:50 +0000 UTC (1 container statuses recorded)
Mar 12 07:34:27.728: INFO: 	Container kube-apiserver ready: true, restart count 0
Mar 12 07:34:27.728: INFO: kube-proxy-node-3 from kube-system started at 2020-03-11 11:52:32 +0000 UTC (1 container statuses recorded)
Mar 12 07:34:27.728: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 12 07:34:27.728: INFO: kube-monitor-1583998200-2nhkk from kube-system started at 2020-03-12 07:30:01 +0000 UTC (1 container statuses recorded)
Mar 12 07:34:27.728: INFO: 	Container kube-monitor ready: false, restart count 0
Mar 12 07:34:27.728: INFO: kube-scheduler-node-3 from kube-system started at 2020-03-11 11:53:06 +0000 UTC (1 container statuses recorded)
Mar 12 07:34:27.728: INFO: 	Container kube-scheduler ready: true, restart count 4
Mar 12 07:34:27.728: INFO: kube-controller-manager-node-3 from kube-system started at 2020-03-11 13:01:20 +0000 UTC (1 container statuses recorded)
Mar 12 07:34:27.728: INFO: 	Container kube-controller-manager ready: true, restart count 3
Mar 12 07:34:27.728: INFO: coredns-v7lrl from kube-system started at 2020-03-11 13:04:04 +0000 UTC (1 container statuses recorded)
Mar 12 07:34:27.728: INFO: 	Container coredns ready: true, restart count 0
Mar 12 07:34:27.728: INFO: sonobuoy-systemd-logs-daemon-set-b6e3a2e4f0de4f41-q7m9f from sonobuoy started at 2020-03-12 07:11:26 +0000 UTC (2 container statuses recorded)
Mar 12 07:34:27.728: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 12 07:34:27.728: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15fb7e36cbabfadd], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:34:28.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7253" for this suite.
Mar 12 07:34:34.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:34:34.897: INFO: namespace sched-pred-7253 deletion completed in 6.12714919s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:7.288 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:34:34.898: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-3574d2aa-8b56-4e9a-811a-fdf153db8895 in namespace container-probe-155
Mar 12 07:34:48.974: INFO: Started pod liveness-3574d2aa-8b56-4e9a-811a-fdf153db8895 in namespace container-probe-155
STEP: checking the pod's current state and verifying that restartCount is present
Mar 12 07:34:48.978: INFO: Initial restart count of pod liveness-3574d2aa-8b56-4e9a-811a-fdf153db8895 is 0
Mar 12 07:35:01.011: INFO: Restart count of pod container-probe-155/liveness-3574d2aa-8b56-4e9a-811a-fdf153db8895 is now 1 (12.032550007s elapsed)
Mar 12 07:35:23.078: INFO: Restart count of pod container-probe-155/liveness-3574d2aa-8b56-4e9a-811a-fdf153db8895 is now 2 (34.099578147s elapsed)
Mar 12 07:35:41.130: INFO: Restart count of pod container-probe-155/liveness-3574d2aa-8b56-4e9a-811a-fdf153db8895 is now 3 (52.152442922s elapsed)
Mar 12 07:36:01.189: INFO: Restart count of pod container-probe-155/liveness-3574d2aa-8b56-4e9a-811a-fdf153db8895 is now 4 (1m12.211216936s elapsed)
Mar 12 07:37:03.353: INFO: Restart count of pod container-probe-155/liveness-3574d2aa-8b56-4e9a-811a-fdf153db8895 is now 5 (2m14.374669762s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:37:03.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-155" for this suite.
Mar 12 07:37:09.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:37:09.486: INFO: namespace container-probe-155 deletion completed in 6.112582378s

• [SLOW TEST:154.589 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:37:09.487: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-21c16e47-dd28-4381-a013-8eff63fcdf51 in namespace container-probe-2469
Mar 12 07:37:23.567: INFO: Started pod liveness-21c16e47-dd28-4381-a013-8eff63fcdf51 in namespace container-probe-2469
STEP: checking the pod's current state and verifying that restartCount is present
Mar 12 07:37:23.572: INFO: Initial restart count of pod liveness-21c16e47-dd28-4381-a013-8eff63fcdf51 is 0
Mar 12 07:37:43.630: INFO: Restart count of pod container-probe-2469/liveness-21c16e47-dd28-4381-a013-8eff63fcdf51 is now 1 (20.057916944s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:37:43.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2469" for this suite.
Mar 12 07:37:49.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:37:49.777: INFO: namespace container-probe-2469 deletion completed in 6.117662163s

• [SLOW TEST:40.290 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:37:49.777: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 12 07:38:17.873: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 12 07:38:17.875: INFO: Pod pod-with-prestop-http-hook still exists
Mar 12 07:38:19.876: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 12 07:38:19.881: INFO: Pod pod-with-prestop-http-hook still exists
Mar 12 07:38:21.876: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 12 07:38:21.881: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:38:21.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8463" for this suite.
Mar 12 07:38:33.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:38:34.031: INFO: namespace container-lifecycle-hook-8463 deletion completed in 12.127828659s

• [SLOW TEST:44.255 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:38:34.032: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 12 07:38:47.165: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:38:47.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5298" for this suite.
Mar 12 07:38:53.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:38:53.303: INFO: namespace container-runtime-5298 deletion completed in 6.11503697s

• [SLOW TEST:19.271 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:38:53.303: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Mar 12 07:39:07.389: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-675793433 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Mar 12 07:39:22.517: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:39:22.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7482" for this suite.
Mar 12 07:39:28.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:39:28.640: INFO: namespace pods-7482 deletion completed in 6.1111755s

• [SLOW TEST:35.337 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:39:28.640: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:39:34.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4267" for this suite.
Mar 12 07:39:40.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:39:40.950: INFO: namespace namespaces-4267 deletion completed in 6.125117723s
STEP: Destroying namespace "nsdeletetest-3306" for this suite.
Mar 12 07:39:40.954: INFO: Namespace nsdeletetest-3306 was already deleted
STEP: Destroying namespace "nsdeletetest-9734" for this suite.
Mar 12 07:39:46.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:39:47.059: INFO: namespace nsdeletetest-9734 deletion completed in 6.104696213s

• [SLOW TEST:18.419 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:39:47.059: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 12 07:39:47.121: INFO: Waiting up to 5m0s for pod "pod-d10d8939-c295-48b3-9ac5-df496653f0d8" in namespace "emptydir-1322" to be "success or failure"
Mar 12 07:39:47.125: INFO: Pod "pod-d10d8939-c295-48b3-9ac5-df496653f0d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.487551ms
Mar 12 07:39:49.132: INFO: Pod "pod-d10d8939-c295-48b3-9ac5-df496653f0d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010990662s
Mar 12 07:39:51.137: INFO: Pod "pod-d10d8939-c295-48b3-9ac5-df496653f0d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016151519s
Mar 12 07:39:53.143: INFO: Pod "pod-d10d8939-c295-48b3-9ac5-df496653f0d8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021789762s
Mar 12 07:39:55.148: INFO: Pod "pod-d10d8939-c295-48b3-9ac5-df496653f0d8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02728067s
Mar 12 07:39:57.154: INFO: Pod "pod-d10d8939-c295-48b3-9ac5-df496653f0d8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.03290957s
Mar 12 07:39:59.160: INFO: Pod "pod-d10d8939-c295-48b3-9ac5-df496653f0d8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.039542839s
Mar 12 07:40:01.166: INFO: Pod "pod-d10d8939-c295-48b3-9ac5-df496653f0d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.045006645s
STEP: Saw pod success
Mar 12 07:40:01.166: INFO: Pod "pod-d10d8939-c295-48b3-9ac5-df496653f0d8" satisfied condition "success or failure"
Mar 12 07:40:01.170: INFO: Trying to get logs from node node-3 pod pod-d10d8939-c295-48b3-9ac5-df496653f0d8 container test-container: <nil>
STEP: delete the pod
Mar 12 07:40:01.204: INFO: Waiting for pod pod-d10d8939-c295-48b3-9ac5-df496653f0d8 to disappear
Mar 12 07:40:01.207: INFO: Pod pod-d10d8939-c295-48b3-9ac5-df496653f0d8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:40:01.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1322" for this suite.
Mar 12 07:40:07.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:40:07.323: INFO: namespace emptydir-1322 deletion completed in 6.111838631s

• [SLOW TEST:20.264 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:40:07.324: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:40:07.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-148" for this suite.
Mar 12 07:40:13.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:40:13.501: INFO: namespace custom-resource-definition-148 deletion completed in 6.119348324s

• [SLOW TEST:6.178 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:40:13.502: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Mar 12 07:40:13.582: INFO: Waiting up to 5m0s for pod "pod-26f0db7f-9f19-43f1-aa7f-986d6860d0d5" in namespace "emptydir-5051" to be "success or failure"
Mar 12 07:40:13.586: INFO: Pod "pod-26f0db7f-9f19-43f1-aa7f-986d6860d0d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008839ms
Mar 12 07:40:15.592: INFO: Pod "pod-26f0db7f-9f19-43f1-aa7f-986d6860d0d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00982233s
Mar 12 07:40:17.597: INFO: Pod "pod-26f0db7f-9f19-43f1-aa7f-986d6860d0d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014295057s
Mar 12 07:40:19.602: INFO: Pod "pod-26f0db7f-9f19-43f1-aa7f-986d6860d0d5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019657709s
Mar 12 07:40:21.607: INFO: Pod "pod-26f0db7f-9f19-43f1-aa7f-986d6860d0d5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024882305s
Mar 12 07:40:23.613: INFO: Pod "pod-26f0db7f-9f19-43f1-aa7f-986d6860d0d5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030986339s
Mar 12 07:40:25.618: INFO: Pod "pod-26f0db7f-9f19-43f1-aa7f-986d6860d0d5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.036038187s
Mar 12 07:40:27.624: INFO: Pod "pod-26f0db7f-9f19-43f1-aa7f-986d6860d0d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.041308134s
STEP: Saw pod success
Mar 12 07:40:27.624: INFO: Pod "pod-26f0db7f-9f19-43f1-aa7f-986d6860d0d5" satisfied condition "success or failure"
Mar 12 07:40:27.627: INFO: Trying to get logs from node node-2 pod pod-26f0db7f-9f19-43f1-aa7f-986d6860d0d5 container test-container: <nil>
STEP: delete the pod
Mar 12 07:40:27.650: INFO: Waiting for pod pod-26f0db7f-9f19-43f1-aa7f-986d6860d0d5 to disappear
Mar 12 07:40:27.653: INFO: Pod pod-26f0db7f-9f19-43f1-aa7f-986d6860d0d5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:40:27.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5051" for this suite.
Mar 12 07:40:33.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:40:33.763: INFO: namespace emptydir-5051 deletion completed in 6.105845018s

• [SLOW TEST:20.262 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:40:33.764: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:41:33.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3847" for this suite.
Mar 12 07:42:01.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:42:01.941: INFO: namespace container-probe-3847 deletion completed in 28.10655403s

• [SLOW TEST:88.177 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:42:01.941: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 12 07:42:02.406: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 12 07:42:04.418: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:42:06.423: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:42:08.423: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:42:10.423: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:42:12.423: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:42:14.424: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719595722, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 12 07:42:17.440: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Mar 12 07:42:17.470: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:42:17.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-583" for this suite.
Mar 12 07:42:23.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:42:23.613: INFO: namespace webhook-583 deletion completed in 6.116602965s
STEP: Destroying namespace "webhook-583-markers" for this suite.
Mar 12 07:42:29.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:42:29.733: INFO: namespace webhook-583-markers deletion completed in 6.12076656s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.810 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:42:29.752: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 12 07:42:29.965: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3dfa527d-255a-4051-92a1-6d7fc5c5b45a" in namespace "downward-api-262" to be "success or failure"
Mar 12 07:42:29.972: INFO: Pod "downwardapi-volume-3dfa527d-255a-4051-92a1-6d7fc5c5b45a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.955176ms
Mar 12 07:42:31.977: INFO: Pod "downwardapi-volume-3dfa527d-255a-4051-92a1-6d7fc5c5b45a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011992172s
Mar 12 07:42:33.982: INFO: Pod "downwardapi-volume-3dfa527d-255a-4051-92a1-6d7fc5c5b45a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016947916s
Mar 12 07:42:35.987: INFO: Pod "downwardapi-volume-3dfa527d-255a-4051-92a1-6d7fc5c5b45a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022521983s
Mar 12 07:42:37.992: INFO: Pod "downwardapi-volume-3dfa527d-255a-4051-92a1-6d7fc5c5b45a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02751978s
Mar 12 07:42:39.998: INFO: Pod "downwardapi-volume-3dfa527d-255a-4051-92a1-6d7fc5c5b45a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.032675026s
Mar 12 07:42:42.004: INFO: Pod "downwardapi-volume-3dfa527d-255a-4051-92a1-6d7fc5c5b45a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.039114379s
Mar 12 07:42:44.010: INFO: Pod "downwardapi-volume-3dfa527d-255a-4051-92a1-6d7fc5c5b45a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.044741591s
STEP: Saw pod success
Mar 12 07:42:44.010: INFO: Pod "downwardapi-volume-3dfa527d-255a-4051-92a1-6d7fc5c5b45a" satisfied condition "success or failure"
Mar 12 07:42:44.013: INFO: Trying to get logs from node node-3 pod downwardapi-volume-3dfa527d-255a-4051-92a1-6d7fc5c5b45a container client-container: <nil>
STEP: delete the pod
Mar 12 07:42:44.051: INFO: Waiting for pod downwardapi-volume-3dfa527d-255a-4051-92a1-6d7fc5c5b45a to disappear
Mar 12 07:42:44.054: INFO: Pod downwardapi-volume-3dfa527d-255a-4051-92a1-6d7fc5c5b45a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:42:44.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-262" for this suite.
Mar 12 07:42:50.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:42:50.168: INFO: namespace downward-api-262 deletion completed in 6.108439009s

• [SLOW TEST:20.416 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:42:50.168: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Mar 12 07:42:50.233: INFO: Waiting up to 5m0s for pod "var-expansion-ff6ce4a0-0f18-4635-a858-bc9a3ca95269" in namespace "var-expansion-2132" to be "success or failure"
Mar 12 07:42:50.236: INFO: Pod "var-expansion-ff6ce4a0-0f18-4635-a858-bc9a3ca95269": Phase="Pending", Reason="", readiness=false. Elapsed: 3.289675ms
Mar 12 07:42:52.242: INFO: Pod "var-expansion-ff6ce4a0-0f18-4635-a858-bc9a3ca95269": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009387396s
Mar 12 07:42:54.247: INFO: Pod "var-expansion-ff6ce4a0-0f18-4635-a858-bc9a3ca95269": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014522312s
Mar 12 07:42:56.253: INFO: Pod "var-expansion-ff6ce4a0-0f18-4635-a858-bc9a3ca95269": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020339057s
Mar 12 07:42:58.258: INFO: Pod "var-expansion-ff6ce4a0-0f18-4635-a858-bc9a3ca95269": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025190879s
Mar 12 07:43:00.263: INFO: Pod "var-expansion-ff6ce4a0-0f18-4635-a858-bc9a3ca95269": Phase="Pending", Reason="", readiness=false. Elapsed: 10.029582896s
Mar 12 07:43:02.267: INFO: Pod "var-expansion-ff6ce4a0-0f18-4635-a858-bc9a3ca95269": Phase="Pending", Reason="", readiness=false. Elapsed: 12.03437608s
Mar 12 07:43:04.273: INFO: Pod "var-expansion-ff6ce4a0-0f18-4635-a858-bc9a3ca95269": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.039867749s
STEP: Saw pod success
Mar 12 07:43:04.273: INFO: Pod "var-expansion-ff6ce4a0-0f18-4635-a858-bc9a3ca95269" satisfied condition "success or failure"
Mar 12 07:43:04.276: INFO: Trying to get logs from node node-3 pod var-expansion-ff6ce4a0-0f18-4635-a858-bc9a3ca95269 container dapi-container: <nil>
STEP: delete the pod
Mar 12 07:43:04.303: INFO: Waiting for pod var-expansion-ff6ce4a0-0f18-4635-a858-bc9a3ca95269 to disappear
Mar 12 07:43:04.306: INFO: Pod var-expansion-ff6ce4a0-0f18-4635-a858-bc9a3ca95269 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:43:04.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2132" for this suite.
Mar 12 07:43:10.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:43:10.418: INFO: namespace var-expansion-2132 deletion completed in 6.106344335s

• [SLOW TEST:20.249 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:43:10.418: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 12 07:43:10.475: INFO: Waiting up to 5m0s for pod "pod-fb668c6a-c912-46ec-940b-e3d6c7456096" in namespace "emptydir-7592" to be "success or failure"
Mar 12 07:43:10.479: INFO: Pod "pod-fb668c6a-c912-46ec-940b-e3d6c7456096": Phase="Pending", Reason="", readiness=false. Elapsed: 3.482706ms
Mar 12 07:43:12.486: INFO: Pod "pod-fb668c6a-c912-46ec-940b-e3d6c7456096": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010581721s
Mar 12 07:43:14.492: INFO: Pod "pod-fb668c6a-c912-46ec-940b-e3d6c7456096": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016457786s
Mar 12 07:43:16.497: INFO: Pod "pod-fb668c6a-c912-46ec-940b-e3d6c7456096": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021539238s
Mar 12 07:43:18.502: INFO: Pod "pod-fb668c6a-c912-46ec-940b-e3d6c7456096": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026634721s
Mar 12 07:43:20.506: INFO: Pod "pod-fb668c6a-c912-46ec-940b-e3d6c7456096": Phase="Pending", Reason="", readiness=false. Elapsed: 10.031088009s
Mar 12 07:43:22.511: INFO: Pod "pod-fb668c6a-c912-46ec-940b-e3d6c7456096": Phase="Pending", Reason="", readiness=false. Elapsed: 12.035882142s
Mar 12 07:43:24.517: INFO: Pod "pod-fb668c6a-c912-46ec-940b-e3d6c7456096": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.041508532s
STEP: Saw pod success
Mar 12 07:43:24.517: INFO: Pod "pod-fb668c6a-c912-46ec-940b-e3d6c7456096" satisfied condition "success or failure"
Mar 12 07:43:24.520: INFO: Trying to get logs from node node-2 pod pod-fb668c6a-c912-46ec-940b-e3d6c7456096 container test-container: <nil>
STEP: delete the pod
Mar 12 07:43:24.561: INFO: Waiting for pod pod-fb668c6a-c912-46ec-940b-e3d6c7456096 to disappear
Mar 12 07:43:24.564: INFO: Pod pod-fb668c6a-c912-46ec-940b-e3d6c7456096 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:43:24.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7592" for this suite.
Mar 12 07:43:30.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:43:30.678: INFO: namespace emptydir-7592 deletion completed in 6.105547099s

• [SLOW TEST:20.260 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:43:30.678: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-eff06198-ff5f-4203-a306-9befee4b6f57
STEP: Creating a pod to test consume configMaps
Mar 12 07:43:30.768: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f7198724-9ff8-4122-bbbe-112fda2ae5fe" in namespace "projected-9851" to be "success or failure"
Mar 12 07:43:30.771: INFO: Pod "pod-projected-configmaps-f7198724-9ff8-4122-bbbe-112fda2ae5fe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.359388ms
Mar 12 07:43:32.776: INFO: Pod "pod-projected-configmaps-f7198724-9ff8-4122-bbbe-112fda2ae5fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007743112s
Mar 12 07:43:34.783: INFO: Pod "pod-projected-configmaps-f7198724-9ff8-4122-bbbe-112fda2ae5fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014622459s
Mar 12 07:43:36.788: INFO: Pod "pod-projected-configmaps-f7198724-9ff8-4122-bbbe-112fda2ae5fe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020391083s
Mar 12 07:43:38.794: INFO: Pod "pod-projected-configmaps-f7198724-9ff8-4122-bbbe-112fda2ae5fe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02587133s
Mar 12 07:43:40.800: INFO: Pod "pod-projected-configmaps-f7198724-9ff8-4122-bbbe-112fda2ae5fe": Phase="Pending", Reason="", readiness=false. Elapsed: 10.03177726s
Mar 12 07:43:42.810: INFO: Pod "pod-projected-configmaps-f7198724-9ff8-4122-bbbe-112fda2ae5fe": Phase="Pending", Reason="", readiness=false. Elapsed: 12.042486642s
Mar 12 07:43:44.824: INFO: Pod "pod-projected-configmaps-f7198724-9ff8-4122-bbbe-112fda2ae5fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.055656607s
STEP: Saw pod success
Mar 12 07:43:44.824: INFO: Pod "pod-projected-configmaps-f7198724-9ff8-4122-bbbe-112fda2ae5fe" satisfied condition "success or failure"
Mar 12 07:43:44.828: INFO: Trying to get logs from node node-2 pod pod-projected-configmaps-f7198724-9ff8-4122-bbbe-112fda2ae5fe container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 12 07:43:44.860: INFO: Waiting for pod pod-projected-configmaps-f7198724-9ff8-4122-bbbe-112fda2ae5fe to disappear
Mar 12 07:43:44.863: INFO: Pod pod-projected-configmaps-f7198724-9ff8-4122-bbbe-112fda2ae5fe no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:43:44.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9851" for this suite.
Mar 12 07:43:50.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:43:50.980: INFO: namespace projected-9851 deletion completed in 6.113236517s

• [SLOW TEST:20.302 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:43:50.981: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 07:43:51.045: INFO: Waiting up to 5m0s for pod "busybox-user-65534-09b1c049-4395-4abb-87c4-907b66de7450" in namespace "security-context-test-6245" to be "success or failure"
Mar 12 07:43:51.049: INFO: Pod "busybox-user-65534-09b1c049-4395-4abb-87c4-907b66de7450": Phase="Pending", Reason="", readiness=false. Elapsed: 4.710165ms
Mar 12 07:43:53.054: INFO: Pod "busybox-user-65534-09b1c049-4395-4abb-87c4-907b66de7450": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009736313s
Mar 12 07:43:55.059: INFO: Pod "busybox-user-65534-09b1c049-4395-4abb-87c4-907b66de7450": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01492409s
Mar 12 07:43:57.066: INFO: Pod "busybox-user-65534-09b1c049-4395-4abb-87c4-907b66de7450": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021533588s
Mar 12 07:43:59.072: INFO: Pod "busybox-user-65534-09b1c049-4395-4abb-87c4-907b66de7450": Phase="Pending", Reason="", readiness=false. Elapsed: 8.027464854s
Mar 12 07:44:01.079: INFO: Pod "busybox-user-65534-09b1c049-4395-4abb-87c4-907b66de7450": Phase="Pending", Reason="", readiness=false. Elapsed: 10.034453865s
Mar 12 07:44:03.797: INFO: Pod "busybox-user-65534-09b1c049-4395-4abb-87c4-907b66de7450": Phase="Pending", Reason="", readiness=false. Elapsed: 12.752705353s
Mar 12 07:44:05.810: INFO: Pod "busybox-user-65534-09b1c049-4395-4abb-87c4-907b66de7450": Phase="Pending", Reason="", readiness=false. Elapsed: 14.765111147s
Mar 12 07:44:07.815: INFO: Pod "busybox-user-65534-09b1c049-4395-4abb-87c4-907b66de7450": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.770603959s
Mar 12 07:44:07.815: INFO: Pod "busybox-user-65534-09b1c049-4395-4abb-87c4-907b66de7450" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:44:07.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6245" for this suite.
Mar 12 07:44:13.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:44:13.946: INFO: namespace security-context-test-6245 deletion completed in 6.126668863s

• [SLOW TEST:22.966 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:44:13.947: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar 12 07:44:28.031: INFO: &Pod{ObjectMeta:{send-events-cd7cc009-4928-47e6-bcc0-8f8f15f0c9e2  events-6135 /api/v1/namespaces/events-6135/pods/send-events-cd7cc009-4928-47e6-bcc0-8f8f15f0c9e2 9cef2129-5bbb-4efd-abc9-f2ef3f3fd3e5 140725 0 2020-03-12 07:44:14 +0000 UTC <nil> <nil> map[name:foo time:993921877] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f52g7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f52g7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f52g7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 07:44:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 07:44:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 07:44:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 07:44:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.20.4,PodIP:10.233.64.24,StartTime:2020-03-12 07:44:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-12 07:44:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:docker://sha256:abecdf3ecffe4649c622f2af16931c861a1d0c26da183652e1ac27d3d7b7d047,ContainerID:docker://7eea169d34493d035002b548e861277dbca4030acaf7740d1a3abd3683f46238,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.24,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Mar 12 07:44:30.037: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar 12 07:44:32.042: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:44:32.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6135" for this suite.
Mar 12 07:45:16.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:45:16.182: INFO: namespace events-6135 deletion completed in 44.125813183s

• [SLOW TEST:62.235 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:45:16.183: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-dksn
STEP: Creating a pod to test atomic-volume-subpath
Mar 12 07:45:16.260: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-dksn" in namespace "subpath-2066" to be "success or failure"
Mar 12 07:45:16.271: INFO: Pod "pod-subpath-test-projected-dksn": Phase="Pending", Reason="", readiness=false. Elapsed: 10.392122ms
Mar 12 07:45:18.276: INFO: Pod "pod-subpath-test-projected-dksn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015790346s
Mar 12 07:45:20.281: INFO: Pod "pod-subpath-test-projected-dksn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021199192s
Mar 12 07:45:22.288: INFO: Pod "pod-subpath-test-projected-dksn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027417134s
Mar 12 07:45:24.293: INFO: Pod "pod-subpath-test-projected-dksn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.03270064s
Mar 12 07:45:26.298: INFO: Pod "pod-subpath-test-projected-dksn": Phase="Pending", Reason="", readiness=false. Elapsed: 10.038235981s
Mar 12 07:45:28.304: INFO: Pod "pod-subpath-test-projected-dksn": Phase="Pending", Reason="", readiness=false. Elapsed: 12.043279742s
Mar 12 07:45:30.310: INFO: Pod "pod-subpath-test-projected-dksn": Phase="Running", Reason="", readiness=true. Elapsed: 14.049314769s
Mar 12 07:45:32.316: INFO: Pod "pod-subpath-test-projected-dksn": Phase="Running", Reason="", readiness=true. Elapsed: 16.05526605s
Mar 12 07:45:34.320: INFO: Pod "pod-subpath-test-projected-dksn": Phase="Running", Reason="", readiness=true. Elapsed: 18.060138839s
Mar 12 07:45:36.326: INFO: Pod "pod-subpath-test-projected-dksn": Phase="Running", Reason="", readiness=true. Elapsed: 20.065975847s
Mar 12 07:45:38.332: INFO: Pod "pod-subpath-test-projected-dksn": Phase="Running", Reason="", readiness=true. Elapsed: 22.071408585s
Mar 12 07:45:40.337: INFO: Pod "pod-subpath-test-projected-dksn": Phase="Running", Reason="", readiness=true. Elapsed: 24.076589261s
Mar 12 07:45:42.342: INFO: Pod "pod-subpath-test-projected-dksn": Phase="Running", Reason="", readiness=true. Elapsed: 26.082042662s
Mar 12 07:45:44.348: INFO: Pod "pod-subpath-test-projected-dksn": Phase="Running", Reason="", readiness=true. Elapsed: 28.087989186s
Mar 12 07:45:46.354: INFO: Pod "pod-subpath-test-projected-dksn": Phase="Running", Reason="", readiness=true. Elapsed: 30.094213931s
Mar 12 07:45:48.360: INFO: Pod "pod-subpath-test-projected-dksn": Phase="Running", Reason="", readiness=true. Elapsed: 32.099862538s
Mar 12 07:45:50.366: INFO: Pod "pod-subpath-test-projected-dksn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.10616641s
STEP: Saw pod success
Mar 12 07:45:50.366: INFO: Pod "pod-subpath-test-projected-dksn" satisfied condition "success or failure"
Mar 12 07:45:50.370: INFO: Trying to get logs from node node-3 pod pod-subpath-test-projected-dksn container test-container-subpath-projected-dksn: <nil>
STEP: delete the pod
Mar 12 07:45:50.402: INFO: Waiting for pod pod-subpath-test-projected-dksn to disappear
Mar 12 07:45:50.405: INFO: Pod pod-subpath-test-projected-dksn no longer exists
STEP: Deleting pod pod-subpath-test-projected-dksn
Mar 12 07:45:50.405: INFO: Deleting pod "pod-subpath-test-projected-dksn" in namespace "subpath-2066"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:45:50.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2066" for this suite.
Mar 12 07:45:56.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:45:56.525: INFO: namespace subpath-2066 deletion completed in 6.110660479s

• [SLOW TEST:40.343 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:45:56.526: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 12 07:45:56.949: INFO: Waiting up to 5m0s for pod "pod-80c80985-1379-447c-83b1-7d2f6e374135" in namespace "emptydir-4331" to be "success or failure"
Mar 12 07:45:56.953: INFO: Pod "pod-80c80985-1379-447c-83b1-7d2f6e374135": Phase="Pending", Reason="", readiness=false. Elapsed: 4.197679ms
Mar 12 07:45:58.958: INFO: Pod "pod-80c80985-1379-447c-83b1-7d2f6e374135": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009452334s
Mar 12 07:46:00.964: INFO: Pod "pod-80c80985-1379-447c-83b1-7d2f6e374135": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014850155s
Mar 12 07:46:02.973: INFO: Pod "pod-80c80985-1379-447c-83b1-7d2f6e374135": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024334113s
Mar 12 07:46:04.978: INFO: Pod "pod-80c80985-1379-447c-83b1-7d2f6e374135": Phase="Pending", Reason="", readiness=false. Elapsed: 8.028969891s
Mar 12 07:46:06.983: INFO: Pod "pod-80c80985-1379-447c-83b1-7d2f6e374135": Phase="Pending", Reason="", readiness=false. Elapsed: 10.034396728s
Mar 12 07:46:08.989: INFO: Pod "pod-80c80985-1379-447c-83b1-7d2f6e374135": Phase="Pending", Reason="", readiness=false. Elapsed: 12.040634126s
Mar 12 07:46:10.995: INFO: Pod "pod-80c80985-1379-447c-83b1-7d2f6e374135": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.046030028s
STEP: Saw pod success
Mar 12 07:46:10.995: INFO: Pod "pod-80c80985-1379-447c-83b1-7d2f6e374135" satisfied condition "success or failure"
Mar 12 07:46:11.003: INFO: Trying to get logs from node node-2 pod pod-80c80985-1379-447c-83b1-7d2f6e374135 container test-container: <nil>
STEP: delete the pod
Mar 12 07:46:11.037: INFO: Waiting for pod pod-80c80985-1379-447c-83b1-7d2f6e374135 to disappear
Mar 12 07:46:11.040: INFO: Pod pod-80c80985-1379-447c-83b1-7d2f6e374135 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:46:11.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4331" for this suite.
Mar 12 07:46:17.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:46:17.145: INFO: namespace emptydir-4331 deletion completed in 6.101062837s

• [SLOW TEST:20.620 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:46:17.146: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 07:46:17.203: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 12 07:46:25.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 --namespace=crd-publish-openapi-1479 create -f -'
Mar 12 07:46:26.515: INFO: stderr: ""
Mar 12 07:46:26.515: INFO: stdout: "e2e-test-crd-publish-openapi-3595-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 12 07:46:26.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 --namespace=crd-publish-openapi-1479 delete e2e-test-crd-publish-openapi-3595-crds test-cr'
Mar 12 07:46:26.621: INFO: stderr: ""
Mar 12 07:46:26.621: INFO: stdout: "e2e-test-crd-publish-openapi-3595-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar 12 07:46:26.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 --namespace=crd-publish-openapi-1479 apply -f -'
Mar 12 07:46:26.930: INFO: stderr: ""
Mar 12 07:46:26.930: INFO: stdout: "e2e-test-crd-publish-openapi-3595-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 12 07:46:26.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 --namespace=crd-publish-openapi-1479 delete e2e-test-crd-publish-openapi-3595-crds test-cr'
Mar 12 07:46:27.069: INFO: stderr: ""
Mar 12 07:46:27.069: INFO: stdout: "e2e-test-crd-publish-openapi-3595-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 12 07:46:27.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 explain e2e-test-crd-publish-openapi-3595-crds'
Mar 12 07:46:27.317: INFO: stderr: ""
Mar 12 07:46:27.317: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3595-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:46:30.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1479" for this suite.
Mar 12 07:46:36.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:46:36.938: INFO: namespace crd-publish-openapi-1479 deletion completed in 6.106342383s

• [SLOW TEST:19.792 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:46:36.938: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:46:36.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6703" for this suite.
Mar 12 07:46:43.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:46:43.097: INFO: namespace services-6703 deletion completed in 6.095944028s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:6.159 seconds]
[sig-network] Services
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:46:43.097: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-5263
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-5263
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5263
Mar 12 07:46:43.170: INFO: Found 0 stateful pods, waiting for 1
Mar 12 07:46:53.176: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Mar 12 07:47:03.174: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar 12 07:47:03.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=statefulset-5263 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 12 07:47:03.357: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 12 07:47:03.357: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 12 07:47:03.357: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 12 07:47:03.362: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 12 07:47:13.368: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 12 07:47:13.368: INFO: Waiting for statefulset status.replicas updated to 0
Mar 12 07:47:13.386: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999942s
Mar 12 07:47:14.391: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995965897s
Mar 12 07:47:15.397: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990641339s
Mar 12 07:47:16.403: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.984667642s
Mar 12 07:47:17.408: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.979381946s
Mar 12 07:47:18.413: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.974399775s
Mar 12 07:47:19.418: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.968757473s
Mar 12 07:47:20.422: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.963823448s
Mar 12 07:47:21.428: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.95950082s
Mar 12 07:47:22.433: INFO: Verifying statefulset ss doesn't scale past 1 for another 954.288242ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5263
Mar 12 07:47:23.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=statefulset-5263 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 12 07:47:23.609: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 12 07:47:23.609: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 12 07:47:23.609: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 12 07:47:23.613: INFO: Found 1 stateful pods, waiting for 3
Mar 12 07:47:33.618: INFO: Found 2 stateful pods, waiting for 3
Mar 12 07:47:43.618: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 12 07:47:43.618: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 12 07:47:43.618: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar 12 07:47:53.618: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 12 07:47:53.618: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 12 07:47:53.618: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar 12 07:47:53.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=statefulset-5263 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 12 07:47:53.803: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 12 07:47:53.803: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 12 07:47:53.803: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 12 07:47:53.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=statefulset-5263 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 12 07:47:54.002: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 12 07:47:54.002: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 12 07:47:54.002: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 12 07:47:54.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=statefulset-5263 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 12 07:47:54.208: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 12 07:47:54.209: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 12 07:47:54.209: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 12 07:47:54.209: INFO: Waiting for statefulset status.replicas updated to 0
Mar 12 07:47:54.213: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar 12 07:48:04.220: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 12 07:48:04.220: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 12 07:48:04.220: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 12 07:48:04.232: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999523s
Mar 12 07:48:05.237: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996334931s
Mar 12 07:48:06.242: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991595966s
Mar 12 07:48:07.248: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986622553s
Mar 12 07:48:08.252: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980990668s
Mar 12 07:48:09.259: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.976224021s
Mar 12 07:48:10.265: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969863414s
Mar 12 07:48:11.270: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.963983152s
Mar 12 07:48:12.276: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.958363456s
Mar 12 07:48:13.281: INFO: Verifying statefulset ss doesn't scale past 3 for another 952.3088ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5263
Mar 12 07:48:14.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=statefulset-5263 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 12 07:48:14.469: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 12 07:48:14.469: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 12 07:48:14.469: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 12 07:48:14.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=statefulset-5263 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 12 07:48:14.654: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 12 07:48:14.654: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 12 07:48:14.654: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 12 07:48:14.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=statefulset-5263 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 12 07:48:14.836: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 12 07:48:14.836: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 12 07:48:14.836: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 12 07:48:14.836: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar 12 07:48:34.863: INFO: Deleting all statefulset in ns statefulset-5263
Mar 12 07:48:34.866: INFO: Scaling statefulset ss to 0
Mar 12 07:48:34.876: INFO: Waiting for statefulset status.replicas updated to 0
Mar 12 07:48:34.878: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:48:34.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5263" for this suite.
Mar 12 07:48:40.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:48:41.014: INFO: namespace statefulset-5263 deletion completed in 6.11531257s

• [SLOW TEST:117.917 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:48:41.015: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-7f082eff-e8e7-4067-b02a-95951fc9786c
STEP: Creating a pod to test consume secrets
Mar 12 07:48:41.098: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-868e8139-10c3-4a28-accc-8cdc0acc3187" in namespace "projected-6898" to be "success or failure"
Mar 12 07:48:41.102: INFO: Pod "pod-projected-secrets-868e8139-10c3-4a28-accc-8cdc0acc3187": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066917ms
Mar 12 07:48:43.107: INFO: Pod "pod-projected-secrets-868e8139-10c3-4a28-accc-8cdc0acc3187": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009290652s
Mar 12 07:48:45.112: INFO: Pod "pod-projected-secrets-868e8139-10c3-4a28-accc-8cdc0acc3187": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013862188s
Mar 12 07:48:47.116: INFO: Pod "pod-projected-secrets-868e8139-10c3-4a28-accc-8cdc0acc3187": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017919994s
Mar 12 07:48:49.121: INFO: Pod "pod-projected-secrets-868e8139-10c3-4a28-accc-8cdc0acc3187": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022709705s
Mar 12 07:48:51.125: INFO: Pod "pod-projected-secrets-868e8139-10c3-4a28-accc-8cdc0acc3187": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02716518s
Mar 12 07:48:53.131: INFO: Pod "pod-projected-secrets-868e8139-10c3-4a28-accc-8cdc0acc3187": Phase="Pending", Reason="", readiness=false. Elapsed: 12.032432499s
Mar 12 07:48:55.135: INFO: Pod "pod-projected-secrets-868e8139-10c3-4a28-accc-8cdc0acc3187": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.037271333s
STEP: Saw pod success
Mar 12 07:48:55.136: INFO: Pod "pod-projected-secrets-868e8139-10c3-4a28-accc-8cdc0acc3187" satisfied condition "success or failure"
Mar 12 07:48:55.139: INFO: Trying to get logs from node node-3 pod pod-projected-secrets-868e8139-10c3-4a28-accc-8cdc0acc3187 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 12 07:48:55.177: INFO: Waiting for pod pod-projected-secrets-868e8139-10c3-4a28-accc-8cdc0acc3187 to disappear
Mar 12 07:48:55.180: INFO: Pod pod-projected-secrets-868e8139-10c3-4a28-accc-8cdc0acc3187 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:48:55.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6898" for this suite.
Mar 12 07:49:01.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:49:01.307: INFO: namespace projected-6898 deletion completed in 6.121304798s

• [SLOW TEST:20.293 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:49:01.308: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Mar 12 07:49:01.368: INFO: Waiting up to 5m0s for pod "client-containers-3d1a467e-272a-43ea-bb03-a2b109d165a8" in namespace "containers-6345" to be "success or failure"
Mar 12 07:49:01.371: INFO: Pod "client-containers-3d1a467e-272a-43ea-bb03-a2b109d165a8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.333567ms
Mar 12 07:49:03.377: INFO: Pod "client-containers-3d1a467e-272a-43ea-bb03-a2b109d165a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008659042s
Mar 12 07:49:05.382: INFO: Pod "client-containers-3d1a467e-272a-43ea-bb03-a2b109d165a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014014242s
Mar 12 07:49:07.387: INFO: Pod "client-containers-3d1a467e-272a-43ea-bb03-a2b109d165a8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019052343s
Mar 12 07:49:09.392: INFO: Pod "client-containers-3d1a467e-272a-43ea-bb03-a2b109d165a8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023963385s
Mar 12 07:49:11.396: INFO: Pod "client-containers-3d1a467e-272a-43ea-bb03-a2b109d165a8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02835546s
Mar 12 07:49:13.401: INFO: Pod "client-containers-3d1a467e-272a-43ea-bb03-a2b109d165a8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.032930033s
Mar 12 07:49:15.410: INFO: Pod "client-containers-3d1a467e-272a-43ea-bb03-a2b109d165a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.041464352s
STEP: Saw pod success
Mar 12 07:49:15.410: INFO: Pod "client-containers-3d1a467e-272a-43ea-bb03-a2b109d165a8" satisfied condition "success or failure"
Mar 12 07:49:15.413: INFO: Trying to get logs from node node-2 pod client-containers-3d1a467e-272a-43ea-bb03-a2b109d165a8 container test-container: <nil>
STEP: delete the pod
Mar 12 07:49:15.459: INFO: Waiting for pod client-containers-3d1a467e-272a-43ea-bb03-a2b109d165a8 to disappear
Mar 12 07:49:15.472: INFO: Pod client-containers-3d1a467e-272a-43ea-bb03-a2b109d165a8 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:49:15.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6345" for this suite.
Mar 12 07:49:21.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:49:21.575: INFO: namespace containers-6345 deletion completed in 6.097738907s

• [SLOW TEST:20.268 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:49:21.576: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 07:49:21.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 create -f - --namespace=kubectl-9683'
Mar 12 07:49:21.879: INFO: stderr: ""
Mar 12 07:49:21.879: INFO: stdout: "replicationcontroller/redis-master created\n"
Mar 12 07:49:21.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 create -f - --namespace=kubectl-9683'
Mar 12 07:49:22.208: INFO: stderr: ""
Mar 12 07:49:22.208: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar 12 07:49:23.214: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:49:23.214: INFO: Found 0 / 1
Mar 12 07:49:24.213: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:49:24.213: INFO: Found 0 / 1
Mar 12 07:49:25.214: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:49:25.214: INFO: Found 0 / 1
Mar 12 07:49:26.213: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:49:26.213: INFO: Found 0 / 1
Mar 12 07:49:27.214: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:49:27.214: INFO: Found 0 / 1
Mar 12 07:49:28.213: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:49:28.213: INFO: Found 0 / 1
Mar 12 07:49:29.214: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:49:29.214: INFO: Found 0 / 1
Mar 12 07:49:30.213: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:49:30.213: INFO: Found 0 / 1
Mar 12 07:49:31.213: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:49:31.213: INFO: Found 0 / 1
Mar 12 07:49:32.214: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:49:32.214: INFO: Found 0 / 1
Mar 12 07:49:33.213: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:49:33.214: INFO: Found 0 / 1
Mar 12 07:49:34.213: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:49:34.213: INFO: Found 1 / 1
Mar 12 07:49:34.213: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 12 07:49:34.216: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 07:49:34.216: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 12 07:49:34.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 describe pod redis-master-9vnsz --namespace=kubectl-9683'
Mar 12 07:49:34.342: INFO: stderr: ""
Mar 12 07:49:34.342: INFO: stdout: "Name:         redis-master-9vnsz\nNamespace:    kubectl-9683\nPriority:     0\nNode:         node-3/192.168.20.5\nStart Time:   Thu, 12 Mar 2020 07:49:21 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nStatus:       Running\nIP:           10.233.65.88\nIPs:\n  IP:           10.233.65.88\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://f0b3a4b20b508f04197fa7041440224f449e4a4795c935233a786824047dd552\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker-pullable://docker.io/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 12 Mar 2020 07:49:34 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jtp4n (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-jtp4n:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-jtp4n\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From               Message\n  ----    ------     ----       ----               -------\n  Normal  Scheduled  <unknown>  default-scheduler  Successfully assigned kubectl-9683/redis-master-9vnsz to node-3\n  Normal  Pulled     1s         kubelet, node-3    Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    1s         kubelet, node-3    Created container redis-master\n  Normal  Started    0s         kubelet, node-3    Started container redis-master\n"
Mar 12 07:49:34.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 describe rc redis-master --namespace=kubectl-9683'
Mar 12 07:49:34.617: INFO: stderr: ""
Mar 12 07:49:34.617: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-9683\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  13s   replication-controller  Created pod: redis-master-9vnsz\n"
Mar 12 07:49:34.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 describe service redis-master --namespace=kubectl-9683'
Mar 12 07:49:34.735: INFO: stderr: ""
Mar 12 07:49:34.735: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-9683\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.233.39.3\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.233.65.88:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 12 07:49:34.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 describe node node-1'
Mar 12 07:49:34.872: INFO: stderr: ""
Mar 12 07:49:34.872: INFO: stdout: "Name:               node-1\nRoles:              master,node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=node-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=true\n                    node-role.kubernetes.io/node=true\nAnnotations:        flannel.alpha.coreos.com/backend-data: null\n                    flannel.alpha.coreos.com/backend-type: host-gw\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.20.3\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 11 Mar 2020 11:52:58 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 12 Mar 2020 07:49:24 +0000   Thu, 12 Mar 2020 06:39:23 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 12 Mar 2020 07:49:24 +0000   Thu, 12 Mar 2020 06:39:23 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 12 Mar 2020 07:49:24 +0000   Thu, 12 Mar 2020 06:39:23 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 12 Mar 2020 07:49:24 +0000   Thu, 12 Mar 2020 06:39:23 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.20.3\n  Hostname:    node-1\nCapacity:\n cpu:                16\n ephemeral-storage:  100290720Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             39939956Ki\n pods:               300\nAllocatable:\n cpu:                14976m\n ephemeral-storage:  100290720Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             25690996Ki\n pods:               300\nSystem Info:\n Machine ID:                 c9c198ef9bc2496bb3678a7609e5a5a4\n System UUID:                C9C198EF-9BC2-496B-B367-8A7609E5A5A4\n Boot ID:                    1c3990af-53cd-415e-a1a3-b62ba5ba7682\n Kernel Version:             3.10.0-693.11.1.el7.es.10.x86_64\n OS Image:                   EasyStack Cloud Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://1.13.1\n Kubelet Version:            v1.16.6-es\n Kube-Proxy Version:         v1.16.6-es\nPodCIDR:                     10.233.66.0/24\nPodCIDRs:                    10.233.66.0/24\nNon-terminated Pods:         (10 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                coredns-p4prs                                              100m (0%)     0 (0%)      70Mi (0%)        500Mi (1%)     15h\n  kube-system                k8s-keystone-auth-7n8pn                                    200m (1%)     0 (0%)      0 (0%)           0 (0%)         15h\n  kube-system                kube-apiserver-node-1                                      100m (0%)     4 (26%)     256M (0%)        4096M (15%)    46m\n  kube-system                kube-controller-manager-node-1                             100m (0%)     250m (1%)   100M (0%)        512M (1%)      18h\n  kube-system                kube-flannel-hgdc4                                         150m (1%)     300m (2%)   64M (0%)         500M (1%)      15h\n  kube-system                kube-proxy-node-1                                          150m (1%)     500m (3%)   64M (0%)         2G (7%)        19h\n  kube-system                kube-scheduler-node-1                                      80m (0%)      250m (1%)   170M (0%)        512M (1%)      19h\n  sonobuoy                   sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         38m\n  sonobuoy                   sonobuoy-e2e-job-0ad0ccbfe03640eb                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         38m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-b6e3a2e4f0de4f41-fktq2    0 (0%)        0 (0%)      0 (0%)           0 (0%)         38m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                880m (5%)       5300m (35%)\n  memory             727400320 (2%)  8144288000 (30%)\n  ephemeral-storage  0 (0%)          0 (0%)\nEvents:              <none>\n"
Mar 12 07:49:34.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 describe namespace kubectl-9683'
Mar 12 07:49:34.970: INFO: stderr: ""
Mar 12 07:49:34.970: INFO: stdout: "Name:         kubectl-9683\nLabels:       e2e-framework=kubectl\n              e2e-run=89cb804f-94de-4d46-9e23-74bab84b7cf4\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:49:34.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9683" for this suite.
Mar 12 07:49:46.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:49:47.076: INFO: namespace kubectl-9683 deletion completed in 12.099869324s

• [SLOW TEST:25.500 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:49:47.076: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:50:03.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-502" for this suite.
Mar 12 07:50:09.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:50:09.383: INFO: namespace resourcequota-502 deletion completed in 6.126667899s

• [SLOW TEST:22.307 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:50:09.384: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 07:50:09.462: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-86b9c658-5003-4b33-9a54-da7872defa2a" in namespace "security-context-test-7963" to be "success or failure"
Mar 12 07:50:09.465: INFO: Pod "busybox-privileged-false-86b9c658-5003-4b33-9a54-da7872defa2a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.152723ms
Mar 12 07:50:11.471: INFO: Pod "busybox-privileged-false-86b9c658-5003-4b33-9a54-da7872defa2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009599855s
Mar 12 07:50:13.477: INFO: Pod "busybox-privileged-false-86b9c658-5003-4b33-9a54-da7872defa2a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01560133s
Mar 12 07:50:15.482: INFO: Pod "busybox-privileged-false-86b9c658-5003-4b33-9a54-da7872defa2a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020610286s
Mar 12 07:50:17.488: INFO: Pod "busybox-privileged-false-86b9c658-5003-4b33-9a54-da7872defa2a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025711026s
Mar 12 07:50:19.492: INFO: Pod "busybox-privileged-false-86b9c658-5003-4b33-9a54-da7872defa2a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030350544s
Mar 12 07:50:21.497: INFO: Pod "busybox-privileged-false-86b9c658-5003-4b33-9a54-da7872defa2a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.034888803s
Mar 12 07:50:23.501: INFO: Pod "busybox-privileged-false-86b9c658-5003-4b33-9a54-da7872defa2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.039236239s
Mar 12 07:50:23.501: INFO: Pod "busybox-privileged-false-86b9c658-5003-4b33-9a54-da7872defa2a" satisfied condition "success or failure"
Mar 12 07:50:23.510: INFO: Got logs for pod "busybox-privileged-false-86b9c658-5003-4b33-9a54-da7872defa2a": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:50:23.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7963" for this suite.
Mar 12 07:50:29.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:50:29.624: INFO: namespace security-context-test-7963 deletion completed in 6.108706355s

• [SLOW TEST:20.240 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:50:29.624: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar 12 07:50:29.691: INFO: Pod name pod-release: Found 0 pods out of 1
Mar 12 07:50:34.696: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:50:35.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6999" for this suite.
Mar 12 07:50:41.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:50:41.861: INFO: namespace replication-controller-6999 deletion completed in 6.135922967s

• [SLOW TEST:12.237 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:50:41.861: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-2691
STEP: creating replication controller nodeport-test in namespace services-2691
I0312 07:50:42.073336      24 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-2691, replica count: 2
I0312 07:50:45.123907      24 runners.go:184] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 07:50:48.124169      24 runners.go:184] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 07:50:51.124480      24 runners.go:184] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 07:50:54.124735      24 runners.go:184] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 12 07:50:57.125: INFO: Creating new exec pod
I0312 07:50:57.124953      24 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 12 07:51:12.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=services-2691 execpod99z42 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Mar 12 07:51:12.758: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar 12 07:51:12.758: INFO: stdout: ""
Mar 12 07:51:12.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=services-2691 execpod99z42 -- /bin/sh -x -c nc -zv -t -w 2 10.233.47.98 80'
Mar 12 07:51:12.942: INFO: stderr: "+ nc -zv -t -w 2 10.233.47.98 80\nConnection to 10.233.47.98 80 port [tcp/http] succeeded!\n"
Mar 12 07:51:12.942: INFO: stdout: ""
Mar 12 07:51:12.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=services-2691 execpod99z42 -- /bin/sh -x -c nc -zv -t -w 2 192.168.20.3 32128'
Mar 12 07:51:13.147: INFO: stderr: "+ nc -zv -t -w 2 192.168.20.3 32128\nConnection to 192.168.20.3 32128 port [tcp/32128] succeeded!\n"
Mar 12 07:51:13.147: INFO: stdout: ""
Mar 12 07:51:13.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=services-2691 execpod99z42 -- /bin/sh -x -c nc -zv -t -w 2 192.168.20.4 32128'
Mar 12 07:51:13.351: INFO: stderr: "+ nc -zv -t -w 2 192.168.20.4 32128\nConnection to 192.168.20.4 32128 port [tcp/32128] succeeded!\n"
Mar 12 07:51:13.351: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:51:13.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2691" for this suite.
Mar 12 07:51:19.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:51:19.450: INFO: namespace services-2691 deletion completed in 6.09300998s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:37.588 seconds]
[sig-network] Services
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:51:19.450: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-d9bea600-5b6a-489e-a0ef-481bdfc203c1
STEP: Creating a pod to test consume configMaps
Mar 12 07:51:19.514: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5748fc52-160d-494a-b64c-8159f77f0a91" in namespace "projected-8954" to be "success or failure"
Mar 12 07:51:19.517: INFO: Pod "pod-projected-configmaps-5748fc52-160d-494a-b64c-8159f77f0a91": Phase="Pending", Reason="", readiness=false. Elapsed: 3.111342ms
Mar 12 07:51:21.521: INFO: Pod "pod-projected-configmaps-5748fc52-160d-494a-b64c-8159f77f0a91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007619765s
Mar 12 07:51:23.527: INFO: Pod "pod-projected-configmaps-5748fc52-160d-494a-b64c-8159f77f0a91": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012897605s
Mar 12 07:51:25.531: INFO: Pod "pod-projected-configmaps-5748fc52-160d-494a-b64c-8159f77f0a91": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017417541s
Mar 12 07:51:27.535: INFO: Pod "pod-projected-configmaps-5748fc52-160d-494a-b64c-8159f77f0a91": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02153391s
Mar 12 07:51:29.540: INFO: Pod "pod-projected-configmaps-5748fc52-160d-494a-b64c-8159f77f0a91": Phase="Pending", Reason="", readiness=false. Elapsed: 10.026567947s
Mar 12 07:51:31.545: INFO: Pod "pod-projected-configmaps-5748fc52-160d-494a-b64c-8159f77f0a91": Phase="Pending", Reason="", readiness=false. Elapsed: 12.031651503s
Mar 12 07:51:33.551: INFO: Pod "pod-projected-configmaps-5748fc52-160d-494a-b64c-8159f77f0a91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.037087898s
STEP: Saw pod success
Mar 12 07:51:33.551: INFO: Pod "pod-projected-configmaps-5748fc52-160d-494a-b64c-8159f77f0a91" satisfied condition "success or failure"
Mar 12 07:51:33.555: INFO: Trying to get logs from node node-2 pod pod-projected-configmaps-5748fc52-160d-494a-b64c-8159f77f0a91 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 12 07:51:33.590: INFO: Waiting for pod pod-projected-configmaps-5748fc52-160d-494a-b64c-8159f77f0a91 to disappear
Mar 12 07:51:33.593: INFO: Pod pod-projected-configmaps-5748fc52-160d-494a-b64c-8159f77f0a91 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:51:33.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8954" for this suite.
Mar 12 07:51:39.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:51:39.728: INFO: namespace projected-8954 deletion completed in 6.130272098s

• [SLOW TEST:20.278 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:51:39.728: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 12 07:51:39.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-6440'
Mar 12 07:51:39.906: INFO: stderr: ""
Mar 12 07:51:39.906: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Mar 12 07:51:54.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pod e2e-test-httpd-pod --namespace=kubectl-6440 -o json'
Mar 12 07:51:55.074: INFO: stderr: ""
Mar 12 07:51:55.074: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-03-12T07:51:39Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6440\",\n        \"resourceVersion\": \"142115\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6440/pods/e2e-test-httpd-pod\",\n        \"uid\": \"c84b808c-fbb7-4a04-952f-17f1ebc7e077\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-ghztj\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"node-2\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-ghztj\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-ghztj\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-12T07:51:39Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-12T07:51:52Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-12T07:51:52Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-12T07:51:39Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://2181bf2d31774e0a9d14a5fd0d9ab95f474623383f3a828827b22d7f2d2e2718\",\n                \"image\": \"docker.io/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-03-12T07:51:52Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.20.4\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.64.32\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.64.32\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-03-12T07:51:39Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar 12 07:51:55.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 replace -f - --namespace=kubectl-6440'
Mar 12 07:51:55.362: INFO: stderr: ""
Mar 12 07:51:55.362: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Mar 12 07:51:55.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 delete pods e2e-test-httpd-pod --namespace=kubectl-6440'
Mar 12 07:51:57.867: INFO: stderr: ""
Mar 12 07:51:57.867: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:51:57.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6440" for this suite.
Mar 12 07:52:03.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:52:03.994: INFO: namespace kubectl-6440 deletion completed in 6.121372245s

• [SLOW TEST:24.265 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:52:03.994: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 12 07:52:04.057: INFO: Waiting up to 5m0s for pod "downwardapi-volume-74e11217-ffbd-46ec-b811-63e8e5ea8871" in namespace "downward-api-349" to be "success or failure"
Mar 12 07:52:04.060: INFO: Pod "downwardapi-volume-74e11217-ffbd-46ec-b811-63e8e5ea8871": Phase="Pending", Reason="", readiness=false. Elapsed: 2.877574ms
Mar 12 07:52:06.065: INFO: Pod "downwardapi-volume-74e11217-ffbd-46ec-b811-63e8e5ea8871": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007769972s
Mar 12 07:52:08.070: INFO: Pod "downwardapi-volume-74e11217-ffbd-46ec-b811-63e8e5ea8871": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012783274s
Mar 12 07:52:10.095: INFO: Pod "downwardapi-volume-74e11217-ffbd-46ec-b811-63e8e5ea8871": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038097926s
Mar 12 07:52:12.099: INFO: Pod "downwardapi-volume-74e11217-ffbd-46ec-b811-63e8e5ea8871": Phase="Pending", Reason="", readiness=false. Elapsed: 8.042703676s
Mar 12 07:52:14.104: INFO: Pod "downwardapi-volume-74e11217-ffbd-46ec-b811-63e8e5ea8871": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047300175s
Mar 12 07:52:16.108: INFO: Pod "downwardapi-volume-74e11217-ffbd-46ec-b811-63e8e5ea8871": Phase="Running", Reason="", readiness=true. Elapsed: 12.051604972s
Mar 12 07:52:18.113: INFO: Pod "downwardapi-volume-74e11217-ffbd-46ec-b811-63e8e5ea8871": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.056485341s
STEP: Saw pod success
Mar 12 07:52:18.113: INFO: Pod "downwardapi-volume-74e11217-ffbd-46ec-b811-63e8e5ea8871" satisfied condition "success or failure"
Mar 12 07:52:18.116: INFO: Trying to get logs from node node-3 pod downwardapi-volume-74e11217-ffbd-46ec-b811-63e8e5ea8871 container client-container: <nil>
STEP: delete the pod
Mar 12 07:52:18.147: INFO: Waiting for pod downwardapi-volume-74e11217-ffbd-46ec-b811-63e8e5ea8871 to disappear
Mar 12 07:52:18.150: INFO: Pod downwardapi-volume-74e11217-ffbd-46ec-b811-63e8e5ea8871 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:52:18.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-349" for this suite.
Mar 12 07:52:24.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:52:24.271: INFO: namespace downward-api-349 deletion completed in 6.115830323s

• [SLOW TEST:20.277 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:52:24.272: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-9ecf987c-d072-4d09-8d12-ef7b72fa583b
STEP: Creating a pod to test consume configMaps
Mar 12 07:52:24.332: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0177af5b-bb01-466a-a215-d50f00280618" in namespace "projected-8662" to be "success or failure"
Mar 12 07:52:24.336: INFO: Pod "pod-projected-configmaps-0177af5b-bb01-466a-a215-d50f00280618": Phase="Pending", Reason="", readiness=false. Elapsed: 3.332472ms
Mar 12 07:52:26.340: INFO: Pod "pod-projected-configmaps-0177af5b-bb01-466a-a215-d50f00280618": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007954047s
Mar 12 07:52:28.345: INFO: Pod "pod-projected-configmaps-0177af5b-bb01-466a-a215-d50f00280618": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012969926s
Mar 12 07:52:30.352: INFO: Pod "pod-projected-configmaps-0177af5b-bb01-466a-a215-d50f00280618": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019242585s
Mar 12 07:52:32.369: INFO: Pod "pod-projected-configmaps-0177af5b-bb01-466a-a215-d50f00280618": Phase="Pending", Reason="", readiness=false. Elapsed: 8.036636862s
Mar 12 07:52:34.374: INFO: Pod "pod-projected-configmaps-0177af5b-bb01-466a-a215-d50f00280618": Phase="Pending", Reason="", readiness=false. Elapsed: 10.042001563s
Mar 12 07:52:36.379: INFO: Pod "pod-projected-configmaps-0177af5b-bb01-466a-a215-d50f00280618": Phase="Pending", Reason="", readiness=false. Elapsed: 12.046509809s
Mar 12 07:52:38.384: INFO: Pod "pod-projected-configmaps-0177af5b-bb01-466a-a215-d50f00280618": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.051291126s
STEP: Saw pod success
Mar 12 07:52:38.384: INFO: Pod "pod-projected-configmaps-0177af5b-bb01-466a-a215-d50f00280618" satisfied condition "success or failure"
Mar 12 07:52:38.387: INFO: Trying to get logs from node node-3 pod pod-projected-configmaps-0177af5b-bb01-466a-a215-d50f00280618 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 12 07:52:38.410: INFO: Waiting for pod pod-projected-configmaps-0177af5b-bb01-466a-a215-d50f00280618 to disappear
Mar 12 07:52:38.413: INFO: Pod pod-projected-configmaps-0177af5b-bb01-466a-a215-d50f00280618 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:52:38.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8662" for this suite.
Mar 12 07:52:44.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:52:44.510: INFO: namespace projected-8662 deletion completed in 6.093505513s

• [SLOW TEST:20.239 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:52:44.510: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-338/configmap-test-fb4933ca-4ded-4cff-8813-48049a1693a0
STEP: Creating a pod to test consume configMaps
Mar 12 07:52:44.584: INFO: Waiting up to 5m0s for pod "pod-configmaps-f58c2642-fbc1-4da6-a499-c1852bcd9fe3" in namespace "configmap-338" to be "success or failure"
Mar 12 07:52:44.587: INFO: Pod "pod-configmaps-f58c2642-fbc1-4da6-a499-c1852bcd9fe3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.31852ms
Mar 12 07:52:46.592: INFO: Pod "pod-configmaps-f58c2642-fbc1-4da6-a499-c1852bcd9fe3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008409395s
Mar 12 07:52:48.596: INFO: Pod "pod-configmaps-f58c2642-fbc1-4da6-a499-c1852bcd9fe3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012503311s
Mar 12 07:52:50.610: INFO: Pod "pod-configmaps-f58c2642-fbc1-4da6-a499-c1852bcd9fe3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026245556s
Mar 12 07:52:52.614: INFO: Pod "pod-configmaps-f58c2642-fbc1-4da6-a499-c1852bcd9fe3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.030201131s
Mar 12 07:52:54.619: INFO: Pod "pod-configmaps-f58c2642-fbc1-4da6-a499-c1852bcd9fe3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.034597886s
Mar 12 07:52:56.623: INFO: Pod "pod-configmaps-f58c2642-fbc1-4da6-a499-c1852bcd9fe3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.038842505s
Mar 12 07:52:58.629: INFO: Pod "pod-configmaps-f58c2642-fbc1-4da6-a499-c1852bcd9fe3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.045285993s
STEP: Saw pod success
Mar 12 07:52:58.629: INFO: Pod "pod-configmaps-f58c2642-fbc1-4da6-a499-c1852bcd9fe3" satisfied condition "success or failure"
Mar 12 07:52:58.633: INFO: Trying to get logs from node node-2 pod pod-configmaps-f58c2642-fbc1-4da6-a499-c1852bcd9fe3 container env-test: <nil>
STEP: delete the pod
Mar 12 07:52:58.660: INFO: Waiting for pod pod-configmaps-f58c2642-fbc1-4da6-a499-c1852bcd9fe3 to disappear
Mar 12 07:52:58.663: INFO: Pod pod-configmaps-f58c2642-fbc1-4da6-a499-c1852bcd9fe3 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:52:58.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-338" for this suite.
Mar 12 07:53:04.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:53:04.773: INFO: namespace configmap-338 deletion completed in 6.105802983s

• [SLOW TEST:20.263 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:53:04.774: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Mar 12 07:53:05.346: INFO: created pod pod-service-account-defaultsa
Mar 12 07:53:05.346: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 12 07:53:05.352: INFO: created pod pod-service-account-mountsa
Mar 12 07:53:05.352: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 12 07:53:05.361: INFO: created pod pod-service-account-nomountsa
Mar 12 07:53:05.361: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 12 07:53:05.374: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 12 07:53:05.374: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 12 07:53:05.388: INFO: created pod pod-service-account-mountsa-mountspec
Mar 12 07:53:05.388: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 12 07:53:05.409: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 12 07:53:05.409: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 12 07:53:05.416: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 12 07:53:05.416: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 12 07:53:05.434: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 12 07:53:05.434: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 12 07:53:05.443: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 12 07:53:05.443: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:53:05.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5442" for this suite.
Mar 12 07:53:33.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:53:33.548: INFO: namespace svcaccounts-5442 deletion completed in 28.095920128s

• [SLOW TEST:28.774 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:53:33.548: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 12 07:53:33.613: INFO: Waiting up to 5m0s for pod "downwardapi-volume-86ebc6ee-6de6-47e6-88ae-3699a6afdc57" in namespace "downward-api-5914" to be "success or failure"
Mar 12 07:53:33.619: INFO: Pod "downwardapi-volume-86ebc6ee-6de6-47e6-88ae-3699a6afdc57": Phase="Pending", Reason="", readiness=false. Elapsed: 5.593671ms
Mar 12 07:53:35.624: INFO: Pod "downwardapi-volume-86ebc6ee-6de6-47e6-88ae-3699a6afdc57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010694118s
Mar 12 07:53:37.629: INFO: Pod "downwardapi-volume-86ebc6ee-6de6-47e6-88ae-3699a6afdc57": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015416034s
Mar 12 07:53:39.634: INFO: Pod "downwardapi-volume-86ebc6ee-6de6-47e6-88ae-3699a6afdc57": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021057615s
Mar 12 07:53:41.639: INFO: Pod "downwardapi-volume-86ebc6ee-6de6-47e6-88ae-3699a6afdc57": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02601774s
Mar 12 07:53:43.644: INFO: Pod "downwardapi-volume-86ebc6ee-6de6-47e6-88ae-3699a6afdc57": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030713323s
Mar 12 07:53:45.649: INFO: Pod "downwardapi-volume-86ebc6ee-6de6-47e6-88ae-3699a6afdc57": Phase="Pending", Reason="", readiness=false. Elapsed: 12.035892255s
Mar 12 07:53:47.653: INFO: Pod "downwardapi-volume-86ebc6ee-6de6-47e6-88ae-3699a6afdc57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.040145871s
STEP: Saw pod success
Mar 12 07:53:47.654: INFO: Pod "downwardapi-volume-86ebc6ee-6de6-47e6-88ae-3699a6afdc57" satisfied condition "success or failure"
Mar 12 07:53:47.656: INFO: Trying to get logs from node node-2 pod downwardapi-volume-86ebc6ee-6de6-47e6-88ae-3699a6afdc57 container client-container: <nil>
STEP: delete the pod
Mar 12 07:53:47.698: INFO: Waiting for pod downwardapi-volume-86ebc6ee-6de6-47e6-88ae-3699a6afdc57 to disappear
Mar 12 07:53:47.701: INFO: Pod downwardapi-volume-86ebc6ee-6de6-47e6-88ae-3699a6afdc57 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:53:47.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5914" for this suite.
Mar 12 07:53:53.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:53:53.800: INFO: namespace downward-api-5914 deletion completed in 6.093419764s

• [SLOW TEST:20.252 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:53:53.800: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 12 07:53:53.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-5405'
Mar 12 07:53:53.947: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 12 07:53:53.947: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Mar 12 07:53:53.952: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Mar 12 07:53:53.956: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Mar 12 07:53:53.969: INFO: scanned /root for discovery docs: <nil>
Mar 12 07:53:53.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-5405'
Mar 12 07:54:18.919: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar 12 07:54:18.919: INFO: stdout: "Created e2e-test-httpd-rc-595692c282e2d9fec7742caa6dbfb053\nScaling up e2e-test-httpd-rc-595692c282e2d9fec7742caa6dbfb053 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-595692c282e2d9fec7742caa6dbfb053 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-595692c282e2d9fec7742caa6dbfb053 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Mar 12 07:54:18.919: INFO: stdout: "Created e2e-test-httpd-rc-595692c282e2d9fec7742caa6dbfb053\nScaling up e2e-test-httpd-rc-595692c282e2d9fec7742caa6dbfb053 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-595692c282e2d9fec7742caa6dbfb053 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-595692c282e2d9fec7742caa6dbfb053 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Mar 12 07:54:18.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-5405'
Mar 12 07:54:19.022: INFO: stderr: ""
Mar 12 07:54:19.022: INFO: stdout: "e2e-test-httpd-rc-595692c282e2d9fec7742caa6dbfb053-jzvlj e2e-test-httpd-rc-r4wd7 "
STEP: Replicas for run=e2e-test-httpd-rc: expected=1 actual=2
Mar 12 07:54:24.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-5405'
Mar 12 07:54:24.139: INFO: stderr: ""
Mar 12 07:54:24.139: INFO: stdout: "e2e-test-httpd-rc-595692c282e2d9fec7742caa6dbfb053-jzvlj "
Mar 12 07:54:24.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods e2e-test-httpd-rc-595692c282e2d9fec7742caa6dbfb053-jzvlj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5405'
Mar 12 07:54:24.230: INFO: stderr: ""
Mar 12 07:54:24.230: INFO: stdout: "true"
Mar 12 07:54:24.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods e2e-test-httpd-rc-595692c282e2d9fec7742caa6dbfb053-jzvlj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5405'
Mar 12 07:54:24.311: INFO: stderr: ""
Mar 12 07:54:24.311: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Mar 12 07:54:24.311: INFO: e2e-test-httpd-rc-595692c282e2d9fec7742caa6dbfb053-jzvlj is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Mar 12 07:54:24.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 delete rc e2e-test-httpd-rc --namespace=kubectl-5405'
Mar 12 07:54:24.408: INFO: stderr: ""
Mar 12 07:54:24.408: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:54:24.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5405" for this suite.
Mar 12 07:54:30.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:54:30.524: INFO: namespace kubectl-5405 deletion completed in 6.109699638s

• [SLOW TEST:36.724 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:54:30.524: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 12 07:54:30.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-9665'
Mar 12 07:54:30.660: INFO: stderr: ""
Mar 12 07:54:30.660: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Mar 12 07:54:30.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 delete pods e2e-test-httpd-pod --namespace=kubectl-9665'
Mar 12 07:54:41.554: INFO: stderr: ""
Mar 12 07:54:41.554: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:54:41.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9665" for this suite.
Mar 12 07:54:47.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:54:47.676: INFO: namespace kubectl-9665 deletion completed in 6.116936437s

• [SLOW TEST:17.152 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:54:47.677: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 07:54:47.720: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar 12 07:54:49.765: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:54:50.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2808" for this suite.
Mar 12 07:54:56.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:54:56.881: INFO: namespace replication-controller-2808 deletion completed in 6.103364371s

• [SLOW TEST:9.205 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:54:56.882: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 12 07:54:56.940: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9ba00ed8-aa26-435b-83a5-5502b858f785" in namespace "downward-api-4356" to be "success or failure"
Mar 12 07:54:56.943: INFO: Pod "downwardapi-volume-9ba00ed8-aa26-435b-83a5-5502b858f785": Phase="Pending", Reason="", readiness=false. Elapsed: 2.784563ms
Mar 12 07:54:58.948: INFO: Pod "downwardapi-volume-9ba00ed8-aa26-435b-83a5-5502b858f785": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008107905s
Mar 12 07:55:00.954: INFO: Pod "downwardapi-volume-9ba00ed8-aa26-435b-83a5-5502b858f785": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013640114s
Mar 12 07:55:02.959: INFO: Pod "downwardapi-volume-9ba00ed8-aa26-435b-83a5-5502b858f785": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018807173s
Mar 12 07:55:04.964: INFO: Pod "downwardapi-volume-9ba00ed8-aa26-435b-83a5-5502b858f785": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024102481s
Mar 12 07:55:06.969: INFO: Pod "downwardapi-volume-9ba00ed8-aa26-435b-83a5-5502b858f785": Phase="Pending", Reason="", readiness=false. Elapsed: 10.029030666s
Mar 12 07:55:08.975: INFO: Pod "downwardapi-volume-9ba00ed8-aa26-435b-83a5-5502b858f785": Phase="Pending", Reason="", readiness=false. Elapsed: 12.034990613s
Mar 12 07:55:10.980: INFO: Pod "downwardapi-volume-9ba00ed8-aa26-435b-83a5-5502b858f785": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.039904792s
STEP: Saw pod success
Mar 12 07:55:10.980: INFO: Pod "downwardapi-volume-9ba00ed8-aa26-435b-83a5-5502b858f785" satisfied condition "success or failure"
Mar 12 07:55:10.983: INFO: Trying to get logs from node node-2 pod downwardapi-volume-9ba00ed8-aa26-435b-83a5-5502b858f785 container client-container: <nil>
STEP: delete the pod
Mar 12 07:55:11.011: INFO: Waiting for pod downwardapi-volume-9ba00ed8-aa26-435b-83a5-5502b858f785 to disappear
Mar 12 07:55:11.014: INFO: Pod downwardapi-volume-9ba00ed8-aa26-435b-83a5-5502b858f785 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:55:11.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4356" for this suite.
Mar 12 07:55:17.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:55:17.128: INFO: namespace downward-api-4356 deletion completed in 6.109471499s

• [SLOW TEST:20.246 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:55:17.128: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-390edc9d-b522-4570-b9c4-cb3320526cf9 in namespace container-probe-1389
Mar 12 07:55:31.194: INFO: Started pod busybox-390edc9d-b522-4570-b9c4-cb3320526cf9 in namespace container-probe-1389
STEP: checking the pod's current state and verifying that restartCount is present
Mar 12 07:55:31.196: INFO: Initial restart count of pod busybox-390edc9d-b522-4570-b9c4-cb3320526cf9 is 0
Mar 12 07:56:17.313: INFO: Restart count of pod container-probe-1389/busybox-390edc9d-b522-4570-b9c4-cb3320526cf9 is now 1 (46.117260811s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:56:17.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1389" for this suite.
Mar 12 07:56:23.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:56:23.436: INFO: namespace container-probe-1389 deletion completed in 6.104672683s

• [SLOW TEST:66.309 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:56:23.437: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-ddsl
STEP: Creating a pod to test atomic-volume-subpath
Mar 12 07:56:23.516: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ddsl" in namespace "subpath-2810" to be "success or failure"
Mar 12 07:56:23.521: INFO: Pod "pod-subpath-test-configmap-ddsl": Phase="Pending", Reason="", readiness=false. Elapsed: 5.080662ms
Mar 12 07:56:25.526: INFO: Pod "pod-subpath-test-configmap-ddsl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010239364s
Mar 12 07:56:27.531: INFO: Pod "pod-subpath-test-configmap-ddsl": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015357115s
Mar 12 07:56:29.536: INFO: Pod "pod-subpath-test-configmap-ddsl": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019990332s
Mar 12 07:56:31.540: INFO: Pod "pod-subpath-test-configmap-ddsl": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024780187s
Mar 12 07:56:33.545: INFO: Pod "pod-subpath-test-configmap-ddsl": Phase="Pending", Reason="", readiness=false. Elapsed: 10.029597522s
Mar 12 07:56:35.550: INFO: Pod "pod-subpath-test-configmap-ddsl": Phase="Pending", Reason="", readiness=false. Elapsed: 12.034512556s
Mar 12 07:56:37.555: INFO: Pod "pod-subpath-test-configmap-ddsl": Phase="Running", Reason="", readiness=true. Elapsed: 14.039186357s
Mar 12 07:56:39.560: INFO: Pod "pod-subpath-test-configmap-ddsl": Phase="Running", Reason="", readiness=true. Elapsed: 16.04412374s
Mar 12 07:56:41.564: INFO: Pod "pod-subpath-test-configmap-ddsl": Phase="Running", Reason="", readiness=true. Elapsed: 18.048381253s
Mar 12 07:56:43.570: INFO: Pod "pod-subpath-test-configmap-ddsl": Phase="Running", Reason="", readiness=true. Elapsed: 20.05458889s
Mar 12 07:56:45.575: INFO: Pod "pod-subpath-test-configmap-ddsl": Phase="Running", Reason="", readiness=true. Elapsed: 22.059269905s
Mar 12 07:56:47.580: INFO: Pod "pod-subpath-test-configmap-ddsl": Phase="Running", Reason="", readiness=true. Elapsed: 24.063967833s
Mar 12 07:56:49.585: INFO: Pod "pod-subpath-test-configmap-ddsl": Phase="Running", Reason="", readiness=true. Elapsed: 26.069128193s
Mar 12 07:56:51.591: INFO: Pod "pod-subpath-test-configmap-ddsl": Phase="Running", Reason="", readiness=true. Elapsed: 28.075342539s
Mar 12 07:56:53.596: INFO: Pod "pod-subpath-test-configmap-ddsl": Phase="Running", Reason="", readiness=true. Elapsed: 30.079998472s
Mar 12 07:56:55.606: INFO: Pod "pod-subpath-test-configmap-ddsl": Phase="Running", Reason="", readiness=true. Elapsed: 32.090869525s
Mar 12 07:56:57.611: INFO: Pod "pod-subpath-test-configmap-ddsl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.095697249s
STEP: Saw pod success
Mar 12 07:56:57.611: INFO: Pod "pod-subpath-test-configmap-ddsl" satisfied condition "success or failure"
Mar 12 07:56:57.615: INFO: Trying to get logs from node node-2 pod pod-subpath-test-configmap-ddsl container test-container-subpath-configmap-ddsl: <nil>
STEP: delete the pod
Mar 12 07:56:57.647: INFO: Waiting for pod pod-subpath-test-configmap-ddsl to disappear
Mar 12 07:56:57.651: INFO: Pod pod-subpath-test-configmap-ddsl no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ddsl
Mar 12 07:56:57.651: INFO: Deleting pod "pod-subpath-test-configmap-ddsl" in namespace "subpath-2810"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:56:57.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2810" for this suite.
Mar 12 07:57:03.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:57:03.757: INFO: namespace subpath-2810 deletion completed in 6.098535759s

• [SLOW TEST:40.320 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:57:03.757: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 12 07:57:04.182: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 12 07:57:06.194: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:57:08.203: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:57:10.200: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:57:12.200: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:57:14.200: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:57:16.200: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596624, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 12 07:57:19.215: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:57:19.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-159" for this suite.
Mar 12 07:57:25.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:57:25.364: INFO: namespace webhook-159 deletion completed in 6.134754918s
STEP: Destroying namespace "webhook-159-markers" for this suite.
Mar 12 07:57:31.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:57:31.489: INFO: namespace webhook-159-markers deletion completed in 6.124960494s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.751 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:57:31.509: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-3649, will wait for the garbage collector to delete the pods
Mar 12 07:57:45.648: INFO: Deleting Job.batch foo took: 10.536543ms
Mar 12 07:57:45.748: INFO: Terminating Job.batch foo pods took: 100.296459ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:58:18.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3649" for this suite.
Mar 12 07:58:24.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:58:24.375: INFO: namespace job-3649 deletion completed in 6.115689607s

• [SLOW TEST:52.866 seconds]
[sig-apps] Job
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:58:24.376: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 07:58:24.472: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"c2b66295-22ae-4937-9738-4b3eb371552a", Controller:(*bool)(0xc004636fca), BlockOwnerDeletion:(*bool)(0xc004636fcb)}}
Mar 12 07:58:24.493: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"c1e66577-1112-42e4-8108-da123616e015", Controller:(*bool)(0xc004306bea), BlockOwnerDeletion:(*bool)(0xc004306beb)}}
Mar 12 07:58:24.508: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"b7da3b22-f454-4b9e-9ad9-213ed12237be", Controller:(*bool)(0xc004306d92), BlockOwnerDeletion:(*bool)(0xc004306d93)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:58:29.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4258" for this suite.
Mar 12 07:58:35.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:58:35.641: INFO: namespace gc-4258 deletion completed in 6.114018222s

• [SLOW TEST:11.266 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:58:35.642: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-4387
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 12 07:58:35.701: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 12 07:59:19.810: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.65.108 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4387 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 12 07:59:19.810: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 07:59:20.912: INFO: Found all expected endpoints: [netserver-0]
Mar 12 07:59:20.917: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.66.29 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4387 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 12 07:59:20.917: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 07:59:22.001: INFO: Found all expected endpoints: [netserver-1]
Mar 12 07:59:22.005: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.45 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4387 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 12 07:59:22.005: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 07:59:23.077: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:59:23.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4387" for this suite.
Mar 12 07:59:35.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 07:59:35.199: INFO: namespace pod-network-test-4387 deletion completed in 12.116407528s

• [SLOW TEST:59.558 seconds]
[sig-network] Networking
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 07:59:35.200: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 12 07:59:35.887: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 12 07:59:37.900: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:59:39.908: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:59:41.905: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:59:43.905: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:59:45.905: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 07:59:47.905: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719596775, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 12 07:59:50.920: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 07:59:50.924: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 07:59:56.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2250" for this suite.
Mar 12 08:00:02.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:00:02.762: INFO: namespace webhook-2250 deletion completed in 6.130756763s
STEP: Destroying namespace "webhook-2250-markers" for this suite.
Mar 12 08:00:08.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:00:08.861: INFO: namespace webhook-2250-markers deletion completed in 6.099621439s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:33.680 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:00:08.880: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Mar 12 08:00:08.928: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 12 08:01:08.951: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 08:01:08.957: INFO: Starting informer...
STEP: Starting pod...
Mar 12 08:01:09.177: INFO: Pod is running on node-2. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Mar 12 08:01:09.199: INFO: Pod wasn't evicted. Proceeding
Mar 12 08:01:09.199: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Mar 12 08:02:24.237: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:02:24.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-715" for this suite.
Mar 12 08:02:52.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:02:52.347: INFO: namespace taint-single-pod-715 deletion completed in 28.102962884s

• [SLOW TEST:163.467 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:02:52.348: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Mar 12 08:02:52.402: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 12 08:02:52.417: INFO: Waiting for terminating namespaces to be deleted...
Mar 12 08:02:52.420: INFO: 
Logging pods the kubelet thinks is on node node-1 before test
Mar 12 08:02:52.439: INFO: kube-proxy-node-1 from kube-system started at 2020-03-11 11:52:32 +0000 UTC (1 container statuses recorded)
Mar 12 08:02:52.439: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 12 08:02:52.439: INFO: kube-scheduler-node-1 from kube-system started at 2020-03-11 11:53:05 +0000 UTC (1 container statuses recorded)
Mar 12 08:02:52.439: INFO: 	Container kube-scheduler ready: true, restart count 3
Mar 12 08:02:52.439: INFO: sonobuoy-systemd-logs-daemon-set-b6e3a2e4f0de4f41-fktq2 from sonobuoy started at 2020-03-12 07:11:26 +0000 UTC (2 container statuses recorded)
Mar 12 08:02:52.439: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 12 08:02:52.439: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 12 08:02:52.439: INFO: kube-apiserver-node-1 from kube-system started at 2020-03-12 07:03:22 +0000 UTC (1 container statuses recorded)
Mar 12 08:02:52.439: INFO: 	Container kube-apiserver ready: true, restart count 1
Mar 12 08:02:52.439: INFO: kube-controller-manager-node-1 from kube-system started at 2020-03-11 13:01:20 +0000 UTC (1 container statuses recorded)
Mar 12 08:02:52.439: INFO: 	Container kube-controller-manager ready: true, restart count 4
Mar 12 08:02:52.439: INFO: coredns-p4prs from kube-system started at 2020-03-11 16:39:41 +0000 UTC (1 container statuses recorded)
Mar 12 08:02:52.439: INFO: 	Container coredns ready: true, restart count 0
Mar 12 08:02:52.439: INFO: sonobuoy from sonobuoy started at 2020-03-12 07:11:13 +0000 UTC (1 container statuses recorded)
Mar 12 08:02:52.439: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 12 08:02:52.439: INFO: k8s-keystone-auth-7n8pn from kube-system started at 2020-03-11 16:39:41 +0000 UTC (1 container statuses recorded)
Mar 12 08:02:52.439: INFO: 	Container k8s-keystone-auth ready: true, restart count 0
Mar 12 08:02:52.439: INFO: sonobuoy-e2e-job-0ad0ccbfe03640eb from sonobuoy started at 2020-03-12 07:11:26 +0000 UTC (2 container statuses recorded)
Mar 12 08:02:52.439: INFO: 	Container e2e ready: true, restart count 0
Mar 12 08:02:52.439: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 12 08:02:52.439: INFO: kube-flannel-hgdc4 from kube-system started at 2020-03-11 16:40:11 +0000 UTC (2 container statuses recorded)
Mar 12 08:02:52.439: INFO: 	Container install-cni ready: true, restart count 0
Mar 12 08:02:52.439: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 12 08:02:52.439: INFO: 
Logging pods the kubelet thinks is on node node-2 before test
Mar 12 08:02:52.454: INFO: kube-scheduler-node-2 from kube-system started at 2020-03-11 11:53:05 +0000 UTC (1 container statuses recorded)
Mar 12 08:02:52.454: INFO: 	Container kube-scheduler ready: true, restart count 2
Mar 12 08:02:52.454: INFO: kube-flannel-rh9nn from kube-system started at 2020-03-12 08:01:51 +0000 UTC (2 container statuses recorded)
Mar 12 08:02:52.454: INFO: 	Container install-cni ready: true, restart count 0
Mar 12 08:02:52.454: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 12 08:02:52.454: INFO: kube-apiserver-node-2 from kube-system started at 2020-03-11 11:52:50 +0000 UTC (1 container statuses recorded)
Mar 12 08:02:52.454: INFO: 	Container kube-apiserver ready: true, restart count 0
Mar 12 08:02:52.454: INFO: kube-proxy-node-2 from kube-system started at 2020-03-11 11:52:32 +0000 UTC (1 container statuses recorded)
Mar 12 08:02:52.454: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 12 08:02:52.454: INFO: coredns-w5zbd from kube-system started at 2020-03-12 08:01:21 +0000 UTC (1 container statuses recorded)
Mar 12 08:02:52.454: INFO: 	Container coredns ready: true, restart count 0
Mar 12 08:02:52.454: INFO: k8s-keystone-auth-n7bw6 from kube-system started at 2020-03-12 08:01:11 +0000 UTC (1 container statuses recorded)
Mar 12 08:02:52.454: INFO: 	Container k8s-keystone-auth ready: true, restart count 0
Mar 12 08:02:52.454: INFO: sonobuoy-systemd-logs-daemon-set-b6e3a2e4f0de4f41-2gk9x from sonobuoy started at 2020-03-12 07:11:26 +0000 UTC (2 container statuses recorded)
Mar 12 08:02:52.454: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 12 08:02:52.454: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 12 08:02:52.454: INFO: kube-controller-manager-node-2 from kube-system started at 2020-03-11 13:01:20 +0000 UTC (1 container statuses recorded)
Mar 12 08:02:52.454: INFO: 	Container kube-controller-manager ready: true, restart count 2
Mar 12 08:02:52.454: INFO: 
Logging pods the kubelet thinks is on node node-3 before test
Mar 12 08:02:52.471: INFO: kube-apiserver-node-3 from kube-system started at 2020-03-11 11:52:50 +0000 UTC (1 container statuses recorded)
Mar 12 08:02:52.471: INFO: 	Container kube-apiserver ready: true, restart count 0
Mar 12 08:02:52.471: INFO: tiller-deploy-7db6d94b4f-nqfpr from kube-system started at 2020-03-11 13:03:14 +0000 UTC (1 container statuses recorded)
Mar 12 08:02:52.471: INFO: 	Container tiller ready: true, restart count 0
Mar 12 08:02:52.471: INFO: k8s-keystone-auth-swhr6 from kube-system started at 2020-03-11 13:42:59 +0000 UTC (1 container statuses recorded)
Mar 12 08:02:52.471: INFO: 	Container k8s-keystone-auth ready: true, restart count 0
Mar 12 08:02:52.471: INFO: kube-scheduler-node-3 from kube-system started at 2020-03-11 11:53:06 +0000 UTC (1 container statuses recorded)
Mar 12 08:02:52.471: INFO: 	Container kube-scheduler ready: true, restart count 4
Mar 12 08:02:52.471: INFO: kube-controller-manager-node-3 from kube-system started at 2020-03-11 13:01:20 +0000 UTC (1 container statuses recorded)
Mar 12 08:02:52.471: INFO: 	Container kube-controller-manager ready: true, restart count 3
Mar 12 08:02:52.471: INFO: coredns-v7lrl from kube-system started at 2020-03-11 13:04:04 +0000 UTC (1 container statuses recorded)
Mar 12 08:02:52.471: INFO: 	Container coredns ready: true, restart count 0
Mar 12 08:02:52.471: INFO: kube-proxy-node-3 from kube-system started at 2020-03-11 11:52:32 +0000 UTC (1 container statuses recorded)
Mar 12 08:02:52.471: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 12 08:02:52.471: INFO: kube-monitor-1584000000-ggvz4 from kube-system started at 2020-03-12 08:00:05 +0000 UTC (1 container statuses recorded)
Mar 12 08:02:52.471: INFO: 	Container kube-monitor ready: false, restart count 0
Mar 12 08:02:52.471: INFO: sonobuoy-systemd-logs-daemon-set-b6e3a2e4f0de4f41-q7m9f from sonobuoy started at 2020-03-12 07:11:26 +0000 UTC (2 container statuses recorded)
Mar 12 08:02:52.471: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 12 08:02:52.471: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 12 08:02:52.471: INFO: kube-flannel-h7vhj from kube-system started at 2020-03-11 13:02:04 +0000 UTC (2 container statuses recorded)
Mar 12 08:02:52.471: INFO: 	Container install-cni ready: true, restart count 0
Mar 12 08:02:52.472: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 12 08:02:52.472: INFO: heapster-659dbd5d99-m5wz6 from kube-system started at 2020-03-12 08:01:09 +0000 UTC (1 container statuses recorded)
Mar 12 08:02:52.472: INFO: 	Container heapster ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node node-1
STEP: verifying the node has the label node node-2
STEP: verifying the node has the label node node-3
Mar 12 08:02:52.533: INFO: Pod coredns-p4prs requesting resource cpu=100m on Node node-1
Mar 12 08:02:52.533: INFO: Pod coredns-v7lrl requesting resource cpu=100m on Node node-3
Mar 12 08:02:52.533: INFO: Pod coredns-w5zbd requesting resource cpu=100m on Node node-2
Mar 12 08:02:52.533: INFO: Pod heapster-659dbd5d99-m5wz6 requesting resource cpu=0m on Node node-3
Mar 12 08:02:52.533: INFO: Pod k8s-keystone-auth-7n8pn requesting resource cpu=200m on Node node-1
Mar 12 08:02:52.533: INFO: Pod k8s-keystone-auth-n7bw6 requesting resource cpu=200m on Node node-2
Mar 12 08:02:52.533: INFO: Pod k8s-keystone-auth-swhr6 requesting resource cpu=200m on Node node-3
Mar 12 08:02:52.533: INFO: Pod kube-apiserver-node-1 requesting resource cpu=100m on Node node-1
Mar 12 08:02:52.533: INFO: Pod kube-apiserver-node-2 requesting resource cpu=100m on Node node-2
Mar 12 08:02:52.533: INFO: Pod kube-apiserver-node-3 requesting resource cpu=100m on Node node-3
Mar 12 08:02:52.533: INFO: Pod kube-controller-manager-node-1 requesting resource cpu=100m on Node node-1
Mar 12 08:02:52.533: INFO: Pod kube-controller-manager-node-2 requesting resource cpu=100m on Node node-2
Mar 12 08:02:52.533: INFO: Pod kube-controller-manager-node-3 requesting resource cpu=100m on Node node-3
Mar 12 08:02:52.533: INFO: Pod kube-flannel-h7vhj requesting resource cpu=150m on Node node-3
Mar 12 08:02:52.533: INFO: Pod kube-flannel-hgdc4 requesting resource cpu=150m on Node node-1
Mar 12 08:02:52.533: INFO: Pod kube-flannel-rh9nn requesting resource cpu=150m on Node node-2
Mar 12 08:02:52.533: INFO: Pod kube-proxy-node-1 requesting resource cpu=150m on Node node-1
Mar 12 08:02:52.533: INFO: Pod kube-proxy-node-2 requesting resource cpu=150m on Node node-2
Mar 12 08:02:52.533: INFO: Pod kube-proxy-node-3 requesting resource cpu=150m on Node node-3
Mar 12 08:02:52.533: INFO: Pod kube-scheduler-node-1 requesting resource cpu=80m on Node node-1
Mar 12 08:02:52.533: INFO: Pod kube-scheduler-node-2 requesting resource cpu=80m on Node node-2
Mar 12 08:02:52.533: INFO: Pod kube-scheduler-node-3 requesting resource cpu=80m on Node node-3
Mar 12 08:02:52.533: INFO: Pod tiller-deploy-7db6d94b4f-nqfpr requesting resource cpu=0m on Node node-3
Mar 12 08:02:52.533: INFO: Pod sonobuoy requesting resource cpu=0m on Node node-1
Mar 12 08:02:52.533: INFO: Pod sonobuoy-e2e-job-0ad0ccbfe03640eb requesting resource cpu=0m on Node node-1
Mar 12 08:02:52.533: INFO: Pod sonobuoy-systemd-logs-daemon-set-b6e3a2e4f0de4f41-2gk9x requesting resource cpu=0m on Node node-2
Mar 12 08:02:52.533: INFO: Pod sonobuoy-systemd-logs-daemon-set-b6e3a2e4f0de4f41-fktq2 requesting resource cpu=0m on Node node-1
Mar 12 08:02:52.533: INFO: Pod sonobuoy-systemd-logs-daemon-set-b6e3a2e4f0de4f41-q7m9f requesting resource cpu=0m on Node node-3
STEP: Starting Pods to consume most of the cluster CPU.
Mar 12 08:02:52.533: INFO: Creating a pod which consumes cpu=9867m on Node node-1
Mar 12 08:02:52.548: INFO: Creating a pod which consumes cpu=9867m on Node node-2
Mar 12 08:02:52.553: INFO: Creating a pod which consumes cpu=9867m on Node node-3
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-42e2f2e1-2201-4bf6-8b8f-e5e5792e86fb.15fb7fc3b9d3647d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3333/filler-pod-42e2f2e1-2201-4bf6-8b8f-e5e5792e86fb to node-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-42e2f2e1-2201-4bf6-8b8f-e5e5792e86fb.15fb7fc65f696e38], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-42e2f2e1-2201-4bf6-8b8f-e5e5792e86fb.15fb7fc6cf39fc0b], Reason = [Created], Message = [Created container filler-pod-42e2f2e1-2201-4bf6-8b8f-e5e5792e86fb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-42e2f2e1-2201-4bf6-8b8f-e5e5792e86fb.15fb7fc6ea9796c0], Reason = [Started], Message = [Started container filler-pod-42e2f2e1-2201-4bf6-8b8f-e5e5792e86fb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-62815709-2958-40ca-93aa-9c6317b7ea4c.15fb7fc3badadad1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3333/filler-pod-62815709-2958-40ca-93aa-9c6317b7ea4c to node-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-62815709-2958-40ca-93aa-9c6317b7ea4c.15fb7fc65e21ee3f], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-62815709-2958-40ca-93aa-9c6317b7ea4c.15fb7fc6dacb8cd6], Reason = [Created], Message = [Created container filler-pod-62815709-2958-40ca-93aa-9c6317b7ea4c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-62815709-2958-40ca-93aa-9c6317b7ea4c.15fb7fc6eb9ef112], Reason = [Started], Message = [Started container filler-pod-62815709-2958-40ca-93aa-9c6317b7ea4c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9cb54872-ed50-4324-adba-97f0412db116.15fb7fc3baf3a062], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3333/filler-pod-9cb54872-ed50-4324-adba-97f0412db116 to node-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9cb54872-ed50-4324-adba-97f0412db116.15fb7fc65c0137f5], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9cb54872-ed50-4324-adba-97f0412db116.15fb7fc6d097bf07], Reason = [Created], Message = [Created container filler-pod-9cb54872-ed50-4324-adba-97f0412db116]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9cb54872-ed50-4324-adba-97f0412db116.15fb7fc6e6f16ea4], Reason = [Started], Message = [Started container filler-pod-9cb54872-ed50-4324-adba-97f0412db116]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15fb7fc6fe00ae10], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node node-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node node-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node node-3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:03:07.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3333" for this suite.
Mar 12 08:03:13.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:03:13.792: INFO: namespace sched-pred-3333 deletion completed in 6.116995589s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:21.445 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:03:13.793: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:03:13.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2060" for this suite.
Mar 12 08:03:19.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:03:19.981: INFO: namespace kubelet-test-2060 deletion completed in 6.105977645s

• [SLOW TEST:6.189 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:03:19.981: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Mar 12 08:03:20.025: INFO: PodSpec: initContainers in spec.initContainers
Mar 12 08:04:17.863: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-1469e1fc-6b51-4748-a3aa-9a181dae4d04", GenerateName:"", Namespace:"init-container-5712", SelfLink:"/api/v1/namespaces/init-container-5712/pods/pod-init-1469e1fc-6b51-4748-a3aa-9a181dae4d04", UID:"1c07200d-5e09-4867-bd7e-6fb2fc809e37", ResourceVersion:"144416", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63719597000, loc:(*time.Location)(0x78896e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"25600371"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-b925x", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00574bd80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-b925x", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-b925x", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-b925x", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001afa9a8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"node-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002e405a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001afaa30)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001afaa50)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001afaa58), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001afaa5c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719597000, loc:(*time.Location)(0x78896e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719597000, loc:(*time.Location)(0x78896e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719597000, loc:(*time.Location)(0x78896e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719597000, loc:(*time.Location)(0x78896e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.20.5", PodIP:"10.233.65.113", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.65.113"}}, StartTime:(*v1.Time)(0xc003726e40), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc003726ec0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0003cdf10)}, Ready:false, RestartCount:3, Image:"docker.io/busybox:1.29", ImageID:"docker-pullable://docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://2f616465106ae97580fae70945848f3134ab68e12b268773a25487ded28f3b21", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003726f00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003726e80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc001afaadf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:04:17.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5712" for this suite.
Mar 12 08:04:45.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:04:45.981: INFO: namespace init-container-5712 deletion completed in 28.11141477s

• [SLOW TEST:86.000 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:04:45.981: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 12 08:05:14.092: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 12 08:05:14.096: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 12 08:05:16.096: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 12 08:05:16.102: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 12 08:05:18.096: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 12 08:05:18.101: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 12 08:05:20.096: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 12 08:05:20.102: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 12 08:05:22.096: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 12 08:05:22.101: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:05:22.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8388" for this suite.
Mar 12 08:05:34.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:05:34.239: INFO: namespace container-lifecycle-hook-8388 deletion completed in 12.109699942s

• [SLOW TEST:48.258 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:05:34.239: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-8174
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Mar 12 08:05:34.327: INFO: Found 0 stateful pods, waiting for 3
Mar 12 08:05:44.366: INFO: Found 1 stateful pods, waiting for 3
Mar 12 08:05:54.332: INFO: Found 2 stateful pods, waiting for 3
Mar 12 08:06:04.333: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 12 08:06:04.333: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 12 08:06:04.333: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar 12 08:06:14.333: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 12 08:06:14.333: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 12 08:06:14.333: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar 12 08:06:24.333: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 12 08:06:24.333: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 12 08:06:24.333: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar 12 08:06:24.367: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar 12 08:06:34.410: INFO: Updating stateful set ss2
Mar 12 08:06:34.421: INFO: Waiting for Pod statefulset-8174/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Mar 12 08:06:44.488: INFO: Found 2 stateful pods, waiting for 3
Mar 12 08:06:54.495: INFO: Found 2 stateful pods, waiting for 3
Mar 12 08:07:04.493: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 12 08:07:04.493: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 12 08:07:04.493: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar 12 08:07:14.494: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 12 08:07:14.494: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 12 08:07:14.494: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar 12 08:07:14.526: INFO: Updating stateful set ss2
Mar 12 08:07:14.538: INFO: Waiting for Pod statefulset-8174/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 12 08:07:24.571: INFO: Updating stateful set ss2
Mar 12 08:07:24.581: INFO: Waiting for StatefulSet statefulset-8174/ss2 to complete update
Mar 12 08:07:24.581: INFO: Waiting for Pod statefulset-8174/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 12 08:07:34.590: INFO: Waiting for StatefulSet statefulset-8174/ss2 to complete update
Mar 12 08:07:34.590: INFO: Waiting for Pod statefulset-8174/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 12 08:07:44.591: INFO: Waiting for StatefulSet statefulset-8174/ss2 to complete update
Mar 12 08:07:54.589: INFO: Waiting for StatefulSet statefulset-8174/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar 12 08:08:04.589: INFO: Deleting all statefulset in ns statefulset-8174
Mar 12 08:08:04.592: INFO: Scaling statefulset ss2 to 0
Mar 12 08:08:14.611: INFO: Waiting for statefulset status.replicas updated to 0
Mar 12 08:08:14.615: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:08:14.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8174" for this suite.
Mar 12 08:08:20.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:08:20.744: INFO: namespace statefulset-8174 deletion completed in 6.103736779s

• [SLOW TEST:166.505 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:08:20.744: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar 12 08:08:35.935: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:08:36.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2428" for this suite.
Mar 12 08:09:04.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:09:05.049: INFO: namespace replicaset-2428 deletion completed in 28.092158767s

• [SLOW TEST:44.304 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:09:05.049: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Mar 12 08:09:05.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 create -f - --namespace=kubectl-508'
Mar 12 08:09:05.700: INFO: stderr: ""
Mar 12 08:09:05.700: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 12 08:09:05.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-508'
Mar 12 08:09:05.780: INFO: stderr: ""
Mar 12 08:09:05.781: INFO: stdout: "update-demo-nautilus-8h8lw update-demo-nautilus-mlwct "
Mar 12 08:09:05.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-8h8lw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-508'
Mar 12 08:09:05.865: INFO: stderr: ""
Mar 12 08:09:05.865: INFO: stdout: ""
Mar 12 08:09:05.865: INFO: update-demo-nautilus-8h8lw is created but not running
Mar 12 08:09:10.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-508'
Mar 12 08:09:10.990: INFO: stderr: ""
Mar 12 08:09:10.990: INFO: stdout: "update-demo-nautilus-8h8lw update-demo-nautilus-mlwct "
Mar 12 08:09:10.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-8h8lw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-508'
Mar 12 08:09:11.084: INFO: stderr: ""
Mar 12 08:09:11.084: INFO: stdout: ""
Mar 12 08:09:11.084: INFO: update-demo-nautilus-8h8lw is created but not running
Mar 12 08:09:16.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-508'
Mar 12 08:09:16.189: INFO: stderr: ""
Mar 12 08:09:16.189: INFO: stdout: "update-demo-nautilus-8h8lw update-demo-nautilus-mlwct "
Mar 12 08:09:16.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-8h8lw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-508'
Mar 12 08:09:16.270: INFO: stderr: ""
Mar 12 08:09:16.270: INFO: stdout: ""
Mar 12 08:09:16.270: INFO: update-demo-nautilus-8h8lw is created but not running
Mar 12 08:09:21.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-508'
Mar 12 08:09:21.369: INFO: stderr: ""
Mar 12 08:09:21.369: INFO: stdout: "update-demo-nautilus-8h8lw update-demo-nautilus-mlwct "
Mar 12 08:09:21.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-8h8lw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-508'
Mar 12 08:09:21.464: INFO: stderr: ""
Mar 12 08:09:21.464: INFO: stdout: "true"
Mar 12 08:09:21.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-8h8lw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-508'
Mar 12 08:09:21.546: INFO: stderr: ""
Mar 12 08:09:21.546: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 12 08:09:21.546: INFO: validating pod update-demo-nautilus-8h8lw
Mar 12 08:09:21.554: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 12 08:09:21.554: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 12 08:09:21.554: INFO: update-demo-nautilus-8h8lw is verified up and running
Mar 12 08:09:21.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-mlwct -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-508'
Mar 12 08:09:21.637: INFO: stderr: ""
Mar 12 08:09:21.637: INFO: stdout: "true"
Mar 12 08:09:21.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-mlwct -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-508'
Mar 12 08:09:21.731: INFO: stderr: ""
Mar 12 08:09:21.731: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 12 08:09:21.731: INFO: validating pod update-demo-nautilus-mlwct
Mar 12 08:09:21.737: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 12 08:09:21.737: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 12 08:09:21.737: INFO: update-demo-nautilus-mlwct is verified up and running
STEP: rolling-update to new replication controller
Mar 12 08:09:21.738: INFO: scanned /root for discovery docs: <nil>
Mar 12 08:09:21.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-508'
Mar 12 08:10:04.449: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar 12 08:10:04.449: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 12 08:10:04.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-508'
Mar 12 08:10:04.559: INFO: stderr: ""
Mar 12 08:10:04.559: INFO: stdout: "update-demo-kitten-bhk28 update-demo-kitten-z8qw4 "
Mar 12 08:10:04.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-kitten-bhk28 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-508'
Mar 12 08:10:04.657: INFO: stderr: ""
Mar 12 08:10:04.657: INFO: stdout: "true"
Mar 12 08:10:04.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-kitten-bhk28 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-508'
Mar 12 08:10:04.751: INFO: stderr: ""
Mar 12 08:10:04.751: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar 12 08:10:04.751: INFO: validating pod update-demo-kitten-bhk28
Mar 12 08:10:04.757: INFO: got data: {
  "image": "kitten.jpg"
}

Mar 12 08:10:04.757: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar 12 08:10:04.757: INFO: update-demo-kitten-bhk28 is verified up and running
Mar 12 08:10:04.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-kitten-z8qw4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-508'
Mar 12 08:10:04.840: INFO: stderr: ""
Mar 12 08:10:04.840: INFO: stdout: "true"
Mar 12 08:10:04.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-kitten-z8qw4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-508'
Mar 12 08:10:04.926: INFO: stderr: ""
Mar 12 08:10:04.926: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar 12 08:10:04.926: INFO: validating pod update-demo-kitten-z8qw4
Mar 12 08:10:04.932: INFO: got data: {
  "image": "kitten.jpg"
}

Mar 12 08:10:04.932: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar 12 08:10:04.932: INFO: update-demo-kitten-z8qw4 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:10:04.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-508" for this suite.
Mar 12 08:10:32.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:10:33.040: INFO: namespace kubectl-508 deletion completed in 28.103796255s

• [SLOW TEST:87.991 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:10:33.040: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-9199/secret-test-530fc11d-1b88-4203-9000-5626392f96b7
STEP: Creating a pod to test consume secrets
Mar 12 08:10:33.363: INFO: Waiting up to 5m0s for pod "pod-configmaps-119c7b34-52d5-43b8-9266-7c41827a7deb" in namespace "secrets-9199" to be "success or failure"
Mar 12 08:10:33.366: INFO: Pod "pod-configmaps-119c7b34-52d5-43b8-9266-7c41827a7deb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.865157ms
Mar 12 08:10:35.370: INFO: Pod "pod-configmaps-119c7b34-52d5-43b8-9266-7c41827a7deb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007553337s
Mar 12 08:10:37.375: INFO: Pod "pod-configmaps-119c7b34-52d5-43b8-9266-7c41827a7deb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01259432s
Mar 12 08:10:39.379: INFO: Pod "pod-configmaps-119c7b34-52d5-43b8-9266-7c41827a7deb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016264921s
Mar 12 08:10:41.384: INFO: Pod "pod-configmaps-119c7b34-52d5-43b8-9266-7c41827a7deb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021202315s
Mar 12 08:10:43.389: INFO: Pod "pod-configmaps-119c7b34-52d5-43b8-9266-7c41827a7deb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.025872142s
Mar 12 08:10:45.394: INFO: Pod "pod-configmaps-119c7b34-52d5-43b8-9266-7c41827a7deb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.031391928s
Mar 12 08:10:47.399: INFO: Pod "pod-configmaps-119c7b34-52d5-43b8-9266-7c41827a7deb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.035841968s
STEP: Saw pod success
Mar 12 08:10:47.399: INFO: Pod "pod-configmaps-119c7b34-52d5-43b8-9266-7c41827a7deb" satisfied condition "success or failure"
Mar 12 08:10:47.401: INFO: Trying to get logs from node node-2 pod pod-configmaps-119c7b34-52d5-43b8-9266-7c41827a7deb container env-test: <nil>
STEP: delete the pod
Mar 12 08:10:47.443: INFO: Waiting for pod pod-configmaps-119c7b34-52d5-43b8-9266-7c41827a7deb to disappear
Mar 12 08:10:47.447: INFO: Pod pod-configmaps-119c7b34-52d5-43b8-9266-7c41827a7deb no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:10:47.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9199" for this suite.
Mar 12 08:10:53.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:10:53.557: INFO: namespace secrets-9199 deletion completed in 6.104785057s

• [SLOW TEST:20.517 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:10:53.557: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2082.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2082.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 12 08:11:13.660: INFO: Unable to read jessie_udp@kubernetes.default.svc.cluster.local from pod dns-2082/dns-test-91b6c202-6b3e-4998-abc1-27f66365990f: the server could not find the requested resource (get pods dns-test-91b6c202-6b3e-4998-abc1-27f66365990f)
Mar 12 08:11:13.664: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod dns-2082/dns-test-91b6c202-6b3e-4998-abc1-27f66365990f: the server could not find the requested resource (get pods dns-test-91b6c202-6b3e-4998-abc1-27f66365990f)
Mar 12 08:11:13.668: INFO: Unable to read jessie_udp@PodARecord from pod dns-2082/dns-test-91b6c202-6b3e-4998-abc1-27f66365990f: the server could not find the requested resource (get pods dns-test-91b6c202-6b3e-4998-abc1-27f66365990f)
Mar 12 08:11:13.672: INFO: Unable to read jessie_tcp@PodARecord from pod dns-2082/dns-test-91b6c202-6b3e-4998-abc1-27f66365990f: the server could not find the requested resource (get pods dns-test-91b6c202-6b3e-4998-abc1-27f66365990f)
Mar 12 08:11:13.672: INFO: Lookups using dns-2082/dns-test-91b6c202-6b3e-4998-abc1-27f66365990f failed for: [jessie_udp@kubernetes.default.svc.cluster.local jessie_tcp@kubernetes.default.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 12 08:11:18.707: INFO: DNS probes using dns-2082/dns-test-91b6c202-6b3e-4998-abc1-27f66365990f succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:11:18.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2082" for this suite.
Mar 12 08:11:24.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:11:24.861: INFO: namespace dns-2082 deletion completed in 6.117752652s

• [SLOW TEST:31.303 seconds]
[sig-network] DNS
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:11:24.861: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Mar 12 08:11:24.907: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:11:47.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3040" for this suite.
Mar 12 08:11:53.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:11:53.246: INFO: namespace crd-publish-openapi-3040 deletion completed in 6.107177405s

• [SLOW TEST:28.385 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:11:53.246: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-dcb987a3-81b5-463c-a28a-5f495d989049
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-dcb987a3-81b5-463c-a28a-5f495d989049
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:12:09.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5254" for this suite.
Mar 12 08:12:23.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:12:23.496: INFO: namespace configmap-5254 deletion completed in 14.103606828s

• [SLOW TEST:30.250 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:12:23.496: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 12 08:12:23.557: INFO: Waiting up to 5m0s for pod "pod-7322eb56-b8b8-4a38-b917-fc489b211477" in namespace "emptydir-3151" to be "success or failure"
Mar 12 08:12:23.560: INFO: Pod "pod-7322eb56-b8b8-4a38-b917-fc489b211477": Phase="Pending", Reason="", readiness=false. Elapsed: 2.629355ms
Mar 12 08:12:25.565: INFO: Pod "pod-7322eb56-b8b8-4a38-b917-fc489b211477": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007514731s
Mar 12 08:12:27.570: INFO: Pod "pod-7322eb56-b8b8-4a38-b917-fc489b211477": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012929364s
Mar 12 08:12:29.575: INFO: Pod "pod-7322eb56-b8b8-4a38-b917-fc489b211477": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017335679s
Mar 12 08:12:31.579: INFO: Pod "pod-7322eb56-b8b8-4a38-b917-fc489b211477": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021911547s
Mar 12 08:12:33.584: INFO: Pod "pod-7322eb56-b8b8-4a38-b917-fc489b211477": Phase="Pending", Reason="", readiness=false. Elapsed: 10.026385075s
Mar 12 08:12:35.588: INFO: Pod "pod-7322eb56-b8b8-4a38-b917-fc489b211477": Phase="Pending", Reason="", readiness=false. Elapsed: 12.031066851s
Mar 12 08:12:37.594: INFO: Pod "pod-7322eb56-b8b8-4a38-b917-fc489b211477": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.036237367s
STEP: Saw pod success
Mar 12 08:12:37.594: INFO: Pod "pod-7322eb56-b8b8-4a38-b917-fc489b211477" satisfied condition "success or failure"
Mar 12 08:12:37.597: INFO: Trying to get logs from node node-3 pod pod-7322eb56-b8b8-4a38-b917-fc489b211477 container test-container: <nil>
STEP: delete the pod
Mar 12 08:12:37.633: INFO: Waiting for pod pod-7322eb56-b8b8-4a38-b917-fc489b211477 to disappear
Mar 12 08:12:37.637: INFO: Pod pod-7322eb56-b8b8-4a38-b917-fc489b211477 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:12:37.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3151" for this suite.
Mar 12 08:12:43.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:12:43.746: INFO: namespace emptydir-3151 deletion completed in 6.10407372s

• [SLOW TEST:20.250 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:12:43.747: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 12 08:12:43.808: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6ccbd567-5fa5-4b94-b385-87d3c6054c9d" in namespace "projected-4979" to be "success or failure"
Mar 12 08:12:43.811: INFO: Pod "downwardapi-volume-6ccbd567-5fa5-4b94-b385-87d3c6054c9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.911228ms
Mar 12 08:12:45.816: INFO: Pod "downwardapi-volume-6ccbd567-5fa5-4b94-b385-87d3c6054c9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008045088s
Mar 12 08:12:47.822: INFO: Pod "downwardapi-volume-6ccbd567-5fa5-4b94-b385-87d3c6054c9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013157344s
Mar 12 08:12:49.826: INFO: Pod "downwardapi-volume-6ccbd567-5fa5-4b94-b385-87d3c6054c9d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017800415s
Mar 12 08:12:51.832: INFO: Pod "downwardapi-volume-6ccbd567-5fa5-4b94-b385-87d3c6054c9d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023499199s
Mar 12 08:12:53.836: INFO: Pod "downwardapi-volume-6ccbd567-5fa5-4b94-b385-87d3c6054c9d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02793569s
Mar 12 08:12:55.845: INFO: Pod "downwardapi-volume-6ccbd567-5fa5-4b94-b385-87d3c6054c9d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.037003004s
Mar 12 08:12:57.850: INFO: Pod "downwardapi-volume-6ccbd567-5fa5-4b94-b385-87d3c6054c9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.041977506s
STEP: Saw pod success
Mar 12 08:12:57.850: INFO: Pod "downwardapi-volume-6ccbd567-5fa5-4b94-b385-87d3c6054c9d" satisfied condition "success or failure"
Mar 12 08:12:57.854: INFO: Trying to get logs from node node-3 pod downwardapi-volume-6ccbd567-5fa5-4b94-b385-87d3c6054c9d container client-container: <nil>
STEP: delete the pod
Mar 12 08:12:57.885: INFO: Waiting for pod downwardapi-volume-6ccbd567-5fa5-4b94-b385-87d3c6054c9d to disappear
Mar 12 08:12:57.888: INFO: Pod downwardapi-volume-6ccbd567-5fa5-4b94-b385-87d3c6054c9d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:12:57.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4979" for this suite.
Mar 12 08:13:03.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:13:03.987: INFO: namespace projected-4979 deletion completed in 6.095603881s

• [SLOW TEST:20.241 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:13:03.988: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Mar 12 08:13:04.036: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:13:20.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1857" for this suite.
Mar 12 08:13:26.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:13:26.493: INFO: namespace init-container-1857 deletion completed in 6.101940747s

• [SLOW TEST:22.505 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:13:26.493: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Mar 12 08:13:26.543: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 12 08:14:26.564: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 08:14:26.570: INFO: Starting informer...
STEP: Starting pods...
Mar 12 08:14:26.793: INFO: Pod1 is running on node-3. Tainting Node
Mar 12 08:14:41.021: INFO: Pod2 is running on node-3. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Mar 12 08:14:52.468: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar 12 08:15:11.529: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:15:11.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-2982" for this suite.
Mar 12 08:15:17.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:15:17.650: INFO: namespace taint-multiple-pods-2982 deletion completed in 6.093768468s

• [SLOW TEST:111.157 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:15:17.650: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-902
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-902
I0312 08:15:17.751607      24 runners.go:184] Created replication controller with name: externalname-service, namespace: services-902, replica count: 2
I0312 08:15:20.802179      24 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 08:15:23.802608      24 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 08:15:26.802926      24 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 08:15:29.803210      24 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 08:15:32.803531      24 runners.go:184] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 12 08:15:35.803: INFO: Creating new exec pod
I0312 08:15:35.803810      24 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 12 08:15:52.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=services-902 execpodvdz6m -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 12 08:15:53.042: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 12 08:15:53.042: INFO: stdout: ""
Mar 12 08:15:53.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=services-902 execpodvdz6m -- /bin/sh -x -c nc -zv -t -w 2 10.233.51.253 80'
Mar 12 08:15:53.238: INFO: stderr: "+ nc -zv -t -w 2 10.233.51.253 80\nConnection to 10.233.51.253 80 port [tcp/http] succeeded!\n"
Mar 12 08:15:53.238: INFO: stdout: ""
Mar 12 08:15:53.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=services-902 execpodvdz6m -- /bin/sh -x -c nc -zv -t -w 2 192.168.20.3 30973'
Mar 12 08:15:53.424: INFO: stderr: "+ nc -zv -t -w 2 192.168.20.3 30973\nConnection to 192.168.20.3 30973 port [tcp/30973] succeeded!\n"
Mar 12 08:15:53.424: INFO: stdout: ""
Mar 12 08:15:53.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=services-902 execpodvdz6m -- /bin/sh -x -c nc -zv -t -w 2 192.168.20.4 30973'
Mar 12 08:15:53.614: INFO: stderr: "+ nc -zv -t -w 2 192.168.20.4 30973\nConnection to 192.168.20.4 30973 port [tcp/30973] succeeded!\n"
Mar 12 08:15:53.614: INFO: stdout: ""
Mar 12 08:15:53.614: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:15:53.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-902" for this suite.
Mar 12 08:15:59.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:15:59.765: INFO: namespace services-902 deletion completed in 6.117080258s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:42.115 seconds]
[sig-network] Services
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:15:59.766: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:16:10.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4530" for this suite.
Mar 12 08:16:16.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:16:17.018: INFO: namespace resourcequota-4530 deletion completed in 6.119747911s

• [SLOW TEST:17.252 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:16:17.018: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-fa870ee8-0ed0-4433-a355-e43ed4f7d191
STEP: Creating a pod to test consume configMaps
Mar 12 08:16:17.095: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1eb90231-6313-4a10-ac37-29a2e63530a3" in namespace "projected-8867" to be "success or failure"
Mar 12 08:16:17.098: INFO: Pod "pod-projected-configmaps-1eb90231-6313-4a10-ac37-29a2e63530a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.989166ms
Mar 12 08:16:19.104: INFO: Pod "pod-projected-configmaps-1eb90231-6313-4a10-ac37-29a2e63530a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008182748s
Mar 12 08:16:21.108: INFO: Pod "pod-projected-configmaps-1eb90231-6313-4a10-ac37-29a2e63530a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012664082s
Mar 12 08:16:23.113: INFO: Pod "pod-projected-configmaps-1eb90231-6313-4a10-ac37-29a2e63530a3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018049553s
Mar 12 08:16:25.119: INFO: Pod "pod-projected-configmaps-1eb90231-6313-4a10-ac37-29a2e63530a3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023795715s
Mar 12 08:16:27.124: INFO: Pod "pod-projected-configmaps-1eb90231-6313-4a10-ac37-29a2e63530a3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02897108s
Mar 12 08:16:29.129: INFO: Pod "pod-projected-configmaps-1eb90231-6313-4a10-ac37-29a2e63530a3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.033480545s
Mar 12 08:16:31.133: INFO: Pod "pod-projected-configmaps-1eb90231-6313-4a10-ac37-29a2e63530a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.037432006s
STEP: Saw pod success
Mar 12 08:16:31.133: INFO: Pod "pod-projected-configmaps-1eb90231-6313-4a10-ac37-29a2e63530a3" satisfied condition "success or failure"
Mar 12 08:16:31.135: INFO: Trying to get logs from node node-2 pod pod-projected-configmaps-1eb90231-6313-4a10-ac37-29a2e63530a3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 12 08:16:31.170: INFO: Waiting for pod pod-projected-configmaps-1eb90231-6313-4a10-ac37-29a2e63530a3 to disappear
Mar 12 08:16:31.173: INFO: Pod pod-projected-configmaps-1eb90231-6313-4a10-ac37-29a2e63530a3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:16:31.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8867" for this suite.
Mar 12 08:16:37.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:16:37.278: INFO: namespace projected-8867 deletion completed in 6.101764535s

• [SLOW TEST:20.261 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:16:37.279: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 12 08:16:50.416: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:16:50.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8769" for this suite.
Mar 12 08:16:56.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:16:56.567: INFO: namespace container-runtime-8769 deletion completed in 6.119643504s

• [SLOW TEST:19.288 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:16:56.568: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-b992e8a2-9163-4fd7-a5ea-9f5ed9ef80e1
STEP: Creating configMap with name cm-test-opt-upd-62945de8-951a-4e67-beb7-f2b6f0968e3d
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b992e8a2-9163-4fd7-a5ea-9f5ed9ef80e1
STEP: Updating configmap cm-test-opt-upd-62945de8-951a-4e67-beb7-f2b6f0968e3d
STEP: Creating configMap with name cm-test-opt-create-b573cf36-a7cb-43b9-9931-a39310091e01
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:18:35.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5469" for this suite.
Mar 12 08:18:47.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:18:47.355: INFO: namespace configmap-5469 deletion completed in 12.099886739s

• [SLOW TEST:110.786 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:18:47.355: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Mar 12 08:18:47.412: INFO: Waiting up to 5m0s for pod "var-expansion-7c13a498-dc0b-43cc-a942-6b980d9ef420" in namespace "var-expansion-1408" to be "success or failure"
Mar 12 08:18:47.420: INFO: Pod "var-expansion-7c13a498-dc0b-43cc-a942-6b980d9ef420": Phase="Pending", Reason="", readiness=false. Elapsed: 7.918688ms
Mar 12 08:18:49.425: INFO: Pod "var-expansion-7c13a498-dc0b-43cc-a942-6b980d9ef420": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012624541s
Mar 12 08:18:51.430: INFO: Pod "var-expansion-7c13a498-dc0b-43cc-a942-6b980d9ef420": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018523143s
Mar 12 08:18:53.437: INFO: Pod "var-expansion-7c13a498-dc0b-43cc-a942-6b980d9ef420": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024849145s
Mar 12 08:18:55.441: INFO: Pod "var-expansion-7c13a498-dc0b-43cc-a942-6b980d9ef420": Phase="Pending", Reason="", readiness=false. Elapsed: 8.029293095s
Mar 12 08:18:57.446: INFO: Pod "var-expansion-7c13a498-dc0b-43cc-a942-6b980d9ef420": Phase="Pending", Reason="", readiness=false. Elapsed: 10.034348945s
Mar 12 08:18:59.451: INFO: Pod "var-expansion-7c13a498-dc0b-43cc-a942-6b980d9ef420": Phase="Pending", Reason="", readiness=false. Elapsed: 12.039262444s
Mar 12 08:19:01.458: INFO: Pod "var-expansion-7c13a498-dc0b-43cc-a942-6b980d9ef420": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.045678496s
STEP: Saw pod success
Mar 12 08:19:01.458: INFO: Pod "var-expansion-7c13a498-dc0b-43cc-a942-6b980d9ef420" satisfied condition "success or failure"
Mar 12 08:19:01.461: INFO: Trying to get logs from node node-3 pod var-expansion-7c13a498-dc0b-43cc-a942-6b980d9ef420 container dapi-container: <nil>
STEP: delete the pod
Mar 12 08:19:01.484: INFO: Waiting for pod var-expansion-7c13a498-dc0b-43cc-a942-6b980d9ef420 to disappear
Mar 12 08:19:01.487: INFO: Pod var-expansion-7c13a498-dc0b-43cc-a942-6b980d9ef420 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:19:01.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1408" for this suite.
Mar 12 08:19:07.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:19:07.604: INFO: namespace var-expansion-1408 deletion completed in 6.108818813s

• [SLOW TEST:20.249 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:19:07.605: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 12 08:19:07.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-356'
Mar 12 08:19:08.099: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 12 08:19:08.099: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Mar 12 08:19:12.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 delete deployment e2e-test-httpd-deployment --namespace=kubectl-356'
Mar 12 08:19:12.242: INFO: stderr: ""
Mar 12 08:19:12.242: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:19:12.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-356" for this suite.
Mar 12 08:19:18.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:19:18.349: INFO: namespace kubectl-356 deletion completed in 6.10042584s

• [SLOW TEST:10.744 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:19:18.349: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 12 08:19:18.454: INFO: Number of nodes with available pods: 0
Mar 12 08:19:18.454: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:19.466: INFO: Number of nodes with available pods: 0
Mar 12 08:19:19.466: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:20.464: INFO: Number of nodes with available pods: 0
Mar 12 08:19:20.464: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:21.465: INFO: Number of nodes with available pods: 0
Mar 12 08:19:21.465: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:22.467: INFO: Number of nodes with available pods: 0
Mar 12 08:19:22.467: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:23.466: INFO: Number of nodes with available pods: 0
Mar 12 08:19:23.466: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:24.467: INFO: Number of nodes with available pods: 0
Mar 12 08:19:24.467: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:25.465: INFO: Number of nodes with available pods: 0
Mar 12 08:19:25.465: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:26.466: INFO: Number of nodes with available pods: 0
Mar 12 08:19:26.466: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:27.466: INFO: Number of nodes with available pods: 0
Mar 12 08:19:27.466: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:28.465: INFO: Number of nodes with available pods: 0
Mar 12 08:19:28.465: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:29.464: INFO: Number of nodes with available pods: 0
Mar 12 08:19:29.464: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:30.465: INFO: Number of nodes with available pods: 0
Mar 12 08:19:30.465: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:31.472: INFO: Number of nodes with available pods: 2
Mar 12 08:19:31.472: INFO: Node node-3 is running more than one daemon pod
Mar 12 08:19:32.465: INFO: Number of nodes with available pods: 3
Mar 12 08:19:32.465: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar 12 08:19:32.486: INFO: Number of nodes with available pods: 2
Mar 12 08:19:32.486: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:33.497: INFO: Number of nodes with available pods: 2
Mar 12 08:19:33.497: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:34.496: INFO: Number of nodes with available pods: 2
Mar 12 08:19:34.496: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:35.497: INFO: Number of nodes with available pods: 2
Mar 12 08:19:35.498: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:36.499: INFO: Number of nodes with available pods: 2
Mar 12 08:19:36.499: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:37.497: INFO: Number of nodes with available pods: 2
Mar 12 08:19:37.497: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:38.496: INFO: Number of nodes with available pods: 2
Mar 12 08:19:38.496: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:39.496: INFO: Number of nodes with available pods: 2
Mar 12 08:19:39.496: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:40.500: INFO: Number of nodes with available pods: 2
Mar 12 08:19:40.500: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:41.497: INFO: Number of nodes with available pods: 2
Mar 12 08:19:41.497: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:42.496: INFO: Number of nodes with available pods: 2
Mar 12 08:19:42.496: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:43.495: INFO: Number of nodes with available pods: 2
Mar 12 08:19:43.495: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:44.496: INFO: Number of nodes with available pods: 2
Mar 12 08:19:44.496: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:45.499: INFO: Number of nodes with available pods: 2
Mar 12 08:19:45.499: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:46.496: INFO: Number of nodes with available pods: 2
Mar 12 08:19:46.496: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:47.498: INFO: Number of nodes with available pods: 2
Mar 12 08:19:47.498: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:48.497: INFO: Number of nodes with available pods: 2
Mar 12 08:19:48.497: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:49.497: INFO: Number of nodes with available pods: 2
Mar 12 08:19:49.497: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:50.496: INFO: Number of nodes with available pods: 2
Mar 12 08:19:50.496: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:51.497: INFO: Number of nodes with available pods: 2
Mar 12 08:19:51.497: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:52.497: INFO: Number of nodes with available pods: 2
Mar 12 08:19:52.497: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:53.497: INFO: Number of nodes with available pods: 2
Mar 12 08:19:53.497: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:19:54.497: INFO: Number of nodes with available pods: 3
Mar 12 08:19:54.497: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1165, will wait for the garbage collector to delete the pods
Mar 12 08:19:54.561: INFO: Deleting DaemonSet.extensions daemon-set took: 9.286204ms
Mar 12 08:19:54.662: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.290952ms
Mar 12 08:20:01.566: INFO: Number of nodes with available pods: 0
Mar 12 08:20:01.566: INFO: Number of running nodes: 0, number of available pods: 0
Mar 12 08:20:01.571: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1165/daemonsets","resourceVersion":"147248"},"items":null}

Mar 12 08:20:01.574: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1165/pods","resourceVersion":"147248"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:20:01.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1165" for this suite.
Mar 12 08:20:07.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:20:07.713: INFO: namespace daemonsets-1165 deletion completed in 6.123407629s

• [SLOW TEST:49.365 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:20:07.714: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Mar 12 08:20:07.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 create -f - --namespace=kubectl-8442'
Mar 12 08:20:08.075: INFO: stderr: ""
Mar 12 08:20:08.075: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 12 08:20:08.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8442'
Mar 12 08:20:08.156: INFO: stderr: ""
Mar 12 08:20:08.156: INFO: stdout: "update-demo-nautilus-gxrkc update-demo-nautilus-z98hc "
Mar 12 08:20:08.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-gxrkc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8442'
Mar 12 08:20:08.241: INFO: stderr: ""
Mar 12 08:20:08.241: INFO: stdout: ""
Mar 12 08:20:08.241: INFO: update-demo-nautilus-gxrkc is created but not running
Mar 12 08:20:13.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8442'
Mar 12 08:20:13.360: INFO: stderr: ""
Mar 12 08:20:13.360: INFO: stdout: "update-demo-nautilus-gxrkc update-demo-nautilus-z98hc "
Mar 12 08:20:13.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-gxrkc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8442'
Mar 12 08:20:13.453: INFO: stderr: ""
Mar 12 08:20:13.453: INFO: stdout: ""
Mar 12 08:20:13.453: INFO: update-demo-nautilus-gxrkc is created but not running
Mar 12 08:20:18.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8442'
Mar 12 08:20:18.554: INFO: stderr: ""
Mar 12 08:20:18.554: INFO: stdout: "update-demo-nautilus-gxrkc update-demo-nautilus-z98hc "
Mar 12 08:20:18.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-gxrkc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8442'
Mar 12 08:20:18.641: INFO: stderr: ""
Mar 12 08:20:18.641: INFO: stdout: ""
Mar 12 08:20:18.641: INFO: update-demo-nautilus-gxrkc is created but not running
Mar 12 08:20:23.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8442'
Mar 12 08:20:23.753: INFO: stderr: ""
Mar 12 08:20:23.753: INFO: stdout: "update-demo-nautilus-gxrkc update-demo-nautilus-z98hc "
Mar 12 08:20:23.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-gxrkc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8442'
Mar 12 08:20:23.844: INFO: stderr: ""
Mar 12 08:20:23.844: INFO: stdout: "true"
Mar 12 08:20:23.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-gxrkc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8442'
Mar 12 08:20:23.943: INFO: stderr: ""
Mar 12 08:20:23.943: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 12 08:20:23.943: INFO: validating pod update-demo-nautilus-gxrkc
Mar 12 08:20:23.951: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 12 08:20:23.951: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 12 08:20:23.951: INFO: update-demo-nautilus-gxrkc is verified up and running
Mar 12 08:20:23.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-z98hc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8442'
Mar 12 08:20:24.059: INFO: stderr: ""
Mar 12 08:20:24.059: INFO: stdout: "true"
Mar 12 08:20:24.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-z98hc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8442'
Mar 12 08:20:24.157: INFO: stderr: ""
Mar 12 08:20:24.157: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 12 08:20:24.157: INFO: validating pod update-demo-nautilus-z98hc
Mar 12 08:20:24.163: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 12 08:20:24.163: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 12 08:20:24.163: INFO: update-demo-nautilus-z98hc is verified up and running
STEP: scaling down the replication controller
Mar 12 08:20:24.165: INFO: scanned /root for discovery docs: <nil>
Mar 12 08:20:24.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-8442'
Mar 12 08:20:25.275: INFO: stderr: ""
Mar 12 08:20:25.275: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 12 08:20:25.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8442'
Mar 12 08:20:25.381: INFO: stderr: ""
Mar 12 08:20:25.382: INFO: stdout: "update-demo-nautilus-gxrkc update-demo-nautilus-z98hc "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 12 08:20:30.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8442'
Mar 12 08:20:30.490: INFO: stderr: ""
Mar 12 08:20:30.490: INFO: stdout: "update-demo-nautilus-z98hc "
Mar 12 08:20:30.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-z98hc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8442'
Mar 12 08:20:30.596: INFO: stderr: ""
Mar 12 08:20:30.596: INFO: stdout: "true"
Mar 12 08:20:30.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-z98hc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8442'
Mar 12 08:20:30.681: INFO: stderr: ""
Mar 12 08:20:30.681: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 12 08:20:30.681: INFO: validating pod update-demo-nautilus-z98hc
Mar 12 08:20:30.685: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 12 08:20:30.685: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 12 08:20:30.685: INFO: update-demo-nautilus-z98hc is verified up and running
STEP: scaling up the replication controller
Mar 12 08:20:30.686: INFO: scanned /root for discovery docs: <nil>
Mar 12 08:20:30.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-8442'
Mar 12 08:20:31.833: INFO: stderr: ""
Mar 12 08:20:31.833: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 12 08:20:31.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8442'
Mar 12 08:20:31.928: INFO: stderr: ""
Mar 12 08:20:31.928: INFO: stdout: "update-demo-nautilus-vr46h update-demo-nautilus-z98hc "
Mar 12 08:20:31.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-vr46h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8442'
Mar 12 08:20:32.031: INFO: stderr: ""
Mar 12 08:20:32.031: INFO: stdout: ""
Mar 12 08:20:32.031: INFO: update-demo-nautilus-vr46h is created but not running
Mar 12 08:20:37.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8442'
Mar 12 08:20:37.132: INFO: stderr: ""
Mar 12 08:20:37.132: INFO: stdout: "update-demo-nautilus-vr46h update-demo-nautilus-z98hc "
Mar 12 08:20:37.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-vr46h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8442'
Mar 12 08:20:37.235: INFO: stderr: ""
Mar 12 08:20:37.235: INFO: stdout: ""
Mar 12 08:20:37.235: INFO: update-demo-nautilus-vr46h is created but not running
Mar 12 08:20:42.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8442'
Mar 12 08:20:42.335: INFO: stderr: ""
Mar 12 08:20:42.335: INFO: stdout: "update-demo-nautilus-vr46h update-demo-nautilus-z98hc "
Mar 12 08:20:42.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-vr46h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8442'
Mar 12 08:20:42.423: INFO: stderr: ""
Mar 12 08:20:42.423: INFO: stdout: ""
Mar 12 08:20:42.423: INFO: update-demo-nautilus-vr46h is created but not running
Mar 12 08:20:47.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8442'
Mar 12 08:20:47.536: INFO: stderr: ""
Mar 12 08:20:47.536: INFO: stdout: "update-demo-nautilus-vr46h update-demo-nautilus-z98hc "
Mar 12 08:20:47.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-vr46h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8442'
Mar 12 08:20:47.634: INFO: stderr: ""
Mar 12 08:20:47.634: INFO: stdout: "true"
Mar 12 08:20:47.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-vr46h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8442'
Mar 12 08:20:47.725: INFO: stderr: ""
Mar 12 08:20:47.725: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 12 08:20:47.725: INFO: validating pod update-demo-nautilus-vr46h
Mar 12 08:20:47.731: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 12 08:20:47.731: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 12 08:20:47.731: INFO: update-demo-nautilus-vr46h is verified up and running
Mar 12 08:20:47.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-z98hc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8442'
Mar 12 08:20:47.818: INFO: stderr: ""
Mar 12 08:20:47.818: INFO: stdout: "true"
Mar 12 08:20:47.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-z98hc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8442'
Mar 12 08:20:47.899: INFO: stderr: ""
Mar 12 08:20:47.899: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 12 08:20:47.899: INFO: validating pod update-demo-nautilus-z98hc
Mar 12 08:20:47.904: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 12 08:20:47.904: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 12 08:20:47.904: INFO: update-demo-nautilus-z98hc is verified up and running
STEP: using delete to clean up resources
Mar 12 08:20:47.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 delete --grace-period=0 --force -f - --namespace=kubectl-8442'
Mar 12 08:20:48.013: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 12 08:20:48.013: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 12 08:20:48.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8442'
Mar 12 08:20:48.117: INFO: stderr: "No resources found in kubectl-8442 namespace.\n"
Mar 12 08:20:48.117: INFO: stdout: ""
Mar 12 08:20:48.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -l name=update-demo --namespace=kubectl-8442 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 12 08:20:48.205: INFO: stderr: ""
Mar 12 08:20:48.205: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:20:48.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8442" for this suite.
Mar 12 08:21:16.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:21:16.351: INFO: namespace kubectl-8442 deletion completed in 28.140641674s

• [SLOW TEST:68.638 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:21:16.352: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 08:21:16.410: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 12 08:21:16.425: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 12 08:21:21.430: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 12 08:21:29.438: INFO: Creating deployment "test-rolling-update-deployment"
Mar 12 08:21:29.445: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 12 08:21:29.452: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar 12 08:21:31.461: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 12 08:21:31.464: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:21:33.469: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:21:35.469: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:21:37.471: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:21:39.469: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:21:41.469: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598089, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:21:43.469: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar 12 08:21:43.479: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-310 /apis/apps/v1/namespaces/deployment-310/deployments/test-rolling-update-deployment 44fef879-9115-4d8a-a1a7-1b23c1193359 147597 1 2020-03-12 08:21:29 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001bddd08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-12 08:21:29 +0000 UTC,LastTransitionTime:2020-03-12 08:21:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-03-12 08:21:42 +0000 UTC,LastTransitionTime:2020-03-12 08:21:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 12 08:21:43.484: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-310 /apis/apps/v1/namespaces/deployment-310/replicasets/test-rolling-update-deployment-55d946486 bc4fe2bf-8636-44e5-bec6-a161eb3d539c 147586 1 2020-03-12 08:21:29 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 44fef879-9115-4d8a-a1a7-1b23c1193359 0xc0041d81f0 0xc0041d81f1}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0041d8258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 12 08:21:43.484: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 12 08:21:43.484: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-310 /apis/apps/v1/namespaces/deployment-310/replicasets/test-rolling-update-controller ad2c424f-a015-464f-bc51-dff3230baaad 147596 2 2020-03-12 08:21:16 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 44fef879-9115-4d8a-a1a7-1b23c1193359 0xc0041d8127 0xc0041d8128}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0041d8188 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 12 08:21:43.488: INFO: Pod "test-rolling-update-deployment-55d946486-5stmn" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-5stmn test-rolling-update-deployment-55d946486- deployment-310 /api/v1/namespaces/deployment-310/pods/test-rolling-update-deployment-55d946486-5stmn 7ce62c78-899a-43e2-8c92-a575434e4515 147585 0 2020-03-12 08:21:29 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 bc4fe2bf-8636-44e5-bec6-a161eb3d539c 0xc0041d86d0 0xc0041d86d1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9pp27,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9pp27,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9pp27,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 08:21:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 08:21:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 08:21:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 08:21:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.20.5,PodIP:10.233.65.135,StartTime:2020-03-12 08:21:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-12 08:21:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/redis:5.0.5-alpine,ImageID:docker-pullable://docker.io/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://0e314238785c5b723d6e331cf2ba4778e6d6569b81fedf1454189a0979455641,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.135,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:21:43.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-310" for this suite.
Mar 12 08:21:49.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:21:49.599: INFO: namespace deployment-310 deletion completed in 6.106312543s

• [SLOW TEST:33.248 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:21:49.600: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Mar 12 08:21:49.660: INFO: Waiting up to 5m0s for pod "client-containers-c2243212-a40e-484a-b83e-fa6c108df18b" in namespace "containers-4817" to be "success or failure"
Mar 12 08:21:49.663: INFO: Pod "client-containers-c2243212-a40e-484a-b83e-fa6c108df18b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.837108ms
Mar 12 08:21:51.667: INFO: Pod "client-containers-c2243212-a40e-484a-b83e-fa6c108df18b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007591547s
Mar 12 08:21:53.672: INFO: Pod "client-containers-c2243212-a40e-484a-b83e-fa6c108df18b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012514733s
Mar 12 08:21:55.677: INFO: Pod "client-containers-c2243212-a40e-484a-b83e-fa6c108df18b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017432088s
Mar 12 08:21:57.682: INFO: Pod "client-containers-c2243212-a40e-484a-b83e-fa6c108df18b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022229707s
Mar 12 08:21:59.686: INFO: Pod "client-containers-c2243212-a40e-484a-b83e-fa6c108df18b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.026202257s
Mar 12 08:22:01.690: INFO: Pod "client-containers-c2243212-a40e-484a-b83e-fa6c108df18b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.030383307s
Mar 12 08:22:03.695: INFO: Pod "client-containers-c2243212-a40e-484a-b83e-fa6c108df18b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.034986693s
STEP: Saw pod success
Mar 12 08:22:03.695: INFO: Pod "client-containers-c2243212-a40e-484a-b83e-fa6c108df18b" satisfied condition "success or failure"
Mar 12 08:22:03.698: INFO: Trying to get logs from node node-3 pod client-containers-c2243212-a40e-484a-b83e-fa6c108df18b container test-container: <nil>
STEP: delete the pod
Mar 12 08:22:03.735: INFO: Waiting for pod client-containers-c2243212-a40e-484a-b83e-fa6c108df18b to disappear
Mar 12 08:22:03.738: INFO: Pod client-containers-c2243212-a40e-484a-b83e-fa6c108df18b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:22:03.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4817" for this suite.
Mar 12 08:22:09.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:22:09.912: INFO: namespace containers-4817 deletion completed in 6.170425333s

• [SLOW TEST:20.313 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:22:09.913: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4738.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4738.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4738.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4738.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4738.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4738.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 12 08:22:26.037: INFO: DNS probes using dns-4738/dns-test-d52c60a7-57c5-47ef-a744-98c7e364b8ce succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:22:26.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4738" for this suite.
Mar 12 08:22:32.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:22:32.183: INFO: namespace dns-4738 deletion completed in 6.107516756s

• [SLOW TEST:22.270 seconds]
[sig-network] DNS
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:22:32.183: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-a29651f9-6500-4aa1-a0cd-0ed33c0f98a3
STEP: Creating a pod to test consume secrets
Mar 12 08:22:32.261: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-09e315a0-9693-4180-b79f-f284484647b0" in namespace "projected-1639" to be "success or failure"
Mar 12 08:22:32.264: INFO: Pod "pod-projected-secrets-09e315a0-9693-4180-b79f-f284484647b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.989113ms
Mar 12 08:22:34.269: INFO: Pod "pod-projected-secrets-09e315a0-9693-4180-b79f-f284484647b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008067643s
Mar 12 08:22:36.274: INFO: Pod "pod-projected-secrets-09e315a0-9693-4180-b79f-f284484647b0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013368275s
Mar 12 08:22:38.279: INFO: Pod "pod-projected-secrets-09e315a0-9693-4180-b79f-f284484647b0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018165094s
Mar 12 08:22:40.284: INFO: Pod "pod-projected-secrets-09e315a0-9693-4180-b79f-f284484647b0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022858686s
Mar 12 08:22:42.289: INFO: Pod "pod-projected-secrets-09e315a0-9693-4180-b79f-f284484647b0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.027947361s
Mar 12 08:22:44.294: INFO: Pod "pod-projected-secrets-09e315a0-9693-4180-b79f-f284484647b0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.033278105s
Mar 12 08:22:46.299: INFO: Pod "pod-projected-secrets-09e315a0-9693-4180-b79f-f284484647b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.037862229s
STEP: Saw pod success
Mar 12 08:22:46.299: INFO: Pod "pod-projected-secrets-09e315a0-9693-4180-b79f-f284484647b0" satisfied condition "success or failure"
Mar 12 08:22:46.302: INFO: Trying to get logs from node node-3 pod pod-projected-secrets-09e315a0-9693-4180-b79f-f284484647b0 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 12 08:22:46.338: INFO: Waiting for pod pod-projected-secrets-09e315a0-9693-4180-b79f-f284484647b0 to disappear
Mar 12 08:22:46.342: INFO: Pod pod-projected-secrets-09e315a0-9693-4180-b79f-f284484647b0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:22:46.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1639" for this suite.
Mar 12 08:22:52.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:22:52.449: INFO: namespace projected-1639 deletion completed in 6.100420626s

• [SLOW TEST:20.266 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:22:52.449: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-379.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-379.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-379.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-379.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-379.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-379.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 12 08:23:08.541: INFO: DNS probes using dns-379/dns-test-e8f2d946-0cf7-4a83-a073-ff3e94f6aef9 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:23:08.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-379" for this suite.
Mar 12 08:23:14.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:23:14.681: INFO: namespace dns-379 deletion completed in 6.119904786s

• [SLOW TEST:22.232 seconds]
[sig-network] DNS
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:23:14.682: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 12 08:23:14.782: INFO: Number of nodes with available pods: 0
Mar 12 08:23:14.782: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:23:15.792: INFO: Number of nodes with available pods: 0
Mar 12 08:23:15.792: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:23:16.792: INFO: Number of nodes with available pods: 0
Mar 12 08:23:16.792: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:23:17.794: INFO: Number of nodes with available pods: 0
Mar 12 08:23:17.794: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:23:18.792: INFO: Number of nodes with available pods: 0
Mar 12 08:23:18.792: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:23:19.791: INFO: Number of nodes with available pods: 0
Mar 12 08:23:19.791: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:23:20.791: INFO: Number of nodes with available pods: 0
Mar 12 08:23:20.792: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:23:21.794: INFO: Number of nodes with available pods: 0
Mar 12 08:23:21.794: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:23:22.791: INFO: Number of nodes with available pods: 0
Mar 12 08:23:22.791: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:23:23.791: INFO: Number of nodes with available pods: 0
Mar 12 08:23:23.791: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:23:24.793: INFO: Number of nodes with available pods: 0
Mar 12 08:23:24.793: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:23:25.792: INFO: Number of nodes with available pods: 0
Mar 12 08:23:25.792: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:23:26.802: INFO: Number of nodes with available pods: 0
Mar 12 08:23:26.802: INFO: Node node-1 is running more than one daemon pod
Mar 12 08:23:27.794: INFO: Number of nodes with available pods: 3
Mar 12 08:23:27.794: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar 12 08:23:27.837: INFO: Number of nodes with available pods: 3
Mar 12 08:23:27.837: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8088, will wait for the garbage collector to delete the pods
Mar 12 08:23:28.913: INFO: Deleting DaemonSet.extensions daemon-set took: 12.109878ms
Mar 12 08:23:29.013: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.272346ms
Mar 12 08:23:41.818: INFO: Number of nodes with available pods: 0
Mar 12 08:23:41.818: INFO: Number of running nodes: 0, number of available pods: 0
Mar 12 08:23:41.821: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8088/daemonsets","resourceVersion":"148027"},"items":null}

Mar 12 08:23:41.826: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8088/pods","resourceVersion":"148027"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:23:41.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8088" for this suite.
Mar 12 08:23:47.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:23:47.953: INFO: namespace daemonsets-8088 deletion completed in 6.108922671s

• [SLOW TEST:33.271 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:23:47.953: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 08:23:48.003: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:24:02.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2461" for this suite.
Mar 12 08:24:46.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:24:46.185: INFO: namespace pods-2461 deletion completed in 44.114399732s

• [SLOW TEST:58.232 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:24:46.185: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-ec19b549-3e9c-40cd-85af-be381ca4ac9a
Mar 12 08:24:46.252: INFO: Pod name my-hostname-basic-ec19b549-3e9c-40cd-85af-be381ca4ac9a: Found 0 pods out of 1
Mar 12 08:24:51.257: INFO: Pod name my-hostname-basic-ec19b549-3e9c-40cd-85af-be381ca4ac9a: Found 1 pods out of 1
Mar 12 08:24:51.257: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-ec19b549-3e9c-40cd-85af-be381ca4ac9a" are running
Mar 12 08:24:59.266: INFO: Pod "my-hostname-basic-ec19b549-3e9c-40cd-85af-be381ca4ac9a-lrd7z" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-12 08:24:46 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-12 08:24:46 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-ec19b549-3e9c-40cd-85af-be381ca4ac9a]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-12 08:24:46 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-ec19b549-3e9c-40cd-85af-be381ca4ac9a]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-12 08:24:46 +0000 UTC Reason: Message:}])
Mar 12 08:24:59.266: INFO: Trying to dial the pod
Mar 12 08:25:04.279: INFO: Controller my-hostname-basic-ec19b549-3e9c-40cd-85af-be381ca4ac9a: Got expected result from replica 1 [my-hostname-basic-ec19b549-3e9c-40cd-85af-be381ca4ac9a-lrd7z]: "my-hostname-basic-ec19b549-3e9c-40cd-85af-be381ca4ac9a-lrd7z", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:25:04.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-497" for this suite.
Mar 12 08:25:10.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:25:10.390: INFO: namespace replication-controller-497 deletion completed in 6.105247268s

• [SLOW TEST:24.205 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:25:10.391: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-jkprl in namespace proxy-2488
I0312 08:25:10.459468      24 runners.go:184] Created replication controller with name: proxy-service-jkprl, namespace: proxy-2488, replica count: 1
I0312 08:25:11.509999      24 runners.go:184] proxy-service-jkprl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 08:25:12.510297      24 runners.go:184] proxy-service-jkprl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 08:25:13.510542      24 runners.go:184] proxy-service-jkprl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 08:25:14.510803      24 runners.go:184] proxy-service-jkprl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 08:25:15.511097      24 runners.go:184] proxy-service-jkprl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 08:25:16.511413      24 runners.go:184] proxy-service-jkprl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 08:25:17.511751      24 runners.go:184] proxy-service-jkprl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 08:25:18.512169      24 runners.go:184] proxy-service-jkprl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 08:25:19.512442      24 runners.go:184] proxy-service-jkprl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 08:25:20.512700      24 runners.go:184] proxy-service-jkprl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 08:25:21.512999      24 runners.go:184] proxy-service-jkprl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 08:25:22.513209      24 runners.go:184] proxy-service-jkprl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 08:25:23.513439      24 runners.go:184] proxy-service-jkprl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0312 08:25:24.513726      24 runners.go:184] proxy-service-jkprl Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 12 08:25:24.518: INFO: setup took 14.087518157s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar 12 08:25:24.528: INFO: (0) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 9.070048ms)
Mar 12 08:25:24.528: INFO: (0) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/rewriteme">test</a> (200; 9.703356ms)
Mar 12 08:25:24.529: INFO: (0) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 10.463368ms)
Mar 12 08:25:24.529: INFO: (0) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 9.496533ms)
Mar 12 08:25:24.529: INFO: (0) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">test<... (200; 9.60719ms)
Mar 12 08:25:24.529: INFO: (0) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 10.016636ms)
Mar 12 08:25:24.529: INFO: (0) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">... (200; 9.496636ms)
Mar 12 08:25:24.529: INFO: (0) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname1/proxy/: foo (200; 9.686584ms)
Mar 12 08:25:24.534: INFO: (0) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname2/proxy/: bar (200; 13.946483ms)
Mar 12 08:25:24.536: INFO: (0) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname2/proxy/: bar (200; 16.638883ms)
Mar 12 08:25:24.536: INFO: (0) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname1/proxy/: foo (200; 17.037008ms)
Mar 12 08:25:24.538: INFO: (0) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/tlsrewritem... (200; 19.877842ms)
Mar 12 08:25:24.538: INFO: (0) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:462/proxy/: tls qux (200; 18.91078ms)
Mar 12 08:25:24.538: INFO: (0) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname2/proxy/: tls qux (200; 19.374736ms)
Mar 12 08:25:24.539: INFO: (0) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:460/proxy/: tls baz (200; 20.003984ms)
Mar 12 08:25:24.539: INFO: (0) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname1/proxy/: tls baz (200; 19.396916ms)
Mar 12 08:25:24.547: INFO: (1) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:460/proxy/: tls baz (200; 8.198114ms)
Mar 12 08:25:24.547: INFO: (1) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 8.232971ms)
Mar 12 08:25:24.547: INFO: (1) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname1/proxy/: foo (200; 8.785953ms)
Mar 12 08:25:24.547: INFO: (1) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/rewriteme">test</a> (200; 8.277456ms)
Mar 12 08:25:24.547: INFO: (1) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">test<... (200; 8.348986ms)
Mar 12 08:25:24.547: INFO: (1) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:462/proxy/: tls qux (200; 8.415366ms)
Mar 12 08:25:24.547: INFO: (1) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname2/proxy/: bar (200; 8.409537ms)
Mar 12 08:25:24.547: INFO: (1) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname2/proxy/: bar (200; 8.558456ms)
Mar 12 08:25:24.547: INFO: (1) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">... (200; 8.47237ms)
Mar 12 08:25:24.548: INFO: (1) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/tlsrewritem... (200; 8.769338ms)
Mar 12 08:25:24.548: INFO: (1) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 8.884688ms)
Mar 12 08:25:24.548: INFO: (1) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 8.511566ms)
Mar 12 08:25:24.548: INFO: (1) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname1/proxy/: tls baz (200; 8.722525ms)
Mar 12 08:25:24.548: INFO: (1) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 8.716693ms)
Mar 12 08:25:24.548: INFO: (1) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname1/proxy/: foo (200; 8.878582ms)
Mar 12 08:25:24.548: INFO: (1) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname2/proxy/: tls qux (200; 9.105922ms)
Mar 12 08:25:24.552: INFO: (2) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 3.476436ms)
Mar 12 08:25:24.554: INFO: (2) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/rewriteme">test</a> (200; 5.637372ms)
Mar 12 08:25:24.558: INFO: (2) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:462/proxy/: tls qux (200; 8.957621ms)
Mar 12 08:25:24.558: INFO: (2) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">test<... (200; 8.525962ms)
Mar 12 08:25:24.558: INFO: (2) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 9.398807ms)
Mar 12 08:25:24.558: INFO: (2) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/tlsrewritem... (200; 10.18575ms)
Mar 12 08:25:24.558: INFO: (2) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname2/proxy/: bar (200; 9.399612ms)
Mar 12 08:25:24.558: INFO: (2) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">... (200; 9.129869ms)
Mar 12 08:25:24.558: INFO: (2) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:460/proxy/: tls baz (200; 9.686147ms)
Mar 12 08:25:24.558: INFO: (2) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 9.783211ms)
Mar 12 08:25:24.558: INFO: (2) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 8.847387ms)
Mar 12 08:25:24.560: INFO: (2) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname2/proxy/: tls qux (200; 10.872301ms)
Mar 12 08:25:24.560: INFO: (2) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname1/proxy/: tls baz (200; 10.971471ms)
Mar 12 08:25:24.560: INFO: (2) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname2/proxy/: bar (200; 11.161005ms)
Mar 12 08:25:24.560: INFO: (2) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname1/proxy/: foo (200; 10.579597ms)
Mar 12 08:25:24.560: INFO: (2) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname1/proxy/: foo (200; 10.765828ms)
Mar 12 08:25:24.564: INFO: (3) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/tlsrewritem... (200; 3.700311ms)
Mar 12 08:25:24.564: INFO: (3) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:462/proxy/: tls qux (200; 3.471959ms)
Mar 12 08:25:24.564: INFO: (3) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 3.878242ms)
Mar 12 08:25:24.564: INFO: (3) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 4.111697ms)
Mar 12 08:25:24.564: INFO: (3) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 4.000894ms)
Mar 12 08:25:24.565: INFO: (3) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">test<... (200; 4.467708ms)
Mar 12 08:25:24.565: INFO: (3) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/rewriteme">test</a> (200; 4.182106ms)
Mar 12 08:25:24.565: INFO: (3) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:460/proxy/: tls baz (200; 4.436644ms)
Mar 12 08:25:24.565: INFO: (3) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 4.981437ms)
Mar 12 08:25:24.565: INFO: (3) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">... (200; 4.334808ms)
Mar 12 08:25:24.566: INFO: (3) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname1/proxy/: foo (200; 6.012251ms)
Mar 12 08:25:24.567: INFO: (3) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname1/proxy/: tls baz (200; 6.276489ms)
Mar 12 08:25:24.567: INFO: (3) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname1/proxy/: foo (200; 6.923452ms)
Mar 12 08:25:24.567: INFO: (3) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname2/proxy/: tls qux (200; 6.323786ms)
Mar 12 08:25:24.567: INFO: (3) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname2/proxy/: bar (200; 6.432132ms)
Mar 12 08:25:24.567: INFO: (3) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname2/proxy/: bar (200; 7.224952ms)
Mar 12 08:25:24.571: INFO: (4) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:460/proxy/: tls baz (200; 3.39803ms)
Mar 12 08:25:24.571: INFO: (4) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">test<... (200; 3.368427ms)
Mar 12 08:25:24.572: INFO: (4) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">... (200; 4.406937ms)
Mar 12 08:25:24.572: INFO: (4) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:462/proxy/: tls qux (200; 4.756504ms)
Mar 12 08:25:24.573: INFO: (4) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 4.670217ms)
Mar 12 08:25:24.573: INFO: (4) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 4.660977ms)
Mar 12 08:25:24.573: INFO: (4) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 5.195303ms)
Mar 12 08:25:24.573: INFO: (4) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/rewriteme">test</a> (200; 5.184777ms)
Mar 12 08:25:24.573: INFO: (4) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 5.426882ms)
Mar 12 08:25:24.573: INFO: (4) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname2/proxy/: bar (200; 5.766864ms)
Mar 12 08:25:24.573: INFO: (4) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/tlsrewritem... (200; 5.343983ms)
Mar 12 08:25:24.574: INFO: (4) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname1/proxy/: foo (200; 5.983551ms)
Mar 12 08:25:24.574: INFO: (4) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname1/proxy/: tls baz (200; 6.28129ms)
Mar 12 08:25:24.574: INFO: (4) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname1/proxy/: foo (200; 6.282618ms)
Mar 12 08:25:24.576: INFO: (4) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname2/proxy/: tls qux (200; 8.249848ms)
Mar 12 08:25:24.576: INFO: (4) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname2/proxy/: bar (200; 8.570786ms)
Mar 12 08:25:24.580: INFO: (5) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:462/proxy/: tls qux (200; 3.847895ms)
Mar 12 08:25:24.582: INFO: (5) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/tlsrewritem... (200; 5.821766ms)
Mar 12 08:25:24.582: INFO: (5) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 6.081496ms)
Mar 12 08:25:24.582: INFO: (5) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 6.289745ms)
Mar 12 08:25:24.583: INFO: (5) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 6.543616ms)
Mar 12 08:25:24.583: INFO: (5) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 6.345838ms)
Mar 12 08:25:24.583: INFO: (5) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:460/proxy/: tls baz (200; 6.505165ms)
Mar 12 08:25:24.583: INFO: (5) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">... (200; 6.429389ms)
Mar 12 08:25:24.583: INFO: (5) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/rewriteme">test</a> (200; 6.432172ms)
Mar 12 08:25:24.583: INFO: (5) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">test<... (200; 6.542302ms)
Mar 12 08:25:24.583: INFO: (5) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname1/proxy/: foo (200; 6.686188ms)
Mar 12 08:25:24.583: INFO: (5) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname1/proxy/: tls baz (200; 7.109756ms)
Mar 12 08:25:24.585: INFO: (5) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname2/proxy/: bar (200; 8.483346ms)
Mar 12 08:25:24.585: INFO: (5) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname1/proxy/: foo (200; 8.755642ms)
Mar 12 08:25:24.585: INFO: (5) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname2/proxy/: bar (200; 8.671596ms)
Mar 12 08:25:24.585: INFO: (5) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname2/proxy/: tls qux (200; 8.712094ms)
Mar 12 08:25:24.588: INFO: (6) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:460/proxy/: tls baz (200; 2.989239ms)
Mar 12 08:25:24.590: INFO: (6) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:462/proxy/: tls qux (200; 3.997785ms)
Mar 12 08:25:24.590: INFO: (6) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">... (200; 4.230299ms)
Mar 12 08:25:24.590: INFO: (6) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/tlsrewritem... (200; 3.71663ms)
Mar 12 08:25:24.591: INFO: (6) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">test<... (200; 4.318095ms)
Mar 12 08:25:24.591: INFO: (6) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 3.452792ms)
Mar 12 08:25:24.591: INFO: (6) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/rewriteme">test</a> (200; 3.679767ms)
Mar 12 08:25:24.591: INFO: (6) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 3.353126ms)
Mar 12 08:25:24.591: INFO: (6) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 4.042384ms)
Mar 12 08:25:24.591: INFO: (6) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname2/proxy/: bar (200; 5.482005ms)
Mar 12 08:25:24.592: INFO: (6) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 4.513377ms)
Mar 12 08:25:24.592: INFO: (6) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname1/proxy/: foo (200; 5.757796ms)
Mar 12 08:25:24.592: INFO: (6) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname1/proxy/: tls baz (200; 6.916362ms)
Mar 12 08:25:24.592: INFO: (6) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname2/proxy/: bar (200; 7.219709ms)
Mar 12 08:25:24.593: INFO: (6) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname1/proxy/: foo (200; 6.62137ms)
Mar 12 08:25:24.593: INFO: (6) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname2/proxy/: tls qux (200; 7.169726ms)
Mar 12 08:25:24.597: INFO: (7) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:462/proxy/: tls qux (200; 3.901854ms)
Mar 12 08:25:24.597: INFO: (7) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">test<... (200; 3.724836ms)
Mar 12 08:25:24.597: INFO: (7) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 3.588075ms)
Mar 12 08:25:24.597: INFO: (7) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 3.91603ms)
Mar 12 08:25:24.597: INFO: (7) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/rewriteme">test</a> (200; 4.060679ms)
Mar 12 08:25:24.598: INFO: (7) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/tlsrewritem... (200; 4.36031ms)
Mar 12 08:25:24.598: INFO: (7) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 5.169179ms)
Mar 12 08:25:24.598: INFO: (7) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 4.810904ms)
Mar 12 08:25:24.599: INFO: (7) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname1/proxy/: foo (200; 5.877235ms)
Mar 12 08:25:24.599: INFO: (7) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:460/proxy/: tls baz (200; 5.230376ms)
Mar 12 08:25:24.599: INFO: (7) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">... (200; 4.794531ms)
Mar 12 08:25:24.599: INFO: (7) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname1/proxy/: foo (200; 6.126538ms)
Mar 12 08:25:24.599: INFO: (7) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname2/proxy/: bar (200; 5.223001ms)
Mar 12 08:25:24.600: INFO: (7) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname2/proxy/: bar (200; 6.632057ms)
Mar 12 08:25:24.600: INFO: (7) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname2/proxy/: tls qux (200; 6.317347ms)
Mar 12 08:25:24.600: INFO: (7) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname1/proxy/: tls baz (200; 6.465859ms)
Mar 12 08:25:24.604: INFO: (8) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">test<... (200; 3.094949ms)
Mar 12 08:25:24.604: INFO: (8) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:460/proxy/: tls baz (200; 3.550318ms)
Mar 12 08:25:24.604: INFO: (8) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 3.437643ms)
Mar 12 08:25:24.605: INFO: (8) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/tlsrewritem... (200; 4.181599ms)
Mar 12 08:25:24.605: INFO: (8) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 4.449113ms)
Mar 12 08:25:24.605: INFO: (8) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 4.432663ms)
Mar 12 08:25:24.605: INFO: (8) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">... (200; 4.436549ms)
Mar 12 08:25:24.606: INFO: (8) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 5.011562ms)
Mar 12 08:25:24.606: INFO: (8) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:462/proxy/: tls qux (200; 5.149831ms)
Mar 12 08:25:24.607: INFO: (8) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname2/proxy/: bar (200; 5.928527ms)
Mar 12 08:25:24.607: INFO: (8) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname1/proxy/: foo (200; 5.914001ms)
Mar 12 08:25:24.607: INFO: (8) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname2/proxy/: bar (200; 6.073872ms)
Mar 12 08:25:24.607: INFO: (8) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/rewriteme">test</a> (200; 6.287366ms)
Mar 12 08:25:24.607: INFO: (8) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname1/proxy/: foo (200; 6.078559ms)
Mar 12 08:25:24.607: INFO: (8) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname1/proxy/: tls baz (200; 6.615105ms)
Mar 12 08:25:24.607: INFO: (8) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname2/proxy/: tls qux (200; 6.649449ms)
Mar 12 08:25:24.611: INFO: (9) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:462/proxy/: tls qux (200; 3.428314ms)
Mar 12 08:25:24.613: INFO: (9) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/rewriteme">test</a> (200; 4.841628ms)
Mar 12 08:25:24.614: INFO: (9) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 5.788969ms)
Mar 12 08:25:24.615: INFO: (9) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">... (200; 5.835376ms)
Mar 12 08:25:24.615: INFO: (9) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname1/proxy/: foo (200; 7.47646ms)
Mar 12 08:25:24.615: INFO: (9) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">test<... (200; 7.515716ms)
Mar 12 08:25:24.615: INFO: (9) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 7.457403ms)
Mar 12 08:25:24.615: INFO: (9) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:460/proxy/: tls baz (200; 7.060094ms)
Mar 12 08:25:24.615: INFO: (9) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 7.21585ms)
Mar 12 08:25:24.615: INFO: (9) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 7.317842ms)
Mar 12 08:25:24.615: INFO: (9) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/tlsrewritem... (200; 7.575637ms)
Mar 12 08:25:24.616: INFO: (9) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname1/proxy/: foo (200; 7.84147ms)
Mar 12 08:25:24.616: INFO: (9) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname2/proxy/: bar (200; 7.31631ms)
Mar 12 08:25:24.616: INFO: (9) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname1/proxy/: tls baz (200; 7.187601ms)
Mar 12 08:25:24.616: INFO: (9) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname2/proxy/: bar (200; 7.269202ms)
Mar 12 08:25:24.617: INFO: (9) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname2/proxy/: tls qux (200; 7.981326ms)
Mar 12 08:25:24.620: INFO: (10) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/tlsrewritem... (200; 3.041111ms)
Mar 12 08:25:24.621: INFO: (10) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">... (200; 3.74551ms)
Mar 12 08:25:24.621: INFO: (10) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:460/proxy/: tls baz (200; 4.213879ms)
Mar 12 08:25:24.621: INFO: (10) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">test<... (200; 4.486918ms)
Mar 12 08:25:24.623: INFO: (10) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 5.473871ms)
Mar 12 08:25:24.623: INFO: (10) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/rewriteme">test</a> (200; 5.607911ms)
Mar 12 08:25:24.623: INFO: (10) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 5.788501ms)
Mar 12 08:25:24.623: INFO: (10) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 5.593045ms)
Mar 12 08:25:24.623: INFO: (10) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 5.654283ms)
Mar 12 08:25:24.623: INFO: (10) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:462/proxy/: tls qux (200; 5.709593ms)
Mar 12 08:25:24.624: INFO: (10) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname1/proxy/: foo (200; 7.543409ms)
Mar 12 08:25:24.626: INFO: (10) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname1/proxy/: tls baz (200; 8.566304ms)
Mar 12 08:25:24.626: INFO: (10) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname2/proxy/: bar (200; 8.632945ms)
Mar 12 08:25:24.626: INFO: (10) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname2/proxy/: tls qux (200; 9.274036ms)
Mar 12 08:25:24.626: INFO: (10) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname1/proxy/: foo (200; 9.063649ms)
Mar 12 08:25:24.626: INFO: (10) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname2/proxy/: bar (200; 8.759578ms)
Mar 12 08:25:24.629: INFO: (11) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">test<... (200; 2.819199ms)
Mar 12 08:25:24.629: INFO: (11) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/rewriteme">test</a> (200; 2.852612ms)
Mar 12 08:25:24.630: INFO: (11) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 3.339746ms)
Mar 12 08:25:24.630: INFO: (11) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:460/proxy/: tls baz (200; 3.168165ms)
Mar 12 08:25:24.630: INFO: (11) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 3.249612ms)
Mar 12 08:25:24.630: INFO: (11) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/tlsrewritem... (200; 3.331309ms)
Mar 12 08:25:24.630: INFO: (11) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 3.110996ms)
Mar 12 08:25:24.631: INFO: (11) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 4.649275ms)
Mar 12 08:25:24.631: INFO: (11) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:462/proxy/: tls qux (200; 4.396633ms)
Mar 12 08:25:24.631: INFO: (11) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname1/proxy/: foo (200; 5.192191ms)
Mar 12 08:25:24.631: INFO: (11) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname2/proxy/: tls qux (200; 4.624476ms)
Mar 12 08:25:24.631: INFO: (11) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">... (200; 4.549575ms)
Mar 12 08:25:24.631: INFO: (11) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname2/proxy/: bar (200; 4.449081ms)
Mar 12 08:25:24.632: INFO: (11) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname1/proxy/: tls baz (200; 4.742005ms)
Mar 12 08:25:24.632: INFO: (11) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname2/proxy/: bar (200; 5.152922ms)
Mar 12 08:25:24.632: INFO: (11) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname1/proxy/: foo (200; 5.176875ms)
Mar 12 08:25:24.635: INFO: (12) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:460/proxy/: tls baz (200; 3.015011ms)
Mar 12 08:25:24.637: INFO: (12) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 4.153055ms)
Mar 12 08:25:24.638: INFO: (12) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 4.24676ms)
Mar 12 08:25:24.638: INFO: (12) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/rewriteme">test</a> (200; 4.421896ms)
Mar 12 08:25:24.639: INFO: (12) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 5.5306ms)
Mar 12 08:25:24.639: INFO: (12) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">test<... (200; 6.294241ms)
Mar 12 08:25:24.639: INFO: (12) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 5.968269ms)
Mar 12 08:25:24.640: INFO: (12) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname2/proxy/: bar (200; 7.343301ms)
Mar 12 08:25:24.640: INFO: (12) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/tlsrewritem... (200; 6.623574ms)
Mar 12 08:25:24.640: INFO: (12) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:462/proxy/: tls qux (200; 7.017466ms)
Mar 12 08:25:24.640: INFO: (12) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">... (200; 7.207382ms)
Mar 12 08:25:24.640: INFO: (12) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname2/proxy/: bar (200; 7.595783ms)
Mar 12 08:25:24.641: INFO: (12) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname1/proxy/: foo (200; 8.548006ms)
Mar 12 08:25:24.641: INFO: (12) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname1/proxy/: foo (200; 8.423571ms)
Mar 12 08:25:24.641: INFO: (12) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname1/proxy/: tls baz (200; 8.976006ms)
Mar 12 08:25:24.641: INFO: (12) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname2/proxy/: tls qux (200; 8.951851ms)
Mar 12 08:25:24.645: INFO: (13) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 3.13854ms)
Mar 12 08:25:24.646: INFO: (13) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/tlsrewritem... (200; 4.374669ms)
Mar 12 08:25:24.647: INFO: (13) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">... (200; 3.913809ms)
Mar 12 08:25:24.647: INFO: (13) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 4.619516ms)
Mar 12 08:25:24.647: INFO: (13) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 4.744377ms)
Mar 12 08:25:24.647: INFO: (13) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/rewriteme">test</a> (200; 5.033887ms)
Mar 12 08:25:24.647: INFO: (13) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:460/proxy/: tls baz (200; 4.592275ms)
Mar 12 08:25:24.647: INFO: (13) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:462/proxy/: tls qux (200; 3.918412ms)
Mar 12 08:25:24.647: INFO: (13) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">test<... (200; 3.878925ms)
Mar 12 08:25:24.647: INFO: (13) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 3.847296ms)
Mar 12 08:25:24.648: INFO: (13) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname2/proxy/: bar (200; 5.351411ms)
Mar 12 08:25:24.648: INFO: (13) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname1/proxy/: foo (200; 5.439106ms)
Mar 12 08:25:24.648: INFO: (13) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname2/proxy/: bar (200; 5.956279ms)
Mar 12 08:25:24.649: INFO: (13) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname2/proxy/: tls qux (200; 6.137871ms)
Mar 12 08:25:24.649: INFO: (13) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname1/proxy/: foo (200; 5.858452ms)
Mar 12 08:25:24.649: INFO: (13) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname1/proxy/: tls baz (200; 6.27854ms)
Mar 12 08:25:24.652: INFO: (14) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 2.72136ms)
Mar 12 08:25:24.652: INFO: (14) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 2.962931ms)
Mar 12 08:25:24.652: INFO: (14) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 3.218281ms)
Mar 12 08:25:24.653: INFO: (14) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/rewriteme">test</a> (200; 3.0102ms)
Mar 12 08:25:24.653: INFO: (14) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/tlsrewritem... (200; 3.691784ms)
Mar 12 08:25:24.653: INFO: (14) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:460/proxy/: tls baz (200; 3.341046ms)
Mar 12 08:25:24.653: INFO: (14) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 3.584315ms)
Mar 12 08:25:24.653: INFO: (14) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:462/proxy/: tls qux (200; 2.955976ms)
Mar 12 08:25:24.653: INFO: (14) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">... (200; 3.332978ms)
Mar 12 08:25:24.654: INFO: (14) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">test<... (200; 3.231611ms)
Mar 12 08:25:24.654: INFO: (14) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname1/proxy/: foo (200; 4.854386ms)
Mar 12 08:25:24.654: INFO: (14) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname1/proxy/: foo (200; 5.023063ms)
Mar 12 08:25:24.655: INFO: (14) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname1/proxy/: tls baz (200; 4.960574ms)
Mar 12 08:25:24.655: INFO: (14) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname2/proxy/: bar (200; 4.864575ms)
Mar 12 08:25:24.655: INFO: (14) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname2/proxy/: bar (200; 5.485498ms)
Mar 12 08:25:24.655: INFO: (14) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname2/proxy/: tls qux (200; 5.526138ms)
Mar 12 08:25:24.659: INFO: (15) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:462/proxy/: tls qux (200; 2.965917ms)
Mar 12 08:25:24.660: INFO: (15) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:460/proxy/: tls baz (200; 4.659496ms)
Mar 12 08:25:24.662: INFO: (15) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 5.547373ms)
Mar 12 08:25:24.662: INFO: (15) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/rewriteme">test</a> (200; 5.957205ms)
Mar 12 08:25:24.662: INFO: (15) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 5.928223ms)
Mar 12 08:25:24.662: INFO: (15) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">test<... (200; 5.933688ms)
Mar 12 08:25:24.662: INFO: (15) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 6.300892ms)
Mar 12 08:25:24.662: INFO: (15) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">... (200; 6.221159ms)
Mar 12 08:25:24.662: INFO: (15) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 6.100404ms)
Mar 12 08:25:24.663: INFO: (15) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/tlsrewritem... (200; 7.413074ms)
Mar 12 08:25:24.664: INFO: (15) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname2/proxy/: bar (200; 7.729734ms)
Mar 12 08:25:24.666: INFO: (15) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname2/proxy/: tls qux (200; 9.620921ms)
Mar 12 08:25:24.666: INFO: (15) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname1/proxy/: foo (200; 9.34541ms)
Mar 12 08:25:24.666: INFO: (15) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname2/proxy/: bar (200; 9.916322ms)
Mar 12 08:25:24.666: INFO: (15) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname1/proxy/: tls baz (200; 9.782039ms)
Mar 12 08:25:24.666: INFO: (15) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname1/proxy/: foo (200; 9.416591ms)
Mar 12 08:25:24.669: INFO: (16) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/tlsrewritem... (200; 3.679914ms)
Mar 12 08:25:24.671: INFO: (16) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 4.551939ms)
Mar 12 08:25:24.671: INFO: (16) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 4.96845ms)
Mar 12 08:25:24.671: INFO: (16) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:462/proxy/: tls qux (200; 4.979432ms)
Mar 12 08:25:24.671: INFO: (16) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 5.168709ms)
Mar 12 08:25:24.672: INFO: (16) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">... (200; 6.33391ms)
Mar 12 08:25:24.672: INFO: (16) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/rewriteme">test</a> (200; 6.461731ms)
Mar 12 08:25:24.672: INFO: (16) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 6.379501ms)
Mar 12 08:25:24.672: INFO: (16) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:460/proxy/: tls baz (200; 6.432906ms)
Mar 12 08:25:24.672: INFO: (16) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">test<... (200; 6.360408ms)
Mar 12 08:25:24.674: INFO: (16) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname2/proxy/: bar (200; 7.710826ms)
Mar 12 08:25:24.674: INFO: (16) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname2/proxy/: bar (200; 8.410897ms)
Mar 12 08:25:24.674: INFO: (16) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname1/proxy/: foo (200; 8.41669ms)
Mar 12 08:25:24.674: INFO: (16) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname1/proxy/: foo (200; 8.454921ms)
Mar 12 08:25:24.675: INFO: (16) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname1/proxy/: tls baz (200; 8.537026ms)
Mar 12 08:25:24.675: INFO: (16) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname2/proxy/: tls qux (200; 8.639851ms)
Mar 12 08:25:24.680: INFO: (17) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/rewriteme">test</a> (200; 5.683883ms)
Mar 12 08:25:24.682: INFO: (17) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">... (200; 6.690469ms)
Mar 12 08:25:24.682: INFO: (17) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 6.566184ms)
Mar 12 08:25:24.682: INFO: (17) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 6.791242ms)
Mar 12 08:25:24.682: INFO: (17) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:462/proxy/: tls qux (200; 6.756174ms)
Mar 12 08:25:24.682: INFO: (17) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 6.916188ms)
Mar 12 08:25:24.682: INFO: (17) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 7.152282ms)
Mar 12 08:25:24.682: INFO: (17) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:460/proxy/: tls baz (200; 6.9115ms)
Mar 12 08:25:24.682: INFO: (17) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">test<... (200; 7.045602ms)
Mar 12 08:25:24.682: INFO: (17) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/tlsrewritem... (200; 7.650519ms)
Mar 12 08:25:24.684: INFO: (17) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname1/proxy/: foo (200; 8.882486ms)
Mar 12 08:25:24.685: INFO: (17) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname2/proxy/: bar (200; 9.70375ms)
Mar 12 08:25:24.685: INFO: (17) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname2/proxy/: bar (200; 9.234374ms)
Mar 12 08:25:24.685: INFO: (17) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname1/proxy/: tls baz (200; 9.77104ms)
Mar 12 08:25:24.685: INFO: (17) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname2/proxy/: tls qux (200; 9.863279ms)
Mar 12 08:25:24.685: INFO: (17) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname1/proxy/: foo (200; 9.901477ms)
Mar 12 08:25:24.690: INFO: (18) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 4.146377ms)
Mar 12 08:25:24.691: INFO: (18) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">test<... (200; 5.316207ms)
Mar 12 08:25:24.693: INFO: (18) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:462/proxy/: tls qux (200; 7.021991ms)
Mar 12 08:25:24.693: INFO: (18) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname2/proxy/: tls qux (200; 8.188437ms)
Mar 12 08:25:24.694: INFO: (18) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 7.559642ms)
Mar 12 08:25:24.694: INFO: (18) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">... (200; 7.719993ms)
Mar 12 08:25:24.694: INFO: (18) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/rewriteme">test</a> (200; 8.160051ms)
Mar 12 08:25:24.694: INFO: (18) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 8.086066ms)
Mar 12 08:25:24.694: INFO: (18) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 7.952575ms)
Mar 12 08:25:24.694: INFO: (18) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname1/proxy/: foo (200; 8.678014ms)
Mar 12 08:25:24.694: INFO: (18) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:460/proxy/: tls baz (200; 8.521356ms)
Mar 12 08:25:24.695: INFO: (18) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname2/proxy/: bar (200; 8.745524ms)
Mar 12 08:25:24.695: INFO: (18) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname2/proxy/: bar (200; 8.796273ms)
Mar 12 08:25:24.695: INFO: (18) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname1/proxy/: tls baz (200; 8.769761ms)
Mar 12 08:25:24.695: INFO: (18) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/tlsrewritem... (200; 9.145523ms)
Mar 12 08:25:24.695: INFO: (18) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname1/proxy/: foo (200; 9.366265ms)
Mar 12 08:25:24.699: INFO: (19) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 3.681835ms)
Mar 12 08:25:24.700: INFO: (19) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5/proxy/rewriteme">test</a> (200; 5.064622ms)
Mar 12 08:25:24.701: INFO: (19) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname1/proxy/: foo (200; 5.539475ms)
Mar 12 08:25:24.701: INFO: (19) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 5.487986ms)
Mar 12 08:25:24.701: INFO: (19) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:460/proxy/: tls baz (200; 5.885777ms)
Mar 12 08:25:24.701: INFO: (19) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:462/proxy/: tls qux (200; 5.715631ms)
Mar 12 08:25:24.701: INFO: (19) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:162/proxy/: bar (200; 5.97512ms)
Mar 12 08:25:24.701: INFO: (19) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">test<... (200; 5.699079ms)
Mar 12 08:25:24.702: INFO: (19) /api/v1/namespaces/proxy-2488/pods/proxy-service-jkprl-9h2d5:160/proxy/: foo (200; 6.850101ms)
Mar 12 08:25:24.702: INFO: (19) /api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/https:proxy-service-jkprl-9h2d5:443/proxy/tlsrewritem... (200; 6.769137ms)
Mar 12 08:25:24.702: INFO: (19) /api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/: <a href="/api/v1/namespaces/proxy-2488/pods/http:proxy-service-jkprl-9h2d5:1080/proxy/rewriteme">... (200; 6.958224ms)
Mar 12 08:25:24.702: INFO: (19) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname2/proxy/: bar (200; 7.284116ms)
Mar 12 08:25:24.703: INFO: (19) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname2/proxy/: tls qux (200; 7.603539ms)
Mar 12 08:25:24.703: INFO: (19) /api/v1/namespaces/proxy-2488/services/https:proxy-service-jkprl:tlsportname1/proxy/: tls baz (200; 7.661424ms)
Mar 12 08:25:24.703: INFO: (19) /api/v1/namespaces/proxy-2488/services/http:proxy-service-jkprl:portname2/proxy/: bar (200; 8.141816ms)
Mar 12 08:25:24.704: INFO: (19) /api/v1/namespaces/proxy-2488/services/proxy-service-jkprl:portname1/proxy/: foo (200; 8.65397ms)
STEP: deleting ReplicationController proxy-service-jkprl in namespace proxy-2488, will wait for the garbage collector to delete the pods
Mar 12 08:25:24.766: INFO: Deleting ReplicationController proxy-service-jkprl took: 9.420932ms
Mar 12 08:25:24.867: INFO: Terminating ReplicationController proxy-service-jkprl pods took: 100.26132ms
[AfterEach] version v1
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:25:31.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2488" for this suite.
Mar 12 08:25:37.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:25:37.679: INFO: namespace proxy-2488 deletion completed in 6.106860669s

• [SLOW TEST:27.288 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:25:37.679: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar 12 08:25:47.833: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:25:47.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0312 08:25:47.833760      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-1655" for this suite.
Mar 12 08:25:55.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:25:55.948: INFO: namespace gc-1655 deletion completed in 8.10959297s

• [SLOW TEST:18.268 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:25:55.948: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar 12 08:25:56.046: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8124 /api/v1/namespaces/watch-8124/configmaps/e2e-watch-test-resource-version 7f920026-747c-49fa-a9ce-76ad91977918 148538 0 2020-03-12 08:25:56 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 12 08:25:56.047: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8124 /api/v1/namespaces/watch-8124/configmaps/e2e-watch-test-resource-version 7f920026-747c-49fa-a9ce-76ad91977918 148539 0 2020-03-12 08:25:56 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:25:56.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8124" for this suite.
Mar 12 08:26:02.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:26:02.143: INFO: namespace watch-8124 deletion completed in 6.090591466s

• [SLOW TEST:6.195 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:26:02.144: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 12 08:26:02.197: INFO: Waiting up to 5m0s for pod "pod-85aa2367-0a91-4dc7-b316-d10d18fbe495" in namespace "emptydir-4656" to be "success or failure"
Mar 12 08:26:02.199: INFO: Pod "pod-85aa2367-0a91-4dc7-b316-d10d18fbe495": Phase="Pending", Reason="", readiness=false. Elapsed: 2.507047ms
Mar 12 08:26:04.204: INFO: Pod "pod-85aa2367-0a91-4dc7-b316-d10d18fbe495": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007080035s
Mar 12 08:26:06.208: INFO: Pod "pod-85aa2367-0a91-4dc7-b316-d10d18fbe495": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011148267s
Mar 12 08:26:08.213: INFO: Pod "pod-85aa2367-0a91-4dc7-b316-d10d18fbe495": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016141066s
Mar 12 08:26:10.218: INFO: Pod "pod-85aa2367-0a91-4dc7-b316-d10d18fbe495": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021081702s
Mar 12 08:26:12.223: INFO: Pod "pod-85aa2367-0a91-4dc7-b316-d10d18fbe495": Phase="Pending", Reason="", readiness=false. Elapsed: 10.026433311s
Mar 12 08:26:14.228: INFO: Pod "pod-85aa2367-0a91-4dc7-b316-d10d18fbe495": Phase="Pending", Reason="", readiness=false. Elapsed: 12.03145834s
Mar 12 08:26:16.233: INFO: Pod "pod-85aa2367-0a91-4dc7-b316-d10d18fbe495": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.036733414s
STEP: Saw pod success
Mar 12 08:26:16.234: INFO: Pod "pod-85aa2367-0a91-4dc7-b316-d10d18fbe495" satisfied condition "success or failure"
Mar 12 08:26:16.238: INFO: Trying to get logs from node node-3 pod pod-85aa2367-0a91-4dc7-b316-d10d18fbe495 container test-container: <nil>
STEP: delete the pod
Mar 12 08:26:16.278: INFO: Waiting for pod pod-85aa2367-0a91-4dc7-b316-d10d18fbe495 to disappear
Mar 12 08:26:16.281: INFO: Pod pod-85aa2367-0a91-4dc7-b316-d10d18fbe495 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:26:16.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4656" for this suite.
Mar 12 08:26:22.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:26:22.396: INFO: namespace emptydir-4656 deletion completed in 6.109056178s

• [SLOW TEST:20.252 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:26:22.397: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 12 08:26:22.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-8799'
Mar 12 08:26:22.577: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 12 08:26:22.577: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Mar 12 08:26:24.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 delete deployment e2e-test-httpd-deployment --namespace=kubectl-8799'
Mar 12 08:26:24.769: INFO: stderr: ""
Mar 12 08:26:24.769: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:26:24.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8799" for this suite.
Mar 12 08:26:30.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:26:30.873: INFO: namespace kubectl-8799 deletion completed in 6.09698431s

• [SLOW TEST:8.476 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:26:30.873: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 08:26:30.919: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Creating first CR 
Mar 12 08:26:36.514: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-12T08:26:36Z generation:1 name:name1 resourceVersion:148677 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:bea9af63-dae2-4d18-ba79-cbd23c968967] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Mar 12 08:26:46.532: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-12T08:26:46Z generation:1 name:name2 resourceVersion:148695 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:4af91e90-8288-4c53-bf98-cbb6d7be315c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Mar 12 08:26:56.541: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-12T08:26:36Z generation:2 name:name1 resourceVersion:148712 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:bea9af63-dae2-4d18-ba79-cbd23c968967] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Mar 12 08:27:06.552: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-12T08:26:46Z generation:2 name:name2 resourceVersion:148730 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:4af91e90-8288-4c53-bf98-cbb6d7be315c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Mar 12 08:27:16.564: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-12T08:26:36Z generation:2 name:name1 resourceVersion:148747 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:bea9af63-dae2-4d18-ba79-cbd23c968967] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Mar 12 08:27:26.574: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-12T08:26:46Z generation:2 name:name2 resourceVersion:148764 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:4af91e90-8288-4c53-bf98-cbb6d7be315c] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:27:37.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7985" for this suite.
Mar 12 08:27:43.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:27:43.202: INFO: namespace crd-watch-7985 deletion completed in 6.102437466s

• [SLOW TEST:72.329 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:27:43.202: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 12 08:28:11.335: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 12 08:28:11.338: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 12 08:28:13.339: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 12 08:28:13.344: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 12 08:28:15.339: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 12 08:28:15.344: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:28:15.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3735" for this suite.
Mar 12 08:28:27.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:28:27.493: INFO: namespace container-lifecycle-hook-3735 deletion completed in 12.143266115s

• [SLOW TEST:44.291 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:28:27.494: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 12 08:28:28.004: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 12 08:28:30.018: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:28:32.023: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:28:34.023: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:28:36.024: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:28:38.025: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:28:40.023: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598508, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 12 08:28:43.034: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:28:43.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3983" for this suite.
Mar 12 08:28:55.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:28:55.203: INFO: namespace webhook-3983 deletion completed in 12.101676273s
STEP: Destroying namespace "webhook-3983-markers" for this suite.
Mar 12 08:29:01.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:29:01.322: INFO: namespace webhook-3983-markers deletion completed in 6.118968611s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:33.849 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:29:01.343: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 12 08:29:01.408: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1f6faad5-68fa-4b65-b1df-8c8e9077e322" in namespace "projected-4245" to be "success or failure"
Mar 12 08:29:01.412: INFO: Pod "downwardapi-volume-1f6faad5-68fa-4b65-b1df-8c8e9077e322": Phase="Pending", Reason="", readiness=false. Elapsed: 3.537436ms
Mar 12 08:29:03.417: INFO: Pod "downwardapi-volume-1f6faad5-68fa-4b65-b1df-8c8e9077e322": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009337976s
Mar 12 08:29:05.422: INFO: Pod "downwardapi-volume-1f6faad5-68fa-4b65-b1df-8c8e9077e322": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014328319s
Mar 12 08:29:07.427: INFO: Pod "downwardapi-volume-1f6faad5-68fa-4b65-b1df-8c8e9077e322": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019115505s
Mar 12 08:29:09.432: INFO: Pod "downwardapi-volume-1f6faad5-68fa-4b65-b1df-8c8e9077e322": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024182384s
Mar 12 08:29:11.439: INFO: Pod "downwardapi-volume-1f6faad5-68fa-4b65-b1df-8c8e9077e322": Phase="Pending", Reason="", readiness=false. Elapsed: 10.031183429s
Mar 12 08:29:13.445: INFO: Pod "downwardapi-volume-1f6faad5-68fa-4b65-b1df-8c8e9077e322": Phase="Pending", Reason="", readiness=false. Elapsed: 12.036698827s
Mar 12 08:29:15.451: INFO: Pod "downwardapi-volume-1f6faad5-68fa-4b65-b1df-8c8e9077e322": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.04337354s
STEP: Saw pod success
Mar 12 08:29:15.451: INFO: Pod "downwardapi-volume-1f6faad5-68fa-4b65-b1df-8c8e9077e322" satisfied condition "success or failure"
Mar 12 08:29:15.455: INFO: Trying to get logs from node node-3 pod downwardapi-volume-1f6faad5-68fa-4b65-b1df-8c8e9077e322 container client-container: <nil>
STEP: delete the pod
Mar 12 08:29:15.500: INFO: Waiting for pod downwardapi-volume-1f6faad5-68fa-4b65-b1df-8c8e9077e322 to disappear
Mar 12 08:29:15.504: INFO: Pod downwardapi-volume-1f6faad5-68fa-4b65-b1df-8c8e9077e322 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:29:15.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4245" for this suite.
Mar 12 08:29:21.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:29:21.617: INFO: namespace projected-4245 deletion completed in 6.106903091s

• [SLOW TEST:20.275 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:29:21.617: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 12 08:29:22.385: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 12 08:29:24.397: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:29:26.402: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:29:28.402: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:29:30.404: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:29:32.402: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:29:34.402: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598562, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 12 08:29:37.419: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:29:47.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4506" for this suite.
Mar 12 08:29:53.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:29:53.695: INFO: namespace webhook-4506 deletion completed in 6.095546561s
STEP: Destroying namespace "webhook-4506-markers" for this suite.
Mar 12 08:29:59.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:29:59.793: INFO: namespace webhook-4506-markers deletion completed in 6.097628357s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:38.194 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:29:59.811: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-9fbc9a69-b129-4b1d-b727-40ea6c8f2c20
STEP: Creating a pod to test consume secrets
Mar 12 08:29:59.882: INFO: Waiting up to 5m0s for pod "pod-secrets-68a9b702-357f-4c82-8bf2-6822c6d92b04" in namespace "secrets-6109" to be "success or failure"
Mar 12 08:29:59.885: INFO: Pod "pod-secrets-68a9b702-357f-4c82-8bf2-6822c6d92b04": Phase="Pending", Reason="", readiness=false. Elapsed: 3.381113ms
Mar 12 08:30:01.890: INFO: Pod "pod-secrets-68a9b702-357f-4c82-8bf2-6822c6d92b04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007940382s
Mar 12 08:30:03.894: INFO: Pod "pod-secrets-68a9b702-357f-4c82-8bf2-6822c6d92b04": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012218473s
Mar 12 08:30:05.899: INFO: Pod "pod-secrets-68a9b702-357f-4c82-8bf2-6822c6d92b04": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016890711s
Mar 12 08:30:07.904: INFO: Pod "pod-secrets-68a9b702-357f-4c82-8bf2-6822c6d92b04": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022142921s
Mar 12 08:30:09.909: INFO: Pod "pod-secrets-68a9b702-357f-4c82-8bf2-6822c6d92b04": Phase="Pending", Reason="", readiness=false. Elapsed: 10.027266169s
Mar 12 08:30:11.914: INFO: Pod "pod-secrets-68a9b702-357f-4c82-8bf2-6822c6d92b04": Phase="Pending", Reason="", readiness=false. Elapsed: 12.032128733s
Mar 12 08:30:13.919: INFO: Pod "pod-secrets-68a9b702-357f-4c82-8bf2-6822c6d92b04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.037030596s
STEP: Saw pod success
Mar 12 08:30:13.919: INFO: Pod "pod-secrets-68a9b702-357f-4c82-8bf2-6822c6d92b04" satisfied condition "success or failure"
Mar 12 08:30:13.922: INFO: Trying to get logs from node node-2 pod pod-secrets-68a9b702-357f-4c82-8bf2-6822c6d92b04 container secret-volume-test: <nil>
STEP: delete the pod
Mar 12 08:30:13.954: INFO: Waiting for pod pod-secrets-68a9b702-357f-4c82-8bf2-6822c6d92b04 to disappear
Mar 12 08:30:13.956: INFO: Pod pod-secrets-68a9b702-357f-4c82-8bf2-6822c6d92b04 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:30:13.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6109" for this suite.
Mar 12 08:30:19.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:30:20.068: INFO: namespace secrets-6109 deletion completed in 6.106642157s

• [SLOW TEST:20.257 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:30:20.069: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-3558/configmap-test-8b806aad-1fc3-46e6-a3f8-2aea8f8fe1c3
STEP: Creating a pod to test consume configMaps
Mar 12 08:30:20.138: INFO: Waiting up to 5m0s for pod "pod-configmaps-2973e4c9-e2b2-4498-8ea4-f0ac25130406" in namespace "configmap-3558" to be "success or failure"
Mar 12 08:30:20.141: INFO: Pod "pod-configmaps-2973e4c9-e2b2-4498-8ea4-f0ac25130406": Phase="Pending", Reason="", readiness=false. Elapsed: 3.185256ms
Mar 12 08:30:22.146: INFO: Pod "pod-configmaps-2973e4c9-e2b2-4498-8ea4-f0ac25130406": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007824025s
Mar 12 08:30:24.150: INFO: Pod "pod-configmaps-2973e4c9-e2b2-4498-8ea4-f0ac25130406": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012511106s
Mar 12 08:30:26.155: INFO: Pod "pod-configmaps-2973e4c9-e2b2-4498-8ea4-f0ac25130406": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017451178s
Mar 12 08:30:28.159: INFO: Pod "pod-configmaps-2973e4c9-e2b2-4498-8ea4-f0ac25130406": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021291091s
Mar 12 08:30:30.167: INFO: Pod "pod-configmaps-2973e4c9-e2b2-4498-8ea4-f0ac25130406": Phase="Pending", Reason="", readiness=false. Elapsed: 10.029302798s
Mar 12 08:30:32.171: INFO: Pod "pod-configmaps-2973e4c9-e2b2-4498-8ea4-f0ac25130406": Phase="Pending", Reason="", readiness=false. Elapsed: 12.033657963s
Mar 12 08:30:34.176: INFO: Pod "pod-configmaps-2973e4c9-e2b2-4498-8ea4-f0ac25130406": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.038370219s
STEP: Saw pod success
Mar 12 08:30:34.176: INFO: Pod "pod-configmaps-2973e4c9-e2b2-4498-8ea4-f0ac25130406" satisfied condition "success or failure"
Mar 12 08:30:34.179: INFO: Trying to get logs from node node-3 pod pod-configmaps-2973e4c9-e2b2-4498-8ea4-f0ac25130406 container env-test: <nil>
STEP: delete the pod
Mar 12 08:30:34.201: INFO: Waiting for pod pod-configmaps-2973e4c9-e2b2-4498-8ea4-f0ac25130406 to disappear
Mar 12 08:30:34.204: INFO: Pod pod-configmaps-2973e4c9-e2b2-4498-8ea4-f0ac25130406 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:30:34.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3558" for this suite.
Mar 12 08:30:40.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:30:40.312: INFO: namespace configmap-3558 deletion completed in 6.104104061s

• [SLOW TEST:20.243 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:30:40.312: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-002d9af3-3ac9-404f-a67a-e38aa4618988
STEP: Creating a pod to test consume secrets
Mar 12 08:30:40.379: INFO: Waiting up to 5m0s for pod "pod-secrets-6982ffdb-910c-440d-89a9-8e6db3fe6303" in namespace "secrets-5691" to be "success or failure"
Mar 12 08:30:40.382: INFO: Pod "pod-secrets-6982ffdb-910c-440d-89a9-8e6db3fe6303": Phase="Pending", Reason="", readiness=false. Elapsed: 3.150724ms
Mar 12 08:30:42.387: INFO: Pod "pod-secrets-6982ffdb-910c-440d-89a9-8e6db3fe6303": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007598404s
Mar 12 08:30:44.391: INFO: Pod "pod-secrets-6982ffdb-910c-440d-89a9-8e6db3fe6303": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012122809s
Mar 12 08:30:46.397: INFO: Pod "pod-secrets-6982ffdb-910c-440d-89a9-8e6db3fe6303": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01792644s
Mar 12 08:30:48.402: INFO: Pod "pod-secrets-6982ffdb-910c-440d-89a9-8e6db3fe6303": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022779686s
Mar 12 08:30:50.407: INFO: Pod "pod-secrets-6982ffdb-910c-440d-89a9-8e6db3fe6303": Phase="Pending", Reason="", readiness=false. Elapsed: 10.027726994s
Mar 12 08:30:52.411: INFO: Pod "pod-secrets-6982ffdb-910c-440d-89a9-8e6db3fe6303": Phase="Pending", Reason="", readiness=false. Elapsed: 12.032344677s
Mar 12 08:30:54.416: INFO: Pod "pod-secrets-6982ffdb-910c-440d-89a9-8e6db3fe6303": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.037222496s
STEP: Saw pod success
Mar 12 08:30:54.416: INFO: Pod "pod-secrets-6982ffdb-910c-440d-89a9-8e6db3fe6303" satisfied condition "success or failure"
Mar 12 08:30:54.420: INFO: Trying to get logs from node node-3 pod pod-secrets-6982ffdb-910c-440d-89a9-8e6db3fe6303 container secret-env-test: <nil>
STEP: delete the pod
Mar 12 08:30:54.444: INFO: Waiting for pod pod-secrets-6982ffdb-910c-440d-89a9-8e6db3fe6303 to disappear
Mar 12 08:30:54.447: INFO: Pod pod-secrets-6982ffdb-910c-440d-89a9-8e6db3fe6303 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:30:54.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5691" for this suite.
Mar 12 08:31:00.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:31:00.561: INFO: namespace secrets-5691 deletion completed in 6.108284451s

• [SLOW TEST:20.249 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:31:00.561: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 12 08:31:01.309: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 12 08:31:03.319: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:31:05.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:31:07.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:31:09.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:31:11.324: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:31:13.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719598661, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 12 08:31:16.338: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:31:16.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5928" for this suite.
Mar 12 08:31:22.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:31:22.511: INFO: namespace webhook-5928 deletion completed in 6.104652948s
STEP: Destroying namespace "webhook-5928-markers" for this suite.
Mar 12 08:31:28.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:31:28.617: INFO: namespace webhook-5928-markers deletion completed in 6.106369309s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:28.074 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:31:28.635: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:31:42.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7783" for this suite.
Mar 12 08:31:48.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:31:48.865: INFO: namespace emptydir-wrapper-7783 deletion completed in 6.110736571s

• [SLOW TEST:20.230 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:31:48.865: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar 12 08:31:48.934: INFO: Waiting up to 5m0s for pod "pod-133bda1b-3240-4017-a74e-13d9fac24d7e" in namespace "emptydir-6116" to be "success or failure"
Mar 12 08:31:48.937: INFO: Pod "pod-133bda1b-3240-4017-a74e-13d9fac24d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.115885ms
Mar 12 08:31:50.941: INFO: Pod "pod-133bda1b-3240-4017-a74e-13d9fac24d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007079643s
Mar 12 08:31:52.945: INFO: Pod "pod-133bda1b-3240-4017-a74e-13d9fac24d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011476903s
Mar 12 08:31:54.951: INFO: Pod "pod-133bda1b-3240-4017-a74e-13d9fac24d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01721546s
Mar 12 08:31:56.956: INFO: Pod "pod-133bda1b-3240-4017-a74e-13d9fac24d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022666096s
Mar 12 08:31:58.961: INFO: Pod "pod-133bda1b-3240-4017-a74e-13d9fac24d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.027532902s
Mar 12 08:32:00.967: INFO: Pod "pod-133bda1b-3240-4017-a74e-13d9fac24d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.032970093s
Mar 12 08:32:03.052: INFO: Pod "pod-133bda1b-3240-4017-a74e-13d9fac24d7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.1179628s
STEP: Saw pod success
Mar 12 08:32:03.052: INFO: Pod "pod-133bda1b-3240-4017-a74e-13d9fac24d7e" satisfied condition "success or failure"
Mar 12 08:32:03.058: INFO: Trying to get logs from node node-3 pod pod-133bda1b-3240-4017-a74e-13d9fac24d7e container test-container: <nil>
STEP: delete the pod
Mar 12 08:32:03.081: INFO: Waiting for pod pod-133bda1b-3240-4017-a74e-13d9fac24d7e to disappear
Mar 12 08:32:03.084: INFO: Pod pod-133bda1b-3240-4017-a74e-13d9fac24d7e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:32:03.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6116" for this suite.
Mar 12 08:32:09.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:32:09.198: INFO: namespace emptydir-6116 deletion completed in 6.109192757s

• [SLOW TEST:20.332 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:32:09.198: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-a6c58368-51c1-435a-b641-67a91beb52f9
STEP: Creating a pod to test consume configMaps
Mar 12 08:32:09.270: INFO: Waiting up to 5m0s for pod "pod-configmaps-5fa95635-b935-4148-9b02-8796789bfe32" in namespace "configmap-5249" to be "success or failure"
Mar 12 08:32:09.274: INFO: Pod "pod-configmaps-5fa95635-b935-4148-9b02-8796789bfe32": Phase="Pending", Reason="", readiness=false. Elapsed: 3.859828ms
Mar 12 08:32:11.282: INFO: Pod "pod-configmaps-5fa95635-b935-4148-9b02-8796789bfe32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011805857s
Mar 12 08:32:13.289: INFO: Pod "pod-configmaps-5fa95635-b935-4148-9b02-8796789bfe32": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019448357s
Mar 12 08:32:15.295: INFO: Pod "pod-configmaps-5fa95635-b935-4148-9b02-8796789bfe32": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024875357s
Mar 12 08:32:17.300: INFO: Pod "pod-configmaps-5fa95635-b935-4148-9b02-8796789bfe32": Phase="Pending", Reason="", readiness=false. Elapsed: 8.029757186s
Mar 12 08:32:19.304: INFO: Pod "pod-configmaps-5fa95635-b935-4148-9b02-8796789bfe32": Phase="Pending", Reason="", readiness=false. Elapsed: 10.034421026s
Mar 12 08:32:21.309: INFO: Pod "pod-configmaps-5fa95635-b935-4148-9b02-8796789bfe32": Phase="Pending", Reason="", readiness=false. Elapsed: 12.039300131s
Mar 12 08:32:23.314: INFO: Pod "pod-configmaps-5fa95635-b935-4148-9b02-8796789bfe32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.044503929s
STEP: Saw pod success
Mar 12 08:32:23.314: INFO: Pod "pod-configmaps-5fa95635-b935-4148-9b02-8796789bfe32" satisfied condition "success or failure"
Mar 12 08:32:23.318: INFO: Trying to get logs from node node-2 pod pod-configmaps-5fa95635-b935-4148-9b02-8796789bfe32 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 12 08:32:23.353: INFO: Waiting for pod pod-configmaps-5fa95635-b935-4148-9b02-8796789bfe32 to disappear
Mar 12 08:32:23.356: INFO: Pod pod-configmaps-5fa95635-b935-4148-9b02-8796789bfe32 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:32:23.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5249" for this suite.
Mar 12 08:32:29.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:32:29.465: INFO: namespace configmap-5249 deletion completed in 6.103147281s

• [SLOW TEST:20.266 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:32:29.465: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar 12 08:32:29.511: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 08:32:38.000: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:32:56.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7601" for this suite.
Mar 12 08:33:02.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:33:02.664: INFO: namespace crd-publish-openapi-7601 deletion completed in 6.109468017s

• [SLOW TEST:33.199 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:33:02.665: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Mar 12 08:33:02.718: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Mar 12 08:33:02.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 create -f - --namespace=kubectl-1706'
Mar 12 08:33:03.401: INFO: stderr: ""
Mar 12 08:33:03.401: INFO: stdout: "service/redis-slave created\n"
Mar 12 08:33:03.401: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Mar 12 08:33:03.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 create -f - --namespace=kubectl-1706'
Mar 12 08:33:03.709: INFO: stderr: ""
Mar 12 08:33:03.709: INFO: stdout: "service/redis-master created\n"
Mar 12 08:33:03.709: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 12 08:33:03.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 create -f - --namespace=kubectl-1706'
Mar 12 08:33:03.987: INFO: stderr: ""
Mar 12 08:33:03.987: INFO: stdout: "service/frontend created\n"
Mar 12 08:33:03.987: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Mar 12 08:33:03.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 create -f - --namespace=kubectl-1706'
Mar 12 08:33:04.255: INFO: stderr: ""
Mar 12 08:33:04.255: INFO: stdout: "deployment.apps/frontend created\n"
Mar 12 08:33:04.256: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 12 08:33:04.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 create -f - --namespace=kubectl-1706'
Mar 12 08:33:04.464: INFO: stderr: ""
Mar 12 08:33:04.464: INFO: stdout: "deployment.apps/redis-master created\n"
Mar 12 08:33:04.464: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Mar 12 08:33:04.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 create -f - --namespace=kubectl-1706'
Mar 12 08:33:04.734: INFO: stderr: ""
Mar 12 08:33:04.734: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Mar 12 08:33:04.734: INFO: Waiting for all frontend pods to be Running.
Mar 12 08:33:24.786: INFO: Waiting for frontend to serve content.
Mar 12 08:33:24.794: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: 
Mar 12 08:33:29.801: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: 
Mar 12 08:33:35.715: INFO: Trying to add a new entry to the guestbook.
Mar 12 08:33:36.332: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar 12 08:33:36.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 delete --grace-period=0 --force -f - --namespace=kubectl-1706'
Mar 12 08:33:36.481: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 12 08:33:36.481: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Mar 12 08:33:36.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 delete --grace-period=0 --force -f - --namespace=kubectl-1706'
Mar 12 08:33:36.605: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 12 08:33:36.605: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar 12 08:33:36.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 delete --grace-period=0 --force -f - --namespace=kubectl-1706'
Mar 12 08:33:36.708: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 12 08:33:36.708: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 12 08:33:36.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 delete --grace-period=0 --force -f - --namespace=kubectl-1706'
Mar 12 08:33:36.801: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 12 08:33:36.801: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 12 08:33:36.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 delete --grace-period=0 --force -f - --namespace=kubectl-1706'
Mar 12 08:33:37.469: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 12 08:33:37.469: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar 12 08:33:37.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 delete --grace-period=0 --force -f - --namespace=kubectl-1706'
Mar 12 08:33:38.104: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 12 08:33:38.104: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:33:38.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1706" for this suite.
Mar 12 08:33:44.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:33:44.226: INFO: namespace kubectl-1706 deletion completed in 6.115813428s

• [SLOW TEST:41.561 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:33:44.226: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar 12 08:33:44.292: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6535 /api/v1/namespaces/watch-6535/configmaps/e2e-watch-test-configmap-a e54c780f-6531-4f0c-9848-14cb23ed8a52 150086 0 2020-03-12 08:33:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 12 08:33:44.292: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6535 /api/v1/namespaces/watch-6535/configmaps/e2e-watch-test-configmap-a e54c780f-6531-4f0c-9848-14cb23ed8a52 150086 0 2020-03-12 08:33:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar 12 08:33:54.303: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6535 /api/v1/namespaces/watch-6535/configmaps/e2e-watch-test-configmap-a e54c780f-6531-4f0c-9848-14cb23ed8a52 150104 0 2020-03-12 08:33:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar 12 08:33:54.303: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6535 /api/v1/namespaces/watch-6535/configmaps/e2e-watch-test-configmap-a e54c780f-6531-4f0c-9848-14cb23ed8a52 150104 0 2020-03-12 08:33:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar 12 08:34:04.314: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6535 /api/v1/namespaces/watch-6535/configmaps/e2e-watch-test-configmap-a e54c780f-6531-4f0c-9848-14cb23ed8a52 150122 0 2020-03-12 08:33:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 12 08:34:04.314: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6535 /api/v1/namespaces/watch-6535/configmaps/e2e-watch-test-configmap-a e54c780f-6531-4f0c-9848-14cb23ed8a52 150122 0 2020-03-12 08:33:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar 12 08:34:14.326: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6535 /api/v1/namespaces/watch-6535/configmaps/e2e-watch-test-configmap-a e54c780f-6531-4f0c-9848-14cb23ed8a52 150139 0 2020-03-12 08:33:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 12 08:34:14.326: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6535 /api/v1/namespaces/watch-6535/configmaps/e2e-watch-test-configmap-a e54c780f-6531-4f0c-9848-14cb23ed8a52 150139 0 2020-03-12 08:33:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar 12 08:34:24.338: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6535 /api/v1/namespaces/watch-6535/configmaps/e2e-watch-test-configmap-b ace8c765-b127-4f42-9ff8-4ab659290953 150156 0 2020-03-12 08:34:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 12 08:34:24.338: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6535 /api/v1/namespaces/watch-6535/configmaps/e2e-watch-test-configmap-b ace8c765-b127-4f42-9ff8-4ab659290953 150156 0 2020-03-12 08:34:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar 12 08:34:34.349: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6535 /api/v1/namespaces/watch-6535/configmaps/e2e-watch-test-configmap-b ace8c765-b127-4f42-9ff8-4ab659290953 150174 0 2020-03-12 08:34:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 12 08:34:34.349: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6535 /api/v1/namespaces/watch-6535/configmaps/e2e-watch-test-configmap-b ace8c765-b127-4f42-9ff8-4ab659290953 150174 0 2020-03-12 08:34:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:34:44.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6535" for this suite.
Mar 12 08:34:50.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:34:50.462: INFO: namespace watch-6535 deletion completed in 6.106356354s

• [SLOW TEST:66.236 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:34:50.463: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-73416913-ab25-417a-bf78-c96d9aba8dfb
STEP: Creating a pod to test consume configMaps
Mar 12 08:34:50.540: INFO: Waiting up to 5m0s for pod "pod-configmaps-34ef87d8-15ae-4088-98e1-2fafff267d12" in namespace "configmap-2410" to be "success or failure"
Mar 12 08:34:50.543: INFO: Pod "pod-configmaps-34ef87d8-15ae-4088-98e1-2fafff267d12": Phase="Pending", Reason="", readiness=false. Elapsed: 3.101071ms
Mar 12 08:34:52.548: INFO: Pod "pod-configmaps-34ef87d8-15ae-4088-98e1-2fafff267d12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008195994s
Mar 12 08:34:54.553: INFO: Pod "pod-configmaps-34ef87d8-15ae-4088-98e1-2fafff267d12": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012845663s
Mar 12 08:34:56.567: INFO: Pod "pod-configmaps-34ef87d8-15ae-4088-98e1-2fafff267d12": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02735463s
Mar 12 08:34:58.572: INFO: Pod "pod-configmaps-34ef87d8-15ae-4088-98e1-2fafff267d12": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032085577s
Mar 12 08:35:00.576: INFO: Pod "pod-configmaps-34ef87d8-15ae-4088-98e1-2fafff267d12": Phase="Pending", Reason="", readiness=false. Elapsed: 10.035819273s
Mar 12 08:35:02.580: INFO: Pod "pod-configmaps-34ef87d8-15ae-4088-98e1-2fafff267d12": Phase="Pending", Reason="", readiness=false. Elapsed: 12.039824034s
Mar 12 08:35:04.584: INFO: Pod "pod-configmaps-34ef87d8-15ae-4088-98e1-2fafff267d12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.044379112s
STEP: Saw pod success
Mar 12 08:35:04.584: INFO: Pod "pod-configmaps-34ef87d8-15ae-4088-98e1-2fafff267d12" satisfied condition "success or failure"
Mar 12 08:35:04.589: INFO: Trying to get logs from node node-2 pod pod-configmaps-34ef87d8-15ae-4088-98e1-2fafff267d12 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 12 08:35:04.621: INFO: Waiting for pod pod-configmaps-34ef87d8-15ae-4088-98e1-2fafff267d12 to disappear
Mar 12 08:35:04.623: INFO: Pod pod-configmaps-34ef87d8-15ae-4088-98e1-2fafff267d12 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:35:04.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2410" for this suite.
Mar 12 08:35:10.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:35:10.740: INFO: namespace configmap-2410 deletion completed in 6.113352264s

• [SLOW TEST:20.277 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:35:10.741: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:36:06.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8069" for this suite.
Mar 12 08:36:12.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:36:12.329: INFO: namespace container-runtime-8069 deletion completed in 6.127107692s

• [SLOW TEST:61.589 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:36:12.330: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-6824
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 12 08:36:12.375: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 12 08:36:58.501: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.65.169:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6824 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 12 08:36:58.501: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 08:36:58.762: INFO: Found all expected endpoints: [netserver-0]
Mar 12 08:36:58.771: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.85:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6824 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 12 08:36:58.771: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 08:36:58.856: INFO: Found all expected endpoints: [netserver-1]
Mar 12 08:36:58.861: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.66.44:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6824 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 12 08:36:58.861: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 08:36:58.939: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:36:58.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6824" for this suite.
Mar 12 08:37:10.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:37:11.077: INFO: namespace pod-network-test-6824 deletion completed in 12.13217213s

• [SLOW TEST:58.748 seconds]
[sig-network] Networking
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:37:11.078: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-2517c1c4-20bb-4eb2-91ac-3405e5fd3955
STEP: Creating a pod to test consume configMaps
Mar 12 08:37:11.155: INFO: Waiting up to 5m0s for pod "pod-configmaps-7700dd22-0ab2-4d97-8baf-6dffd5afe9cb" in namespace "configmap-2687" to be "success or failure"
Mar 12 08:37:11.170: INFO: Pod "pod-configmaps-7700dd22-0ab2-4d97-8baf-6dffd5afe9cb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.80697ms
Mar 12 08:37:13.174: INFO: Pod "pod-configmaps-7700dd22-0ab2-4d97-8baf-6dffd5afe9cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019554772s
Mar 12 08:37:15.181: INFO: Pod "pod-configmaps-7700dd22-0ab2-4d97-8baf-6dffd5afe9cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025839566s
Mar 12 08:37:17.185: INFO: Pod "pod-configmaps-7700dd22-0ab2-4d97-8baf-6dffd5afe9cb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030199755s
Mar 12 08:37:19.189: INFO: Pod "pod-configmaps-7700dd22-0ab2-4d97-8baf-6dffd5afe9cb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.0343782s
Mar 12 08:37:21.194: INFO: Pod "pod-configmaps-7700dd22-0ab2-4d97-8baf-6dffd5afe9cb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.039227209s
Mar 12 08:37:23.199: INFO: Pod "pod-configmaps-7700dd22-0ab2-4d97-8baf-6dffd5afe9cb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.04414293s
Mar 12 08:37:25.204: INFO: Pod "pod-configmaps-7700dd22-0ab2-4d97-8baf-6dffd5afe9cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.048803202s
STEP: Saw pod success
Mar 12 08:37:25.204: INFO: Pod "pod-configmaps-7700dd22-0ab2-4d97-8baf-6dffd5afe9cb" satisfied condition "success or failure"
Mar 12 08:37:25.207: INFO: Trying to get logs from node node-3 pod pod-configmaps-7700dd22-0ab2-4d97-8baf-6dffd5afe9cb container configmap-volume-test: <nil>
STEP: delete the pod
Mar 12 08:37:25.237: INFO: Waiting for pod pod-configmaps-7700dd22-0ab2-4d97-8baf-6dffd5afe9cb to disappear
Mar 12 08:37:25.239: INFO: Pod pod-configmaps-7700dd22-0ab2-4d97-8baf-6dffd5afe9cb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:37:25.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2687" for this suite.
Mar 12 08:37:31.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:37:31.350: INFO: namespace configmap-2687 deletion completed in 6.106281892s

• [SLOW TEST:20.272 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:37:31.350: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Mar 12 08:37:45.938: INFO: Successfully updated pod "adopt-release-d9t7s"
STEP: Checking that the Job readopts the Pod
Mar 12 08:37:45.938: INFO: Waiting up to 15m0s for pod "adopt-release-d9t7s" in namespace "job-5248" to be "adopted"
Mar 12 08:37:45.944: INFO: Pod "adopt-release-d9t7s": Phase="Running", Reason="", readiness=true. Elapsed: 6.1443ms
Mar 12 08:37:47.949: INFO: Pod "adopt-release-d9t7s": Phase="Running", Reason="", readiness=true. Elapsed: 2.011055686s
Mar 12 08:37:47.949: INFO: Pod "adopt-release-d9t7s" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Mar 12 08:37:48.474: INFO: Successfully updated pod "adopt-release-d9t7s"
STEP: Checking that the Job releases the Pod
Mar 12 08:37:48.474: INFO: Waiting up to 15m0s for pod "adopt-release-d9t7s" in namespace "job-5248" to be "released"
Mar 12 08:37:48.485: INFO: Pod "adopt-release-d9t7s": Phase="Running", Reason="", readiness=true. Elapsed: 10.176475ms
Mar 12 08:37:48.485: INFO: Pod "adopt-release-d9t7s" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:37:48.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5248" for this suite.
Mar 12 08:38:32.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:38:32.625: INFO: namespace job-5248 deletion completed in 44.135027215s

• [SLOW TEST:61.275 seconds]
[sig-apps] Job
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:38:32.625: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 08:38:32.689: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar 12 08:38:37.694: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 12 08:38:45.703: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar 12 08:38:45.731: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6388 /apis/apps/v1/namespaces/deployment-6388/deployments/test-cleanup-deployment 8240f2ac-f6e5-499e-ba68-560277fc6680 150903 1 2020-03-12 08:38:45 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0045905d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar 12 08:38:45.740: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-6388 /apis/apps/v1/namespaces/deployment-6388/replicasets/test-cleanup-deployment-65db99849b f3f57082-f798-4034-801a-669d5ad5e162 150905 1 2020-03-12 08:38:45 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 8240f2ac-f6e5-499e-ba68-560277fc6680 0xc0046b99b7 0xc0046b99b8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0046b9a18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 12 08:38:45.740: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar 12 08:38:45.740: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-6388 /apis/apps/v1/namespaces/deployment-6388/replicasets/test-cleanup-controller 86f343ad-ab50-4fcf-91cf-d52acf06c3a7 150904 1 2020-03-12 08:38:32 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 8240f2ac-f6e5-499e-ba68-560277fc6680 0xc0046b98e7 0xc0046b98e8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0046b9948 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 12 08:38:45.747: INFO: Pod "test-cleanup-controller-qwlm9" is available:
&Pod{ObjectMeta:{test-cleanup-controller-qwlm9 test-cleanup-controller- deployment-6388 /api/v1/namespaces/deployment-6388/pods/test-cleanup-controller-qwlm9 0f2021c3-5f98-4595-bdf2-731fcc2bf2f9 150900 0 2020-03-12 08:38:32 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 86f343ad-ab50-4fcf-91cf-d52acf06c3a7 0xc005b2d057 0xc005b2d058}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2zlnn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2zlnn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2zlnn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 08:38:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 08:38:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 08:38:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 08:38:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.20.4,PodIP:10.233.64.87,StartTime:2020-03-12 08:38:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-12 08:38:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://6a0184c82330b42ccc108724d9a6700c60dde24aea29642f563c9626e378913d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:38:45.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6388" for this suite.
Mar 12 08:38:51.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:38:51.921: INFO: namespace deployment-6388 deletion completed in 6.164503299s

• [SLOW TEST:19.296 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:38:51.921: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-61c464c0-43ca-4c72-a399-28566bec10a5
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:39:06.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7154" for this suite.
Mar 12 08:39:24.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:39:24.181: INFO: namespace configmap-7154 deletion completed in 18.134212724s

• [SLOW TEST:32.260 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:39:24.182: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 08:39:24.234: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:39:38.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9382" for this suite.
Mar 12 08:40:22.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:40:22.596: INFO: namespace pods-9382 deletion completed in 44.241444051s

• [SLOW TEST:58.415 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:40:22.596: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 12 08:40:22.666: INFO: Waiting up to 5m0s for pod "pod-a9d02b50-1f6a-4d70-9ce6-f6ba134b2b7c" in namespace "emptydir-625" to be "success or failure"
Mar 12 08:40:22.670: INFO: Pod "pod-a9d02b50-1f6a-4d70-9ce6-f6ba134b2b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.219918ms
Mar 12 08:40:24.675: INFO: Pod "pod-a9d02b50-1f6a-4d70-9ce6-f6ba134b2b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009121891s
Mar 12 08:40:26.680: INFO: Pod "pod-a9d02b50-1f6a-4d70-9ce6-f6ba134b2b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014584699s
Mar 12 08:40:28.685: INFO: Pod "pod-a9d02b50-1f6a-4d70-9ce6-f6ba134b2b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019079448s
Mar 12 08:40:30.690: INFO: Pod "pod-a9d02b50-1f6a-4d70-9ce6-f6ba134b2b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024086154s
Mar 12 08:40:32.694: INFO: Pod "pod-a9d02b50-1f6a-4d70-9ce6-f6ba134b2b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.028553645s
Mar 12 08:40:34.699: INFO: Pod "pod-a9d02b50-1f6a-4d70-9ce6-f6ba134b2b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.033272206s
Mar 12 08:40:36.705: INFO: Pod "pod-a9d02b50-1f6a-4d70-9ce6-f6ba134b2b7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.038950627s
STEP: Saw pod success
Mar 12 08:40:36.705: INFO: Pod "pod-a9d02b50-1f6a-4d70-9ce6-f6ba134b2b7c" satisfied condition "success or failure"
Mar 12 08:40:36.708: INFO: Trying to get logs from node node-3 pod pod-a9d02b50-1f6a-4d70-9ce6-f6ba134b2b7c container test-container: <nil>
STEP: delete the pod
Mar 12 08:40:36.744: INFO: Waiting for pod pod-a9d02b50-1f6a-4d70-9ce6-f6ba134b2b7c to disappear
Mar 12 08:40:36.747: INFO: Pod pod-a9d02b50-1f6a-4d70-9ce6-f6ba134b2b7c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:40:36.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-625" for this suite.
Mar 12 08:40:42.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:40:42.875: INFO: namespace emptydir-625 deletion completed in 6.121629669s

• [SLOW TEST:20.279 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:40:42.875: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 12 08:40:43.358: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 12 08:40:45.371: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:40:47.376: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:40:49.377: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:40:51.375: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:40:53.376: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:40:55.375: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599243, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 12 08:40:58.391: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:41:10.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6736" for this suite.
Mar 12 08:41:16.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:41:16.686: INFO: namespace webhook-6736 deletion completed in 6.123264765s
STEP: Destroying namespace "webhook-6736-markers" for this suite.
Mar 12 08:41:22.704: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:41:22.792: INFO: namespace webhook-6736-markers deletion completed in 6.106436494s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:39.936 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:41:22.811: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 12 08:41:22.873: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6159e1e8-1935-4c36-ad67-52de9eab2a20" in namespace "projected-2087" to be "success or failure"
Mar 12 08:41:22.876: INFO: Pod "downwardapi-volume-6159e1e8-1935-4c36-ad67-52de9eab2a20": Phase="Pending", Reason="", readiness=false. Elapsed: 3.765152ms
Mar 12 08:41:24.881: INFO: Pod "downwardapi-volume-6159e1e8-1935-4c36-ad67-52de9eab2a20": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008772564s
Mar 12 08:41:26.886: INFO: Pod "downwardapi-volume-6159e1e8-1935-4c36-ad67-52de9eab2a20": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013333111s
Mar 12 08:41:28.891: INFO: Pod "downwardapi-volume-6159e1e8-1935-4c36-ad67-52de9eab2a20": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018315834s
Mar 12 08:41:30.895: INFO: Pod "downwardapi-volume-6159e1e8-1935-4c36-ad67-52de9eab2a20": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022478871s
Mar 12 08:41:32.900: INFO: Pod "downwardapi-volume-6159e1e8-1935-4c36-ad67-52de9eab2a20": Phase="Pending", Reason="", readiness=false. Elapsed: 10.027363692s
Mar 12 08:41:34.905: INFO: Pod "downwardapi-volume-6159e1e8-1935-4c36-ad67-52de9eab2a20": Phase="Pending", Reason="", readiness=false. Elapsed: 12.032254532s
Mar 12 08:41:36.910: INFO: Pod "downwardapi-volume-6159e1e8-1935-4c36-ad67-52de9eab2a20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.037373971s
STEP: Saw pod success
Mar 12 08:41:36.910: INFO: Pod "downwardapi-volume-6159e1e8-1935-4c36-ad67-52de9eab2a20" satisfied condition "success or failure"
Mar 12 08:41:36.913: INFO: Trying to get logs from node node-2 pod downwardapi-volume-6159e1e8-1935-4c36-ad67-52de9eab2a20 container client-container: <nil>
STEP: delete the pod
Mar 12 08:41:36.951: INFO: Waiting for pod downwardapi-volume-6159e1e8-1935-4c36-ad67-52de9eab2a20 to disappear
Mar 12 08:41:36.954: INFO: Pod downwardapi-volume-6159e1e8-1935-4c36-ad67-52de9eab2a20 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:41:36.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2087" for this suite.
Mar 12 08:41:42.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:41:43.074: INFO: namespace projected-2087 deletion completed in 6.115899332s

• [SLOW TEST:20.263 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:41:43.076: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 12 08:41:43.140: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b7ae1547-f05d-46d3-97b4-d163c0bba8b6" in namespace "downward-api-3824" to be "success or failure"
Mar 12 08:41:43.142: INFO: Pod "downwardapi-volume-b7ae1547-f05d-46d3-97b4-d163c0bba8b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.803281ms
Mar 12 08:41:45.147: INFO: Pod "downwardapi-volume-b7ae1547-f05d-46d3-97b4-d163c0bba8b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007761195s
Mar 12 08:41:47.153: INFO: Pod "downwardapi-volume-b7ae1547-f05d-46d3-97b4-d163c0bba8b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01301327s
Mar 12 08:41:49.159: INFO: Pod "downwardapi-volume-b7ae1547-f05d-46d3-97b4-d163c0bba8b6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019464745s
Mar 12 08:41:51.164: INFO: Pod "downwardapi-volume-b7ae1547-f05d-46d3-97b4-d163c0bba8b6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024122955s
Mar 12 08:41:53.168: INFO: Pod "downwardapi-volume-b7ae1547-f05d-46d3-97b4-d163c0bba8b6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.028673896s
Mar 12 08:41:55.172: INFO: Pod "downwardapi-volume-b7ae1547-f05d-46d3-97b4-d163c0bba8b6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.032785823s
Mar 12 08:41:57.177: INFO: Pod "downwardapi-volume-b7ae1547-f05d-46d3-97b4-d163c0bba8b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.037569836s
STEP: Saw pod success
Mar 12 08:41:57.177: INFO: Pod "downwardapi-volume-b7ae1547-f05d-46d3-97b4-d163c0bba8b6" satisfied condition "success or failure"
Mar 12 08:41:57.180: INFO: Trying to get logs from node node-3 pod downwardapi-volume-b7ae1547-f05d-46d3-97b4-d163c0bba8b6 container client-container: <nil>
STEP: delete the pod
Mar 12 08:41:57.202: INFO: Waiting for pod downwardapi-volume-b7ae1547-f05d-46d3-97b4-d163c0bba8b6 to disappear
Mar 12 08:41:57.206: INFO: Pod downwardapi-volume-b7ae1547-f05d-46d3-97b4-d163c0bba8b6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:41:57.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3824" for this suite.
Mar 12 08:42:03.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:42:03.319: INFO: namespace downward-api-3824 deletion completed in 6.107923255s

• [SLOW TEST:20.243 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:42:03.319: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:42:14.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1985" for this suite.
Mar 12 08:42:20.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:42:20.557: INFO: namespace resourcequota-1985 deletion completed in 6.132705262s

• [SLOW TEST:17.238 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:42:20.558: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-6480
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6480 to expose endpoints map[]
Mar 12 08:42:20.661: INFO: successfully validated that service multi-endpoint-test in namespace services-6480 exposes endpoints map[] (8.285322ms elapsed)
STEP: Creating pod pod1 in namespace services-6480
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6480 to expose endpoints map[pod1:[100]]
Mar 12 08:42:24.735: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (4.05335473s elapsed, will retry)
Mar 12 08:42:29.774: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (9.092134925s elapsed, will retry)
Mar 12 08:42:33.811: INFO: successfully validated that service multi-endpoint-test in namespace services-6480 exposes endpoints map[pod1:[100]] (13.129479803s elapsed)
STEP: Creating pod pod2 in namespace services-6480
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6480 to expose endpoints map[pod1:[100] pod2:[101]]
Mar 12 08:42:37.884: INFO: Unexpected endpoints: found map[3f675304-8cdc-4fbd-8f07-bfa84d9f39d2:[100]], expected map[pod1:[100] pod2:[101]] (4.060773813s elapsed, will retry)
Mar 12 08:42:42.943: INFO: Unexpected endpoints: found map[3f675304-8cdc-4fbd-8f07-bfa84d9f39d2:[100]], expected map[pod1:[100] pod2:[101]] (9.119560329s elapsed, will retry)
Mar 12 08:42:46.992: INFO: successfully validated that service multi-endpoint-test in namespace services-6480 exposes endpoints map[pod1:[100] pod2:[101]] (13.168404845s elapsed)
STEP: Deleting pod pod1 in namespace services-6480
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6480 to expose endpoints map[pod2:[101]]
Mar 12 08:42:48.020: INFO: successfully validated that service multi-endpoint-test in namespace services-6480 exposes endpoints map[pod2:[101]] (1.017518297s elapsed)
STEP: Deleting pod pod2 in namespace services-6480
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6480 to expose endpoints map[]
Mar 12 08:42:49.035: INFO: successfully validated that service multi-endpoint-test in namespace services-6480 exposes endpoints map[] (1.007625229s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:42:49.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6480" for this suite.
Mar 12 08:42:55.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:42:55.163: INFO: namespace services-6480 deletion completed in 6.100849168s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:34.605 seconds]
[sig-network] Services
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:42:55.163: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-5777
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 12 08:42:55.208: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 12 08:43:39.309: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.182:8080/dial?request=hostName&protocol=http&host=10.233.64.92&port=8080&tries=1'] Namespace:pod-network-test-5777 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 12 08:43:39.309: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 08:43:39.445: INFO: Waiting for endpoints: map[]
Mar 12 08:43:39.449: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.182:8080/dial?request=hostName&protocol=http&host=10.233.65.181&port=8080&tries=1'] Namespace:pod-network-test-5777 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 12 08:43:39.449: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 08:43:39.540: INFO: Waiting for endpoints: map[]
Mar 12 08:43:39.544: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.182:8080/dial?request=hostName&protocol=http&host=10.233.66.45&port=8080&tries=1'] Namespace:pod-network-test-5777 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 12 08:43:39.544: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 08:43:39.614: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:43:39.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5777" for this suite.
Mar 12 08:43:51.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:43:51.743: INFO: namespace pod-network-test-5777 deletion completed in 12.122133109s

• [SLOW TEST:56.580 seconds]
[sig-network] Networking
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:43:51.743: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 12 08:43:51.818: INFO: Waiting up to 5m0s for pod "pod-26a5d8c7-98f9-45e0-bd84-758d5da30a88" in namespace "emptydir-901" to be "success or failure"
Mar 12 08:43:51.822: INFO: Pod "pod-26a5d8c7-98f9-45e0-bd84-758d5da30a88": Phase="Pending", Reason="", readiness=false. Elapsed: 3.485835ms
Mar 12 08:43:53.827: INFO: Pod "pod-26a5d8c7-98f9-45e0-bd84-758d5da30a88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008534916s
Mar 12 08:43:55.831: INFO: Pod "pod-26a5d8c7-98f9-45e0-bd84-758d5da30a88": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013347852s
Mar 12 08:43:57.837: INFO: Pod "pod-26a5d8c7-98f9-45e0-bd84-758d5da30a88": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018519969s
Mar 12 08:43:59.842: INFO: Pod "pod-26a5d8c7-98f9-45e0-bd84-758d5da30a88": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023464466s
Mar 12 08:44:01.846: INFO: Pod "pod-26a5d8c7-98f9-45e0-bd84-758d5da30a88": Phase="Pending", Reason="", readiness=false. Elapsed: 10.028275092s
Mar 12 08:44:03.852: INFO: Pod "pod-26a5d8c7-98f9-45e0-bd84-758d5da30a88": Phase="Pending", Reason="", readiness=false. Elapsed: 12.033558251s
Mar 12 08:44:05.856: INFO: Pod "pod-26a5d8c7-98f9-45e0-bd84-758d5da30a88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.03819407s
STEP: Saw pod success
Mar 12 08:44:05.856: INFO: Pod "pod-26a5d8c7-98f9-45e0-bd84-758d5da30a88" satisfied condition "success or failure"
Mar 12 08:44:05.860: INFO: Trying to get logs from node node-3 pod pod-26a5d8c7-98f9-45e0-bd84-758d5da30a88 container test-container: <nil>
STEP: delete the pod
Mar 12 08:44:05.893: INFO: Waiting for pod pod-26a5d8c7-98f9-45e0-bd84-758d5da30a88 to disappear
Mar 12 08:44:05.896: INFO: Pod pod-26a5d8c7-98f9-45e0-bd84-758d5da30a88 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:44:05.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-901" for this suite.
Mar 12 08:44:11.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:44:12.021: INFO: namespace emptydir-901 deletion completed in 6.120291962s

• [SLOW TEST:20.277 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:44:12.021: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 12 08:44:12.089: INFO: Waiting up to 5m0s for pod "pod-cd67b417-eb0c-49d6-b273-75e75e6e1921" in namespace "emptydir-879" to be "success or failure"
Mar 12 08:44:12.092: INFO: Pod "pod-cd67b417-eb0c-49d6-b273-75e75e6e1921": Phase="Pending", Reason="", readiness=false. Elapsed: 3.239001ms
Mar 12 08:44:14.097: INFO: Pod "pod-cd67b417-eb0c-49d6-b273-75e75e6e1921": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007780932s
Mar 12 08:44:16.102: INFO: Pod "pod-cd67b417-eb0c-49d6-b273-75e75e6e1921": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01281637s
Mar 12 08:44:18.107: INFO: Pod "pod-cd67b417-eb0c-49d6-b273-75e75e6e1921": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018130808s
Mar 12 08:44:20.113: INFO: Pod "pod-cd67b417-eb0c-49d6-b273-75e75e6e1921": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023916068s
Mar 12 08:44:22.118: INFO: Pod "pod-cd67b417-eb0c-49d6-b273-75e75e6e1921": Phase="Pending", Reason="", readiness=false. Elapsed: 10.028693627s
Mar 12 08:44:24.122: INFO: Pod "pod-cd67b417-eb0c-49d6-b273-75e75e6e1921": Phase="Pending", Reason="", readiness=false. Elapsed: 12.033583561s
Mar 12 08:44:26.128: INFO: Pod "pod-cd67b417-eb0c-49d6-b273-75e75e6e1921": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.039240207s
STEP: Saw pod success
Mar 12 08:44:26.128: INFO: Pod "pod-cd67b417-eb0c-49d6-b273-75e75e6e1921" satisfied condition "success or failure"
Mar 12 08:44:26.132: INFO: Trying to get logs from node node-2 pod pod-cd67b417-eb0c-49d6-b273-75e75e6e1921 container test-container: <nil>
STEP: delete the pod
Mar 12 08:44:26.165: INFO: Waiting for pod pod-cd67b417-eb0c-49d6-b273-75e75e6e1921 to disappear
Mar 12 08:44:26.168: INFO: Pod pod-cd67b417-eb0c-49d6-b273-75e75e6e1921 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:44:26.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-879" for this suite.
Mar 12 08:44:32.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:44:32.267: INFO: namespace emptydir-879 deletion completed in 6.095259773s

• [SLOW TEST:20.246 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:44:32.268: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 12 08:44:46.858: INFO: Successfully updated pod "pod-update-fd38c5ce-ac1b-47d2-beb6-589049eb9694"
STEP: verifying the updated pod is in kubernetes
Mar 12 08:44:46.866: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:44:46.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1933" for this suite.
Mar 12 08:44:58.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:44:58.979: INFO: namespace pods-1933 deletion completed in 12.10492421s

• [SLOW TEST:26.711 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:44:58.979: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 12 08:45:27.106: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 12 08:45:27.110: INFO: Pod pod-with-poststart-http-hook still exists
Mar 12 08:45:29.110: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 12 08:45:29.115: INFO: Pod pod-with-poststart-http-hook still exists
Mar 12 08:45:31.110: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 12 08:45:31.116: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:45:31.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7031" for this suite.
Mar 12 08:45:43.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:45:43.233: INFO: namespace container-lifecycle-hook-7031 deletion completed in 12.111709875s

• [SLOW TEST:44.254 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:45:43.233: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-727a05d7-390b-4b2d-bd4f-b26bce257e76
STEP: Creating secret with name s-test-opt-upd-bf43f8b6-8081-4392-bbe8-3abe2c58b3f8
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-727a05d7-390b-4b2d-bd4f-b26bce257e76
STEP: Updating secret s-test-opt-upd-bf43f8b6-8081-4392-bbe8-3abe2c58b3f8
STEP: Creating secret with name s-test-opt-create-91d1dd6f-9efb-4d2b-b7c4-1a682734633e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:46:03.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7722" for this suite.
Mar 12 08:46:27.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:46:27.563: INFO: namespace projected-7722 deletion completed in 24.110131681s

• [SLOW TEST:44.330 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:46:27.564: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 08:47:03.632: INFO: Container started at 2020-03-12 08:46:39 +0000 UTC, pod became ready at 2020-03-12 08:47:03 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:47:03.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6202" for this suite.
Mar 12 08:47:15.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:47:15.750: INFO: namespace container-probe-6202 deletion completed in 12.113010548s

• [SLOW TEST:48.186 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:47:15.750: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 08:47:29.844: INFO: Waiting up to 5m0s for pod "client-envvars-587c5dae-13bc-4bd2-8de5-ace6f516626c" in namespace "pods-1520" to be "success or failure"
Mar 12 08:47:29.848: INFO: Pod "client-envvars-587c5dae-13bc-4bd2-8de5-ace6f516626c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.165307ms
Mar 12 08:47:31.853: INFO: Pod "client-envvars-587c5dae-13bc-4bd2-8de5-ace6f516626c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008365583s
Mar 12 08:47:33.858: INFO: Pod "client-envvars-587c5dae-13bc-4bd2-8de5-ace6f516626c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013485032s
Mar 12 08:47:35.863: INFO: Pod "client-envvars-587c5dae-13bc-4bd2-8de5-ace6f516626c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018837882s
Mar 12 08:47:37.870: INFO: Pod "client-envvars-587c5dae-13bc-4bd2-8de5-ace6f516626c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025570686s
Mar 12 08:47:39.875: INFO: Pod "client-envvars-587c5dae-13bc-4bd2-8de5-ace6f516626c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.03096862s
Mar 12 08:47:41.880: INFO: Pod "client-envvars-587c5dae-13bc-4bd2-8de5-ace6f516626c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.035607063s
Mar 12 08:47:43.885: INFO: Pod "client-envvars-587c5dae-13bc-4bd2-8de5-ace6f516626c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.040509767s
STEP: Saw pod success
Mar 12 08:47:43.885: INFO: Pod "client-envvars-587c5dae-13bc-4bd2-8de5-ace6f516626c" satisfied condition "success or failure"
Mar 12 08:47:43.889: INFO: Trying to get logs from node node-2 pod client-envvars-587c5dae-13bc-4bd2-8de5-ace6f516626c container env3cont: <nil>
STEP: delete the pod
Mar 12 08:47:43.931: INFO: Waiting for pod client-envvars-587c5dae-13bc-4bd2-8de5-ace6f516626c to disappear
Mar 12 08:47:43.936: INFO: Pod client-envvars-587c5dae-13bc-4bd2-8de5-ace6f516626c no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:47:43.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1520" for this suite.
Mar 12 08:48:11.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:48:12.064: INFO: namespace pods-1520 deletion completed in 28.122320453s

• [SLOW TEST:56.314 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:48:12.065: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Mar 12 08:48:12.116: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 12 08:48:12.130: INFO: Waiting for terminating namespaces to be deleted...
Mar 12 08:48:12.134: INFO: 
Logging pods the kubelet thinks is on node node-1 before test
Mar 12 08:48:12.159: INFO: kube-apiserver-node-1 from kube-system started at 2020-03-12 07:03:22 +0000 UTC (1 container statuses recorded)
Mar 12 08:48:12.159: INFO: 	Container kube-apiserver ready: true, restart count 1
Mar 12 08:48:12.159: INFO: kube-controller-manager-node-1 from kube-system started at 2020-03-11 13:01:20 +0000 UTC (1 container statuses recorded)
Mar 12 08:48:12.159: INFO: 	Container kube-controller-manager ready: true, restart count 4
Mar 12 08:48:12.159: INFO: coredns-p4prs from kube-system started at 2020-03-11 16:39:41 +0000 UTC (1 container statuses recorded)
Mar 12 08:48:12.159: INFO: 	Container coredns ready: true, restart count 0
Mar 12 08:48:12.159: INFO: sonobuoy from sonobuoy started at 2020-03-12 07:11:13 +0000 UTC (1 container statuses recorded)
Mar 12 08:48:12.159: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 12 08:48:12.159: INFO: k8s-keystone-auth-7n8pn from kube-system started at 2020-03-11 16:39:41 +0000 UTC (1 container statuses recorded)
Mar 12 08:48:12.159: INFO: 	Container k8s-keystone-auth ready: true, restart count 0
Mar 12 08:48:12.159: INFO: sonobuoy-e2e-job-0ad0ccbfe03640eb from sonobuoy started at 2020-03-12 07:11:26 +0000 UTC (2 container statuses recorded)
Mar 12 08:48:12.159: INFO: 	Container e2e ready: true, restart count 0
Mar 12 08:48:12.159: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 12 08:48:12.159: INFO: kube-flannel-hgdc4 from kube-system started at 2020-03-11 16:40:11 +0000 UTC (2 container statuses recorded)
Mar 12 08:48:12.159: INFO: 	Container install-cni ready: true, restart count 0
Mar 12 08:48:12.159: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 12 08:48:12.159: INFO: kube-proxy-node-1 from kube-system started at 2020-03-11 11:52:32 +0000 UTC (1 container statuses recorded)
Mar 12 08:48:12.159: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 12 08:48:12.159: INFO: kube-scheduler-node-1 from kube-system started at 2020-03-11 11:53:05 +0000 UTC (1 container statuses recorded)
Mar 12 08:48:12.159: INFO: 	Container kube-scheduler ready: true, restart count 3
Mar 12 08:48:12.159: INFO: sonobuoy-systemd-logs-daemon-set-b6e3a2e4f0de4f41-fktq2 from sonobuoy started at 2020-03-12 07:11:26 +0000 UTC (2 container statuses recorded)
Mar 12 08:48:12.159: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 12 08:48:12.159: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 12 08:48:12.159: INFO: 
Logging pods the kubelet thinks is on node node-2 before test
Mar 12 08:48:12.170: INFO: kube-proxy-node-2 from kube-system started at 2020-03-11 11:52:32 +0000 UTC (1 container statuses recorded)
Mar 12 08:48:12.170: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 12 08:48:12.170: INFO: kube-scheduler-node-2 from kube-system started at 2020-03-11 11:53:05 +0000 UTC (1 container statuses recorded)
Mar 12 08:48:12.170: INFO: 	Container kube-scheduler ready: true, restart count 2
Mar 12 08:48:12.170: INFO: kube-flannel-rh9nn from kube-system started at 2020-03-12 08:01:51 +0000 UTC (2 container statuses recorded)
Mar 12 08:48:12.170: INFO: 	Container install-cni ready: true, restart count 0
Mar 12 08:48:12.170: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 12 08:48:12.170: INFO: kube-apiserver-node-2 from kube-system started at 2020-03-11 11:52:50 +0000 UTC (1 container statuses recorded)
Mar 12 08:48:12.170: INFO: 	Container kube-apiserver ready: true, restart count 0
Mar 12 08:48:12.170: INFO: k8s-keystone-auth-n7bw6 from kube-system started at 2020-03-12 08:01:11 +0000 UTC (1 container statuses recorded)
Mar 12 08:48:12.170: INFO: 	Container k8s-keystone-auth ready: true, restart count 0
Mar 12 08:48:12.170: INFO: coredns-w5zbd from kube-system started at 2020-03-12 08:01:21 +0000 UTC (1 container statuses recorded)
Mar 12 08:48:12.170: INFO: 	Container coredns ready: true, restart count 0
Mar 12 08:48:12.170: INFO: heapster-659dbd5d99-c8kfc from kube-system started at 2020-03-12 08:14:41 +0000 UTC (1 container statuses recorded)
Mar 12 08:48:12.170: INFO: 	Container heapster ready: true, restart count 0
Mar 12 08:48:12.170: INFO: kube-controller-manager-node-2 from kube-system started at 2020-03-11 13:01:20 +0000 UTC (1 container statuses recorded)
Mar 12 08:48:12.170: INFO: 	Container kube-controller-manager ready: true, restart count 2
Mar 12 08:48:12.170: INFO: tiller-deploy-7db6d94b4f-zcl5s from kube-system started at 2020-03-12 08:14:41 +0000 UTC (1 container statuses recorded)
Mar 12 08:48:12.170: INFO: 	Container tiller ready: true, restart count 0
Mar 12 08:48:12.170: INFO: sonobuoy-systemd-logs-daemon-set-b6e3a2e4f0de4f41-2gk9x from sonobuoy started at 2020-03-12 07:11:26 +0000 UTC (2 container statuses recorded)
Mar 12 08:48:12.170: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 12 08:48:12.170: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 12 08:48:12.170: INFO: 
Logging pods the kubelet thinks is on node node-3 before test
Mar 12 08:48:12.189: INFO: sonobuoy-systemd-logs-daemon-set-b6e3a2e4f0de4f41-q7m9f from sonobuoy started at 2020-03-12 07:11:26 +0000 UTC (2 container statuses recorded)
Mar 12 08:48:12.190: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 12 08:48:12.190: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 12 08:48:12.190: INFO: k8s-keystone-auth-qlh29 from kube-system started at 2020-03-12 08:15:11 +0000 UTC (1 container statuses recorded)
Mar 12 08:48:12.190: INFO: 	Container k8s-keystone-auth ready: true, restart count 0
Mar 12 08:48:12.190: INFO: coredns-rbm58 from kube-system started at 2020-03-12 08:15:11 +0000 UTC (1 container statuses recorded)
Mar 12 08:48:12.190: INFO: 	Container coredns ready: true, restart count 0
Mar 12 08:48:12.190: INFO: kube-apiserver-node-3 from kube-system started at 2020-03-11 11:52:50 +0000 UTC (1 container statuses recorded)
Mar 12 08:48:12.190: INFO: 	Container kube-apiserver ready: true, restart count 0
Mar 12 08:48:12.190: INFO: kube-monitor-1584002700-x84wn from kube-system started at 2020-03-12 08:45:01 +0000 UTC (1 container statuses recorded)
Mar 12 08:48:12.190: INFO: 	Container kube-monitor ready: false, restart count 0
Mar 12 08:48:12.190: INFO: kube-proxy-node-3 from kube-system started at 2020-03-11 11:52:32 +0000 UTC (1 container statuses recorded)
Mar 12 08:48:12.190: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 12 08:48:12.190: INFO: kube-flannel-fzr9c from kube-system started at 2020-03-12 08:15:16 +0000 UTC (2 container statuses recorded)
Mar 12 08:48:12.190: INFO: 	Container install-cni ready: true, restart count 0
Mar 12 08:48:12.190: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 12 08:48:12.190: INFO: kube-scheduler-node-3 from kube-system started at 2020-03-11 11:53:06 +0000 UTC (1 container statuses recorded)
Mar 12 08:48:12.190: INFO: 	Container kube-scheduler ready: true, restart count 4
Mar 12 08:48:12.190: INFO: kube-controller-manager-node-3 from kube-system started at 2020-03-11 13:01:20 +0000 UTC (1 container statuses recorded)
Mar 12 08:48:12.190: INFO: 	Container kube-controller-manager ready: true, restart count 3
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-a0804c8c-0248-4b1d-8530-748c3a27a5dc 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-a0804c8c-0248-4b1d-8530-748c3a27a5dc off the node node-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a0804c8c-0248-4b1d-8530-748c3a27a5dc
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:49:08.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7798" for this suite.
Mar 12 08:49:26.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:49:26.426: INFO: namespace sched-pred-7798 deletion completed in 18.105890964s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:74.361 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:49:26.426: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 12 08:49:26.977: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 12 08:49:28.995: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599767, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599767, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599767, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599766, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:49:31.000: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599767, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599767, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599767, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599766, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:49:33.000: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599767, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599767, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599767, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599766, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:49:35.000: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599767, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599767, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599767, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599766, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:49:37.000: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599767, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599767, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599767, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599766, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:49:39.000: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599767, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599767, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599767, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599766, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:49:41.000: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599767, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599767, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599767, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599766, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 12 08:49:44.014: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:49:44.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8563" for this suite.
Mar 12 08:49:50.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:49:50.187: INFO: namespace webhook-8563 deletion completed in 6.111752309s
STEP: Destroying namespace "webhook-8563-markers" for this suite.
Mar 12 08:49:56.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:49:56.289: INFO: namespace webhook-8563-markers deletion completed in 6.101875255s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:29.880 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:49:56.307: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 08:49:56.373: INFO: (0) /api/v1/nodes/node-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 16.762007ms)
Mar 12 08:49:56.379: INFO: (1) /api/v1/nodes/node-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 5.244528ms)
Mar 12 08:49:56.385: INFO: (2) /api/v1/nodes/node-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 5.741137ms)
Mar 12 08:49:56.390: INFO: (3) /api/v1/nodes/node-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 4.982465ms)
Mar 12 08:49:56.394: INFO: (4) /api/v1/nodes/node-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 4.034381ms)
Mar 12 08:49:56.397: INFO: (5) /api/v1/nodes/node-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 3.846221ms)
Mar 12 08:49:56.404: INFO: (6) /api/v1/nodes/node-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 6.122712ms)
Mar 12 08:49:56.409: INFO: (7) /api/v1/nodes/node-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 5.382368ms)
Mar 12 08:49:56.414: INFO: (8) /api/v1/nodes/node-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 4.859454ms)
Mar 12 08:49:56.419: INFO: (9) /api/v1/nodes/node-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 5.177198ms)
Mar 12 08:49:56.424: INFO: (10) /api/v1/nodes/node-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 5.229737ms)
Mar 12 08:49:56.431: INFO: (11) /api/v1/nodes/node-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 6.180457ms)
Mar 12 08:49:56.438: INFO: (12) /api/v1/nodes/node-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 7.494045ms)
Mar 12 08:49:56.443: INFO: (13) /api/v1/nodes/node-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 5.170682ms)
Mar 12 08:49:56.448: INFO: (14) /api/v1/nodes/node-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 4.66049ms)
Mar 12 08:49:56.453: INFO: (15) /api/v1/nodes/node-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 4.892946ms)
Mar 12 08:49:56.459: INFO: (16) /api/v1/nodes/node-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 5.614453ms)
Mar 12 08:49:56.464: INFO: (17) /api/v1/nodes/node-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 4.925144ms)
Mar 12 08:49:56.469: INFO: (18) /api/v1/nodes/node-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 4.948219ms)
Mar 12 08:49:56.473: INFO: (19) /api/v1/nodes/node-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="ansible.log">ansible.log</a>
<a href="audit/">au... (200; 4.474792ms)
[AfterEach] version v1
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:49:56.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3739" for this suite.
Mar 12 08:50:02.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:50:02.601: INFO: namespace proxy-3739 deletion completed in 6.124187526s

• [SLOW TEST:6.295 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:50:02.602: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 12 08:50:02.882: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 12 08:50:04.894: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:50:06.899: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:50:08.899: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:50:10.899: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:50:12.899: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 08:50:14.898: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719599802, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 12 08:50:17.918: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:50:18.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4640" for this suite.
Mar 12 08:50:24.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:50:24.341: INFO: namespace webhook-4640 deletion completed in 6.101946015s
STEP: Destroying namespace "webhook-4640-markers" for this suite.
Mar 12 08:50:30.357: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:50:30.446: INFO: namespace webhook-4640-markers deletion completed in 6.105061441s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.863 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:50:30.465: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Mar 12 08:51:01.076: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:51:01.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0312 08:51:01.076638      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-1625" for this suite.
Mar 12 08:51:07.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:51:07.176: INFO: namespace gc-1625 deletion completed in 6.094440715s

• [SLOW TEST:36.710 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:51:07.176: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 12 08:51:07.229: INFO: Waiting up to 5m0s for pod "pod-2535a051-4a81-44c3-9fd9-50921bfd4615" in namespace "emptydir-8860" to be "success or failure"
Mar 12 08:51:07.233: INFO: Pod "pod-2535a051-4a81-44c3-9fd9-50921bfd4615": Phase="Pending", Reason="", readiness=false. Elapsed: 4.126348ms
Mar 12 08:51:09.239: INFO: Pod "pod-2535a051-4a81-44c3-9fd9-50921bfd4615": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009473088s
Mar 12 08:51:11.244: INFO: Pod "pod-2535a051-4a81-44c3-9fd9-50921bfd4615": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014459488s
Mar 12 08:51:13.250: INFO: Pod "pod-2535a051-4a81-44c3-9fd9-50921bfd4615": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021161832s
Mar 12 08:51:15.255: INFO: Pod "pod-2535a051-4a81-44c3-9fd9-50921bfd4615": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025503686s
Mar 12 08:51:17.260: INFO: Pod "pod-2535a051-4a81-44c3-9fd9-50921bfd4615": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030655983s
Mar 12 08:51:19.265: INFO: Pod "pod-2535a051-4a81-44c3-9fd9-50921bfd4615": Phase="Pending", Reason="", readiness=false. Elapsed: 12.035367456s
Mar 12 08:51:21.269: INFO: Pod "pod-2535a051-4a81-44c3-9fd9-50921bfd4615": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.040310177s
STEP: Saw pod success
Mar 12 08:51:21.270: INFO: Pod "pod-2535a051-4a81-44c3-9fd9-50921bfd4615" satisfied condition "success or failure"
Mar 12 08:51:21.273: INFO: Trying to get logs from node node-2 pod pod-2535a051-4a81-44c3-9fd9-50921bfd4615 container test-container: <nil>
STEP: delete the pod
Mar 12 08:51:21.310: INFO: Waiting for pod pod-2535a051-4a81-44c3-9fd9-50921bfd4615 to disappear
Mar 12 08:51:21.314: INFO: Pod pod-2535a051-4a81-44c3-9fd9-50921bfd4615 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:51:21.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8860" for this suite.
Mar 12 08:51:27.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:51:27.420: INFO: namespace emptydir-8860 deletion completed in 6.102509647s

• [SLOW TEST:20.245 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:51:27.421: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Mar 12 08:51:27.469: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 12 08:51:27.481: INFO: Waiting for terminating namespaces to be deleted...
Mar 12 08:51:27.486: INFO: 
Logging pods the kubelet thinks is on node node-1 before test
Mar 12 08:51:27.503: INFO: coredns-p4prs from kube-system started at 2020-03-11 16:39:41 +0000 UTC (1 container statuses recorded)
Mar 12 08:51:27.503: INFO: 	Container coredns ready: true, restart count 0
Mar 12 08:51:27.503: INFO: kube-controller-manager-node-1 from kube-system started at 2020-03-11 13:01:20 +0000 UTC (1 container statuses recorded)
Mar 12 08:51:27.503: INFO: 	Container kube-controller-manager ready: true, restart count 4
Mar 12 08:51:27.503: INFO: k8s-keystone-auth-7n8pn from kube-system started at 2020-03-11 16:39:41 +0000 UTC (1 container statuses recorded)
Mar 12 08:51:27.503: INFO: 	Container k8s-keystone-auth ready: true, restart count 0
Mar 12 08:51:27.503: INFO: sonobuoy-e2e-job-0ad0ccbfe03640eb from sonobuoy started at 2020-03-12 07:11:26 +0000 UTC (2 container statuses recorded)
Mar 12 08:51:27.503: INFO: 	Container e2e ready: true, restart count 0
Mar 12 08:51:27.503: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 12 08:51:27.503: INFO: kube-flannel-hgdc4 from kube-system started at 2020-03-11 16:40:11 +0000 UTC (2 container statuses recorded)
Mar 12 08:51:27.503: INFO: 	Container install-cni ready: true, restart count 0
Mar 12 08:51:27.503: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 12 08:51:27.503: INFO: sonobuoy from sonobuoy started at 2020-03-12 07:11:13 +0000 UTC (1 container statuses recorded)
Mar 12 08:51:27.503: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 12 08:51:27.503: INFO: kube-scheduler-node-1 from kube-system started at 2020-03-11 11:53:05 +0000 UTC (1 container statuses recorded)
Mar 12 08:51:27.503: INFO: 	Container kube-scheduler ready: true, restart count 3
Mar 12 08:51:27.503: INFO: sonobuoy-systemd-logs-daemon-set-b6e3a2e4f0de4f41-fktq2 from sonobuoy started at 2020-03-12 07:11:26 +0000 UTC (2 container statuses recorded)
Mar 12 08:51:27.503: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 12 08:51:27.503: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 12 08:51:27.503: INFO: kube-proxy-node-1 from kube-system started at 2020-03-11 11:52:32 +0000 UTC (1 container statuses recorded)
Mar 12 08:51:27.503: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 12 08:51:27.503: INFO: kube-apiserver-node-1 from kube-system started at 2020-03-12 07:03:22 +0000 UTC (1 container statuses recorded)
Mar 12 08:51:27.503: INFO: 	Container kube-apiserver ready: true, restart count 1
Mar 12 08:51:27.503: INFO: 
Logging pods the kubelet thinks is on node node-2 before test
Mar 12 08:51:27.511: INFO: kube-monitor-1584003000-spcc2 from kube-system started at 2020-03-12 08:50:02 +0000 UTC (1 container statuses recorded)
Mar 12 08:51:27.511: INFO: 	Container kube-monitor ready: false, restart count 0
Mar 12 08:51:27.511: INFO: kube-proxy-node-2 from kube-system started at 2020-03-11 11:52:32 +0000 UTC (1 container statuses recorded)
Mar 12 08:51:27.511: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 12 08:51:27.512: INFO: kube-scheduler-node-2 from kube-system started at 2020-03-11 11:53:05 +0000 UTC (1 container statuses recorded)
Mar 12 08:51:27.512: INFO: 	Container kube-scheduler ready: true, restart count 2
Mar 12 08:51:27.512: INFO: kube-flannel-rh9nn from kube-system started at 2020-03-12 08:01:51 +0000 UTC (2 container statuses recorded)
Mar 12 08:51:27.512: INFO: 	Container install-cni ready: true, restart count 0
Mar 12 08:51:27.512: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 12 08:51:27.512: INFO: kube-apiserver-node-2 from kube-system started at 2020-03-11 11:52:50 +0000 UTC (1 container statuses recorded)
Mar 12 08:51:27.512: INFO: 	Container kube-apiserver ready: true, restart count 0
Mar 12 08:51:27.512: INFO: k8s-keystone-auth-n7bw6 from kube-system started at 2020-03-12 08:01:11 +0000 UTC (1 container statuses recorded)
Mar 12 08:51:27.512: INFO: 	Container k8s-keystone-auth ready: true, restart count 0
Mar 12 08:51:27.512: INFO: coredns-w5zbd from kube-system started at 2020-03-12 08:01:21 +0000 UTC (1 container statuses recorded)
Mar 12 08:51:27.512: INFO: 	Container coredns ready: true, restart count 0
Mar 12 08:51:27.512: INFO: heapster-659dbd5d99-c8kfc from kube-system started at 2020-03-12 08:14:41 +0000 UTC (1 container statuses recorded)
Mar 12 08:51:27.512: INFO: 	Container heapster ready: true, restart count 0
Mar 12 08:51:27.512: INFO: kube-controller-manager-node-2 from kube-system started at 2020-03-11 13:01:20 +0000 UTC (1 container statuses recorded)
Mar 12 08:51:27.512: INFO: 	Container kube-controller-manager ready: true, restart count 2
Mar 12 08:51:27.512: INFO: tiller-deploy-7db6d94b4f-zcl5s from kube-system started at 2020-03-12 08:14:41 +0000 UTC (1 container statuses recorded)
Mar 12 08:51:27.512: INFO: 	Container tiller ready: true, restart count 0
Mar 12 08:51:27.512: INFO: sonobuoy-systemd-logs-daemon-set-b6e3a2e4f0de4f41-2gk9x from sonobuoy started at 2020-03-12 07:11:26 +0000 UTC (2 container statuses recorded)
Mar 12 08:51:27.512: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 12 08:51:27.512: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 12 08:51:27.512: INFO: 
Logging pods the kubelet thinks is on node node-3 before test
Mar 12 08:51:27.529: INFO: kube-flannel-fzr9c from kube-system started at 2020-03-12 08:15:16 +0000 UTC (2 container statuses recorded)
Mar 12 08:51:27.529: INFO: 	Container install-cni ready: true, restart count 0
Mar 12 08:51:27.529: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 12 08:51:27.529: INFO: kube-scheduler-node-3 from kube-system started at 2020-03-11 11:53:06 +0000 UTC (1 container statuses recorded)
Mar 12 08:51:27.529: INFO: 	Container kube-scheduler ready: true, restart count 4
Mar 12 08:51:27.529: INFO: kube-controller-manager-node-3 from kube-system started at 2020-03-11 13:01:20 +0000 UTC (1 container statuses recorded)
Mar 12 08:51:27.529: INFO: 	Container kube-controller-manager ready: true, restart count 3
Mar 12 08:51:27.529: INFO: kube-proxy-node-3 from kube-system started at 2020-03-11 11:52:32 +0000 UTC (1 container statuses recorded)
Mar 12 08:51:27.529: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 12 08:51:27.529: INFO: k8s-keystone-auth-qlh29 from kube-system started at 2020-03-12 08:15:11 +0000 UTC (1 container statuses recorded)
Mar 12 08:51:27.529: INFO: 	Container k8s-keystone-auth ready: true, restart count 0
Mar 12 08:51:27.529: INFO: coredns-rbm58 from kube-system started at 2020-03-12 08:15:11 +0000 UTC (1 container statuses recorded)
Mar 12 08:51:27.529: INFO: 	Container coredns ready: true, restart count 0
Mar 12 08:51:27.529: INFO: sonobuoy-systemd-logs-daemon-set-b6e3a2e4f0de4f41-q7m9f from sonobuoy started at 2020-03-12 07:11:26 +0000 UTC (2 container statuses recorded)
Mar 12 08:51:27.529: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 12 08:51:27.529: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 12 08:51:27.529: INFO: kube-apiserver-node-3 from kube-system started at 2020-03-11 11:52:50 +0000 UTC (1 container statuses recorded)
Mar 12 08:51:27.529: INFO: 	Container kube-apiserver ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-064d797f-eb97-4f75-9246-398c48a3799e 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-064d797f-eb97-4f75-9246-398c48a3799e off the node node-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-064d797f-eb97-4f75-9246-398c48a3799e
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 08:56:55.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2232" for this suite.
Mar 12 08:57:13.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 08:57:13.787: INFO: namespace sched-pred-2232 deletion completed in 18.109033171s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:346.366 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 08:57:13.787: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-a27fba20-4ec9-4158-aa20-14476b192734 in namespace container-probe-189
Mar 12 08:57:27.869: INFO: Started pod busybox-a27fba20-4ec9-4158-aa20-14476b192734 in namespace container-probe-189
STEP: checking the pod's current state and verifying that restartCount is present
Mar 12 08:57:27.872: INFO: Initial restart count of pod busybox-a27fba20-4ec9-4158-aa20-14476b192734 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:01:28.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-189" for this suite.
Mar 12 09:01:34.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:01:34.653: INFO: namespace container-probe-189 deletion completed in 6.11214709s

• [SLOW TEST:260.866 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:01:34.654: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 12 09:01:35.202: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Mar 12 09:01:37.217: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:01:39.223: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:01:41.227: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:01:43.223: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:01:45.222: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:01:47.222: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719600495, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 12 09:01:50.260: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 09:01:50.264: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:01:56.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6738" for this suite.
Mar 12 09:02:02.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:02:02.602: INFO: namespace crd-webhook-6738 deletion completed in 6.098780659s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:28.022 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:02:02.676: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:02:16.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6262" for this suite.
Mar 12 09:03:00.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:03:00.896: INFO: namespace kubelet-test-6262 deletion completed in 44.11558158s

• [SLOW TEST:58.220 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:03:00.896: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-e5adb01c-1060-4b49-a2ee-3e7096016c4b
STEP: Creating a pod to test consume secrets
Mar 12 09:03:00.973: INFO: Waiting up to 5m0s for pod "pod-secrets-9a74727d-6f4c-43a2-be77-8646979ab393" in namespace "secrets-5756" to be "success or failure"
Mar 12 09:03:00.976: INFO: Pod "pod-secrets-9a74727d-6f4c-43a2-be77-8646979ab393": Phase="Pending", Reason="", readiness=false. Elapsed: 3.180739ms
Mar 12 09:03:02.981: INFO: Pod "pod-secrets-9a74727d-6f4c-43a2-be77-8646979ab393": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0079042s
Mar 12 09:03:04.986: INFO: Pod "pod-secrets-9a74727d-6f4c-43a2-be77-8646979ab393": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012902974s
Mar 12 09:03:06.991: INFO: Pod "pod-secrets-9a74727d-6f4c-43a2-be77-8646979ab393": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018400549s
Mar 12 09:03:08.996: INFO: Pod "pod-secrets-9a74727d-6f4c-43a2-be77-8646979ab393": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023177208s
Mar 12 09:03:11.001: INFO: Pod "pod-secrets-9a74727d-6f4c-43a2-be77-8646979ab393": Phase="Pending", Reason="", readiness=false. Elapsed: 10.027984263s
Mar 12 09:03:13.006: INFO: Pod "pod-secrets-9a74727d-6f4c-43a2-be77-8646979ab393": Phase="Pending", Reason="", readiness=false. Elapsed: 12.033327826s
Mar 12 09:03:15.011: INFO: Pod "pod-secrets-9a74727d-6f4c-43a2-be77-8646979ab393": Phase="Pending", Reason="", readiness=false. Elapsed: 14.037923096s
Mar 12 09:03:17.016: INFO: Pod "pod-secrets-9a74727d-6f4c-43a2-be77-8646979ab393": Phase="Running", Reason="", readiness=true. Elapsed: 16.042660029s
Mar 12 09:03:19.021: INFO: Pod "pod-secrets-9a74727d-6f4c-43a2-be77-8646979ab393": Phase="Succeeded", Reason="", readiness=false. Elapsed: 18.047670654s
STEP: Saw pod success
Mar 12 09:03:19.021: INFO: Pod "pod-secrets-9a74727d-6f4c-43a2-be77-8646979ab393" satisfied condition "success or failure"
Mar 12 09:03:19.024: INFO: Trying to get logs from node node-3 pod pod-secrets-9a74727d-6f4c-43a2-be77-8646979ab393 container secret-volume-test: <nil>
STEP: delete the pod
Mar 12 09:03:19.052: INFO: Waiting for pod pod-secrets-9a74727d-6f4c-43a2-be77-8646979ab393 to disappear
Mar 12 09:03:19.058: INFO: Pod pod-secrets-9a74727d-6f4c-43a2-be77-8646979ab393 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:03:19.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5756" for this suite.
Mar 12 09:03:25.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:03:25.181: INFO: namespace secrets-5756 deletion completed in 6.117561607s

• [SLOW TEST:24.285 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:03:25.181: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 12 09:03:25.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-381'
Mar 12 09:03:26.290: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 12 09:03:26.290: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Mar 12 09:03:26.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 delete jobs e2e-test-httpd-job --namespace=kubectl-381'
Mar 12 09:03:26.405: INFO: stderr: ""
Mar 12 09:03:26.405: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:03:26.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-381" for this suite.
Mar 12 09:03:38.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:03:38.515: INFO: namespace kubectl-381 deletion completed in 12.10459874s

• [SLOW TEST:13.334 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:03:38.515: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-aaa83941-d418-4dd3-acfa-d6ba35b1a298
STEP: Creating a pod to test consume secrets
Mar 12 09:03:38.601: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8d1c68ac-5e5a-4e5b-bfa6-ac472c0291b8" in namespace "projected-659" to be "success or failure"
Mar 12 09:03:38.607: INFO: Pod "pod-projected-secrets-8d1c68ac-5e5a-4e5b-bfa6-ac472c0291b8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.605954ms
Mar 12 09:03:40.612: INFO: Pod "pod-projected-secrets-8d1c68ac-5e5a-4e5b-bfa6-ac472c0291b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010859407s
Mar 12 09:03:42.622: INFO: Pod "pod-projected-secrets-8d1c68ac-5e5a-4e5b-bfa6-ac472c0291b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021328546s
Mar 12 09:03:44.628: INFO: Pod "pod-projected-secrets-8d1c68ac-5e5a-4e5b-bfa6-ac472c0291b8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026962431s
Mar 12 09:03:46.633: INFO: Pod "pod-projected-secrets-8d1c68ac-5e5a-4e5b-bfa6-ac472c0291b8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031801925s
Mar 12 09:03:48.638: INFO: Pod "pod-projected-secrets-8d1c68ac-5e5a-4e5b-bfa6-ac472c0291b8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.036644112s
Mar 12 09:03:50.642: INFO: Pod "pod-projected-secrets-8d1c68ac-5e5a-4e5b-bfa6-ac472c0291b8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.040989551s
Mar 12 09:03:52.647: INFO: Pod "pod-projected-secrets-8d1c68ac-5e5a-4e5b-bfa6-ac472c0291b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.045903671s
STEP: Saw pod success
Mar 12 09:03:52.647: INFO: Pod "pod-projected-secrets-8d1c68ac-5e5a-4e5b-bfa6-ac472c0291b8" satisfied condition "success or failure"
Mar 12 09:03:52.716: INFO: Trying to get logs from node node-3 pod pod-projected-secrets-8d1c68ac-5e5a-4e5b-bfa6-ac472c0291b8 container secret-volume-test: <nil>
STEP: delete the pod
Mar 12 09:03:52.743: INFO: Waiting for pod pod-projected-secrets-8d1c68ac-5e5a-4e5b-bfa6-ac472c0291b8 to disappear
Mar 12 09:03:52.746: INFO: Pod pod-projected-secrets-8d1c68ac-5e5a-4e5b-bfa6-ac472c0291b8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:03:52.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-659" for this suite.
Mar 12 09:03:58.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:03:58.850: INFO: namespace projected-659 deletion completed in 6.099775198s

• [SLOW TEST:20.335 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:03:58.850: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9635.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9635.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9635.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9635.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9635.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9635.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9635.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9635.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9635.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9635.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9635.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9635.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9635.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9635.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9635.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9635.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9635.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9635.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 12 09:04:14.940: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9635.svc.cluster.local from pod dns-9635/dns-test-cd984a55-b4f9-4a8c-af8b-f438dcb80d05: the server could not find the requested resource (get pods dns-test-cd984a55-b4f9-4a8c-af8b-f438dcb80d05)
Mar 12 09:04:14.945: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9635.svc.cluster.local from pod dns-9635/dns-test-cd984a55-b4f9-4a8c-af8b-f438dcb80d05: the server could not find the requested resource (get pods dns-test-cd984a55-b4f9-4a8c-af8b-f438dcb80d05)
Mar 12 09:04:14.949: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9635.svc.cluster.local from pod dns-9635/dns-test-cd984a55-b4f9-4a8c-af8b-f438dcb80d05: the server could not find the requested resource (get pods dns-test-cd984a55-b4f9-4a8c-af8b-f438dcb80d05)
Mar 12 09:04:14.953: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9635.svc.cluster.local from pod dns-9635/dns-test-cd984a55-b4f9-4a8c-af8b-f438dcb80d05: the server could not find the requested resource (get pods dns-test-cd984a55-b4f9-4a8c-af8b-f438dcb80d05)
Mar 12 09:04:14.968: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9635.svc.cluster.local from pod dns-9635/dns-test-cd984a55-b4f9-4a8c-af8b-f438dcb80d05: the server could not find the requested resource (get pods dns-test-cd984a55-b4f9-4a8c-af8b-f438dcb80d05)
Mar 12 09:04:14.972: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9635.svc.cluster.local from pod dns-9635/dns-test-cd984a55-b4f9-4a8c-af8b-f438dcb80d05: the server could not find the requested resource (get pods dns-test-cd984a55-b4f9-4a8c-af8b-f438dcb80d05)
Mar 12 09:04:14.976: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9635.svc.cluster.local from pod dns-9635/dns-test-cd984a55-b4f9-4a8c-af8b-f438dcb80d05: the server could not find the requested resource (get pods dns-test-cd984a55-b4f9-4a8c-af8b-f438dcb80d05)
Mar 12 09:04:14.980: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9635.svc.cluster.local from pod dns-9635/dns-test-cd984a55-b4f9-4a8c-af8b-f438dcb80d05: the server could not find the requested resource (get pods dns-test-cd984a55-b4f9-4a8c-af8b-f438dcb80d05)
Mar 12 09:04:14.987: INFO: Lookups using dns-9635/dns-test-cd984a55-b4f9-4a8c-af8b-f438dcb80d05 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9635.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9635.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9635.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9635.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9635.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9635.svc.cluster.local jessie_udp@dns-test-service-2.dns-9635.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9635.svc.cluster.local]

Mar 12 09:04:20.039: INFO: DNS probes using dns-9635/dns-test-cd984a55-b4f9-4a8c-af8b-f438dcb80d05 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:04:20.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9635" for this suite.
Mar 12 09:04:26.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:04:26.718: INFO: namespace dns-9635 deletion completed in 6.253282209s

• [SLOW TEST:27.868 seconds]
[sig-network] DNS
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:04:26.718: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-09ca1674-5dc0-49d3-bafe-3110519f32fe in namespace container-probe-3234
Mar 12 09:04:40.781: INFO: Started pod test-webserver-09ca1674-5dc0-49d3-bafe-3110519f32fe in namespace container-probe-3234
STEP: checking the pod's current state and verifying that restartCount is present
Mar 12 09:04:40.785: INFO: Initial restart count of pod test-webserver-09ca1674-5dc0-49d3-bafe-3110519f32fe is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:08:41.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3234" for this suite.
Mar 12 09:08:47.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:08:47.541: INFO: namespace container-probe-3234 deletion completed in 6.126144385s

• [SLOW TEST:260.823 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:08:47.542: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-1698
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1698
STEP: Creating statefulset with conflicting port in namespace statefulset-1698
STEP: Waiting until pod test-pod will start running in namespace statefulset-1698
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1698
Mar 12 09:09:01.651: INFO: Observed stateful pod in namespace: statefulset-1698, name: ss-0, uid: a2749b0e-6dd8-4d47-ae1c-34f9fd25ecc4, status phase: Pending. Waiting for statefulset controller to delete.
Mar 12 09:09:02.046: INFO: Observed stateful pod in namespace: statefulset-1698, name: ss-0, uid: a2749b0e-6dd8-4d47-ae1c-34f9fd25ecc4, status phase: Failed. Waiting for statefulset controller to delete.
Mar 12 09:09:02.057: INFO: Observed stateful pod in namespace: statefulset-1698, name: ss-0, uid: a2749b0e-6dd8-4d47-ae1c-34f9fd25ecc4, status phase: Failed. Waiting for statefulset controller to delete.
Mar 12 09:09:02.069: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1698
STEP: Removing pod with conflicting port in namespace statefulset-1698
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1698 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar 12 09:09:18.134: INFO: Deleting all statefulset in ns statefulset-1698
Mar 12 09:09:18.138: INFO: Scaling statefulset ss to 0
Mar 12 09:09:28.160: INFO: Waiting for statefulset status.replicas updated to 0
Mar 12 09:09:28.164: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:09:28.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1698" for this suite.
Mar 12 09:09:34.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:09:34.285: INFO: namespace statefulset-1698 deletion completed in 6.099108365s

• [SLOW TEST:46.743 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:09:34.285: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Mar 12 09:09:34.342: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 12 09:09:34.357: INFO: Waiting for terminating namespaces to be deleted...
Mar 12 09:09:34.360: INFO: 
Logging pods the kubelet thinks is on node node-1 before test
Mar 12 09:09:34.383: INFO: kube-apiserver-node-1 from kube-system started at 2020-03-12 07:03:22 +0000 UTC (1 container statuses recorded)
Mar 12 09:09:34.383: INFO: 	Container kube-apiserver ready: true, restart count 1
Mar 12 09:09:34.383: INFO: kube-controller-manager-node-1 from kube-system started at 2020-03-11 13:01:20 +0000 UTC (1 container statuses recorded)
Mar 12 09:09:34.383: INFO: 	Container kube-controller-manager ready: true, restart count 4
Mar 12 09:09:34.383: INFO: coredns-p4prs from kube-system started at 2020-03-11 16:39:41 +0000 UTC (1 container statuses recorded)
Mar 12 09:09:34.383: INFO: 	Container coredns ready: true, restart count 0
Mar 12 09:09:34.383: INFO: sonobuoy from sonobuoy started at 2020-03-12 07:11:13 +0000 UTC (1 container statuses recorded)
Mar 12 09:09:34.383: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 12 09:09:34.383: INFO: k8s-keystone-auth-7n8pn from kube-system started at 2020-03-11 16:39:41 +0000 UTC (1 container statuses recorded)
Mar 12 09:09:34.383: INFO: 	Container k8s-keystone-auth ready: true, restart count 0
Mar 12 09:09:34.383: INFO: sonobuoy-e2e-job-0ad0ccbfe03640eb from sonobuoy started at 2020-03-12 07:11:26 +0000 UTC (2 container statuses recorded)
Mar 12 09:09:34.383: INFO: 	Container e2e ready: true, restart count 0
Mar 12 09:09:34.383: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 12 09:09:34.383: INFO: kube-flannel-hgdc4 from kube-system started at 2020-03-11 16:40:11 +0000 UTC (2 container statuses recorded)
Mar 12 09:09:34.383: INFO: 	Container install-cni ready: true, restart count 0
Mar 12 09:09:34.383: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 12 09:09:34.383: INFO: kube-proxy-node-1 from kube-system started at 2020-03-11 11:52:32 +0000 UTC (1 container statuses recorded)
Mar 12 09:09:34.383: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 12 09:09:34.383: INFO: kube-scheduler-node-1 from kube-system started at 2020-03-11 11:53:05 +0000 UTC (1 container statuses recorded)
Mar 12 09:09:34.383: INFO: 	Container kube-scheduler ready: true, restart count 3
Mar 12 09:09:34.383: INFO: sonobuoy-systemd-logs-daemon-set-b6e3a2e4f0de4f41-fktq2 from sonobuoy started at 2020-03-12 07:11:26 +0000 UTC (2 container statuses recorded)
Mar 12 09:09:34.383: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 12 09:09:34.383: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 12 09:09:34.383: INFO: 
Logging pods the kubelet thinks is on node node-2 before test
Mar 12 09:09:34.401: INFO: sonobuoy-systemd-logs-daemon-set-b6e3a2e4f0de4f41-2gk9x from sonobuoy started at 2020-03-12 07:11:26 +0000 UTC (2 container statuses recorded)
Mar 12 09:09:34.401: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 12 09:09:34.401: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 12 09:09:34.401: INFO: kube-controller-manager-node-2 from kube-system started at 2020-03-11 13:01:20 +0000 UTC (1 container statuses recorded)
Mar 12 09:09:34.401: INFO: 	Container kube-controller-manager ready: true, restart count 2
Mar 12 09:09:34.401: INFO: tiller-deploy-7db6d94b4f-zcl5s from kube-system started at 2020-03-12 08:14:41 +0000 UTC (1 container statuses recorded)
Mar 12 09:09:34.401: INFO: 	Container tiller ready: true, restart count 0
Mar 12 09:09:34.401: INFO: kube-flannel-rh9nn from kube-system started at 2020-03-12 08:01:51 +0000 UTC (2 container statuses recorded)
Mar 12 09:09:34.401: INFO: 	Container install-cni ready: true, restart count 0
Mar 12 09:09:34.401: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 12 09:09:34.401: INFO: kube-apiserver-node-2 from kube-system started at 2020-03-11 11:52:50 +0000 UTC (1 container statuses recorded)
Mar 12 09:09:34.401: INFO: 	Container kube-apiserver ready: true, restart count 0
Mar 12 09:09:34.401: INFO: kube-proxy-node-2 from kube-system started at 2020-03-11 11:52:32 +0000 UTC (1 container statuses recorded)
Mar 12 09:09:34.401: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 12 09:09:34.401: INFO: kube-scheduler-node-2 from kube-system started at 2020-03-11 11:53:05 +0000 UTC (1 container statuses recorded)
Mar 12 09:09:34.401: INFO: 	Container kube-scheduler ready: true, restart count 2
Mar 12 09:09:34.401: INFO: k8s-keystone-auth-n7bw6 from kube-system started at 2020-03-12 08:01:11 +0000 UTC (1 container statuses recorded)
Mar 12 09:09:34.401: INFO: 	Container k8s-keystone-auth ready: true, restart count 0
Mar 12 09:09:34.401: INFO: coredns-w5zbd from kube-system started at 2020-03-12 08:01:21 +0000 UTC (1 container statuses recorded)
Mar 12 09:09:34.401: INFO: 	Container coredns ready: true, restart count 0
Mar 12 09:09:34.401: INFO: heapster-659dbd5d99-c8kfc from kube-system started at 2020-03-12 08:14:41 +0000 UTC (1 container statuses recorded)
Mar 12 09:09:34.401: INFO: 	Container heapster ready: true, restart count 0
Mar 12 09:09:34.401: INFO: 
Logging pods the kubelet thinks is on node node-3 before test
Mar 12 09:09:34.419: INFO: kube-monitor-1584003900-ws999 from kube-system started at 2020-03-12 09:05:04 +0000 UTC (1 container statuses recorded)
Mar 12 09:09:34.419: INFO: 	Container kube-monitor ready: false, restart count 0
Mar 12 09:09:34.419: INFO: kube-apiserver-node-3 from kube-system started at 2020-03-11 11:52:50 +0000 UTC (1 container statuses recorded)
Mar 12 09:09:34.419: INFO: 	Container kube-apiserver ready: true, restart count 0
Mar 12 09:09:34.419: INFO: kube-proxy-node-3 from kube-system started at 2020-03-11 11:52:32 +0000 UTC (1 container statuses recorded)
Mar 12 09:09:34.419: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 12 09:09:34.419: INFO: kube-flannel-fzr9c from kube-system started at 2020-03-12 08:15:16 +0000 UTC (2 container statuses recorded)
Mar 12 09:09:34.419: INFO: 	Container install-cni ready: true, restart count 0
Mar 12 09:09:34.419: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 12 09:09:34.419: INFO: kube-scheduler-node-3 from kube-system started at 2020-03-11 11:53:06 +0000 UTC (1 container statuses recorded)
Mar 12 09:09:34.419: INFO: 	Container kube-scheduler ready: true, restart count 4
Mar 12 09:09:34.419: INFO: kube-controller-manager-node-3 from kube-system started at 2020-03-11 13:01:20 +0000 UTC (1 container statuses recorded)
Mar 12 09:09:34.419: INFO: 	Container kube-controller-manager ready: true, restart count 3
Mar 12 09:09:34.419: INFO: sonobuoy-systemd-logs-daemon-set-b6e3a2e4f0de4f41-q7m9f from sonobuoy started at 2020-03-12 07:11:26 +0000 UTC (2 container statuses recorded)
Mar 12 09:09:34.419: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 12 09:09:34.419: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 12 09:09:34.419: INFO: k8s-keystone-auth-qlh29 from kube-system started at 2020-03-12 08:15:11 +0000 UTC (1 container statuses recorded)
Mar 12 09:09:34.419: INFO: 	Container k8s-keystone-auth ready: true, restart count 0
Mar 12 09:09:34.419: INFO: coredns-rbm58 from kube-system started at 2020-03-12 08:15:11 +0000 UTC (1 container statuses recorded)
Mar 12 09:09:34.419: INFO: 	Container coredns ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-5b645468-1ecf-4d01-aaf4-1fb0b025a4c4 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-5b645468-1ecf-4d01-aaf4-1fb0b025a4c4 off the node node-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-5b645468-1ecf-4d01-aaf4-1fb0b025a4c4
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:10:02.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6350" for this suite.
Mar 12 09:10:12.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:10:12.674: INFO: namespace sched-pred-6350 deletion completed in 10.137546298s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:38.389 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:10:12.674: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-8863
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8863 to expose endpoints map[]
Mar 12 09:10:12.782: INFO: successfully validated that service endpoint-test2 in namespace services-8863 exposes endpoints map[] (6.518858ms elapsed)
STEP: Creating pod pod1 in namespace services-8863
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8863 to expose endpoints map[pod1:[80]]
Mar 12 09:10:16.846: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.043153572s elapsed, will retry)
Mar 12 09:10:21.889: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (9.086108546s elapsed, will retry)
Mar 12 09:10:25.919: INFO: successfully validated that service endpoint-test2 in namespace services-8863 exposes endpoints map[pod1:[80]] (13.116591272s elapsed)
STEP: Creating pod pod2 in namespace services-8863
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8863 to expose endpoints map[pod1:[80] pod2:[80]]
Mar 12 09:10:29.996: INFO: Unexpected endpoints: found map[17213621-6397-4d19-8416-e2f4f0cde974:[80]], expected map[pod1:[80] pod2:[80]] (4.06984815s elapsed, will retry)
Mar 12 09:10:35.061: INFO: Unexpected endpoints: found map[17213621-6397-4d19-8416-e2f4f0cde974:[80]], expected map[pod1:[80] pod2:[80]] (9.134415357s elapsed, will retry)
Mar 12 09:10:39.104: INFO: successfully validated that service endpoint-test2 in namespace services-8863 exposes endpoints map[pod1:[80] pod2:[80]] (13.177450362s elapsed)
STEP: Deleting pod pod1 in namespace services-8863
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8863 to expose endpoints map[pod2:[80]]
Mar 12 09:10:40.129: INFO: successfully validated that service endpoint-test2 in namespace services-8863 exposes endpoints map[pod2:[80]] (1.014794922s elapsed)
STEP: Deleting pod pod2 in namespace services-8863
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8863 to expose endpoints map[]
Mar 12 09:10:41.147: INFO: successfully validated that service endpoint-test2 in namespace services-8863 exposes endpoints map[] (1.009624418s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:10:41.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8863" for this suite.
Mar 12 09:10:53.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:10:53.446: INFO: namespace services-8863 deletion completed in 12.115772791s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:40.772 seconds]
[sig-network] Services
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:10:53.446: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Mar 12 09:10:53.500: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:11:09.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5894" for this suite.
Mar 12 09:11:37.488: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:11:37.594: INFO: namespace init-container-5894 deletion completed in 28.125498343s

• [SLOW TEST:44.148 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:11:37.595: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Mar 12 09:11:37.640: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 09:11:46.126: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:12:03.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-173" for this suite.
Mar 12 09:12:09.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:12:09.328: INFO: namespace crd-publish-openapi-173 deletion completed in 6.097203567s

• [SLOW TEST:31.733 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:12:09.329: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-0ec92386-43fd-431c-8cb8-7ba5da5f785b
STEP: Creating a pod to test consume configMaps
Mar 12 09:12:09.404: INFO: Waiting up to 5m0s for pod "pod-configmaps-56405c16-5e4b-4184-867d-097fac3c6297" in namespace "configmap-5392" to be "success or failure"
Mar 12 09:12:09.407: INFO: Pod "pod-configmaps-56405c16-5e4b-4184-867d-097fac3c6297": Phase="Pending", Reason="", readiness=false. Elapsed: 2.981378ms
Mar 12 09:12:11.412: INFO: Pod "pod-configmaps-56405c16-5e4b-4184-867d-097fac3c6297": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007757194s
Mar 12 09:12:13.418: INFO: Pod "pod-configmaps-56405c16-5e4b-4184-867d-097fac3c6297": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013868223s
Mar 12 09:12:15.422: INFO: Pod "pod-configmaps-56405c16-5e4b-4184-867d-097fac3c6297": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018140166s
Mar 12 09:12:17.427: INFO: Pod "pod-configmaps-56405c16-5e4b-4184-867d-097fac3c6297": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023311301s
Mar 12 09:12:19.433: INFO: Pod "pod-configmaps-56405c16-5e4b-4184-867d-097fac3c6297": Phase="Pending", Reason="", readiness=false. Elapsed: 10.028613597s
Mar 12 09:12:21.442: INFO: Pod "pod-configmaps-56405c16-5e4b-4184-867d-097fac3c6297": Phase="Pending", Reason="", readiness=false. Elapsed: 12.038163294s
Mar 12 09:12:23.448: INFO: Pod "pod-configmaps-56405c16-5e4b-4184-867d-097fac3c6297": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.043744066s
STEP: Saw pod success
Mar 12 09:12:23.448: INFO: Pod "pod-configmaps-56405c16-5e4b-4184-867d-097fac3c6297" satisfied condition "success or failure"
Mar 12 09:12:23.451: INFO: Trying to get logs from node node-2 pod pod-configmaps-56405c16-5e4b-4184-867d-097fac3c6297 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 12 09:12:23.488: INFO: Waiting for pod pod-configmaps-56405c16-5e4b-4184-867d-097fac3c6297 to disappear
Mar 12 09:12:23.491: INFO: Pod pod-configmaps-56405c16-5e4b-4184-867d-097fac3c6297 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:12:23.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5392" for this suite.
Mar 12 09:12:29.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:12:29.597: INFO: namespace configmap-5392 deletion completed in 6.100957997s

• [SLOW TEST:20.268 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:12:29.597: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Mar 12 09:12:29.645: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-675793433 proxy --unix-socket=/tmp/kubectl-proxy-unix761943700/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:12:29.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6515" for this suite.
Mar 12 09:12:35.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:12:35.859: INFO: namespace kubectl-6515 deletion completed in 6.10155221s

• [SLOW TEST:6.262 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:12:35.859: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Mar 12 09:12:35.908: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Mar 12 09:12:36.172: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Mar 12 09:12:38.254: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:12:40.258: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:12:42.259: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:12:44.259: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:12:46.260: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:12:48.259: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601156, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:12:51.024: INFO: Waited 749.345388ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:12:51.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-6000" for this suite.
Mar 12 09:12:57.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:12:57.643: INFO: namespace aggregator-6000 deletion completed in 6.186243425s

• [SLOW TEST:21.784 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:12:57.644: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 12 09:12:58.064: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Mar 12 09:13:00.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:13:02.080: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:13:04.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:13:06.080: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:13:08.080: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:13:10.080: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601178, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 12 09:13:13.096: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 09:13:13.100: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:13:19.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9917" for this suite.
Mar 12 09:13:25.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:13:25.427: INFO: namespace crd-webhook-9917 deletion completed in 6.084872967s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:27.799 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:13:25.443: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 09:13:25.491: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:13:31.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4967" for this suite.
Mar 12 09:13:37.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:13:37.157: INFO: namespace custom-resource-definition-4967 deletion completed in 6.097367029s

• [SLOW TEST:11.713 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:13:37.157: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7479
I0312 09:13:37.221936      24 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7479, replica count: 1
I0312 09:13:38.272436      24 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 09:13:39.272664      24 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 09:13:40.272981      24 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 09:13:41.273302      24 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 09:13:42.273566      24 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 09:13:43.273777      24 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 09:13:44.274068      24 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 09:13:45.274339      24 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 09:13:46.274570      24 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 09:13:47.274817      24 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 09:13:48.275153      24 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 09:13:49.275365      24 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 09:13:50.275638      24 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 12 09:13:50.431: INFO: Created: latency-svc-5g8xr
Mar 12 09:13:50.438: INFO: Got endpoints: latency-svc-5g8xr [62.347531ms]
Mar 12 09:13:50.457: INFO: Created: latency-svc-pz6kr
Mar 12 09:13:50.465: INFO: Got endpoints: latency-svc-pz6kr [26.374389ms]
Mar 12 09:13:50.467: INFO: Created: latency-svc-9g2j7
Mar 12 09:13:50.476: INFO: Got endpoints: latency-svc-9g2j7 [37.365993ms]
Mar 12 09:13:50.480: INFO: Created: latency-svc-tffhp
Mar 12 09:13:50.492: INFO: Got endpoints: latency-svc-tffhp [49.230566ms]
Mar 12 09:13:50.496: INFO: Created: latency-svc-6pqxn
Mar 12 09:13:50.506: INFO: Got endpoints: latency-svc-6pqxn [63.790335ms]
Mar 12 09:13:50.506: INFO: Created: latency-svc-fwh8f
Mar 12 09:13:50.513: INFO: Got endpoints: latency-svc-fwh8f [71.407417ms]
Mar 12 09:13:50.523: INFO: Created: latency-svc-g54vx
Mar 12 09:13:50.526: INFO: Created: latency-svc-448bx
Mar 12 09:13:50.531: INFO: Got endpoints: latency-svc-g54vx [88.906139ms]
Mar 12 09:13:50.534: INFO: Got endpoints: latency-svc-448bx [91.403321ms]
Mar 12 09:13:50.534: INFO: Created: latency-svc-7zrft
Mar 12 09:13:50.540: INFO: Got endpoints: latency-svc-7zrft [97.675404ms]
Mar 12 09:13:50.544: INFO: Created: latency-svc-tl57d
Mar 12 09:13:50.553: INFO: Created: latency-svc-2245z
Mar 12 09:13:50.558: INFO: Got endpoints: latency-svc-tl57d [115.788122ms]
Mar 12 09:13:50.560: INFO: Got endpoints: latency-svc-2245z [117.840135ms]
Mar 12 09:13:50.562: INFO: Created: latency-svc-p6nbn
Mar 12 09:13:50.571: INFO: Got endpoints: latency-svc-p6nbn [128.204036ms]
Mar 12 09:13:50.573: INFO: Created: latency-svc-rvsfv
Mar 12 09:13:50.579: INFO: Got endpoints: latency-svc-rvsfv [136.53644ms]
Mar 12 09:13:50.581: INFO: Created: latency-svc-pr5tl
Mar 12 09:13:50.592: INFO: Created: latency-svc-tgr7w
Mar 12 09:13:50.596: INFO: Got endpoints: latency-svc-pr5tl [153.769712ms]
Mar 12 09:13:50.599: INFO: Got endpoints: latency-svc-tgr7w [156.399295ms]
Mar 12 09:13:50.604: INFO: Created: latency-svc-rxz64
Mar 12 09:13:50.611: INFO: Got endpoints: latency-svc-rxz64 [169.801262ms]
Mar 12 09:13:50.613: INFO: Created: latency-svc-m95n2
Mar 12 09:13:50.622: INFO: Got endpoints: latency-svc-m95n2 [156.744693ms]
Mar 12 09:13:50.622: INFO: Created: latency-svc-5t698
Mar 12 09:13:50.629: INFO: Got endpoints: latency-svc-5t698 [153.262015ms]
Mar 12 09:13:50.633: INFO: Created: latency-svc-k8p7g
Mar 12 09:13:50.638: INFO: Created: latency-svc-kcjp2
Mar 12 09:13:50.641: INFO: Got endpoints: latency-svc-k8p7g [148.985289ms]
Mar 12 09:13:50.648: INFO: Got endpoints: latency-svc-kcjp2 [142.190588ms]
Mar 12 09:13:50.648: INFO: Created: latency-svc-gkg4t
Mar 12 09:13:50.655: INFO: Got endpoints: latency-svc-gkg4t [142.434021ms]
Mar 12 09:13:50.656: INFO: Created: latency-svc-5c8mq
Mar 12 09:13:50.662: INFO: Got endpoints: latency-svc-5c8mq [131.185955ms]
Mar 12 09:13:50.664: INFO: Created: latency-svc-6nxjv
Mar 12 09:13:50.676: INFO: Created: latency-svc-xbbkc
Mar 12 09:13:50.684: INFO: Created: latency-svc-mz4sx
Mar 12 09:13:50.691: INFO: Created: latency-svc-lmg4n
Mar 12 09:13:50.702: INFO: Created: latency-svc-rnw6h
Mar 12 09:13:50.713: INFO: Created: latency-svc-rtswm
Mar 12 09:13:50.727: INFO: Created: latency-svc-s2tnh
Mar 12 09:13:50.734: INFO: Created: latency-svc-fphbb
Mar 12 09:13:50.743: INFO: Created: latency-svc-rwlbx
Mar 12 09:13:50.754: INFO: Created: latency-svc-85zzb
Mar 12 09:13:50.757: INFO: Created: latency-svc-fvq9h
Mar 12 09:13:50.767: INFO: Created: latency-svc-n9dfb
Mar 12 09:13:50.773: INFO: Got endpoints: latency-svc-6nxjv [238.890837ms]
Mar 12 09:13:50.773: INFO: Got endpoints: latency-svc-xbbkc [232.612088ms]
Mar 12 09:13:50.777: INFO: Got endpoints: latency-svc-lmg4n [216.426033ms]
Mar 12 09:13:50.777: INFO: Got endpoints: latency-svc-rnw6h [206.137242ms]
Mar 12 09:13:50.777: INFO: Got endpoints: latency-svc-mz4sx [218.734877ms]
Mar 12 09:13:50.779: INFO: Created: latency-svc-hd2pw
Mar 12 09:13:50.785: INFO: Got endpoints: latency-svc-rtswm [205.925426ms]
Mar 12 09:13:50.786: INFO: Got endpoints: latency-svc-s2tnh [189.520653ms]
Mar 12 09:13:50.787: INFO: Got endpoints: latency-svc-rwlbx [175.149511ms]
Mar 12 09:13:50.787: INFO: Got endpoints: latency-svc-fphbb [187.962772ms]
Mar 12 09:13:50.789: INFO: Got endpoints: latency-svc-85zzb [167.401602ms]
Mar 12 09:13:50.795: INFO: Created: latency-svc-2d5wh
Mar 12 09:13:50.795: INFO: Got endpoints: latency-svc-fvq9h [165.457089ms]
Mar 12 09:13:50.797: INFO: Got endpoints: latency-svc-hd2pw [149.589421ms]
Mar 12 09:13:50.798: INFO: Got endpoints: latency-svc-n9dfb [156.665198ms]
Mar 12 09:13:50.808: INFO: Got endpoints: latency-svc-2d5wh [35.337334ms]
Mar 12 09:13:50.812: INFO: Created: latency-svc-2b8wt
Mar 12 09:13:50.819: INFO: Got endpoints: latency-svc-2b8wt [163.456048ms]
Mar 12 09:13:50.823: INFO: Created: latency-svc-k52kl
Mar 12 09:13:50.834: INFO: Got endpoints: latency-svc-k52kl [172.235273ms]
Mar 12 09:13:50.837: INFO: Created: latency-svc-qzq86
Mar 12 09:13:50.846: INFO: Got endpoints: latency-svc-qzq86 [73.131375ms]
Mar 12 09:13:50.848: INFO: Created: latency-svc-d9prc
Mar 12 09:13:50.857: INFO: Got endpoints: latency-svc-d9prc [79.81451ms]
Mar 12 09:13:50.858: INFO: Created: latency-svc-qqz4n
Mar 12 09:13:50.870: INFO: Created: latency-svc-shb6h
Mar 12 09:13:50.873: INFO: Got endpoints: latency-svc-qqz4n [95.614857ms]
Mar 12 09:13:50.880: INFO: Got endpoints: latency-svc-shb6h [102.745297ms]
Mar 12 09:13:50.882: INFO: Created: latency-svc-n98lx
Mar 12 09:13:50.892: INFO: Created: latency-svc-7wmlh
Mar 12 09:13:50.892: INFO: Got endpoints: latency-svc-n98lx [106.685986ms]
Mar 12 09:13:50.902: INFO: Got endpoints: latency-svc-7wmlh [115.767919ms]
Mar 12 09:13:50.905: INFO: Created: latency-svc-cv2n8
Mar 12 09:13:50.915: INFO: Got endpoints: latency-svc-cv2n8 [127.581019ms]
Mar 12 09:13:50.918: INFO: Created: latency-svc-g8j4h
Mar 12 09:13:50.927: INFO: Got endpoints: latency-svc-g8j4h [140.236114ms]
Mar 12 09:13:50.928: INFO: Created: latency-svc-sm4tr
Mar 12 09:13:50.936: INFO: Got endpoints: latency-svc-sm4tr [146.908612ms]
Mar 12 09:13:50.938: INFO: Created: latency-svc-fcbq9
Mar 12 09:13:50.947: INFO: Got endpoints: latency-svc-fcbq9 [151.851787ms]
Mar 12 09:13:50.961: INFO: Created: latency-svc-kq7cs
Mar 12 09:13:50.970: INFO: Created: latency-svc-m7mq7
Mar 12 09:13:50.975: INFO: Got endpoints: latency-svc-kq7cs [177.90832ms]
Mar 12 09:13:50.979: INFO: Created: latency-svc-zw9kq
Mar 12 09:13:50.990: INFO: Created: latency-svc-v7b4x
Mar 12 09:13:50.990: INFO: Got endpoints: latency-svc-zw9kq [181.97167ms]
Mar 12 09:13:50.990: INFO: Got endpoints: latency-svc-m7mq7 [192.591123ms]
Mar 12 09:13:51.003: INFO: Got endpoints: latency-svc-v7b4x [184.247323ms]
Mar 12 09:13:51.006: INFO: Created: latency-svc-g68ln
Mar 12 09:13:51.022: INFO: Got endpoints: latency-svc-g68ln [187.498555ms]
Mar 12 09:13:51.024: INFO: Created: latency-svc-qqtf8
Mar 12 09:13:51.033: INFO: Created: latency-svc-pcksj
Mar 12 09:13:51.037: INFO: Got endpoints: latency-svc-qqtf8 [190.593315ms]
Mar 12 09:13:51.043: INFO: Created: latency-svc-jtpg9
Mar 12 09:13:51.043: INFO: Got endpoints: latency-svc-pcksj [186.377415ms]
Mar 12 09:13:51.053: INFO: Got endpoints: latency-svc-jtpg9 [179.89458ms]
Mar 12 09:13:51.058: INFO: Created: latency-svc-6t87b
Mar 12 09:13:51.068: INFO: Got endpoints: latency-svc-6t87b [188.394387ms]
Mar 12 09:13:51.070: INFO: Created: latency-svc-q2g7q
Mar 12 09:13:51.084: INFO: Created: latency-svc-8k8ld
Mar 12 09:13:51.091: INFO: Got endpoints: latency-svc-q2g7q [199.442198ms]
Mar 12 09:13:51.094: INFO: Created: latency-svc-222km
Mar 12 09:13:51.096: INFO: Got endpoints: latency-svc-8k8ld [194.006651ms]
Mar 12 09:13:51.104: INFO: Got endpoints: latency-svc-222km [188.767194ms]
Mar 12 09:13:51.107: INFO: Created: latency-svc-q7j5t
Mar 12 09:13:51.115: INFO: Got endpoints: latency-svc-q7j5t [187.241385ms]
Mar 12 09:13:51.116: INFO: Created: latency-svc-pkknq
Mar 12 09:13:51.127: INFO: Got endpoints: latency-svc-pkknq [190.260885ms]
Mar 12 09:13:51.127: INFO: Created: latency-svc-bx9r7
Mar 12 09:13:51.137: INFO: Got endpoints: latency-svc-bx9r7 [189.985955ms]
Mar 12 09:13:51.138: INFO: Created: latency-svc-fxtjw
Mar 12 09:13:51.147: INFO: Got endpoints: latency-svc-fxtjw [171.690015ms]
Mar 12 09:13:51.153: INFO: Created: latency-svc-ttkwl
Mar 12 09:13:51.165: INFO: Got endpoints: latency-svc-ttkwl [175.079611ms]
Mar 12 09:13:51.166: INFO: Created: latency-svc-sk9ng
Mar 12 09:13:51.177: INFO: Got endpoints: latency-svc-sk9ng [186.858908ms]
Mar 12 09:13:51.178: INFO: Created: latency-svc-f6kzr
Mar 12 09:13:51.186: INFO: Created: latency-svc-2gfpc
Mar 12 09:13:51.191: INFO: Got endpoints: latency-svc-f6kzr [187.758239ms]
Mar 12 09:13:51.195: INFO: Got endpoints: latency-svc-2gfpc [173.413457ms]
Mar 12 09:13:51.197: INFO: Created: latency-svc-x8k64
Mar 12 09:13:51.209: INFO: Got endpoints: latency-svc-x8k64 [171.829637ms]
Mar 12 09:13:51.210: INFO: Created: latency-svc-t4vbd
Mar 12 09:13:51.219: INFO: Got endpoints: latency-svc-t4vbd [175.332977ms]
Mar 12 09:13:51.222: INFO: Created: latency-svc-fs6fr
Mar 12 09:13:51.231: INFO: Got endpoints: latency-svc-fs6fr [178.531109ms]
Mar 12 09:13:51.236: INFO: Created: latency-svc-24lxj
Mar 12 09:13:51.244: INFO: Got endpoints: latency-svc-24lxj [175.617001ms]
Mar 12 09:13:51.249: INFO: Created: latency-svc-w4cq5
Mar 12 09:13:51.259: INFO: Got endpoints: latency-svc-w4cq5 [167.299495ms]
Mar 12 09:13:51.260: INFO: Created: latency-svc-2nrw7
Mar 12 09:13:51.270: INFO: Created: latency-svc-bd276
Mar 12 09:13:51.270: INFO: Got endpoints: latency-svc-2nrw7 [173.306181ms]
Mar 12 09:13:51.279: INFO: Got endpoints: latency-svc-bd276 [175.767967ms]
Mar 12 09:13:51.280: INFO: Created: latency-svc-6lp8g
Mar 12 09:13:51.290: INFO: Created: latency-svc-962bp
Mar 12 09:13:51.290: INFO: Got endpoints: latency-svc-6lp8g [175.069224ms]
Mar 12 09:13:51.301: INFO: Got endpoints: latency-svc-962bp [174.709191ms]
Mar 12 09:13:51.303: INFO: Created: latency-svc-94zq7
Mar 12 09:13:51.311: INFO: Got endpoints: latency-svc-94zq7 [173.732364ms]
Mar 12 09:13:51.315: INFO: Created: latency-svc-lgzfn
Mar 12 09:13:51.323: INFO: Got endpoints: latency-svc-lgzfn [176.032327ms]
Mar 12 09:13:51.327: INFO: Created: latency-svc-n6bss
Mar 12 09:13:51.337: INFO: Got endpoints: latency-svc-n6bss [171.603949ms]
Mar 12 09:13:51.342: INFO: Created: latency-svc-nlsht
Mar 12 09:13:51.354: INFO: Got endpoints: latency-svc-nlsht [176.539218ms]
Mar 12 09:13:51.354: INFO: Created: latency-svc-fcwm7
Mar 12 09:13:51.363: INFO: Got endpoints: latency-svc-fcwm7 [171.685886ms]
Mar 12 09:13:51.366: INFO: Created: latency-svc-szj45
Mar 12 09:13:51.377: INFO: Got endpoints: latency-svc-szj45 [181.195193ms]
Mar 12 09:13:51.378: INFO: Created: latency-svc-5j86j
Mar 12 09:13:51.388: INFO: Created: latency-svc-m7vq4
Mar 12 09:13:51.388: INFO: Got endpoints: latency-svc-5j86j [179.789889ms]
Mar 12 09:13:51.400: INFO: Created: latency-svc-z7qtm
Mar 12 09:13:51.400: INFO: Got endpoints: latency-svc-m7vq4 [181.490739ms]
Mar 12 09:13:51.411: INFO: Got endpoints: latency-svc-z7qtm [179.484273ms]
Mar 12 09:13:51.414: INFO: Created: latency-svc-jg6g2
Mar 12 09:13:51.421: INFO: Got endpoints: latency-svc-jg6g2 [177.530315ms]
Mar 12 09:13:51.427: INFO: Created: latency-svc-fs9rx
Mar 12 09:13:51.430: INFO: Created: latency-svc-n67wj
Mar 12 09:13:51.436: INFO: Got endpoints: latency-svc-fs9rx [177.1227ms]
Mar 12 09:13:51.449: INFO: Got endpoints: latency-svc-n67wj [178.970542ms]
Mar 12 09:13:51.451: INFO: Created: latency-svc-n5r2t
Mar 12 09:13:51.459: INFO: Got endpoints: latency-svc-n5r2t [179.023294ms]
Mar 12 09:13:51.461: INFO: Created: latency-svc-65mwj
Mar 12 09:13:51.468: INFO: Got endpoints: latency-svc-65mwj [178.11708ms]
Mar 12 09:13:51.479: INFO: Created: latency-svc-8bm9v
Mar 12 09:13:51.495: INFO: Created: latency-svc-fj4mm
Mar 12 09:13:51.498: INFO: Got endpoints: latency-svc-8bm9v [196.329836ms]
Mar 12 09:13:51.499: INFO: Created: latency-svc-n2wh8
Mar 12 09:13:51.509: INFO: Got endpoints: latency-svc-n2wh8 [185.694371ms]
Mar 12 09:13:51.509: INFO: Got endpoints: latency-svc-fj4mm [198.249649ms]
Mar 12 09:13:51.510: INFO: Created: latency-svc-4wlkt
Mar 12 09:13:51.520: INFO: Got endpoints: latency-svc-4wlkt [182.940756ms]
Mar 12 09:13:51.522: INFO: Created: latency-svc-wk72b
Mar 12 09:13:51.529: INFO: Got endpoints: latency-svc-wk72b [175.285527ms]
Mar 12 09:13:51.532: INFO: Created: latency-svc-gkmhp
Mar 12 09:13:51.540: INFO: Got endpoints: latency-svc-gkmhp [176.7312ms]
Mar 12 09:13:51.546: INFO: Created: latency-svc-h2qrl
Mar 12 09:13:51.554: INFO: Got endpoints: latency-svc-h2qrl [177.325486ms]
Mar 12 09:13:51.559: INFO: Created: latency-svc-gd87x
Mar 12 09:13:51.574: INFO: Created: latency-svc-hrvdl
Mar 12 09:13:51.574: INFO: Got endpoints: latency-svc-gd87x [185.338229ms]
Mar 12 09:13:51.579: INFO: Got endpoints: latency-svc-hrvdl [178.608222ms]
Mar 12 09:13:51.581: INFO: Created: latency-svc-74w9n
Mar 12 09:13:51.588: INFO: Got endpoints: latency-svc-74w9n [177.168617ms]
Mar 12 09:13:51.591: INFO: Created: latency-svc-jv5dh
Mar 12 09:13:51.603: INFO: Created: latency-svc-bmv6l
Mar 12 09:13:51.608: INFO: Got endpoints: latency-svc-jv5dh [186.235697ms]
Mar 12 09:13:51.614: INFO: Created: latency-svc-tw5s5
Mar 12 09:13:51.617: INFO: Got endpoints: latency-svc-bmv6l [181.059957ms]
Mar 12 09:13:51.624: INFO: Got endpoints: latency-svc-tw5s5 [175.385718ms]
Mar 12 09:13:51.628: INFO: Created: latency-svc-fnz4f
Mar 12 09:13:51.639: INFO: Got endpoints: latency-svc-fnz4f [180.54554ms]
Mar 12 09:13:51.642: INFO: Created: latency-svc-htkch
Mar 12 09:13:51.653: INFO: Created: latency-svc-qftbk
Mar 12 09:13:51.653: INFO: Got endpoints: latency-svc-htkch [184.727556ms]
Mar 12 09:13:51.665: INFO: Got endpoints: latency-svc-qftbk [167.203505ms]
Mar 12 09:13:51.669: INFO: Created: latency-svc-btjh2
Mar 12 09:13:51.681: INFO: Created: latency-svc-c8pvh
Mar 12 09:13:51.688: INFO: Got endpoints: latency-svc-btjh2 [178.856201ms]
Mar 12 09:13:51.695: INFO: Got endpoints: latency-svc-c8pvh [186.066794ms]
Mar 12 09:13:51.697: INFO: Created: latency-svc-d9pqp
Mar 12 09:13:51.705: INFO: Got endpoints: latency-svc-d9pqp [185.011434ms]
Mar 12 09:13:51.705: INFO: Created: latency-svc-qkdz6
Mar 12 09:13:51.713: INFO: Got endpoints: latency-svc-qkdz6 [183.834765ms]
Mar 12 09:13:51.714: INFO: Created: latency-svc-gjvfm
Mar 12 09:13:51.723: INFO: Created: latency-svc-9rtjz
Mar 12 09:13:51.727: INFO: Got endpoints: latency-svc-gjvfm [187.306679ms]
Mar 12 09:13:51.733: INFO: Got endpoints: latency-svc-9rtjz [179.019282ms]
Mar 12 09:13:51.736: INFO: Created: latency-svc-nv4dm
Mar 12 09:13:51.742: INFO: Got endpoints: latency-svc-nv4dm [168.434067ms]
Mar 12 09:13:51.746: INFO: Created: latency-svc-wmrxr
Mar 12 09:13:51.753: INFO: Created: latency-svc-l9g8m
Mar 12 09:13:51.756: INFO: Got endpoints: latency-svc-wmrxr [177.262846ms]
Mar 12 09:13:51.761: INFO: Created: latency-svc-llrgt
Mar 12 09:13:51.761: INFO: Got endpoints: latency-svc-l9g8m [173.308163ms]
Mar 12 09:13:51.770: INFO: Got endpoints: latency-svc-llrgt [162.419653ms]
Mar 12 09:13:51.771: INFO: Created: latency-svc-77hf8
Mar 12 09:13:51.778: INFO: Got endpoints: latency-svc-77hf8 [160.690974ms]
Mar 12 09:13:51.781: INFO: Created: latency-svc-zx75d
Mar 12 09:13:51.789: INFO: Got endpoints: latency-svc-zx75d [164.993183ms]
Mar 12 09:13:51.793: INFO: Created: latency-svc-7ttjz
Mar 12 09:13:51.804: INFO: Got endpoints: latency-svc-7ttjz [165.349583ms]
Mar 12 09:13:51.805: INFO: Created: latency-svc-sn48c
Mar 12 09:13:51.811: INFO: Got endpoints: latency-svc-sn48c [158.168616ms]
Mar 12 09:13:51.812: INFO: Created: latency-svc-44q6s
Mar 12 09:13:51.821: INFO: Got endpoints: latency-svc-44q6s [155.959636ms]
Mar 12 09:13:51.821: INFO: Created: latency-svc-h5dww
Mar 12 09:13:51.830: INFO: Got endpoints: latency-svc-h5dww [142.08987ms]
Mar 12 09:13:51.830: INFO: Created: latency-svc-m7kdr
Mar 12 09:13:51.840: INFO: Got endpoints: latency-svc-m7kdr [144.5968ms]
Mar 12 09:13:51.845: INFO: Created: latency-svc-s7x4m
Mar 12 09:13:51.855: INFO: Got endpoints: latency-svc-s7x4m [150.2901ms]
Mar 12 09:13:51.858: INFO: Created: latency-svc-bwxt9
Mar 12 09:13:51.867: INFO: Got endpoints: latency-svc-bwxt9 [153.788028ms]
Mar 12 09:13:51.867: INFO: Created: latency-svc-bdhh6
Mar 12 09:13:51.877: INFO: Got endpoints: latency-svc-bdhh6 [150.413397ms]
Mar 12 09:13:51.884: INFO: Created: latency-svc-rth9h
Mar 12 09:13:51.888: INFO: Got endpoints: latency-svc-rth9h [155.113303ms]
Mar 12 09:13:51.891: INFO: Created: latency-svc-nss9s
Mar 12 09:13:51.898: INFO: Got endpoints: latency-svc-nss9s [155.619112ms]
Mar 12 09:13:51.909: INFO: Created: latency-svc-krvhw
Mar 12 09:13:51.916: INFO: Got endpoints: latency-svc-krvhw [159.660452ms]
Mar 12 09:13:51.919: INFO: Created: latency-svc-rdz75
Mar 12 09:13:51.927: INFO: Got endpoints: latency-svc-rdz75 [165.946824ms]
Mar 12 09:13:51.930: INFO: Created: latency-svc-ssbph
Mar 12 09:13:51.939: INFO: Got endpoints: latency-svc-ssbph [168.829266ms]
Mar 12 09:13:51.941: INFO: Created: latency-svc-kqbh5
Mar 12 09:13:51.950: INFO: Got endpoints: latency-svc-kqbh5 [172.244155ms]
Mar 12 09:13:51.951: INFO: Created: latency-svc-g7tgw
Mar 12 09:13:51.959: INFO: Got endpoints: latency-svc-g7tgw [169.74492ms]
Mar 12 09:13:51.963: INFO: Created: latency-svc-kmvjl
Mar 12 09:13:51.968: INFO: Created: latency-svc-v5kx9
Mar 12 09:13:51.970: INFO: Got endpoints: latency-svc-kmvjl [165.88872ms]
Mar 12 09:13:51.977: INFO: Got endpoints: latency-svc-v5kx9 [165.706909ms]
Mar 12 09:13:51.983: INFO: Created: latency-svc-52hvm
Mar 12 09:13:51.992: INFO: Got endpoints: latency-svc-52hvm [170.754587ms]
Mar 12 09:13:52.000: INFO: Created: latency-svc-478z6
Mar 12 09:13:52.008: INFO: Got endpoints: latency-svc-478z6 [177.544671ms]
Mar 12 09:13:52.010: INFO: Created: latency-svc-6tbqt
Mar 12 09:13:52.020: INFO: Created: latency-svc-2jgbw
Mar 12 09:13:52.023: INFO: Got endpoints: latency-svc-6tbqt [183.610471ms]
Mar 12 09:13:52.026: INFO: Got endpoints: latency-svc-2jgbw [170.894491ms]
Mar 12 09:13:52.032: INFO: Created: latency-svc-fj4w5
Mar 12 09:13:52.041: INFO: Got endpoints: latency-svc-fj4w5 [173.721937ms]
Mar 12 09:13:52.043: INFO: Created: latency-svc-mcvdv
Mar 12 09:13:52.051: INFO: Got endpoints: latency-svc-mcvdv [173.405085ms]
Mar 12 09:13:52.053: INFO: Created: latency-svc-7ffw4
Mar 12 09:13:52.065: INFO: Created: latency-svc-rwbpr
Mar 12 09:13:52.065: INFO: Got endpoints: latency-svc-7ffw4 [176.616644ms]
Mar 12 09:13:52.071: INFO: Created: latency-svc-ps44f
Mar 12 09:13:52.076: INFO: Got endpoints: latency-svc-rwbpr [178.301905ms]
Mar 12 09:13:52.081: INFO: Got endpoints: latency-svc-ps44f [165.006937ms]
Mar 12 09:13:52.094: INFO: Created: latency-svc-5ldgc
Mar 12 09:13:52.096: INFO: Got endpoints: latency-svc-5ldgc [169.126961ms]
Mar 12 09:13:52.100: INFO: Created: latency-svc-6hpc7
Mar 12 09:13:52.108: INFO: Got endpoints: latency-svc-6hpc7 [168.553585ms]
Mar 12 09:13:52.111: INFO: Created: latency-svc-x5fh9
Mar 12 09:13:52.122: INFO: Created: latency-svc-dj68l
Mar 12 09:13:52.126: INFO: Got endpoints: latency-svc-x5fh9 [175.575629ms]
Mar 12 09:13:52.129: INFO: Got endpoints: latency-svc-dj68l [169.66144ms]
Mar 12 09:13:52.137: INFO: Created: latency-svc-btncz
Mar 12 09:13:52.146: INFO: Created: latency-svc-cqvnm
Mar 12 09:13:52.160: INFO: Got endpoints: latency-svc-btncz [189.664111ms]
Mar 12 09:13:52.161: INFO: Got endpoints: latency-svc-cqvnm [183.701492ms]
Mar 12 09:13:52.162: INFO: Created: latency-svc-t4vff
Mar 12 09:13:52.171: INFO: Got endpoints: latency-svc-t4vff [179.571199ms]
Mar 12 09:13:52.172: INFO: Created: latency-svc-56nsw
Mar 12 09:13:52.193: INFO: Created: latency-svc-8859v
Mar 12 09:13:52.193: INFO: Got endpoints: latency-svc-56nsw [185.131628ms]
Mar 12 09:13:52.210: INFO: Got endpoints: latency-svc-8859v [186.854618ms]
Mar 12 09:13:52.216: INFO: Created: latency-svc-kxpxg
Mar 12 09:13:52.222: INFO: Created: latency-svc-2v6sn
Mar 12 09:13:52.226: INFO: Got endpoints: latency-svc-kxpxg [199.610909ms]
Mar 12 09:13:52.231: INFO: Got endpoints: latency-svc-2v6sn [190.24207ms]
Mar 12 09:13:52.234: INFO: Created: latency-svc-c8jlk
Mar 12 09:13:52.241: INFO: Got endpoints: latency-svc-c8jlk [190.711402ms]
Mar 12 09:13:52.245: INFO: Created: latency-svc-26xvw
Mar 12 09:13:52.254: INFO: Created: latency-svc-4p4p4
Mar 12 09:13:52.259: INFO: Got endpoints: latency-svc-26xvw [194.36791ms]
Mar 12 09:13:52.263: INFO: Got endpoints: latency-svc-4p4p4 [186.43878ms]
Mar 12 09:13:52.268: INFO: Created: latency-svc-pvp9b
Mar 12 09:13:52.277: INFO: Got endpoints: latency-svc-pvp9b [196.53903ms]
Mar 12 09:13:52.281: INFO: Created: latency-svc-gscg4
Mar 12 09:13:52.290: INFO: Created: latency-svc-bpl47
Mar 12 09:13:52.293: INFO: Got endpoints: latency-svc-gscg4 [196.090645ms]
Mar 12 09:13:52.297: INFO: Got endpoints: latency-svc-bpl47 [189.267306ms]
Mar 12 09:13:52.298: INFO: Created: latency-svc-78dzw
Mar 12 09:13:52.308: INFO: Got endpoints: latency-svc-78dzw [182.547435ms]
Mar 12 09:13:52.312: INFO: Created: latency-svc-7g5mj
Mar 12 09:13:52.321: INFO: Got endpoints: latency-svc-7g5mj [192.634496ms]
Mar 12 09:13:52.324: INFO: Created: latency-svc-krxwb
Mar 12 09:13:52.333: INFO: Got endpoints: latency-svc-krxwb [172.399628ms]
Mar 12 09:13:52.335: INFO: Created: latency-svc-22m5j
Mar 12 09:13:52.343: INFO: Got endpoints: latency-svc-22m5j [182.41551ms]
Mar 12 09:13:52.351: INFO: Created: latency-svc-lgd7m
Mar 12 09:13:52.359: INFO: Created: latency-svc-fkfcs
Mar 12 09:13:52.368: INFO: Got endpoints: latency-svc-lgd7m [197.013097ms]
Mar 12 09:13:52.369: INFO: Got endpoints: latency-svc-fkfcs [175.568223ms]
Mar 12 09:13:52.370: INFO: Created: latency-svc-ffw7j
Mar 12 09:13:52.378: INFO: Got endpoints: latency-svc-ffw7j [167.886136ms]
Mar 12 09:13:52.381: INFO: Created: latency-svc-j8q9s
Mar 12 09:13:52.390: INFO: Got endpoints: latency-svc-j8q9s [163.686226ms]
Mar 12 09:13:52.391: INFO: Created: latency-svc-98fkj
Mar 12 09:13:52.399: INFO: Got endpoints: latency-svc-98fkj [167.623571ms]
Mar 12 09:13:52.401: INFO: Created: latency-svc-gjgmc
Mar 12 09:13:52.412: INFO: Got endpoints: latency-svc-gjgmc [170.436969ms]
Mar 12 09:13:52.413: INFO: Created: latency-svc-gz5sc
Mar 12 09:13:52.421: INFO: Got endpoints: latency-svc-gz5sc [161.453321ms]
Mar 12 09:13:52.422: INFO: Created: latency-svc-wgv2p
Mar 12 09:13:52.430: INFO: Got endpoints: latency-svc-wgv2p [167.187786ms]
Mar 12 09:13:52.432: INFO: Created: latency-svc-dwskb
Mar 12 09:13:52.439: INFO: Got endpoints: latency-svc-dwskb [161.654692ms]
Mar 12 09:13:52.443: INFO: Created: latency-svc-kdhk9
Mar 12 09:13:52.451: INFO: Got endpoints: latency-svc-kdhk9 [157.979502ms]
Mar 12 09:13:52.452: INFO: Created: latency-svc-kvszh
Mar 12 09:13:52.459: INFO: Got endpoints: latency-svc-kvszh [162.535978ms]
Mar 12 09:13:52.463: INFO: Created: latency-svc-bhphq
Mar 12 09:13:52.472: INFO: Got endpoints: latency-svc-bhphq [163.743222ms]
Mar 12 09:13:52.472: INFO: Created: latency-svc-nmv9p
Mar 12 09:13:52.481: INFO: Got endpoints: latency-svc-nmv9p [159.550269ms]
Mar 12 09:13:52.481: INFO: Created: latency-svc-ql882
Mar 12 09:13:52.492: INFO: Got endpoints: latency-svc-ql882 [158.954587ms]
Mar 12 09:13:52.494: INFO: Created: latency-svc-lxvfv
Mar 12 09:13:52.505: INFO: Got endpoints: latency-svc-lxvfv [161.170213ms]
Mar 12 09:13:52.510: INFO: Created: latency-svc-qm5wm
Mar 12 09:13:52.528: INFO: Got endpoints: latency-svc-qm5wm [159.851973ms]
Mar 12 09:13:52.533: INFO: Created: latency-svc-6khhp
Mar 12 09:13:52.541: INFO: Got endpoints: latency-svc-6khhp [171.693252ms]
Mar 12 09:13:52.544: INFO: Created: latency-svc-5bstb
Mar 12 09:13:52.554: INFO: Created: latency-svc-swxr7
Mar 12 09:13:52.554: INFO: Got endpoints: latency-svc-5bstb [175.611292ms]
Mar 12 09:13:52.567: INFO: Got endpoints: latency-svc-swxr7 [177.141093ms]
Mar 12 09:13:52.568: INFO: Created: latency-svc-6r28j
Mar 12 09:13:52.579: INFO: Got endpoints: latency-svc-6r28j [180.758058ms]
Mar 12 09:13:52.580: INFO: Created: latency-svc-rnrgf
Mar 12 09:13:52.590: INFO: Got endpoints: latency-svc-rnrgf [178.159181ms]
Mar 12 09:13:52.592: INFO: Created: latency-svc-nwsjl
Mar 12 09:13:52.605: INFO: Got endpoints: latency-svc-nwsjl [184.077696ms]
Mar 12 09:13:52.607: INFO: Created: latency-svc-kw5j5
Mar 12 09:13:52.619: INFO: Got endpoints: latency-svc-kw5j5 [189.330347ms]
Mar 12 09:13:52.623: INFO: Created: latency-svc-9js9s
Mar 12 09:13:52.636: INFO: Got endpoints: latency-svc-9js9s [196.670202ms]
Mar 12 09:13:52.639: INFO: Created: latency-svc-25xxh
Mar 12 09:13:52.646: INFO: Got endpoints: latency-svc-25xxh [195.687618ms]
Mar 12 09:13:52.650: INFO: Created: latency-svc-w6rnz
Mar 12 09:13:52.659: INFO: Created: latency-svc-pbwfb
Mar 12 09:13:52.664: INFO: Got endpoints: latency-svc-w6rnz [204.424174ms]
Mar 12 09:13:52.668: INFO: Created: latency-svc-xbvqj
Mar 12 09:13:52.668: INFO: Got endpoints: latency-svc-pbwfb [196.030505ms]
Mar 12 09:13:52.676: INFO: Created: latency-svc-mf92z
Mar 12 09:13:52.679: INFO: Got endpoints: latency-svc-xbvqj [197.93032ms]
Mar 12 09:13:52.685: INFO: Got endpoints: latency-svc-mf92z [193.087913ms]
Mar 12 09:13:52.688: INFO: Created: latency-svc-zstd9
Mar 12 09:13:52.694: INFO: Created: latency-svc-8ks4l
Mar 12 09:13:52.696: INFO: Got endpoints: latency-svc-zstd9 [190.981171ms]
Mar 12 09:13:52.704: INFO: Created: latency-svc-cfpm8
Mar 12 09:13:52.704: INFO: Got endpoints: latency-svc-8ks4l [175.610837ms]
Mar 12 09:13:52.710: INFO: Got endpoints: latency-svc-cfpm8 [169.228256ms]
Mar 12 09:13:52.713: INFO: Created: latency-svc-fp4cg
Mar 12 09:13:52.721: INFO: Got endpoints: latency-svc-fp4cg [167.209678ms]
Mar 12 09:13:52.723: INFO: Created: latency-svc-86kr9
Mar 12 09:13:52.732: INFO: Got endpoints: latency-svc-86kr9 [165.219298ms]
Mar 12 09:13:52.732: INFO: Created: latency-svc-55csp
Mar 12 09:13:52.743: INFO: Got endpoints: latency-svc-55csp [163.500993ms]
Mar 12 09:13:52.743: INFO: Latencies: [26.374389ms 35.337334ms 37.365993ms 49.230566ms 63.790335ms 71.407417ms 73.131375ms 79.81451ms 88.906139ms 91.403321ms 95.614857ms 97.675404ms 102.745297ms 106.685986ms 115.767919ms 115.788122ms 117.840135ms 127.581019ms 128.204036ms 131.185955ms 136.53644ms 140.236114ms 142.08987ms 142.190588ms 142.434021ms 144.5968ms 146.908612ms 148.985289ms 149.589421ms 150.2901ms 150.413397ms 151.851787ms 153.262015ms 153.769712ms 153.788028ms 155.113303ms 155.619112ms 155.959636ms 156.399295ms 156.665198ms 156.744693ms 157.979502ms 158.168616ms 158.954587ms 159.550269ms 159.660452ms 159.851973ms 160.690974ms 161.170213ms 161.453321ms 161.654692ms 162.419653ms 162.535978ms 163.456048ms 163.500993ms 163.686226ms 163.743222ms 164.993183ms 165.006937ms 165.219298ms 165.349583ms 165.457089ms 165.706909ms 165.88872ms 165.946824ms 167.187786ms 167.203505ms 167.209678ms 167.299495ms 167.401602ms 167.623571ms 167.886136ms 168.434067ms 168.553585ms 168.829266ms 169.126961ms 169.228256ms 169.66144ms 169.74492ms 169.801262ms 170.436969ms 170.754587ms 170.894491ms 171.603949ms 171.685886ms 171.690015ms 171.693252ms 171.829637ms 172.235273ms 172.244155ms 172.399628ms 173.306181ms 173.308163ms 173.405085ms 173.413457ms 173.721937ms 173.732364ms 174.709191ms 175.069224ms 175.079611ms 175.149511ms 175.285527ms 175.332977ms 175.385718ms 175.568223ms 175.575629ms 175.610837ms 175.611292ms 175.617001ms 175.767967ms 176.032327ms 176.539218ms 176.616644ms 176.7312ms 177.1227ms 177.141093ms 177.168617ms 177.262846ms 177.325486ms 177.530315ms 177.544671ms 177.90832ms 178.11708ms 178.159181ms 178.301905ms 178.531109ms 178.608222ms 178.856201ms 178.970542ms 179.019282ms 179.023294ms 179.484273ms 179.571199ms 179.789889ms 179.89458ms 180.54554ms 180.758058ms 181.059957ms 181.195193ms 181.490739ms 181.97167ms 182.41551ms 182.547435ms 182.940756ms 183.610471ms 183.701492ms 183.834765ms 184.077696ms 184.247323ms 184.727556ms 185.011434ms 185.131628ms 185.338229ms 185.694371ms 186.066794ms 186.235697ms 186.377415ms 186.43878ms 186.854618ms 186.858908ms 187.241385ms 187.306679ms 187.498555ms 187.758239ms 187.962772ms 188.394387ms 188.767194ms 189.267306ms 189.330347ms 189.520653ms 189.664111ms 189.985955ms 190.24207ms 190.260885ms 190.593315ms 190.711402ms 190.981171ms 192.591123ms 192.634496ms 193.087913ms 194.006651ms 194.36791ms 195.687618ms 196.030505ms 196.090645ms 196.329836ms 196.53903ms 196.670202ms 197.013097ms 197.93032ms 198.249649ms 199.442198ms 199.610909ms 204.424174ms 205.925426ms 206.137242ms 216.426033ms 218.734877ms 232.612088ms 238.890837ms]
Mar 12 09:13:52.743: INFO: 50 %ile: 175.149511ms
Mar 12 09:13:52.743: INFO: 90 %ile: 194.006651ms
Mar 12 09:13:52.743: INFO: 99 %ile: 232.612088ms
Mar 12 09:13:52.743: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:13:52.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7479" for this suite.
Mar 12 09:14:16.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:14:16.849: INFO: namespace svc-latency-7479 deletion completed in 24.100561095s

• [SLOW TEST:39.692 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:14:16.849: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-64b1463f-99cb-4d8d-bfd5-261cd5b73b70
STEP: Creating configMap with name cm-test-opt-upd-517b4ba7-31d0-4ac5-a8cd-9d73ada18c48
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-64b1463f-99cb-4d8d-bfd5-261cd5b73b70
STEP: Updating configmap cm-test-opt-upd-517b4ba7-31d0-4ac5-a8cd-9d73ada18c48
STEP: Creating configMap with name cm-test-opt-create-105fbd8c-2b3c-40a5-864f-2b6d465d1418
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:14:33.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9457" for this suite.
Mar 12 09:14:45.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:14:45.156: INFO: namespace projected-9457 deletion completed in 12.104736071s

• [SLOW TEST:28.306 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:14:45.156: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar 12 09:14:45.212: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Mar 12 09:15:04.254: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:15:04.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3859" for this suite.
Mar 12 09:15:10.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:15:10.359: INFO: namespace pods-3859 deletion completed in 6.098260058s

• [SLOW TEST:25.203 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:15:10.360: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 12 09:15:10.417: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b449f4ff-c420-468e-9d72-d1fac9865b09" in namespace "projected-5729" to be "success or failure"
Mar 12 09:15:10.420: INFO: Pod "downwardapi-volume-b449f4ff-c420-468e-9d72-d1fac9865b09": Phase="Pending", Reason="", readiness=false. Elapsed: 3.290453ms
Mar 12 09:15:12.426: INFO: Pod "downwardapi-volume-b449f4ff-c420-468e-9d72-d1fac9865b09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008637357s
Mar 12 09:15:14.431: INFO: Pod "downwardapi-volume-b449f4ff-c420-468e-9d72-d1fac9865b09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013344488s
Mar 12 09:15:16.435: INFO: Pod "downwardapi-volume-b449f4ff-c420-468e-9d72-d1fac9865b09": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017731187s
Mar 12 09:15:18.440: INFO: Pod "downwardapi-volume-b449f4ff-c420-468e-9d72-d1fac9865b09": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022945592s
Mar 12 09:15:20.446: INFO: Pod "downwardapi-volume-b449f4ff-c420-468e-9d72-d1fac9865b09": Phase="Pending", Reason="", readiness=false. Elapsed: 10.028559563s
Mar 12 09:15:22.451: INFO: Pod "downwardapi-volume-b449f4ff-c420-468e-9d72-d1fac9865b09": Phase="Pending", Reason="", readiness=false. Elapsed: 12.033418783s
Mar 12 09:15:24.455: INFO: Pod "downwardapi-volume-b449f4ff-c420-468e-9d72-d1fac9865b09": Phase="Running", Reason="", readiness=true. Elapsed: 14.03775877s
Mar 12 09:15:26.459: INFO: Pod "downwardapi-volume-b449f4ff-c420-468e-9d72-d1fac9865b09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.042231291s
STEP: Saw pod success
Mar 12 09:15:26.459: INFO: Pod "downwardapi-volume-b449f4ff-c420-468e-9d72-d1fac9865b09" satisfied condition "success or failure"
Mar 12 09:15:26.462: INFO: Trying to get logs from node node-1 pod downwardapi-volume-b449f4ff-c420-468e-9d72-d1fac9865b09 container client-container: <nil>
STEP: delete the pod
Mar 12 09:15:26.495: INFO: Waiting for pod downwardapi-volume-b449f4ff-c420-468e-9d72-d1fac9865b09 to disappear
Mar 12 09:15:26.499: INFO: Pod downwardapi-volume-b449f4ff-c420-468e-9d72-d1fac9865b09 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:15:26.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5729" for this suite.
Mar 12 09:15:32.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:15:32.608: INFO: namespace projected-5729 deletion completed in 6.103072346s

• [SLOW TEST:22.248 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:15:32.608: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 12 09:15:32.665: INFO: Waiting up to 5m0s for pod "pod-cf197ea9-c718-475b-badf-6424d6682535" in namespace "emptydir-4240" to be "success or failure"
Mar 12 09:15:32.669: INFO: Pod "pod-cf197ea9-c718-475b-badf-6424d6682535": Phase="Pending", Reason="", readiness=false. Elapsed: 3.791429ms
Mar 12 09:15:34.674: INFO: Pod "pod-cf197ea9-c718-475b-badf-6424d6682535": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008411437s
Mar 12 09:15:36.679: INFO: Pod "pod-cf197ea9-c718-475b-badf-6424d6682535": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013433238s
Mar 12 09:15:38.683: INFO: Pod "pod-cf197ea9-c718-475b-badf-6424d6682535": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017942576s
Mar 12 09:15:40.688: INFO: Pod "pod-cf197ea9-c718-475b-badf-6424d6682535": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022431529s
Mar 12 09:15:42.693: INFO: Pod "pod-cf197ea9-c718-475b-badf-6424d6682535": Phase="Pending", Reason="", readiness=false. Elapsed: 10.027232395s
Mar 12 09:15:44.699: INFO: Pod "pod-cf197ea9-c718-475b-badf-6424d6682535": Phase="Pending", Reason="", readiness=false. Elapsed: 12.033420922s
Mar 12 09:15:46.704: INFO: Pod "pod-cf197ea9-c718-475b-badf-6424d6682535": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.038691469s
STEP: Saw pod success
Mar 12 09:15:46.704: INFO: Pod "pod-cf197ea9-c718-475b-badf-6424d6682535" satisfied condition "success or failure"
Mar 12 09:15:46.708: INFO: Trying to get logs from node node-3 pod pod-cf197ea9-c718-475b-badf-6424d6682535 container test-container: <nil>
STEP: delete the pod
Mar 12 09:15:46.734: INFO: Waiting for pod pod-cf197ea9-c718-475b-badf-6424d6682535 to disappear
Mar 12 09:15:46.737: INFO: Pod pod-cf197ea9-c718-475b-badf-6424d6682535 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:15:46.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4240" for this suite.
Mar 12 09:15:52.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:15:52.852: INFO: namespace emptydir-4240 deletion completed in 6.108232715s

• [SLOW TEST:20.244 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:15:52.853: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar 12 09:16:10.938: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2073 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 12 09:16:10.938: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 09:16:11.026: INFO: Exec stderr: ""
Mar 12 09:16:11.026: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2073 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 12 09:16:11.026: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 09:16:11.105: INFO: Exec stderr: ""
Mar 12 09:16:11.105: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2073 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 12 09:16:11.105: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 09:16:11.178: INFO: Exec stderr: ""
Mar 12 09:16:11.178: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2073 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 12 09:16:11.178: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 09:16:11.254: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar 12 09:16:11.255: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2073 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 12 09:16:11.255: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 09:16:11.328: INFO: Exec stderr: ""
Mar 12 09:16:11.328: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2073 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 12 09:16:11.328: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 09:16:11.404: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar 12 09:16:11.404: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2073 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 12 09:16:11.404: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 09:16:11.486: INFO: Exec stderr: ""
Mar 12 09:16:11.486: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2073 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 12 09:16:11.486: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 09:16:11.561: INFO: Exec stderr: ""
Mar 12 09:16:11.561: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2073 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 12 09:16:11.562: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 09:16:11.636: INFO: Exec stderr: ""
Mar 12 09:16:11.636: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2073 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 12 09:16:11.636: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
Mar 12 09:16:11.713: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:16:11.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2073" for this suite.
Mar 12 09:16:55.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:16:55.831: INFO: namespace e2e-kubelet-etc-hosts-2073 deletion completed in 44.111541087s

• [SLOW TEST:62.979 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:16:55.832: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Mar 12 09:16:55.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 --namespace=kubectl-5545 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Mar 12 09:17:08.987: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Mar 12 09:17:08.987: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:17:11.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5545" for this suite.
Mar 12 09:17:25.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:17:25.128: INFO: namespace kubectl-5545 deletion completed in 14.115889534s

• [SLOW TEST:29.297 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:17:25.129: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 12 09:17:25.187: INFO: Waiting up to 5m0s for pod "downwardapi-volume-59fc8e80-53e4-481e-bf31-6a973b4c97a0" in namespace "projected-1853" to be "success or failure"
Mar 12 09:17:25.190: INFO: Pod "downwardapi-volume-59fc8e80-53e4-481e-bf31-6a973b4c97a0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.46692ms
Mar 12 09:17:27.200: INFO: Pod "downwardapi-volume-59fc8e80-53e4-481e-bf31-6a973b4c97a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013756034s
Mar 12 09:17:29.205: INFO: Pod "downwardapi-volume-59fc8e80-53e4-481e-bf31-6a973b4c97a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018378707s
Mar 12 09:17:31.209: INFO: Pod "downwardapi-volume-59fc8e80-53e4-481e-bf31-6a973b4c97a0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021999431s
Mar 12 09:17:33.249: INFO: Pod "downwardapi-volume-59fc8e80-53e4-481e-bf31-6a973b4c97a0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.062533461s
Mar 12 09:17:35.253: INFO: Pod "downwardapi-volume-59fc8e80-53e4-481e-bf31-6a973b4c97a0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.066736276s
Mar 12 09:17:37.257: INFO: Pod "downwardapi-volume-59fc8e80-53e4-481e-bf31-6a973b4c97a0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.070670865s
Mar 12 09:17:39.263: INFO: Pod "downwardapi-volume-59fc8e80-53e4-481e-bf31-6a973b4c97a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.076206709s
STEP: Saw pod success
Mar 12 09:17:39.263: INFO: Pod "downwardapi-volume-59fc8e80-53e4-481e-bf31-6a973b4c97a0" satisfied condition "success or failure"
Mar 12 09:17:39.266: INFO: Trying to get logs from node node-3 pod downwardapi-volume-59fc8e80-53e4-481e-bf31-6a973b4c97a0 container client-container: <nil>
STEP: delete the pod
Mar 12 09:17:39.301: INFO: Waiting for pod downwardapi-volume-59fc8e80-53e4-481e-bf31-6a973b4c97a0 to disappear
Mar 12 09:17:39.304: INFO: Pod downwardapi-volume-59fc8e80-53e4-481e-bf31-6a973b4c97a0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:17:39.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1853" for this suite.
Mar 12 09:17:45.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:17:45.414: INFO: namespace projected-1853 deletion completed in 6.099628994s

• [SLOW TEST:20.285 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:17:45.416: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-0bc16d9f-9231-4a27-8a10-f8b478ab6ff9
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:17:45.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2610" for this suite.
Mar 12 09:17:51.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:17:51.587: INFO: namespace secrets-2610 deletion completed in 6.116081733s

• [SLOW TEST:6.171 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:17:51.587: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Mar 12 09:18:31.680: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:18:31.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0312 09:18:31.680707      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-3483" for this suite.
Mar 12 09:18:41.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:18:41.810: INFO: namespace gc-3483 deletion completed in 10.126028633s

• [SLOW TEST:50.223 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:18:41.811: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-c5dr
STEP: Creating a pod to test atomic-volume-subpath
Mar 12 09:18:41.894: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-c5dr" in namespace "subpath-528" to be "success or failure"
Mar 12 09:18:41.897: INFO: Pod "pod-subpath-test-secret-c5dr": Phase="Pending", Reason="", readiness=false. Elapsed: 3.124556ms
Mar 12 09:18:43.901: INFO: Pod "pod-subpath-test-secret-c5dr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007891288s
Mar 12 09:18:45.906: INFO: Pod "pod-subpath-test-secret-c5dr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01287986s
Mar 12 09:18:47.911: INFO: Pod "pod-subpath-test-secret-c5dr": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017460487s
Mar 12 09:18:49.917: INFO: Pod "pod-subpath-test-secret-c5dr": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022970664s
Mar 12 09:18:51.921: INFO: Pod "pod-subpath-test-secret-c5dr": Phase="Pending", Reason="", readiness=false. Elapsed: 10.027854083s
Mar 12 09:18:53.927: INFO: Pod "pod-subpath-test-secret-c5dr": Phase="Pending", Reason="", readiness=false. Elapsed: 12.033053767s
Mar 12 09:18:55.931: INFO: Pod "pod-subpath-test-secret-c5dr": Phase="Running", Reason="", readiness=true. Elapsed: 14.037887984s
Mar 12 09:18:57.937: INFO: Pod "pod-subpath-test-secret-c5dr": Phase="Running", Reason="", readiness=true. Elapsed: 16.043828804s
Mar 12 09:18:59.942: INFO: Pod "pod-subpath-test-secret-c5dr": Phase="Running", Reason="", readiness=true. Elapsed: 18.048870864s
Mar 12 09:19:01.947: INFO: Pod "pod-subpath-test-secret-c5dr": Phase="Running", Reason="", readiness=true. Elapsed: 20.053642496s
Mar 12 09:19:03.952: INFO: Pod "pod-subpath-test-secret-c5dr": Phase="Running", Reason="", readiness=true. Elapsed: 22.058645963s
Mar 12 09:19:05.958: INFO: Pod "pod-subpath-test-secret-c5dr": Phase="Running", Reason="", readiness=true. Elapsed: 24.064064416s
Mar 12 09:19:07.962: INFO: Pod "pod-subpath-test-secret-c5dr": Phase="Running", Reason="", readiness=true. Elapsed: 26.068536389s
Mar 12 09:19:09.967: INFO: Pod "pod-subpath-test-secret-c5dr": Phase="Running", Reason="", readiness=true. Elapsed: 28.073669953s
Mar 12 09:19:11.971: INFO: Pod "pod-subpath-test-secret-c5dr": Phase="Running", Reason="", readiness=true. Elapsed: 30.077829022s
Mar 12 09:19:13.976: INFO: Pod "pod-subpath-test-secret-c5dr": Phase="Running", Reason="", readiness=true. Elapsed: 32.082685204s
Mar 12 09:19:15.983: INFO: Pod "pod-subpath-test-secret-c5dr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.089577668s
STEP: Saw pod success
Mar 12 09:19:15.983: INFO: Pod "pod-subpath-test-secret-c5dr" satisfied condition "success or failure"
Mar 12 09:19:15.986: INFO: Trying to get logs from node node-2 pod pod-subpath-test-secret-c5dr container test-container-subpath-secret-c5dr: <nil>
STEP: delete the pod
Mar 12 09:19:16.020: INFO: Waiting for pod pod-subpath-test-secret-c5dr to disappear
Mar 12 09:19:16.023: INFO: Pod pod-subpath-test-secret-c5dr no longer exists
STEP: Deleting pod pod-subpath-test-secret-c5dr
Mar 12 09:19:16.023: INFO: Deleting pod "pod-subpath-test-secret-c5dr" in namespace "subpath-528"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:19:16.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-528" for this suite.
Mar 12 09:19:22.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:19:22.131: INFO: namespace subpath-528 deletion completed in 6.101212651s

• [SLOW TEST:40.320 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:19:22.131: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Mar 12 09:19:36.738: INFO: Successfully updated pod "annotationupdatef331d0a3-742e-4071-99be-84466768c86a"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:19:40.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3685" for this suite.
Mar 12 09:19:52.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:19:52.873: INFO: namespace projected-3685 deletion completed in 12.100670169s

• [SLOW TEST:30.742 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:19:52.873: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:20:18.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8763" for this suite.
Mar 12 09:20:24.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:20:24.148: INFO: namespace namespaces-8763 deletion completed in 6.098705629s
STEP: Destroying namespace "nsdeletetest-818" for this suite.
Mar 12 09:20:24.151: INFO: Namespace nsdeletetest-818 was already deleted
STEP: Destroying namespace "nsdeletetest-2242" for this suite.
Mar 12 09:20:30.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:20:30.248: INFO: namespace nsdeletetest-2242 deletion completed in 6.097417609s

• [SLOW TEST:37.375 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:20:30.248: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 12 09:20:44.843: INFO: Successfully updated pod "pod-update-activedeadlineseconds-9bb53cfd-978d-4d3a-8ed1-e3b65c4b1830"
Mar 12 09:20:44.843: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-9bb53cfd-978d-4d3a-8ed1-e3b65c4b1830" in namespace "pods-284" to be "terminated due to deadline exceeded"
Mar 12 09:20:44.847: INFO: Pod "pod-update-activedeadlineseconds-9bb53cfd-978d-4d3a-8ed1-e3b65c4b1830": Phase="Running", Reason="", readiness=true. Elapsed: 3.862688ms
Mar 12 09:20:46.852: INFO: Pod "pod-update-activedeadlineseconds-9bb53cfd-978d-4d3a-8ed1-e3b65c4b1830": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.00911235s
Mar 12 09:20:46.852: INFO: Pod "pod-update-activedeadlineseconds-9bb53cfd-978d-4d3a-8ed1-e3b65c4b1830" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:20:46.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-284" for this suite.
Mar 12 09:20:52.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:20:52.983: INFO: namespace pods-284 deletion completed in 6.12104182s

• [SLOW TEST:22.735 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:20:52.984: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar 12 09:20:53.415: INFO: Pod name wrapped-volume-race-e4af8cc1-893c-4acc-90b9-70d3a7a6a51b: Found 0 pods out of 5
Mar 12 09:20:58.422: INFO: Pod name wrapped-volume-race-e4af8cc1-893c-4acc-90b9-70d3a7a6a51b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e4af8cc1-893c-4acc-90b9-70d3a7a6a51b in namespace emptydir-wrapper-4380, will wait for the garbage collector to delete the pods
Mar 12 09:21:22.518: INFO: Deleting ReplicationController wrapped-volume-race-e4af8cc1-893c-4acc-90b9-70d3a7a6a51b took: 12.990529ms
Mar 12 09:21:22.618: INFO: Terminating ReplicationController wrapped-volume-race-e4af8cc1-893c-4acc-90b9-70d3a7a6a51b pods took: 100.278471ms
STEP: Creating RC which spawns configmap-volume pods
Mar 12 09:22:02.843: INFO: Pod name wrapped-volume-race-09140491-4891-4d59-b6c1-c622df0c21d0: Found 0 pods out of 5
Mar 12 09:22:07.858: INFO: Pod name wrapped-volume-race-09140491-4891-4d59-b6c1-c622df0c21d0: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-09140491-4891-4d59-b6c1-c622df0c21d0 in namespace emptydir-wrapper-4380, will wait for the garbage collector to delete the pods
Mar 12 09:22:31.954: INFO: Deleting ReplicationController wrapped-volume-race-09140491-4891-4d59-b6c1-c622df0c21d0 took: 15.501374ms
Mar 12 09:22:32.054: INFO: Terminating ReplicationController wrapped-volume-race-09140491-4891-4d59-b6c1-c622df0c21d0 pods took: 100.281274ms
STEP: Creating RC which spawns configmap-volume pods
Mar 12 09:23:12.178: INFO: Pod name wrapped-volume-race-a63b0ab3-dfd3-40fc-ba00-da701bee90c5: Found 0 pods out of 5
Mar 12 09:23:17.187: INFO: Pod name wrapped-volume-race-a63b0ab3-dfd3-40fc-ba00-da701bee90c5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a63b0ab3-dfd3-40fc-ba00-da701bee90c5 in namespace emptydir-wrapper-4380, will wait for the garbage collector to delete the pods
Mar 12 09:23:41.284: INFO: Deleting ReplicationController wrapped-volume-race-a63b0ab3-dfd3-40fc-ba00-da701bee90c5 took: 17.199494ms
Mar 12 09:23:41.586: INFO: Terminating ReplicationController wrapped-volume-race-a63b0ab3-dfd3-40fc-ba00-da701bee90c5 pods took: 301.754382ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:24:22.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4380" for this suite.
Mar 12 09:24:30.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:24:30.327: INFO: namespace emptydir-wrapper-4380 deletion completed in 8.107440216s

• [SLOW TEST:217.343 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:24:30.328: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar 12 09:24:30.388: INFO: Waiting up to 5m0s for pod "downward-api-e02c35d5-12e2-4d4b-8a68-4946dbc02592" in namespace "downward-api-7238" to be "success or failure"
Mar 12 09:24:30.391: INFO: Pod "downward-api-e02c35d5-12e2-4d4b-8a68-4946dbc02592": Phase="Pending", Reason="", readiness=false. Elapsed: 2.860774ms
Mar 12 09:24:32.396: INFO: Pod "downward-api-e02c35d5-12e2-4d4b-8a68-4946dbc02592": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008261172s
Mar 12 09:24:34.401: INFO: Pod "downward-api-e02c35d5-12e2-4d4b-8a68-4946dbc02592": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013259336s
Mar 12 09:24:36.406: INFO: Pod "downward-api-e02c35d5-12e2-4d4b-8a68-4946dbc02592": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018419064s
Mar 12 09:24:38.411: INFO: Pod "downward-api-e02c35d5-12e2-4d4b-8a68-4946dbc02592": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02300362s
Mar 12 09:24:40.416: INFO: Pod "downward-api-e02c35d5-12e2-4d4b-8a68-4946dbc02592": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02792857s
Mar 12 09:24:42.421: INFO: Pod "downward-api-e02c35d5-12e2-4d4b-8a68-4946dbc02592": Phase="Pending", Reason="", readiness=false. Elapsed: 12.032875913s
Mar 12 09:24:44.425: INFO: Pod "downward-api-e02c35d5-12e2-4d4b-8a68-4946dbc02592": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.037136405s
STEP: Saw pod success
Mar 12 09:24:44.425: INFO: Pod "downward-api-e02c35d5-12e2-4d4b-8a68-4946dbc02592" satisfied condition "success or failure"
Mar 12 09:24:44.428: INFO: Trying to get logs from node node-2 pod downward-api-e02c35d5-12e2-4d4b-8a68-4946dbc02592 container dapi-container: <nil>
STEP: delete the pod
Mar 12 09:24:44.466: INFO: Waiting for pod downward-api-e02c35d5-12e2-4d4b-8a68-4946dbc02592 to disappear
Mar 12 09:24:44.469: INFO: Pod downward-api-e02c35d5-12e2-4d4b-8a68-4946dbc02592 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:24:44.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7238" for this suite.
Mar 12 09:24:50.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:24:50.589: INFO: namespace downward-api-7238 deletion completed in 6.115492869s

• [SLOW TEST:20.261 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:24:50.589: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Mar 12 09:24:50.802: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-4011" to be "success or failure"
Mar 12 09:24:50.805: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.998131ms
Mar 12 09:24:52.810: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008201182s
Mar 12 09:24:54.815: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013094718s
Mar 12 09:24:56.820: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018644627s
Mar 12 09:24:58.825: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02344133s
Mar 12 09:25:00.830: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 10.028135483s
Mar 12 09:25:02.836: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 12.034089172s
Mar 12 09:25:04.841: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 14.039337041s
Mar 12 09:25:06.846: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.044283469s
STEP: Saw pod success
Mar 12 09:25:06.846: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Mar 12 09:25:06.850: INFO: Trying to get logs from node node-3 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Mar 12 09:25:07.445: INFO: Waiting for pod pod-host-path-test to disappear
Mar 12 09:25:07.448: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:25:07.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-4011" for this suite.
Mar 12 09:25:13.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:25:13.609: INFO: namespace hostpath-4011 deletion completed in 6.156362222s

• [SLOW TEST:23.020 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:25:13.610: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 12 09:25:14.343: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 12 09:25:16.355: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:25:18.360: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:25:20.360: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:25:22.360: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:25:24.360: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:25:26.360: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719601914, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 12 09:25:29.373: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 09:25:29.378: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5253-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:25:35.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9456" for this suite.
Mar 12 09:25:41.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:25:41.811: INFO: namespace webhook-9456 deletion completed in 6.148859587s
STEP: Destroying namespace "webhook-9456-markers" for this suite.
Mar 12 09:25:47.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:25:47.992: INFO: namespace webhook-9456-markers deletion completed in 6.181134845s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:34.411 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:25:48.021: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Mar 12 09:25:48.106: INFO: Waiting up to 5m0s for pod "var-expansion-484d3f09-6548-42a0-a195-df14f453c1c4" in namespace "var-expansion-6817" to be "success or failure"
Mar 12 09:25:48.111: INFO: Pod "var-expansion-484d3f09-6548-42a0-a195-df14f453c1c4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.326439ms
Mar 12 09:25:50.117: INFO: Pod "var-expansion-484d3f09-6548-42a0-a195-df14f453c1c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01095316s
Mar 12 09:25:52.122: INFO: Pod "var-expansion-484d3f09-6548-42a0-a195-df14f453c1c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015816057s
Mar 12 09:25:54.126: INFO: Pod "var-expansion-484d3f09-6548-42a0-a195-df14f453c1c4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020386246s
Mar 12 09:25:56.133: INFO: Pod "var-expansion-484d3f09-6548-42a0-a195-df14f453c1c4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026523212s
Mar 12 09:25:58.137: INFO: Pod "var-expansion-484d3f09-6548-42a0-a195-df14f453c1c4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.0309739s
Mar 12 09:26:00.142: INFO: Pod "var-expansion-484d3f09-6548-42a0-a195-df14f453c1c4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.035635575s
Mar 12 09:26:02.146: INFO: Pod "var-expansion-484d3f09-6548-42a0-a195-df14f453c1c4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.039748149s
Mar 12 09:26:04.150: INFO: Pod "var-expansion-484d3f09-6548-42a0-a195-df14f453c1c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.044250659s
STEP: Saw pod success
Mar 12 09:26:04.150: INFO: Pod "var-expansion-484d3f09-6548-42a0-a195-df14f453c1c4" satisfied condition "success or failure"
Mar 12 09:26:04.153: INFO: Trying to get logs from node node-3 pod var-expansion-484d3f09-6548-42a0-a195-df14f453c1c4 container dapi-container: <nil>
STEP: delete the pod
Mar 12 09:26:04.177: INFO: Waiting for pod var-expansion-484d3f09-6548-42a0-a195-df14f453c1c4 to disappear
Mar 12 09:26:04.179: INFO: Pod var-expansion-484d3f09-6548-42a0-a195-df14f453c1c4 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:26:04.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6817" for this suite.
Mar 12 09:26:10.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:26:10.278: INFO: namespace var-expansion-6817 deletion completed in 6.093597727s

• [SLOW TEST:22.256 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:26:10.278: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 12 09:26:15.396: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:26:15.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-365" for this suite.
Mar 12 09:26:21.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:26:21.542: INFO: namespace container-runtime-365 deletion completed in 6.109863335s

• [SLOW TEST:11.264 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:26:21.542: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 12 09:26:21.604: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e13d184-25f7-4229-8781-8f005e370cb5" in namespace "projected-8533" to be "success or failure"
Mar 12 09:26:21.607: INFO: Pod "downwardapi-volume-6e13d184-25f7-4229-8781-8f005e370cb5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.05343ms
Mar 12 09:26:23.612: INFO: Pod "downwardapi-volume-6e13d184-25f7-4229-8781-8f005e370cb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008189832s
Mar 12 09:26:25.617: INFO: Pod "downwardapi-volume-6e13d184-25f7-4229-8781-8f005e370cb5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012962179s
Mar 12 09:26:27.622: INFO: Pod "downwardapi-volume-6e13d184-25f7-4229-8781-8f005e370cb5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01813516s
Mar 12 09:26:29.628: INFO: Pod "downwardapi-volume-6e13d184-25f7-4229-8781-8f005e370cb5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024586766s
Mar 12 09:26:31.633: INFO: Pod "downwardapi-volume-6e13d184-25f7-4229-8781-8f005e370cb5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.029506285s
Mar 12 09:26:33.638: INFO: Pod "downwardapi-volume-6e13d184-25f7-4229-8781-8f005e370cb5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.034036336s
Mar 12 09:26:35.643: INFO: Pod "downwardapi-volume-6e13d184-25f7-4229-8781-8f005e370cb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.038964476s
STEP: Saw pod success
Mar 12 09:26:35.643: INFO: Pod "downwardapi-volume-6e13d184-25f7-4229-8781-8f005e370cb5" satisfied condition "success or failure"
Mar 12 09:26:35.646: INFO: Trying to get logs from node node-2 pod downwardapi-volume-6e13d184-25f7-4229-8781-8f005e370cb5 container client-container: <nil>
STEP: delete the pod
Mar 12 09:26:35.684: INFO: Waiting for pod downwardapi-volume-6e13d184-25f7-4229-8781-8f005e370cb5 to disappear
Mar 12 09:26:35.687: INFO: Pod downwardapi-volume-6e13d184-25f7-4229-8781-8f005e370cb5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:26:35.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8533" for this suite.
Mar 12 09:26:41.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:26:41.810: INFO: namespace projected-8533 deletion completed in 6.118954657s

• [SLOW TEST:20.268 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:26:41.810: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar 12 09:26:41.870: INFO: Waiting up to 5m0s for pod "downward-api-a7161957-156c-41f8-9d2c-2356df5fa0e2" in namespace "downward-api-572" to be "success or failure"
Mar 12 09:26:41.878: INFO: Pod "downward-api-a7161957-156c-41f8-9d2c-2356df5fa0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.696292ms
Mar 12 09:26:43.884: INFO: Pod "downward-api-a7161957-156c-41f8-9d2c-2356df5fa0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01389067s
Mar 12 09:26:45.889: INFO: Pod "downward-api-a7161957-156c-41f8-9d2c-2356df5fa0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018736924s
Mar 12 09:26:47.895: INFO: Pod "downward-api-a7161957-156c-41f8-9d2c-2356df5fa0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024478988s
Mar 12 09:26:49.900: INFO: Pod "downward-api-a7161957-156c-41f8-9d2c-2356df5fa0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.029257931s
Mar 12 09:26:51.905: INFO: Pod "downward-api-a7161957-156c-41f8-9d2c-2356df5fa0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.034595464s
Mar 12 09:26:53.910: INFO: Pod "downward-api-a7161957-156c-41f8-9d2c-2356df5fa0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.039288746s
Mar 12 09:26:55.915: INFO: Pod "downward-api-a7161957-156c-41f8-9d2c-2356df5fa0e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.044193066s
STEP: Saw pod success
Mar 12 09:26:55.915: INFO: Pod "downward-api-a7161957-156c-41f8-9d2c-2356df5fa0e2" satisfied condition "success or failure"
Mar 12 09:26:55.918: INFO: Trying to get logs from node node-3 pod downward-api-a7161957-156c-41f8-9d2c-2356df5fa0e2 container dapi-container: <nil>
STEP: delete the pod
Mar 12 09:26:55.977: INFO: Waiting for pod downward-api-a7161957-156c-41f8-9d2c-2356df5fa0e2 to disappear
Mar 12 09:26:55.980: INFO: Pod downward-api-a7161957-156c-41f8-9d2c-2356df5fa0e2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:26:55.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-572" for this suite.
Mar 12 09:27:02.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:27:02.107: INFO: namespace downward-api-572 deletion completed in 6.121980633s

• [SLOW TEST:20.297 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:27:02.107: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 12 09:27:02.189: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3578a368-0935-43c4-b8d7-be8f45d6a4fa" in namespace "downward-api-9623" to be "success or failure"
Mar 12 09:27:02.192: INFO: Pod "downwardapi-volume-3578a368-0935-43c4-b8d7-be8f45d6a4fa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.339733ms
Mar 12 09:27:04.197: INFO: Pod "downwardapi-volume-3578a368-0935-43c4-b8d7-be8f45d6a4fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008163158s
Mar 12 09:27:06.208: INFO: Pod "downwardapi-volume-3578a368-0935-43c4-b8d7-be8f45d6a4fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0193647s
Mar 12 09:27:08.213: INFO: Pod "downwardapi-volume-3578a368-0935-43c4-b8d7-be8f45d6a4fa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023803496s
Mar 12 09:27:10.219: INFO: Pod "downwardapi-volume-3578a368-0935-43c4-b8d7-be8f45d6a4fa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.0303071s
Mar 12 09:27:12.224: INFO: Pod "downwardapi-volume-3578a368-0935-43c4-b8d7-be8f45d6a4fa": Phase="Pending", Reason="", readiness=false. Elapsed: 10.035112999s
Mar 12 09:27:14.229: INFO: Pod "downwardapi-volume-3578a368-0935-43c4-b8d7-be8f45d6a4fa": Phase="Pending", Reason="", readiness=false. Elapsed: 12.039992892s
Mar 12 09:27:16.234: INFO: Pod "downwardapi-volume-3578a368-0935-43c4-b8d7-be8f45d6a4fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.045150399s
STEP: Saw pod success
Mar 12 09:27:16.234: INFO: Pod "downwardapi-volume-3578a368-0935-43c4-b8d7-be8f45d6a4fa" satisfied condition "success or failure"
Mar 12 09:27:16.238: INFO: Trying to get logs from node node-3 pod downwardapi-volume-3578a368-0935-43c4-b8d7-be8f45d6a4fa container client-container: <nil>
STEP: delete the pod
Mar 12 09:27:16.264: INFO: Waiting for pod downwardapi-volume-3578a368-0935-43c4-b8d7-be8f45d6a4fa to disappear
Mar 12 09:27:16.267: INFO: Pod downwardapi-volume-3578a368-0935-43c4-b8d7-be8f45d6a4fa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:27:16.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9623" for this suite.
Mar 12 09:27:22.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:27:22.473: INFO: namespace downward-api-9623 deletion completed in 6.201222173s

• [SLOW TEST:20.365 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:27:22.473: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Mar 12 09:27:22.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 create -f - --namespace=kubectl-6410'
Mar 12 09:27:23.772: INFO: stderr: ""
Mar 12 09:27:23.772: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 12 09:27:23.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6410'
Mar 12 09:27:23.870: INFO: stderr: ""
Mar 12 09:27:23.870: INFO: stdout: "update-demo-nautilus-q2wvc update-demo-nautilus-rm2bf "
Mar 12 09:27:23.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-q2wvc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6410'
Mar 12 09:27:23.952: INFO: stderr: ""
Mar 12 09:27:23.952: INFO: stdout: ""
Mar 12 09:27:23.952: INFO: update-demo-nautilus-q2wvc is created but not running
Mar 12 09:27:28.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6410'
Mar 12 09:27:29.078: INFO: stderr: ""
Mar 12 09:27:29.078: INFO: stdout: "update-demo-nautilus-q2wvc update-demo-nautilus-rm2bf "
Mar 12 09:27:29.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-q2wvc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6410'
Mar 12 09:27:29.173: INFO: stderr: ""
Mar 12 09:27:29.173: INFO: stdout: ""
Mar 12 09:27:29.173: INFO: update-demo-nautilus-q2wvc is created but not running
Mar 12 09:27:34.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6410'
Mar 12 09:27:34.278: INFO: stderr: ""
Mar 12 09:27:34.278: INFO: stdout: "update-demo-nautilus-q2wvc update-demo-nautilus-rm2bf "
Mar 12 09:27:34.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-q2wvc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6410'
Mar 12 09:27:34.377: INFO: stderr: ""
Mar 12 09:27:34.377: INFO: stdout: ""
Mar 12 09:27:34.377: INFO: update-demo-nautilus-q2wvc is created but not running
Mar 12 09:27:39.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6410'
Mar 12 09:27:39.473: INFO: stderr: ""
Mar 12 09:27:39.473: INFO: stdout: "update-demo-nautilus-q2wvc update-demo-nautilus-rm2bf "
Mar 12 09:27:39.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-q2wvc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6410'
Mar 12 09:27:39.569: INFO: stderr: ""
Mar 12 09:27:39.569: INFO: stdout: "true"
Mar 12 09:27:39.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-q2wvc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6410'
Mar 12 09:27:39.652: INFO: stderr: ""
Mar 12 09:27:39.652: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 12 09:27:39.652: INFO: validating pod update-demo-nautilus-q2wvc
Mar 12 09:27:39.659: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 12 09:27:39.659: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 12 09:27:39.659: INFO: update-demo-nautilus-q2wvc is verified up and running
Mar 12 09:27:39.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-rm2bf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6410'
Mar 12 09:27:39.747: INFO: stderr: ""
Mar 12 09:27:39.747: INFO: stdout: "true"
Mar 12 09:27:39.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods update-demo-nautilus-rm2bf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6410'
Mar 12 09:27:39.845: INFO: stderr: ""
Mar 12 09:27:39.845: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 12 09:27:39.845: INFO: validating pod update-demo-nautilus-rm2bf
Mar 12 09:27:39.907: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 12 09:27:39.907: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 12 09:27:39.907: INFO: update-demo-nautilus-rm2bf is verified up and running
STEP: using delete to clean up resources
Mar 12 09:27:39.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 delete --grace-period=0 --force -f - --namespace=kubectl-6410'
Mar 12 09:27:40.019: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 12 09:27:40.019: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 12 09:27:40.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6410'
Mar 12 09:27:40.114: INFO: stderr: "No resources found in kubectl-6410 namespace.\n"
Mar 12 09:27:40.114: INFO: stdout: ""
Mar 12 09:27:40.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -l name=update-demo --namespace=kubectl-6410 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 12 09:27:40.211: INFO: stderr: ""
Mar 12 09:27:40.211: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:27:40.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6410" for this suite.
Mar 12 09:27:46.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:27:46.528: INFO: namespace kubectl-6410 deletion completed in 6.106174311s

• [SLOW TEST:24.055 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:27:46.528: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar 12 09:27:46.611: INFO: Waiting up to 5m0s for pod "downward-api-13dfcc3f-d3d5-4448-9bf4-297f9f941902" in namespace "downward-api-3840" to be "success or failure"
Mar 12 09:27:46.614: INFO: Pod "downward-api-13dfcc3f-d3d5-4448-9bf4-297f9f941902": Phase="Pending", Reason="", readiness=false. Elapsed: 3.004059ms
Mar 12 09:27:48.619: INFO: Pod "downward-api-13dfcc3f-d3d5-4448-9bf4-297f9f941902": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008283329s
Mar 12 09:27:50.625: INFO: Pod "downward-api-13dfcc3f-d3d5-4448-9bf4-297f9f941902": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013530174s
Mar 12 09:27:52.630: INFO: Pod "downward-api-13dfcc3f-d3d5-4448-9bf4-297f9f941902": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018391765s
Mar 12 09:27:54.635: INFO: Pod "downward-api-13dfcc3f-d3d5-4448-9bf4-297f9f941902": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023631232s
Mar 12 09:27:56.640: INFO: Pod "downward-api-13dfcc3f-d3d5-4448-9bf4-297f9f941902": Phase="Pending", Reason="", readiness=false. Elapsed: 10.028509016s
Mar 12 09:27:58.645: INFO: Pod "downward-api-13dfcc3f-d3d5-4448-9bf4-297f9f941902": Phase="Pending", Reason="", readiness=false. Elapsed: 12.03382103s
Mar 12 09:28:00.650: INFO: Pod "downward-api-13dfcc3f-d3d5-4448-9bf4-297f9f941902": Phase="Pending", Reason="", readiness=false. Elapsed: 14.039111374s
Mar 12 09:28:02.656: INFO: Pod "downward-api-13dfcc3f-d3d5-4448-9bf4-297f9f941902": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.04446716s
STEP: Saw pod success
Mar 12 09:28:02.656: INFO: Pod "downward-api-13dfcc3f-d3d5-4448-9bf4-297f9f941902" satisfied condition "success or failure"
Mar 12 09:28:02.659: INFO: Trying to get logs from node node-3 pod downward-api-13dfcc3f-d3d5-4448-9bf4-297f9f941902 container dapi-container: <nil>
STEP: delete the pod
Mar 12 09:28:02.685: INFO: Waiting for pod downward-api-13dfcc3f-d3d5-4448-9bf4-297f9f941902 to disappear
Mar 12 09:28:02.688: INFO: Pod downward-api-13dfcc3f-d3d5-4448-9bf4-297f9f941902 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:28:02.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3840" for this suite.
Mar 12 09:28:08.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:28:08.785: INFO: namespace downward-api-3840 deletion completed in 6.092285847s

• [SLOW TEST:22.256 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:28:08.785: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 09:28:08.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 version'
Mar 12 09:28:09.067: INFO: stderr: ""
Mar 12 09:28:09.067: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.6\", GitCommit:\"72c30166b2105cd7d3350f2c28a219e6abcd79eb\", GitTreeState:\"clean\", BuildDate:\"2020-01-18T23:31:31Z\", GoVersion:\"go1.13.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16+\", GitVersion:\"v1.16.6-es\", GitCommit:\"4a66548aaecf522a8d6aaf69066c4a257420b81d\", GitTreeState:\"clean\", BuildDate:\"2020-02-19T12:34:35Z\", GoVersion:\"go1.13.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:28:09.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4689" for this suite.
Mar 12 09:28:15.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:28:15.178: INFO: namespace kubectl-4689 deletion completed in 6.103076522s

• [SLOW TEST:6.393 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:28:15.178: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:28:15.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7170" for this suite.
Mar 12 09:28:21.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:28:21.378: INFO: namespace resourcequota-7170 deletion completed in 6.107502476s

• [SLOW TEST:6.200 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:28:21.378: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 09:28:21.431: INFO: Creating ReplicaSet my-hostname-basic-d6e5adca-33a7-40d7-b535-fe8c88a9174c
Mar 12 09:28:21.444: INFO: Pod name my-hostname-basic-d6e5adca-33a7-40d7-b535-fe8c88a9174c: Found 0 pods out of 1
Mar 12 09:28:26.449: INFO: Pod name my-hostname-basic-d6e5adca-33a7-40d7-b535-fe8c88a9174c: Found 1 pods out of 1
Mar 12 09:28:26.449: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-d6e5adca-33a7-40d7-b535-fe8c88a9174c" is running
Mar 12 09:28:34.457: INFO: Pod "my-hostname-basic-d6e5adca-33a7-40d7-b535-fe8c88a9174c-pz6nc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-12 09:28:21 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-12 09:28:21 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-d6e5adca-33a7-40d7-b535-fe8c88a9174c]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-12 09:28:21 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-d6e5adca-33a7-40d7-b535-fe8c88a9174c]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-12 09:28:21 +0000 UTC Reason: Message:}])
Mar 12 09:28:34.457: INFO: Trying to dial the pod
Mar 12 09:28:39.472: INFO: Controller my-hostname-basic-d6e5adca-33a7-40d7-b535-fe8c88a9174c: Got expected result from replica 1 [my-hostname-basic-d6e5adca-33a7-40d7-b535-fe8c88a9174c-pz6nc]: "my-hostname-basic-d6e5adca-33a7-40d7-b535-fe8c88a9174c-pz6nc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:28:39.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4631" for this suite.
Mar 12 09:28:45.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:28:45.584: INFO: namespace replicaset-4631 deletion completed in 6.106808533s

• [SLOW TEST:24.206 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:28:45.584: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Mar 12 09:29:00.186: INFO: Successfully updated pod "labelsupdate3e25ffa3-d1cf-4b9e-aa29-b2ea371c5bbf"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:29:02.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4598" for this suite.
Mar 12 09:29:14.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:29:14.321: INFO: namespace downward-api-4598 deletion completed in 12.108724485s

• [SLOW TEST:28.737 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:29:14.322: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-8f5c3166-6122-46c1-ac75-89d7b62304ba
STEP: Creating secret with name s-test-opt-upd-be2ca81d-5a79-4859-bdd9-23f752ccb149
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-8f5c3166-6122-46c1-ac75-89d7b62304ba
STEP: Updating secret s-test-opt-upd-be2ca81d-5a79-4859-bdd9-23f752ccb149
STEP: Creating secret with name s-test-opt-create-38dfcc71-fe38-44b3-964a-7afc99e9e65d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:30:48.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9758" for this suite.
Mar 12 09:31:00.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:31:01.087: INFO: namespace secrets-9758 deletion completed in 12.104005522s

• [SLOW TEST:106.765 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:31:01.087: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7963.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7963.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7963.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7963.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7963.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7963.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7963.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7963.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7963.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7963.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7963.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7963.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7963.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 121.49.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.49.121_udp@PTR;check="$$(dig +tcp +noall +answer +search 121.49.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.49.121_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7963.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7963.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7963.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7963.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7963.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7963.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7963.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7963.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7963.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7963.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7963.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7963.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7963.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 121.49.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.49.121_udp@PTR;check="$$(dig +tcp +noall +answer +search 121.49.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.49.121_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 12 09:31:17.188: INFO: Unable to read wheezy_udp@dns-test-service.dns-7963.svc.cluster.local from pod dns-7963/dns-test-f1dd9dab-2f1a-430c-84ff-e94dda4952b8: the server could not find the requested resource (get pods dns-test-f1dd9dab-2f1a-430c-84ff-e94dda4952b8)
Mar 12 09:31:17.192: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7963.svc.cluster.local from pod dns-7963/dns-test-f1dd9dab-2f1a-430c-84ff-e94dda4952b8: the server could not find the requested resource (get pods dns-test-f1dd9dab-2f1a-430c-84ff-e94dda4952b8)
Mar 12 09:31:17.196: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7963.svc.cluster.local from pod dns-7963/dns-test-f1dd9dab-2f1a-430c-84ff-e94dda4952b8: the server could not find the requested resource (get pods dns-test-f1dd9dab-2f1a-430c-84ff-e94dda4952b8)
Mar 12 09:31:17.200: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7963.svc.cluster.local from pod dns-7963/dns-test-f1dd9dab-2f1a-430c-84ff-e94dda4952b8: the server could not find the requested resource (get pods dns-test-f1dd9dab-2f1a-430c-84ff-e94dda4952b8)
Mar 12 09:31:17.232: INFO: Unable to read jessie_udp@dns-test-service.dns-7963.svc.cluster.local from pod dns-7963/dns-test-f1dd9dab-2f1a-430c-84ff-e94dda4952b8: the server could not find the requested resource (get pods dns-test-f1dd9dab-2f1a-430c-84ff-e94dda4952b8)
Mar 12 09:31:17.237: INFO: Unable to read jessie_tcp@dns-test-service.dns-7963.svc.cluster.local from pod dns-7963/dns-test-f1dd9dab-2f1a-430c-84ff-e94dda4952b8: the server could not find the requested resource (get pods dns-test-f1dd9dab-2f1a-430c-84ff-e94dda4952b8)
Mar 12 09:31:17.242: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7963.svc.cluster.local from pod dns-7963/dns-test-f1dd9dab-2f1a-430c-84ff-e94dda4952b8: the server could not find the requested resource (get pods dns-test-f1dd9dab-2f1a-430c-84ff-e94dda4952b8)
Mar 12 09:31:17.246: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7963.svc.cluster.local from pod dns-7963/dns-test-f1dd9dab-2f1a-430c-84ff-e94dda4952b8: the server could not find the requested resource (get pods dns-test-f1dd9dab-2f1a-430c-84ff-e94dda4952b8)
Mar 12 09:31:17.274: INFO: Lookups using dns-7963/dns-test-f1dd9dab-2f1a-430c-84ff-e94dda4952b8 failed for: [wheezy_udp@dns-test-service.dns-7963.svc.cluster.local wheezy_tcp@dns-test-service.dns-7963.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7963.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7963.svc.cluster.local jessie_udp@dns-test-service.dns-7963.svc.cluster.local jessie_tcp@dns-test-service.dns-7963.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7963.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7963.svc.cluster.local]

Mar 12 09:31:22.372: INFO: DNS probes using dns-7963/dns-test-f1dd9dab-2f1a-430c-84ff-e94dda4952b8 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:31:22.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7963" for this suite.
Mar 12 09:31:28.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:31:28.566: INFO: namespace dns-7963 deletion completed in 6.097968064s

• [SLOW TEST:27.479 seconds]
[sig-network] DNS
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:31:28.566: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Mar 12 09:31:28.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 create -f - --namespace=kubectl-3966'
Mar 12 09:31:28.839: INFO: stderr: ""
Mar 12 09:31:28.839: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar 12 09:31:29.844: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 09:31:29.844: INFO: Found 0 / 1
Mar 12 09:31:30.845: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 09:31:30.845: INFO: Found 0 / 1
Mar 12 09:31:31.845: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 09:31:31.845: INFO: Found 0 / 1
Mar 12 09:31:32.844: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 09:31:32.844: INFO: Found 0 / 1
Mar 12 09:31:33.844: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 09:31:33.844: INFO: Found 0 / 1
Mar 12 09:31:34.844: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 09:31:34.844: INFO: Found 0 / 1
Mar 12 09:31:35.845: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 09:31:35.845: INFO: Found 0 / 1
Mar 12 09:31:36.844: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 09:31:36.844: INFO: Found 0 / 1
Mar 12 09:31:37.844: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 09:31:37.844: INFO: Found 0 / 1
Mar 12 09:31:38.845: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 09:31:38.845: INFO: Found 0 / 1
Mar 12 09:31:39.844: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 09:31:39.844: INFO: Found 0 / 1
Mar 12 09:31:40.844: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 09:31:40.844: INFO: Found 0 / 1
Mar 12 09:31:41.844: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 09:31:41.844: INFO: Found 1 / 1
Mar 12 09:31:41.844: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar 12 09:31:41.848: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 09:31:41.848: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 12 09:31:41.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 patch pod redis-master-9dpmh --namespace=kubectl-3966 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 12 09:31:41.958: INFO: stderr: ""
Mar 12 09:31:41.958: INFO: stdout: "pod/redis-master-9dpmh patched\n"
STEP: checking annotations
Mar 12 09:31:41.962: INFO: Selector matched 1 pods for map[app:redis]
Mar 12 09:31:41.962: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:31:41.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3966" for this suite.
Mar 12 09:31:53.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:31:54.079: INFO: namespace kubectl-3966 deletion completed in 12.111848981s

• [SLOW TEST:25.513 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:31:54.079: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Mar 12 09:31:54.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 api-versions'
Mar 12 09:31:54.222: INFO: stderr: ""
Mar 12 09:31:54.222: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:31:54.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4384" for this suite.
Mar 12 09:32:00.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:32:00.322: INFO: namespace kubectl-4384 deletion completed in 6.094008806s

• [SLOW TEST:6.243 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:32:00.322: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 12 09:32:01.189: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 12 09:32:03.202: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:32:05.207: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:32:07.207: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:32:09.223: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:32:11.209: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:32:13.207: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719602321, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 12 09:32:16.226: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
Mar 12 09:32:16.262: INFO: Waiting for webhook configuration to be ready...
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:32:16.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8478" for this suite.
Mar 12 09:32:22.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:32:22.593: INFO: namespace webhook-8478 deletion completed in 6.127983571s
STEP: Destroying namespace "webhook-8478-markers" for this suite.
Mar 12 09:32:28.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:32:28.708: INFO: namespace webhook-8478-markers deletion completed in 6.114706075s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:28.405 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:32:28.728: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 09:32:28.776: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 12 09:32:36.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 --namespace=crd-publish-openapi-2500 create -f -'
Mar 12 09:32:37.434: INFO: stderr: ""
Mar 12 09:32:37.434: INFO: stdout: "e2e-test-crd-publish-openapi-6792-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 12 09:32:37.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 --namespace=crd-publish-openapi-2500 delete e2e-test-crd-publish-openapi-6792-crds test-cr'
Mar 12 09:32:37.591: INFO: stderr: ""
Mar 12 09:32:37.591: INFO: stdout: "e2e-test-crd-publish-openapi-6792-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar 12 09:32:37.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 --namespace=crd-publish-openapi-2500 apply -f -'
Mar 12 09:32:37.838: INFO: stderr: ""
Mar 12 09:32:37.838: INFO: stdout: "e2e-test-crd-publish-openapi-6792-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 12 09:32:37.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 --namespace=crd-publish-openapi-2500 delete e2e-test-crd-publish-openapi-6792-crds test-cr'
Mar 12 09:32:37.983: INFO: stderr: ""
Mar 12 09:32:37.983: INFO: stdout: "e2e-test-crd-publish-openapi-6792-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Mar 12 09:32:37.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 explain e2e-test-crd-publish-openapi-6792-crds'
Mar 12 09:32:38.253: INFO: stderr: ""
Mar 12 09:32:38.253: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6792-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:32:41.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2500" for this suite.
Mar 12 09:32:47.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:32:47.350: INFO: namespace crd-publish-openapi-2500 deletion completed in 6.110004222s

• [SLOW TEST:18.623 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:32:47.351: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:33:01.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7035" for this suite.
Mar 12 09:33:09.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:33:09.549: INFO: namespace containers-7035 deletion completed in 8.102610272s

• [SLOW TEST:22.197 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:33:09.549: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Mar 12 09:33:09.600: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:33:24.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-849" for this suite.
Mar 12 09:33:30.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:33:30.319: INFO: namespace init-container-849 deletion completed in 6.120489569s

• [SLOW TEST:20.770 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:33:30.320: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Mar 12 09:33:30.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-8411 -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar 12 09:33:30.506: INFO: stderr: ""
Mar 12 09:33:30.506: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Mar 12 09:33:30.506: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar 12 09:33:30.506: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8411" to be "running and ready, or succeeded"
Mar 12 09:33:30.516: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 9.70237ms
Mar 12 09:33:32.521: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014362915s
Mar 12 09:33:34.525: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019246779s
Mar 12 09:33:36.530: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023643482s
Mar 12 09:33:38.534: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 8.028210743s
Mar 12 09:33:40.539: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 10.03293954s
Mar 12 09:33:42.544: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 12.037814258s
Mar 12 09:33:44.549: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 14.042576159s
Mar 12 09:33:44.549: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar 12 09:33:44.549: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Mar 12 09:33:44.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 logs logs-generator logs-generator --namespace=kubectl-8411'
Mar 12 09:33:44.690: INFO: stderr: ""
Mar 12 09:33:44.691: INFO: stdout: "I0312 09:33:43.529008       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/8r4 502\nI0312 09:33:43.729136       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/lbm 547\nI0312 09:33:43.929199       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/b87 311\nI0312 09:33:44.129206       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/qbc 342\nI0312 09:33:44.329173       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/8zl5 382\nI0312 09:33:44.529200       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/wc24 270\n"
STEP: limiting log lines
Mar 12 09:33:44.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 logs logs-generator logs-generator --namespace=kubectl-8411 --tail=1'
Mar 12 09:33:44.791: INFO: stderr: ""
Mar 12 09:33:44.791: INFO: stdout: "I0312 09:33:44.729175       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/mqs7 356\n"
STEP: limiting log bytes
Mar 12 09:33:44.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 logs logs-generator logs-generator --namespace=kubectl-8411 --limit-bytes=1'
Mar 12 09:33:44.927: INFO: stderr: ""
Mar 12 09:33:44.927: INFO: stdout: "I"
STEP: exposing timestamps
Mar 12 09:33:44.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 logs logs-generator logs-generator --namespace=kubectl-8411 --tail=1 --timestamps'
Mar 12 09:33:45.078: INFO: stderr: ""
Mar 12 09:33:45.079: INFO: stdout: "2020-03-12T09:33:44.929644475Z I0312 09:33:44.929239       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/4wn 378\n"
STEP: restricting to a time range
Mar 12 09:33:47.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 logs logs-generator logs-generator --namespace=kubectl-8411 --since=1s'
Mar 12 09:33:47.695: INFO: stderr: ""
Mar 12 09:33:47.695: INFO: stdout: "I0312 09:33:46.729166       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/sgr5 260\nI0312 09:33:46.929172       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/n7z 516\nI0312 09:33:47.129140       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/slts 572\nI0312 09:33:47.329484       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/7dl2 320\nI0312 09:33:47.529172       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/9rb 358\n"
Mar 12 09:33:47.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 logs logs-generator logs-generator --namespace=kubectl-8411 --since=24h'
Mar 12 09:33:47.807: INFO: stderr: ""
Mar 12 09:33:47.807: INFO: stdout: "I0312 09:33:43.529008       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/8r4 502\nI0312 09:33:43.729136       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/lbm 547\nI0312 09:33:43.929199       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/b87 311\nI0312 09:33:44.129206       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/qbc 342\nI0312 09:33:44.329173       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/8zl5 382\nI0312 09:33:44.529200       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/wc24 270\nI0312 09:33:44.729175       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/mqs7 356\nI0312 09:33:44.929239       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/4wn 378\nI0312 09:33:45.129225       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/txhk 379\nI0312 09:33:45.329228       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/t6nl 426\nI0312 09:33:45.529132       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/pm8 316\nI0312 09:33:45.729170       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/vndm 390\nI0312 09:33:45.929191       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/n6hk 332\nI0312 09:33:46.129166       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/4vmp 564\nI0312 09:33:46.329157       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/sgk5 247\nI0312 09:33:46.529652       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/bq7x 281\nI0312 09:33:46.729166       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/sgr5 260\nI0312 09:33:46.929172       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/n7z 516\nI0312 09:33:47.129140       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/slts 572\nI0312 09:33:47.329484       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/7dl2 320\nI0312 09:33:47.529172       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/9rb 358\nI0312 09:33:47.729163       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/l2k 355\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Mar 12 09:33:47.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 delete pod logs-generator --namespace=kubectl-8411'
Mar 12 09:33:49.809: INFO: stderr: ""
Mar 12 09:33:49.809: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:33:49.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8411" for this suite.
Mar 12 09:33:55.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:33:55.929: INFO: namespace kubectl-8411 deletion completed in 6.115148536s

• [SLOW TEST:25.610 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:33:55.930: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-ff60d20e-b5a9-4a2e-81a4-cc977bc8d7ca
STEP: Creating a pod to test consume configMaps
Mar 12 09:33:56.000: INFO: Waiting up to 5m0s for pod "pod-configmaps-b2508a86-3771-4d43-918f-1ab1e8a3aa86" in namespace "configmap-6415" to be "success or failure"
Mar 12 09:33:56.010: INFO: Pod "pod-configmaps-b2508a86-3771-4d43-918f-1ab1e8a3aa86": Phase="Pending", Reason="", readiness=false. Elapsed: 9.819324ms
Mar 12 09:33:58.015: INFO: Pod "pod-configmaps-b2508a86-3771-4d43-918f-1ab1e8a3aa86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015026925s
Mar 12 09:34:00.020: INFO: Pod "pod-configmaps-b2508a86-3771-4d43-918f-1ab1e8a3aa86": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019805638s
Mar 12 09:34:02.026: INFO: Pod "pod-configmaps-b2508a86-3771-4d43-918f-1ab1e8a3aa86": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025774516s
Mar 12 09:34:04.031: INFO: Pod "pod-configmaps-b2508a86-3771-4d43-918f-1ab1e8a3aa86": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031195029s
Mar 12 09:34:06.036: INFO: Pod "pod-configmaps-b2508a86-3771-4d43-918f-1ab1e8a3aa86": Phase="Pending", Reason="", readiness=false. Elapsed: 10.03605249s
Mar 12 09:34:08.040: INFO: Pod "pod-configmaps-b2508a86-3771-4d43-918f-1ab1e8a3aa86": Phase="Pending", Reason="", readiness=false. Elapsed: 12.040210882s
Mar 12 09:34:10.045: INFO: Pod "pod-configmaps-b2508a86-3771-4d43-918f-1ab1e8a3aa86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.044984877s
STEP: Saw pod success
Mar 12 09:34:10.045: INFO: Pod "pod-configmaps-b2508a86-3771-4d43-918f-1ab1e8a3aa86" satisfied condition "success or failure"
Mar 12 09:34:10.048: INFO: Trying to get logs from node node-3 pod pod-configmaps-b2508a86-3771-4d43-918f-1ab1e8a3aa86 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 12 09:34:10.074: INFO: Waiting for pod pod-configmaps-b2508a86-3771-4d43-918f-1ab1e8a3aa86 to disappear
Mar 12 09:34:10.077: INFO: Pod pod-configmaps-b2508a86-3771-4d43-918f-1ab1e8a3aa86 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:34:10.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6415" for this suite.
Mar 12 09:34:16.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:34:16.214: INFO: namespace configmap-6415 deletion completed in 6.129876281s

• [SLOW TEST:20.284 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:34:16.214: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 09:34:16.300: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar 12 09:34:16.316: INFO: Number of nodes with available pods: 0
Mar 12 09:34:16.316: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar 12 09:34:16.344: INFO: Number of nodes with available pods: 0
Mar 12 09:34:16.344: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:17.350: INFO: Number of nodes with available pods: 0
Mar 12 09:34:17.350: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:18.350: INFO: Number of nodes with available pods: 0
Mar 12 09:34:18.350: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:19.350: INFO: Number of nodes with available pods: 0
Mar 12 09:34:19.350: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:20.352: INFO: Number of nodes with available pods: 0
Mar 12 09:34:20.352: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:21.350: INFO: Number of nodes with available pods: 0
Mar 12 09:34:21.350: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:22.350: INFO: Number of nodes with available pods: 0
Mar 12 09:34:22.350: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:23.358: INFO: Number of nodes with available pods: 0
Mar 12 09:34:23.358: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:24.349: INFO: Number of nodes with available pods: 0
Mar 12 09:34:24.349: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:25.350: INFO: Number of nodes with available pods: 0
Mar 12 09:34:25.350: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:26.349: INFO: Number of nodes with available pods: 0
Mar 12 09:34:26.350: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:27.349: INFO: Number of nodes with available pods: 0
Mar 12 09:34:27.349: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:28.349: INFO: Number of nodes with available pods: 0
Mar 12 09:34:28.349: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:29.350: INFO: Number of nodes with available pods: 1
Mar 12 09:34:29.350: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar 12 09:34:29.379: INFO: Number of nodes with available pods: 1
Mar 12 09:34:29.379: INFO: Number of running nodes: 0, number of available pods: 1
Mar 12 09:34:30.384: INFO: Number of nodes with available pods: 0
Mar 12 09:34:30.384: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar 12 09:34:30.406: INFO: Number of nodes with available pods: 0
Mar 12 09:34:30.406: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:31.411: INFO: Number of nodes with available pods: 0
Mar 12 09:34:31.411: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:32.411: INFO: Number of nodes with available pods: 0
Mar 12 09:34:32.411: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:33.415: INFO: Number of nodes with available pods: 0
Mar 12 09:34:33.415: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:34.411: INFO: Number of nodes with available pods: 0
Mar 12 09:34:34.411: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:35.411: INFO: Number of nodes with available pods: 0
Mar 12 09:34:35.411: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:36.411: INFO: Number of nodes with available pods: 0
Mar 12 09:34:36.411: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:37.411: INFO: Number of nodes with available pods: 0
Mar 12 09:34:37.411: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:38.411: INFO: Number of nodes with available pods: 0
Mar 12 09:34:38.411: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:39.411: INFO: Number of nodes with available pods: 0
Mar 12 09:34:39.411: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:40.411: INFO: Number of nodes with available pods: 0
Mar 12 09:34:40.412: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:41.411: INFO: Number of nodes with available pods: 0
Mar 12 09:34:41.411: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:42.411: INFO: Number of nodes with available pods: 0
Mar 12 09:34:42.411: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:43.411: INFO: Number of nodes with available pods: 0
Mar 12 09:34:43.411: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:44.411: INFO: Number of nodes with available pods: 0
Mar 12 09:34:44.411: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:45.411: INFO: Number of nodes with available pods: 0
Mar 12 09:34:45.411: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:46.411: INFO: Number of nodes with available pods: 0
Mar 12 09:34:46.411: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:47.411: INFO: Number of nodes with available pods: 0
Mar 12 09:34:47.411: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:48.411: INFO: Number of nodes with available pods: 0
Mar 12 09:34:48.411: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:49.412: INFO: Number of nodes with available pods: 0
Mar 12 09:34:49.412: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:50.411: INFO: Number of nodes with available pods: 0
Mar 12 09:34:50.411: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:51.412: INFO: Number of nodes with available pods: 0
Mar 12 09:34:51.412: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:52.411: INFO: Number of nodes with available pods: 0
Mar 12 09:34:52.411: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:53.411: INFO: Number of nodes with available pods: 0
Mar 12 09:34:53.411: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:34:54.411: INFO: Number of nodes with available pods: 1
Mar 12 09:34:54.411: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9874, will wait for the garbage collector to delete the pods
Mar 12 09:34:54.483: INFO: Deleting DaemonSet.extensions daemon-set took: 14.110774ms
Mar 12 09:34:54.583: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.221829ms
Mar 12 09:34:58.187: INFO: Number of nodes with available pods: 0
Mar 12 09:34:58.187: INFO: Number of running nodes: 0, number of available pods: 0
Mar 12 09:34:58.190: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9874/daemonsets","resourceVersion":"162078"},"items":null}

Mar 12 09:34:58.193: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9874/pods","resourceVersion":"162078"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:34:58.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9874" for this suite.
Mar 12 09:35:04.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:35:04.353: INFO: namespace daemonsets-9874 deletion completed in 6.118962954s

• [SLOW TEST:48.139 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:35:04.353: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Mar 12 09:35:18.985: INFO: Successfully updated pod "labelsupdate59de0aee-82e2-47b8-ab10-24d7c2bec57d"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:35:21.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6825" for this suite.
Mar 12 09:35:33.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:35:33.129: INFO: namespace projected-6825 deletion completed in 12.11552197s

• [SLOW TEST:28.776 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:35:33.130: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Mar 12 09:35:33.178: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:35:56.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8710" for this suite.
Mar 12 09:36:02.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:36:02.946: INFO: namespace crd-publish-openapi-8710 deletion completed in 6.117129022s

• [SLOW TEST:29.816 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:36:02.947: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:36:14.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5060" for this suite.
Mar 12 09:36:20.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:36:20.192: INFO: namespace resourcequota-5060 deletion completed in 6.126521258s

• [SLOW TEST:17.245 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:36:20.192: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Mar 12 09:36:20.265: INFO: Waiting up to 5m0s for pod "client-containers-50d3d434-e5bb-4fe5-b6d2-cdb570328e89" in namespace "containers-2821" to be "success or failure"
Mar 12 09:36:20.268: INFO: Pod "client-containers-50d3d434-e5bb-4fe5-b6d2-cdb570328e89": Phase="Pending", Reason="", readiness=false. Elapsed: 3.79909ms
Mar 12 09:36:22.274: INFO: Pod "client-containers-50d3d434-e5bb-4fe5-b6d2-cdb570328e89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009328488s
Mar 12 09:36:24.280: INFO: Pod "client-containers-50d3d434-e5bb-4fe5-b6d2-cdb570328e89": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014900752s
Mar 12 09:36:26.284: INFO: Pod "client-containers-50d3d434-e5bb-4fe5-b6d2-cdb570328e89": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019214123s
Mar 12 09:36:28.289: INFO: Pod "client-containers-50d3d434-e5bb-4fe5-b6d2-cdb570328e89": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024196133s
Mar 12 09:36:30.294: INFO: Pod "client-containers-50d3d434-e5bb-4fe5-b6d2-cdb570328e89": Phase="Pending", Reason="", readiness=false. Elapsed: 10.029459825s
Mar 12 09:36:32.299: INFO: Pod "client-containers-50d3d434-e5bb-4fe5-b6d2-cdb570328e89": Phase="Pending", Reason="", readiness=false. Elapsed: 12.034122755s
Mar 12 09:36:34.304: INFO: Pod "client-containers-50d3d434-e5bb-4fe5-b6d2-cdb570328e89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.039242604s
STEP: Saw pod success
Mar 12 09:36:34.304: INFO: Pod "client-containers-50d3d434-e5bb-4fe5-b6d2-cdb570328e89" satisfied condition "success or failure"
Mar 12 09:36:34.308: INFO: Trying to get logs from node node-3 pod client-containers-50d3d434-e5bb-4fe5-b6d2-cdb570328e89 container test-container: <nil>
STEP: delete the pod
Mar 12 09:36:34.340: INFO: Waiting for pod client-containers-50d3d434-e5bb-4fe5-b6d2-cdb570328e89 to disappear
Mar 12 09:36:34.343: INFO: Pod client-containers-50d3d434-e5bb-4fe5-b6d2-cdb570328e89 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:36:34.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2821" for this suite.
Mar 12 09:36:40.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:36:40.448: INFO: namespace containers-2821 deletion completed in 6.100140913s

• [SLOW TEST:20.256 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:36:40.448: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar 12 09:36:40.512: INFO: Waiting up to 5m0s for pod "downward-api-6ac0b9c3-0e39-4cb2-a0dd-cfe40b3ce204" in namespace "downward-api-6088" to be "success or failure"
Mar 12 09:36:40.515: INFO: Pod "downward-api-6ac0b9c3-0e39-4cb2-a0dd-cfe40b3ce204": Phase="Pending", Reason="", readiness=false. Elapsed: 2.771283ms
Mar 12 09:36:42.520: INFO: Pod "downward-api-6ac0b9c3-0e39-4cb2-a0dd-cfe40b3ce204": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008077043s
Mar 12 09:36:44.525: INFO: Pod "downward-api-6ac0b9c3-0e39-4cb2-a0dd-cfe40b3ce204": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012957138s
Mar 12 09:36:46.530: INFO: Pod "downward-api-6ac0b9c3-0e39-4cb2-a0dd-cfe40b3ce204": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017791233s
Mar 12 09:36:48.535: INFO: Pod "downward-api-6ac0b9c3-0e39-4cb2-a0dd-cfe40b3ce204": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023366945s
Mar 12 09:36:50.540: INFO: Pod "downward-api-6ac0b9c3-0e39-4cb2-a0dd-cfe40b3ce204": Phase="Pending", Reason="", readiness=false. Elapsed: 10.028299917s
Mar 12 09:36:52.546: INFO: Pod "downward-api-6ac0b9c3-0e39-4cb2-a0dd-cfe40b3ce204": Phase="Pending", Reason="", readiness=false. Elapsed: 12.03377286s
Mar 12 09:36:54.553: INFO: Pod "downward-api-6ac0b9c3-0e39-4cb2-a0dd-cfe40b3ce204": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.041237924s
STEP: Saw pod success
Mar 12 09:36:54.553: INFO: Pod "downward-api-6ac0b9c3-0e39-4cb2-a0dd-cfe40b3ce204" satisfied condition "success or failure"
Mar 12 09:36:54.557: INFO: Trying to get logs from node node-3 pod downward-api-6ac0b9c3-0e39-4cb2-a0dd-cfe40b3ce204 container dapi-container: <nil>
STEP: delete the pod
Mar 12 09:36:54.581: INFO: Waiting for pod downward-api-6ac0b9c3-0e39-4cb2-a0dd-cfe40b3ce204 to disappear
Mar 12 09:36:54.583: INFO: Pod downward-api-6ac0b9c3-0e39-4cb2-a0dd-cfe40b3ce204 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:36:54.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6088" for this suite.
Mar 12 09:37:00.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:37:00.695: INFO: namespace downward-api-6088 deletion completed in 6.107928115s

• [SLOW TEST:20.247 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:37:00.696: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:37:16.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6790" for this suite.
Mar 12 09:37:22.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:37:22.902: INFO: namespace kubelet-test-6790 deletion completed in 6.104090073s

• [SLOW TEST:22.207 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:37:22.903: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Mar 12 09:37:33.001: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:37:33.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0312 09:37:33.000921      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-1216" for this suite.
Mar 12 09:37:39.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:37:39.148: INFO: namespace gc-1216 deletion completed in 6.141079456s

• [SLOW TEST:16.245 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:37:39.148: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 09:37:39.200: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:37:45.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9106" for this suite.
Mar 12 09:37:51.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:37:51.368: INFO: namespace custom-resource-definition-9106 deletion completed in 6.109206407s

• [SLOW TEST:12.220 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:37:51.368: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3149
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3149
STEP: creating replication controller externalsvc in namespace services-3149
I0312 09:37:51.452087      24 runners.go:184] Created replication controller with name: externalsvc, namespace: services-3149, replica count: 2
I0312 09:37:54.502650      24 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 09:37:57.502910      24 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 09:38:00.503259      24 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 09:38:03.503458      24 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 09:38:06.503777      24 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Mar 12 09:38:06.526: INFO: Creating new exec pod
Mar 12 09:38:20.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=services-3149 execpod28422 -- /bin/sh -x -c nslookup clusterip-service'
Mar 12 09:38:21.159: INFO: stderr: "+ nslookup clusterip-service\n"
Mar 12 09:38:21.159: INFO: stdout: "Server:\t\t10.233.0.3\nAddress:\t10.233.0.3#53\n\nclusterip-service.services-3149.svc.cluster.local\tcanonical name = externalsvc.services-3149.svc.cluster.local.\nName:\texternalsvc.services-3149.svc.cluster.local\nAddress: 10.233.32.249\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3149, will wait for the garbage collector to delete the pods
Mar 12 09:38:21.225: INFO: Deleting ReplicationController externalsvc took: 11.132592ms
Mar 12 09:38:21.325: INFO: Terminating ReplicationController externalsvc pods took: 100.237814ms
Mar 12 09:38:31.653: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:38:31.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3149" for this suite.
Mar 12 09:38:37.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:38:37.784: INFO: namespace services-3149 deletion completed in 6.111613179s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:46.416 seconds]
[sig-network] Services
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:38:37.785: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 12 09:38:37.853: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cd23bdab-1f8b-4e33-bd9f-6e8655567acc" in namespace "downward-api-33" to be "success or failure"
Mar 12 09:38:37.859: INFO: Pod "downwardapi-volume-cd23bdab-1f8b-4e33-bd9f-6e8655567acc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.290095ms
Mar 12 09:38:39.864: INFO: Pod "downwardapi-volume-cd23bdab-1f8b-4e33-bd9f-6e8655567acc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011362963s
Mar 12 09:38:41.869: INFO: Pod "downwardapi-volume-cd23bdab-1f8b-4e33-bd9f-6e8655567acc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015688331s
Mar 12 09:38:43.874: INFO: Pod "downwardapi-volume-cd23bdab-1f8b-4e33-bd9f-6e8655567acc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020477351s
Mar 12 09:38:45.878: INFO: Pod "downwardapi-volume-cd23bdab-1f8b-4e33-bd9f-6e8655567acc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024825196s
Mar 12 09:38:47.883: INFO: Pod "downwardapi-volume-cd23bdab-1f8b-4e33-bd9f-6e8655567acc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.029891508s
Mar 12 09:38:49.888: INFO: Pod "downwardapi-volume-cd23bdab-1f8b-4e33-bd9f-6e8655567acc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.034677605s
Mar 12 09:38:51.897: INFO: Pod "downwardapi-volume-cd23bdab-1f8b-4e33-bd9f-6e8655567acc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.044389417s
STEP: Saw pod success
Mar 12 09:38:51.897: INFO: Pod "downwardapi-volume-cd23bdab-1f8b-4e33-bd9f-6e8655567acc" satisfied condition "success or failure"
Mar 12 09:38:51.902: INFO: Trying to get logs from node node-3 pod downwardapi-volume-cd23bdab-1f8b-4e33-bd9f-6e8655567acc container client-container: <nil>
STEP: delete the pod
Mar 12 09:38:51.945: INFO: Waiting for pod downwardapi-volume-cd23bdab-1f8b-4e33-bd9f-6e8655567acc to disappear
Mar 12 09:38:51.949: INFO: Pod downwardapi-volume-cd23bdab-1f8b-4e33-bd9f-6e8655567acc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:38:51.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-33" for this suite.
Mar 12 09:38:57.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:38:58.079: INFO: namespace downward-api-33 deletion completed in 6.125172806s

• [SLOW TEST:20.295 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:38:58.080: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 09:38:58.128: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:39:58.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6708" for this suite.
Mar 12 09:40:04.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:40:04.593: INFO: namespace custom-resource-definition-6708 deletion completed in 6.103871383s

• [SLOW TEST:66.514 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:40:04.594: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:40:20.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2302" for this suite.
Mar 12 09:40:26.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:40:26.843: INFO: namespace resourcequota-2302 deletion completed in 6.134716232s

• [SLOW TEST:22.249 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:40:26.843: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:40:31.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8006" for this suite.
Mar 12 09:40:37.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:40:37.847: INFO: namespace watch-8006 deletion completed in 6.195842871s

• [SLOW TEST:11.004 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:40:37.847: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:40:52.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3470" for this suite.
Mar 12 09:41:04.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:41:05.052: INFO: namespace replication-controller-3470 deletion completed in 12.103663763s

• [SLOW TEST:27.205 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:41:05.052: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Mar 12 09:41:19.666: INFO: Successfully updated pod "annotationupdate3c498b72-4a15-4afe-81bb-e5a53e00107d"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:41:21.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7083" for this suite.
Mar 12 09:41:33.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:41:33.802: INFO: namespace downward-api-7083 deletion completed in 12.105695027s

• [SLOW TEST:28.750 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:41:33.803: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-01dee2bd-d7a6-44fb-82ef-af62f27d51fc
STEP: Creating a pod to test consume secrets
Mar 12 09:41:33.878: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1f0fd3b3-f505-4395-a9d1-8544468ed332" in namespace "projected-3874" to be "success or failure"
Mar 12 09:41:33.886: INFO: Pod "pod-projected-secrets-1f0fd3b3-f505-4395-a9d1-8544468ed332": Phase="Pending", Reason="", readiness=false. Elapsed: 8.346837ms
Mar 12 09:41:35.890: INFO: Pod "pod-projected-secrets-1f0fd3b3-f505-4395-a9d1-8544468ed332": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012664071s
Mar 12 09:41:37.895: INFO: Pod "pod-projected-secrets-1f0fd3b3-f505-4395-a9d1-8544468ed332": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017495559s
Mar 12 09:41:39.912: INFO: Pod "pod-projected-secrets-1f0fd3b3-f505-4395-a9d1-8544468ed332": Phase="Pending", Reason="", readiness=false. Elapsed: 6.034325043s
Mar 12 09:41:41.917: INFO: Pod "pod-projected-secrets-1f0fd3b3-f505-4395-a9d1-8544468ed332": Phase="Pending", Reason="", readiness=false. Elapsed: 8.039514283s
Mar 12 09:41:43.922: INFO: Pod "pod-projected-secrets-1f0fd3b3-f505-4395-a9d1-8544468ed332": Phase="Pending", Reason="", readiness=false. Elapsed: 10.0439904s
Mar 12 09:41:45.928: INFO: Pod "pod-projected-secrets-1f0fd3b3-f505-4395-a9d1-8544468ed332": Phase="Pending", Reason="", readiness=false. Elapsed: 12.050046678s
Mar 12 09:41:47.933: INFO: Pod "pod-projected-secrets-1f0fd3b3-f505-4395-a9d1-8544468ed332": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.055270998s
STEP: Saw pod success
Mar 12 09:41:47.933: INFO: Pod "pod-projected-secrets-1f0fd3b3-f505-4395-a9d1-8544468ed332" satisfied condition "success or failure"
Mar 12 09:41:47.937: INFO: Trying to get logs from node node-2 pod pod-projected-secrets-1f0fd3b3-f505-4395-a9d1-8544468ed332 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 12 09:41:47.973: INFO: Waiting for pod pod-projected-secrets-1f0fd3b3-f505-4395-a9d1-8544468ed332 to disappear
Mar 12 09:41:47.976: INFO: Pod pod-projected-secrets-1f0fd3b3-f505-4395-a9d1-8544468ed332 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:41:47.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3874" for this suite.
Mar 12 09:41:53.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:41:54.079: INFO: namespace projected-3874 deletion completed in 6.098395567s

• [SLOW TEST:20.277 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:41:54.080: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:42:08.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8862" for this suite.
Mar 12 09:42:52.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:42:52.280: INFO: namespace kubelet-test-8862 deletion completed in 44.115985206s

• [SLOW TEST:58.201 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:42:52.281: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-bvkx
STEP: Creating a pod to test atomic-volume-subpath
Mar 12 09:42:52.364: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-bvkx" in namespace "subpath-7868" to be "success or failure"
Mar 12 09:42:52.369: INFO: Pod "pod-subpath-test-configmap-bvkx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.714111ms
Mar 12 09:42:54.374: INFO: Pod "pod-subpath-test-configmap-bvkx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010000914s
Mar 12 09:42:56.379: INFO: Pod "pod-subpath-test-configmap-bvkx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015071089s
Mar 12 09:42:58.384: INFO: Pod "pod-subpath-test-configmap-bvkx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019648638s
Mar 12 09:43:00.389: INFO: Pod "pod-subpath-test-configmap-bvkx": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025163043s
Mar 12 09:43:02.395: INFO: Pod "pod-subpath-test-configmap-bvkx": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030458765s
Mar 12 09:43:04.400: INFO: Pod "pod-subpath-test-configmap-bvkx": Phase="Pending", Reason="", readiness=false. Elapsed: 12.035810453s
Mar 12 09:43:06.407: INFO: Pod "pod-subpath-test-configmap-bvkx": Phase="Running", Reason="", readiness=true. Elapsed: 14.042680625s
Mar 12 09:43:08.411: INFO: Pod "pod-subpath-test-configmap-bvkx": Phase="Running", Reason="", readiness=true. Elapsed: 16.04693536s
Mar 12 09:43:10.416: INFO: Pod "pod-subpath-test-configmap-bvkx": Phase="Running", Reason="", readiness=true. Elapsed: 18.051474262s
Mar 12 09:43:12.420: INFO: Pod "pod-subpath-test-configmap-bvkx": Phase="Running", Reason="", readiness=true. Elapsed: 20.056199294s
Mar 12 09:43:14.425: INFO: Pod "pod-subpath-test-configmap-bvkx": Phase="Running", Reason="", readiness=true. Elapsed: 22.060547624s
Mar 12 09:43:16.430: INFO: Pod "pod-subpath-test-configmap-bvkx": Phase="Running", Reason="", readiness=true. Elapsed: 24.065496051s
Mar 12 09:43:18.435: INFO: Pod "pod-subpath-test-configmap-bvkx": Phase="Running", Reason="", readiness=true. Elapsed: 26.070700211s
Mar 12 09:43:20.440: INFO: Pod "pod-subpath-test-configmap-bvkx": Phase="Running", Reason="", readiness=true. Elapsed: 28.075585654s
Mar 12 09:43:22.445: INFO: Pod "pod-subpath-test-configmap-bvkx": Phase="Running", Reason="", readiness=true. Elapsed: 30.080540178s
Mar 12 09:43:24.449: INFO: Pod "pod-subpath-test-configmap-bvkx": Phase="Running", Reason="", readiness=true. Elapsed: 32.085092788s
Mar 12 09:43:26.454: INFO: Pod "pod-subpath-test-configmap-bvkx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.090057444s
STEP: Saw pod success
Mar 12 09:43:26.454: INFO: Pod "pod-subpath-test-configmap-bvkx" satisfied condition "success or failure"
Mar 12 09:43:26.457: INFO: Trying to get logs from node node-2 pod pod-subpath-test-configmap-bvkx container test-container-subpath-configmap-bvkx: <nil>
STEP: delete the pod
Mar 12 09:43:26.489: INFO: Waiting for pod pod-subpath-test-configmap-bvkx to disappear
Mar 12 09:43:26.497: INFO: Pod pod-subpath-test-configmap-bvkx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-bvkx
Mar 12 09:43:26.497: INFO: Deleting pod "pod-subpath-test-configmap-bvkx" in namespace "subpath-7868"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:43:26.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7868" for this suite.
Mar 12 09:43:32.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:43:32.629: INFO: namespace subpath-7868 deletion completed in 6.122253282s

• [SLOW TEST:40.348 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:43:32.630: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-b71f27bc-9d09-4b77-8db9-a1c989523a21
STEP: Creating a pod to test consume secrets
Mar 12 09:43:32.729: INFO: Waiting up to 5m0s for pod "pod-secrets-0e75be67-49f3-4d11-8e7f-ee3c3e26e73e" in namespace "secrets-2526" to be "success or failure"
Mar 12 09:43:32.732: INFO: Pod "pod-secrets-0e75be67-49f3-4d11-8e7f-ee3c3e26e73e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.146331ms
Mar 12 09:43:34.737: INFO: Pod "pod-secrets-0e75be67-49f3-4d11-8e7f-ee3c3e26e73e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00784542s
Mar 12 09:43:36.742: INFO: Pod "pod-secrets-0e75be67-49f3-4d11-8e7f-ee3c3e26e73e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012564225s
Mar 12 09:43:38.746: INFO: Pod "pod-secrets-0e75be67-49f3-4d11-8e7f-ee3c3e26e73e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016990149s
Mar 12 09:43:40.751: INFO: Pod "pod-secrets-0e75be67-49f3-4d11-8e7f-ee3c3e26e73e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021991494s
Mar 12 09:43:42.755: INFO: Pod "pod-secrets-0e75be67-49f3-4d11-8e7f-ee3c3e26e73e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.026234981s
Mar 12 09:43:44.760: INFO: Pod "pod-secrets-0e75be67-49f3-4d11-8e7f-ee3c3e26e73e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.030950483s
Mar 12 09:43:46.766: INFO: Pod "pod-secrets-0e75be67-49f3-4d11-8e7f-ee3c3e26e73e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.036933015s
STEP: Saw pod success
Mar 12 09:43:46.766: INFO: Pod "pod-secrets-0e75be67-49f3-4d11-8e7f-ee3c3e26e73e" satisfied condition "success or failure"
Mar 12 09:43:46.769: INFO: Trying to get logs from node node-2 pod pod-secrets-0e75be67-49f3-4d11-8e7f-ee3c3e26e73e container secret-volume-test: <nil>
STEP: delete the pod
Mar 12 09:43:46.795: INFO: Waiting for pod pod-secrets-0e75be67-49f3-4d11-8e7f-ee3c3e26e73e to disappear
Mar 12 09:43:46.802: INFO: Pod pod-secrets-0e75be67-49f3-4d11-8e7f-ee3c3e26e73e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:43:46.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2526" for this suite.
Mar 12 09:43:52.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:43:52.918: INFO: namespace secrets-2526 deletion completed in 6.1110925s

• [SLOW TEST:20.289 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:43:52.919: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 09:43:52.965: INFO: Creating deployment "webserver-deployment"
Mar 12 09:43:52.977: INFO: Waiting for observed generation 1
Mar 12 09:43:54.987: INFO: Waiting for all required pods to come up
Mar 12 09:43:54.993: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar 12 09:44:11.004: INFO: Waiting for deployment "webserver-deployment" to complete
Mar 12 09:44:11.011: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar 12 09:44:11.021: INFO: Updating deployment webserver-deployment
Mar 12 09:44:11.021: INFO: Waiting for observed generation 2
Mar 12 09:44:13.032: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 12 09:44:13.037: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 12 09:44:13.041: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 12 09:44:13.055: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 12 09:44:13.055: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 12 09:44:13.059: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 12 09:44:13.064: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar 12 09:44:13.064: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar 12 09:44:13.073: INFO: Updating deployment webserver-deployment
Mar 12 09:44:13.073: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar 12 09:44:13.094: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 12 09:44:13.101: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar 12 09:44:13.126: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-3109 /apis/apps/v1/namespaces/deployment-3109/deployments/webserver-deployment e03c7d34-e940-4694-a9e4-3580a25ee072 163890 3 2020-03-12 09:43:52 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0062aa538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-03-12 09:44:11 +0000 UTC,LastTransitionTime:2020-03-12 09:43:52 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-03-12 09:44:13 +0000 UTC,LastTransitionTime:2020-03-12 09:44:13 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar 12 09:44:13.139: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-3109 /apis/apps/v1/namespaces/deployment-3109/replicasets/webserver-deployment-c7997dcc8 d01326dc-3d02-40fd-b58f-2c6c1b7fcd3d 163886 3 2020-03-12 09:44:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e03c7d34-e940-4694-a9e4-3580a25ee072 0xc0062aaa47 0xc0062aaa48}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0062aaab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 12 09:44:13.139: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar 12 09:44:13.139: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-3109 /apis/apps/v1/namespaces/deployment-3109/replicasets/webserver-deployment-595b5b9587 06d63b49-4e83-44e9-ae84-b7464103a24e 163885 3 2020-03-12 09:43:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e03c7d34-e940-4694-a9e4-3580a25ee072 0xc0062aa987 0xc0062aa988}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0062aa9e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar 12 09:44:13.144: INFO: Pod "webserver-deployment-595b5b9587-2tn6s" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2tn6s webserver-deployment-595b5b9587- deployment-3109 /api/v1/namespaces/deployment-3109/pods/webserver-deployment-595b5b9587-2tn6s 81896ec4-d480-4757-b6f1-93815639f5e6 163891 0 2020-03-12 09:44:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 06d63b49-4e83-44e9-ae84-b7464103a24e 0xc002be54c7 0xc002be54c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vpb6h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vpb6h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vpb6h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 12 09:44:13.144: INFO: Pod "webserver-deployment-595b5b9587-4lz6d" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4lz6d webserver-deployment-595b5b9587- deployment-3109 /api/v1/namespaces/deployment-3109/pods/webserver-deployment-595b5b9587-4lz6d 762a23f3-8c3e-4b1d-96d8-6ec60dcf5b47 163767 0 2020-03-12 09:43:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 06d63b49-4e83-44e9-ae84-b7464103a24e 0xc002be55e0 0xc002be55e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vpb6h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vpb6h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vpb6h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:43:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:43:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.20.5,PodIP:10.233.65.248,StartTime:2020-03-12 09:43:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-12 09:44:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a1c0feada00f6e658b58f12630923e23f6065fea178f17370aa36901fd749416,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.248,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 12 09:44:13.144: INFO: Pod "webserver-deployment-595b5b9587-5d527" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5d527 webserver-deployment-595b5b9587- deployment-3109 /api/v1/namespaces/deployment-3109/pods/webserver-deployment-595b5b9587-5d527 b4876bc9-c7de-47cf-822a-184e9cbf8ce1 163785 0 2020-03-12 09:43:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 06d63b49-4e83-44e9-ae84-b7464103a24e 0xc002be5757 0xc002be5758}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vpb6h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vpb6h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vpb6h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:43:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:43:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.20.4,PodIP:10.233.64.138,StartTime:2020-03-12 09:43:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-12 09:44:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://8aa605cbf6fd764f5e997d13b3f1e878db7187bb371581097dfc83819d5fea78,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.138,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 12 09:44:13.144: INFO: Pod "webserver-deployment-595b5b9587-5rctz" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5rctz webserver-deployment-595b5b9587- deployment-3109 /api/v1/namespaces/deployment-3109/pods/webserver-deployment-595b5b9587-5rctz f45a4f10-48ce-40fd-9627-6c18936ff9d1 163792 0 2020-03-12 09:43:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 06d63b49-4e83-44e9-ae84-b7464103a24e 0xc002be58d7 0xc002be58d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vpb6h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vpb6h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vpb6h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:43:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:43:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.20.4,PodIP:10.233.64.137,StartTime:2020-03-12 09:43:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-12 09:44:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c72265886b62ee0f69626b5559110a05ed2d152ba725e7317b0f8d05292f0b15,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.137,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 12 09:44:13.144: INFO: Pod "webserver-deployment-595b5b9587-p9bnz" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-p9bnz webserver-deployment-595b5b9587- deployment-3109 /api/v1/namespaces/deployment-3109/pods/webserver-deployment-595b5b9587-p9bnz f59c225e-083d-48d5-acf0-031672cedf8a 163799 0 2020-03-12 09:43:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 06d63b49-4e83-44e9-ae84-b7464103a24e 0xc002be5a57 0xc002be5a58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vpb6h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vpb6h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vpb6h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:43:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:43:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.20.3,PodIP:10.233.66.70,StartTime:2020-03-12 09:43:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-12 09:44:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c32ab3179360c25df9d3f69cdd6a5955b1f4d17ce339c45edd42c00316b1fb42,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.70,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 12 09:44:13.145: INFO: Pod "webserver-deployment-595b5b9587-pj4qc" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pj4qc webserver-deployment-595b5b9587- deployment-3109 /api/v1/namespaces/deployment-3109/pods/webserver-deployment-595b5b9587-pj4qc 7d98ffaf-b172-44f2-b57b-9fd8e5dae06a 163806 0 2020-03-12 09:43:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 06d63b49-4e83-44e9-ae84-b7464103a24e 0xc002be5bd7 0xc002be5bd8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vpb6h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vpb6h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vpb6h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:43:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:43:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.20.3,PodIP:10.233.66.69,StartTime:2020-03-12 09:43:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-12 09:44:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://d3b78079e0330875a1eb23561db8ce1475cb5923492c8c73863ed9dbd821d87f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.69,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 12 09:44:13.145: INFO: Pod "webserver-deployment-595b5b9587-qqvbc" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qqvbc webserver-deployment-595b5b9587- deployment-3109 /api/v1/namespaces/deployment-3109/pods/webserver-deployment-595b5b9587-qqvbc 001b59e1-5cdf-4020-84f5-75adcc2147e7 163814 0 2020-03-12 09:43:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 06d63b49-4e83-44e9-ae84-b7464103a24e 0xc002be5d67 0xc002be5d68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vpb6h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vpb6h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vpb6h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:43:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:43:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.20.5,PodIP:10.233.65.249,StartTime:2020-03-12 09:43:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-12 09:44:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://6f9ebda4221c23199aa59105d39d0513c0e647f56b82853dfcbb1cfaac4a1801,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.249,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 12 09:44:13.145: INFO: Pod "webserver-deployment-595b5b9587-scwth" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-scwth webserver-deployment-595b5b9587- deployment-3109 /api/v1/namespaces/deployment-3109/pods/webserver-deployment-595b5b9587-scwth 112d11b4-545c-4066-9aa2-fd27370443f1 163818 0 2020-03-12 09:43:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 06d63b49-4e83-44e9-ae84-b7464103a24e 0xc002be5ee7 0xc002be5ee8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vpb6h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vpb6h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vpb6h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:43:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:43:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.20.5,PodIP:10.233.65.250,StartTime:2020-03-12 09:43:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-12 09:44:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://b7053c81eec48d7f7e7bb13f75202d54722313dc4c197121eb72b1a58c91ad6c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 12 09:44:13.145: INFO: Pod "webserver-deployment-595b5b9587-xh6z6" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xh6z6 webserver-deployment-595b5b9587- deployment-3109 /api/v1/namespaces/deployment-3109/pods/webserver-deployment-595b5b9587-xh6z6 d963363a-4c5e-48da-b1ef-9d2dd042a6ea 163809 0 2020-03-12 09:43:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 06d63b49-4e83-44e9-ae84-b7464103a24e 0xc005fa6067 0xc005fa6068}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vpb6h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vpb6h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vpb6h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:43:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:43:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.20.4,PodIP:10.233.64.139,StartTime:2020-03-12 09:43:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-12 09:44:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://3c03993e2e0b4f5cf4289835dab45b7f42e537cc3c901e728467a3b8fd72b070,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 12 09:44:13.145: INFO: Pod "webserver-deployment-c7997dcc8-56v2c" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-56v2c webserver-deployment-c7997dcc8- deployment-3109 /api/v1/namespaces/deployment-3109/pods/webserver-deployment-c7997dcc8-56v2c 98a97907-1abd-49f4-8855-70f0fd69b32a 163872 0 2020-03-12 09:44:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d01326dc-3d02-40fd-b58f-2c6c1b7fcd3d 0xc005fa61e7 0xc005fa61e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vpb6h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vpb6h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vpb6h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.20.4,PodIP:,StartTime:2020-03-12 09:44:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 12 09:44:13.146: INFO: Pod "webserver-deployment-c7997dcc8-cpgbd" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-cpgbd webserver-deployment-c7997dcc8- deployment-3109 /api/v1/namespaces/deployment-3109/pods/webserver-deployment-c7997dcc8-cpgbd 0eb9d922-0c17-46ee-a012-bd45f4b24513 163880 0 2020-03-12 09:44:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d01326dc-3d02-40fd-b58f-2c6c1b7fcd3d 0xc005fa6367 0xc005fa6368}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vpb6h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vpb6h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vpb6h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.20.3,PodIP:,StartTime:2020-03-12 09:44:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 12 09:44:13.146: INFO: Pod "webserver-deployment-c7997dcc8-fd6md" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-fd6md webserver-deployment-c7997dcc8- deployment-3109 /api/v1/namespaces/deployment-3109/pods/webserver-deployment-c7997dcc8-fd6md ec956ece-a5c1-4d38-88ef-7c373efe6fb8 163849 0 2020-03-12 09:44:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d01326dc-3d02-40fd-b58f-2c6c1b7fcd3d 0xc005fa64e7 0xc005fa64e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vpb6h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vpb6h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vpb6h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.20.5,PodIP:,StartTime:2020-03-12 09:44:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 12 09:44:13.146: INFO: Pod "webserver-deployment-c7997dcc8-p9k5s" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-p9k5s webserver-deployment-c7997dcc8- deployment-3109 /api/v1/namespaces/deployment-3109/pods/webserver-deployment-c7997dcc8-p9k5s f7576cf7-4ed9-4f0a-b45d-4033c55db5b1 163892 0 2020-03-12 09:44:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d01326dc-3d02-40fd-b58f-2c6c1b7fcd3d 0xc005fa6667 0xc005fa6668}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vpb6h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vpb6h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vpb6h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 12 09:44:13.146: INFO: Pod "webserver-deployment-c7997dcc8-sn2hc" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-sn2hc webserver-deployment-c7997dcc8- deployment-3109 /api/v1/namespaces/deployment-3109/pods/webserver-deployment-c7997dcc8-sn2hc 10146eac-81e5-4dc9-859c-56c89e57b43a 163850 0 2020-03-12 09:44:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d01326dc-3d02-40fd-b58f-2c6c1b7fcd3d 0xc005fa67b0 0xc005fa67b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vpb6h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vpb6h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vpb6h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.20.4,PodIP:,StartTime:2020-03-12 09:44:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 12 09:44:13.146: INFO: Pod "webserver-deployment-c7997dcc8-z9wwn" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-z9wwn webserver-deployment-c7997dcc8- deployment-3109 /api/v1/namespaces/deployment-3109/pods/webserver-deployment-c7997dcc8-z9wwn 4f440991-9ad5-4e58-89c3-bc0505abdbad 163854 0 2020-03-12 09:44:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d01326dc-3d02-40fd-b58f-2c6c1b7fcd3d 0xc005fa6927 0xc005fa6928}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vpb6h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vpb6h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vpb6h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:44:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.20.3,PodIP:,StartTime:2020-03-12 09:44:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:44:13.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3109" for this suite.
Mar 12 09:44:21.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:44:21.263: INFO: namespace deployment-3109 deletion completed in 8.108562587s

• [SLOW TEST:28.345 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:44:21.264: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:44:38.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8167" for this suite.
Mar 12 09:44:44.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:44:44.496: INFO: namespace resourcequota-8167 deletion completed in 6.111638157s

• [SLOW TEST:23.232 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:44:44.496: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-1148
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-1148
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1148
Mar 12 09:44:44.575: INFO: Found 0 stateful pods, waiting for 1
Mar 12 09:44:54.580: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Mar 12 09:45:04.581: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar 12 09:45:04.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=statefulset-1148 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 12 09:45:05.128: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 12 09:45:05.128: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 12 09:45:05.128: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 12 09:45:05.133: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 12 09:45:15.138: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 12 09:45:15.138: INFO: Waiting for statefulset status.replicas updated to 0
Mar 12 09:45:15.154: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Mar 12 09:45:15.154: INFO: ss-0  node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:44:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:44:44 +0000 UTC  }]
Mar 12 09:45:15.154: INFO: 
Mar 12 09:45:15.154: INFO: StatefulSet ss has not reached scale 3, at 1
Mar 12 09:45:16.160: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996878943s
Mar 12 09:45:17.165: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990947953s
Mar 12 09:45:18.170: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985840623s
Mar 12 09:45:19.176: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980673395s
Mar 12 09:45:20.182: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.974590513s
Mar 12 09:45:21.187: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.968781535s
Mar 12 09:45:22.192: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.964003771s
Mar 12 09:45:23.196: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.958561851s
Mar 12 09:45:24.201: INFO: Verifying statefulset ss doesn't scale past 3 for another 954.496888ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1148
Mar 12 09:45:25.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=statefulset-1148 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 12 09:45:25.395: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 12 09:45:25.395: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 12 09:45:25.395: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 12 09:45:25.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=statefulset-1148 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 12 09:45:25.531: INFO: rc: 1
Mar 12 09:45:25.531: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=statefulset-1148 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Mar 12 09:45:35.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=statefulset-1148 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 12 09:45:35.730: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 12 09:45:35.730: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 12 09:45:35.730: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 12 09:45:35.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=statefulset-1148 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 12 09:45:35.925: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 12 09:45:35.925: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 12 09:45:35.925: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 12 09:45:35.929: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 12 09:45:35.929: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 12 09:45:35.929: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar 12 09:45:35.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=statefulset-1148 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 12 09:45:36.114: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 12 09:45:36.114: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 12 09:45:36.114: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 12 09:45:36.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=statefulset-1148 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 12 09:45:36.312: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 12 09:45:36.312: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 12 09:45:36.312: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 12 09:45:36.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=statefulset-1148 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 12 09:45:36.528: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 12 09:45:36.528: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 12 09:45:36.528: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 12 09:45:36.528: INFO: Waiting for statefulset status.replicas updated to 0
Mar 12 09:45:36.533: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 12 09:45:46.542: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 12 09:45:46.542: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 12 09:45:46.542: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 12 09:45:46.556: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Mar 12 09:45:46.556: INFO: ss-0  node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:44:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:44:44 +0000 UTC  }]
Mar 12 09:45:46.556: INFO: ss-1  node-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:15 +0000 UTC  }]
Mar 12 09:45:46.556: INFO: ss-2  node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:15 +0000 UTC  }]
Mar 12 09:45:46.556: INFO: 
Mar 12 09:45:46.556: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 12 09:45:47.562: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Mar 12 09:45:47.562: INFO: ss-0  node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:44:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:44:44 +0000 UTC  }]
Mar 12 09:45:47.562: INFO: ss-1  node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:15 +0000 UTC  }]
Mar 12 09:45:47.562: INFO: ss-2  node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:15 +0000 UTC  }]
Mar 12 09:45:47.562: INFO: 
Mar 12 09:45:47.562: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 12 09:45:48.567: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Mar 12 09:45:48.567: INFO: ss-2  node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:15 +0000 UTC  }]
Mar 12 09:45:48.567: INFO: 
Mar 12 09:45:48.567: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 12 09:45:49.573: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Mar 12 09:45:49.573: INFO: ss-2  node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:15 +0000 UTC  }]
Mar 12 09:45:49.573: INFO: 
Mar 12 09:45:49.573: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 12 09:45:50.578: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Mar 12 09:45:50.578: INFO: ss-2  node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:15 +0000 UTC  }]
Mar 12 09:45:50.578: INFO: 
Mar 12 09:45:50.578: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 12 09:45:51.584: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Mar 12 09:45:51.584: INFO: ss-2  node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-12 09:45:15 +0000 UTC  }]
Mar 12 09:45:51.584: INFO: 
Mar 12 09:45:51.584: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 12 09:45:52.589: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.967463853s
Mar 12 09:45:53.594: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.962251813s
Mar 12 09:45:54.599: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.957229221s
Mar 12 09:45:55.603: INFO: Verifying statefulset ss doesn't scale past 0 for another 952.534366ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1148
Mar 12 09:45:56.608: INFO: Scaling statefulset ss to 0
Mar 12 09:45:56.618: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar 12 09:45:56.620: INFO: Deleting all statefulset in ns statefulset-1148
Mar 12 09:45:56.623: INFO: Scaling statefulset ss to 0
Mar 12 09:45:56.632: INFO: Waiting for statefulset status.replicas updated to 0
Mar 12 09:45:56.634: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:45:56.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1148" for this suite.
Mar 12 09:46:02.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:46:02.745: INFO: namespace statefulset-1148 deletion completed in 6.092897113s

• [SLOW TEST:78.249 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:46:02.746: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar 12 09:46:08.920: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:46:08.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0312 09:46:08.920415      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-1661" for this suite.
Mar 12 09:46:14.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:46:15.026: INFO: namespace gc-1661 deletion completed in 6.102076801s

• [SLOW TEST:12.280 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:46:15.026: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Mar 12 09:46:15.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 create -f - --namespace=kubectl-3551'
Mar 12 09:46:15.352: INFO: stderr: ""
Mar 12 09:46:15.352: INFO: stdout: "pod/pause created\n"
Mar 12 09:46:15.352: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 12 09:46:15.352: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3551" to be "running and ready"
Mar 12 09:46:15.355: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.980076ms
Mar 12 09:46:17.360: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007643244s
Mar 12 09:46:19.365: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012841065s
Mar 12 09:46:21.369: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016669362s
Mar 12 09:46:23.373: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021234993s
Mar 12 09:46:25.378: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02571572s
Mar 12 09:46:27.383: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.031117405s
Mar 12 09:46:29.389: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 14.036996735s
Mar 12 09:46:29.389: INFO: Pod "pause" satisfied condition "running and ready"
Mar 12 09:46:29.389: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Mar 12 09:46:29.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 label pods pause testing-label=testing-label-value --namespace=kubectl-3551'
Mar 12 09:46:29.510: INFO: stderr: ""
Mar 12 09:46:29.510: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar 12 09:46:29.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pod pause -L testing-label --namespace=kubectl-3551'
Mar 12 09:46:29.612: INFO: stderr: ""
Mar 12 09:46:29.612: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          14s   testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar 12 09:46:29.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 label pods pause testing-label- --namespace=kubectl-3551'
Mar 12 09:46:29.714: INFO: stderr: ""
Mar 12 09:46:29.714: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar 12 09:46:29.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pod pause -L testing-label --namespace=kubectl-3551'
Mar 12 09:46:29.810: INFO: stderr: ""
Mar 12 09:46:29.810: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          14s   \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Mar 12 09:46:29.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 delete --grace-period=0 --force -f - --namespace=kubectl-3551'
Mar 12 09:46:29.912: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 12 09:46:29.912: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 12 09:46:29.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get rc,svc -l name=pause --no-headers --namespace=kubectl-3551'
Mar 12 09:46:30.016: INFO: stderr: "No resources found in kubectl-3551 namespace.\n"
Mar 12 09:46:30.016: INFO: stdout: ""
Mar 12 09:46:30.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 get pods -l name=pause --namespace=kubectl-3551 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 12 09:46:30.124: INFO: stderr: ""
Mar 12 09:46:30.124: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:46:30.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3551" for this suite.
Mar 12 09:46:36.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:46:36.250: INFO: namespace kubectl-3551 deletion completed in 6.12078598s

• [SLOW TEST:21.224 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:46:36.250: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 12 09:46:36.996: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 12 09:46:39.010: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603197, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603197, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603197, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603196, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:46:41.015: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603197, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603197, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603197, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603196, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:46:43.015: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603197, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603197, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603197, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603196, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:46:45.014: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603197, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603197, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603197, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603196, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:46:47.014: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603197, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603197, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603197, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603196, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:46:49.014: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603197, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603197, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603197, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603196, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 12 09:46:52.031: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:46:52.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5942" for this suite.
Mar 12 09:46:58.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:46:58.229: INFO: namespace webhook-5942 deletion completed in 6.097905391s
STEP: Destroying namespace "webhook-5942-markers" for this suite.
Mar 12 09:47:04.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:47:04.334: INFO: namespace webhook-5942-markers deletion completed in 6.104921556s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:28.104 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:47:04.355: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar 12 09:47:04.424: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7486 /api/v1/namespaces/watch-7486/configmaps/e2e-watch-test-watch-closed eb07331a-3a4d-4957-aac5-4b2dc146e478 164718 0 2020-03-12 09:47:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 12 09:47:04.425: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7486 /api/v1/namespaces/watch-7486/configmaps/e2e-watch-test-watch-closed eb07331a-3a4d-4957-aac5-4b2dc146e478 164719 0 2020-03-12 09:47:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar 12 09:47:04.441: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7486 /api/v1/namespaces/watch-7486/configmaps/e2e-watch-test-watch-closed eb07331a-3a4d-4957-aac5-4b2dc146e478 164720 0 2020-03-12 09:47:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 12 09:47:04.441: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7486 /api/v1/namespaces/watch-7486/configmaps/e2e-watch-test-watch-closed eb07331a-3a4d-4957-aac5-4b2dc146e478 164721 0 2020-03-12 09:47:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:47:04.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7486" for this suite.
Mar 12 09:47:10.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:47:10.544: INFO: namespace watch-7486 deletion completed in 6.099066373s

• [SLOW TEST:6.190 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:47:10.544: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar 12 09:47:10.615: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7301 /api/v1/namespaces/watch-7301/configmaps/e2e-watch-test-label-changed 61fe8bf5-891a-4832-b2fa-1dab0c800fa6 164741 0 2020-03-12 09:47:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 12 09:47:10.615: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7301 /api/v1/namespaces/watch-7301/configmaps/e2e-watch-test-label-changed 61fe8bf5-891a-4832-b2fa-1dab0c800fa6 164742 0 2020-03-12 09:47:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar 12 09:47:10.615: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7301 /api/v1/namespaces/watch-7301/configmaps/e2e-watch-test-label-changed 61fe8bf5-891a-4832-b2fa-1dab0c800fa6 164743 0 2020-03-12 09:47:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar 12 09:47:20.654: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7301 /api/v1/namespaces/watch-7301/configmaps/e2e-watch-test-label-changed 61fe8bf5-891a-4832-b2fa-1dab0c800fa6 164761 0 2020-03-12 09:47:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 12 09:47:20.654: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7301 /api/v1/namespaces/watch-7301/configmaps/e2e-watch-test-label-changed 61fe8bf5-891a-4832-b2fa-1dab0c800fa6 164762 0 2020-03-12 09:47:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Mar 12 09:47:20.654: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7301 /api/v1/namespaces/watch-7301/configmaps/e2e-watch-test-label-changed 61fe8bf5-891a-4832-b2fa-1dab0c800fa6 164763 0 2020-03-12 09:47:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:47:20.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7301" for this suite.
Mar 12 09:47:26.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:47:26.770: INFO: namespace watch-7301 deletion completed in 6.109195276s

• [SLOW TEST:16.226 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:47:26.770: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 12 09:47:26.837: INFO: Waiting up to 5m0s for pod "downwardapi-volume-832df543-1fe4-407f-b5ba-20837ba94dac" in namespace "projected-8338" to be "success or failure"
Mar 12 09:47:26.840: INFO: Pod "downwardapi-volume-832df543-1fe4-407f-b5ba-20837ba94dac": Phase="Pending", Reason="", readiness=false. Elapsed: 3.190607ms
Mar 12 09:47:28.845: INFO: Pod "downwardapi-volume-832df543-1fe4-407f-b5ba-20837ba94dac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008025196s
Mar 12 09:47:30.850: INFO: Pod "downwardapi-volume-832df543-1fe4-407f-b5ba-20837ba94dac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012954243s
Mar 12 09:47:32.855: INFO: Pod "downwardapi-volume-832df543-1fe4-407f-b5ba-20837ba94dac": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017610429s
Mar 12 09:47:34.860: INFO: Pod "downwardapi-volume-832df543-1fe4-407f-b5ba-20837ba94dac": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022577933s
Mar 12 09:47:36.864: INFO: Pod "downwardapi-volume-832df543-1fe4-407f-b5ba-20837ba94dac": Phase="Pending", Reason="", readiness=false. Elapsed: 10.026931327s
Mar 12 09:47:38.869: INFO: Pod "downwardapi-volume-832df543-1fe4-407f-b5ba-20837ba94dac": Phase="Pending", Reason="", readiness=false. Elapsed: 12.031848635s
Mar 12 09:47:40.891: INFO: Pod "downwardapi-volume-832df543-1fe4-407f-b5ba-20837ba94dac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.053983538s
STEP: Saw pod success
Mar 12 09:47:40.891: INFO: Pod "downwardapi-volume-832df543-1fe4-407f-b5ba-20837ba94dac" satisfied condition "success or failure"
Mar 12 09:47:40.895: INFO: Trying to get logs from node node-2 pod downwardapi-volume-832df543-1fe4-407f-b5ba-20837ba94dac container client-container: <nil>
STEP: delete the pod
Mar 12 09:47:40.929: INFO: Waiting for pod downwardapi-volume-832df543-1fe4-407f-b5ba-20837ba94dac to disappear
Mar 12 09:47:40.934: INFO: Pod downwardapi-volume-832df543-1fe4-407f-b5ba-20837ba94dac no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:47:40.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8338" for this suite.
Mar 12 09:47:46.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:47:47.049: INFO: namespace projected-8338 deletion completed in 6.110876366s

• [SLOW TEST:20.279 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:47:47.050: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar 12 09:47:47.116: INFO: Waiting up to 5m0s for pod "downward-api-e6a09baf-46f8-4be0-8c43-de9115fdf7d0" in namespace "downward-api-4932" to be "success or failure"
Mar 12 09:47:47.120: INFO: Pod "downward-api-e6a09baf-46f8-4be0-8c43-de9115fdf7d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.551474ms
Mar 12 09:47:49.125: INFO: Pod "downward-api-e6a09baf-46f8-4be0-8c43-de9115fdf7d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009506729s
Mar 12 09:47:51.131: INFO: Pod "downward-api-e6a09baf-46f8-4be0-8c43-de9115fdf7d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015584461s
Mar 12 09:47:53.137: INFO: Pod "downward-api-e6a09baf-46f8-4be0-8c43-de9115fdf7d0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021008535s
Mar 12 09:47:55.143: INFO: Pod "downward-api-e6a09baf-46f8-4be0-8c43-de9115fdf7d0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02712505s
Mar 12 09:47:57.147: INFO: Pod "downward-api-e6a09baf-46f8-4be0-8c43-de9115fdf7d0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.031719746s
Mar 12 09:47:59.154: INFO: Pod "downward-api-e6a09baf-46f8-4be0-8c43-de9115fdf7d0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.038591658s
Mar 12 09:48:01.160: INFO: Pod "downward-api-e6a09baf-46f8-4be0-8c43-de9115fdf7d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.04440283s
STEP: Saw pod success
Mar 12 09:48:01.160: INFO: Pod "downward-api-e6a09baf-46f8-4be0-8c43-de9115fdf7d0" satisfied condition "success or failure"
Mar 12 09:48:01.163: INFO: Trying to get logs from node node-3 pod downward-api-e6a09baf-46f8-4be0-8c43-de9115fdf7d0 container dapi-container: <nil>
STEP: delete the pod
Mar 12 09:48:01.197: INFO: Waiting for pod downward-api-e6a09baf-46f8-4be0-8c43-de9115fdf7d0 to disappear
Mar 12 09:48:01.200: INFO: Pod downward-api-e6a09baf-46f8-4be0-8c43-de9115fdf7d0 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:48:01.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4932" for this suite.
Mar 12 09:48:07.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:48:07.305: INFO: namespace downward-api-4932 deletion completed in 6.099864661s

• [SLOW TEST:20.256 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:48:07.306: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 12 09:48:07.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-100'
Mar 12 09:48:07.498: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 12 09:48:07.498: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Mar 12 09:48:07.517: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-7j6q2]
Mar 12 09:48:07.517: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-7j6q2" in namespace "kubectl-100" to be "running and ready"
Mar 12 09:48:07.520: INFO: Pod "e2e-test-httpd-rc-7j6q2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.770736ms
Mar 12 09:48:09.527: INFO: Pod "e2e-test-httpd-rc-7j6q2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01054764s
Mar 12 09:48:11.532: INFO: Pod "e2e-test-httpd-rc-7j6q2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015572222s
Mar 12 09:48:13.538: INFO: Pod "e2e-test-httpd-rc-7j6q2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020865235s
Mar 12 09:48:15.543: INFO: Pod "e2e-test-httpd-rc-7j6q2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025871712s
Mar 12 09:48:17.547: INFO: Pod "e2e-test-httpd-rc-7j6q2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030654592s
Mar 12 09:48:19.553: INFO: Pod "e2e-test-httpd-rc-7j6q2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.036332844s
Mar 12 09:48:21.557: INFO: Pod "e2e-test-httpd-rc-7j6q2": Phase="Running", Reason="", readiness=true. Elapsed: 14.040224636s
Mar 12 09:48:21.557: INFO: Pod "e2e-test-httpd-rc-7j6q2" satisfied condition "running and ready"
Mar 12 09:48:21.557: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-7j6q2]
Mar 12 09:48:21.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 logs rc/e2e-test-httpd-rc --namespace=kubectl-100'
Mar 12 09:48:21.689: INFO: stderr: ""
Mar 12 09:48:21.689: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.233.65.9. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.233.65.9. Set the 'ServerName' directive globally to suppress this message\n[Thu Mar 12 09:48:20.780569 2020] [mpm_event:notice] [pid 1:tid 140275495750504] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Thu Mar 12 09:48:20.780621 2020] [core:notice] [pid 1:tid 140275495750504] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Mar 12 09:48:21.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 delete rc e2e-test-httpd-rc --namespace=kubectl-100'
Mar 12 09:48:21.802: INFO: stderr: ""
Mar 12 09:48:21.802: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:48:21.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-100" for this suite.
Mar 12 09:48:33.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:48:33.910: INFO: namespace kubectl-100 deletion completed in 12.102067939s

• [SLOW TEST:26.605 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:48:33.911: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-58023eae-abb3-40dc-bbf7-2d7d84430f12
STEP: Creating a pod to test consume secrets
Mar 12 09:48:33.985: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a83ba09e-8622-4c40-b647-3756a9a8bc39" in namespace "projected-913" to be "success or failure"
Mar 12 09:48:33.989: INFO: Pod "pod-projected-secrets-a83ba09e-8622-4c40-b647-3756a9a8bc39": Phase="Pending", Reason="", readiness=false. Elapsed: 3.417312ms
Mar 12 09:48:35.993: INFO: Pod "pod-projected-secrets-a83ba09e-8622-4c40-b647-3756a9a8bc39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007855872s
Mar 12 09:48:37.998: INFO: Pod "pod-projected-secrets-a83ba09e-8622-4c40-b647-3756a9a8bc39": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012412368s
Mar 12 09:48:40.002: INFO: Pod "pod-projected-secrets-a83ba09e-8622-4c40-b647-3756a9a8bc39": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016885504s
Mar 12 09:48:42.007: INFO: Pod "pod-projected-secrets-a83ba09e-8622-4c40-b647-3756a9a8bc39": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02176503s
Mar 12 09:48:44.012: INFO: Pod "pod-projected-secrets-a83ba09e-8622-4c40-b647-3756a9a8bc39": Phase="Pending", Reason="", readiness=false. Elapsed: 10.026763505s
Mar 12 09:48:46.016: INFO: Pod "pod-projected-secrets-a83ba09e-8622-4c40-b647-3756a9a8bc39": Phase="Pending", Reason="", readiness=false. Elapsed: 12.031012185s
Mar 12 09:48:48.020: INFO: Pod "pod-projected-secrets-a83ba09e-8622-4c40-b647-3756a9a8bc39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.035152873s
STEP: Saw pod success
Mar 12 09:48:48.020: INFO: Pod "pod-projected-secrets-a83ba09e-8622-4c40-b647-3756a9a8bc39" satisfied condition "success or failure"
Mar 12 09:48:48.024: INFO: Trying to get logs from node node-2 pod pod-projected-secrets-a83ba09e-8622-4c40-b647-3756a9a8bc39 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 12 09:48:48.047: INFO: Waiting for pod pod-projected-secrets-a83ba09e-8622-4c40-b647-3756a9a8bc39 to disappear
Mar 12 09:48:48.050: INFO: Pod pod-projected-secrets-a83ba09e-8622-4c40-b647-3756a9a8bc39 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:48:48.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-913" for this suite.
Mar 12 09:48:54.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:48:54.162: INFO: namespace projected-913 deletion completed in 6.107963612s

• [SLOW TEST:20.251 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:48:54.162: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Mar 12 09:48:54.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 cluster-info'
Mar 12 09:48:54.292: INFO: stderr: ""
Mar 12 09:48:54.292: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:48:54.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1849" for this suite.
Mar 12 09:49:00.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:49:00.400: INFO: namespace kubectl-1849 deletion completed in 6.102793656s

• [SLOW TEST:6.238 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:49:00.402: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-f2b5fe0c-3f68-4ee6-8513-2d1cf98f8e64
STEP: Creating a pod to test consume configMaps
Mar 12 09:49:00.462: INFO: Waiting up to 5m0s for pod "pod-configmaps-c1bd258a-4ad6-45ea-a2be-6184126eb742" in namespace "configmap-4033" to be "success or failure"
Mar 12 09:49:00.465: INFO: Pod "pod-configmaps-c1bd258a-4ad6-45ea-a2be-6184126eb742": Phase="Pending", Reason="", readiness=false. Elapsed: 3.010723ms
Mar 12 09:49:02.470: INFO: Pod "pod-configmaps-c1bd258a-4ad6-45ea-a2be-6184126eb742": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008078292s
Mar 12 09:49:04.474: INFO: Pod "pod-configmaps-c1bd258a-4ad6-45ea-a2be-6184126eb742": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01231235s
Mar 12 09:49:06.479: INFO: Pod "pod-configmaps-c1bd258a-4ad6-45ea-a2be-6184126eb742": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017110572s
Mar 12 09:49:08.484: INFO: Pod "pod-configmaps-c1bd258a-4ad6-45ea-a2be-6184126eb742": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022053356s
Mar 12 09:49:10.489: INFO: Pod "pod-configmaps-c1bd258a-4ad6-45ea-a2be-6184126eb742": Phase="Pending", Reason="", readiness=false. Elapsed: 10.026431951s
Mar 12 09:49:12.493: INFO: Pod "pod-configmaps-c1bd258a-4ad6-45ea-a2be-6184126eb742": Phase="Pending", Reason="", readiness=false. Elapsed: 12.031112499s
Mar 12 09:49:14.499: INFO: Pod "pod-configmaps-c1bd258a-4ad6-45ea-a2be-6184126eb742": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.03661511s
STEP: Saw pod success
Mar 12 09:49:14.499: INFO: Pod "pod-configmaps-c1bd258a-4ad6-45ea-a2be-6184126eb742" satisfied condition "success or failure"
Mar 12 09:49:14.502: INFO: Trying to get logs from node node-2 pod pod-configmaps-c1bd258a-4ad6-45ea-a2be-6184126eb742 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 12 09:49:14.530: INFO: Waiting for pod pod-configmaps-c1bd258a-4ad6-45ea-a2be-6184126eb742 to disappear
Mar 12 09:49:14.533: INFO: Pod pod-configmaps-c1bd258a-4ad6-45ea-a2be-6184126eb742 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:49:14.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4033" for this suite.
Mar 12 09:49:20.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:49:20.660: INFO: namespace configmap-4033 deletion completed in 6.122081275s

• [SLOW TEST:20.259 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:49:20.660: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Mar 12 09:49:34.735: INFO: Pod pod-hostip-c7fac1ce-d782-4083-9a27-ee096d170328 has hostIP: 192.168.20.5
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:49:34.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2683" for this suite.
Mar 12 09:50:02.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:50:02.836: INFO: namespace pods-2683 deletion completed in 28.096823098s

• [SLOW TEST:42.176 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:50:02.837: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-55d440ea-c145-4b9b-9a31-84b951527ddc
STEP: Creating a pod to test consume configMaps
Mar 12 09:50:02.907: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0e818a87-cff8-4b4f-b5ac-5f1cc27ee952" in namespace "projected-8461" to be "success or failure"
Mar 12 09:50:02.910: INFO: Pod "pod-projected-configmaps-0e818a87-cff8-4b4f-b5ac-5f1cc27ee952": Phase="Pending", Reason="", readiness=false. Elapsed: 2.815349ms
Mar 12 09:50:04.914: INFO: Pod "pod-projected-configmaps-0e818a87-cff8-4b4f-b5ac-5f1cc27ee952": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007483324s
Mar 12 09:50:06.919: INFO: Pod "pod-projected-configmaps-0e818a87-cff8-4b4f-b5ac-5f1cc27ee952": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011850562s
Mar 12 09:50:08.923: INFO: Pod "pod-projected-configmaps-0e818a87-cff8-4b4f-b5ac-5f1cc27ee952": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016235756s
Mar 12 09:50:10.927: INFO: Pod "pod-projected-configmaps-0e818a87-cff8-4b4f-b5ac-5f1cc27ee952": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020539133s
Mar 12 09:50:12.932: INFO: Pod "pod-projected-configmaps-0e818a87-cff8-4b4f-b5ac-5f1cc27ee952": Phase="Pending", Reason="", readiness=false. Elapsed: 10.024797702s
Mar 12 09:50:14.936: INFO: Pod "pod-projected-configmaps-0e818a87-cff8-4b4f-b5ac-5f1cc27ee952": Phase="Pending", Reason="", readiness=false. Elapsed: 12.029507949s
Mar 12 09:50:16.941: INFO: Pod "pod-projected-configmaps-0e818a87-cff8-4b4f-b5ac-5f1cc27ee952": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.034051438s
STEP: Saw pod success
Mar 12 09:50:16.941: INFO: Pod "pod-projected-configmaps-0e818a87-cff8-4b4f-b5ac-5f1cc27ee952" satisfied condition "success or failure"
Mar 12 09:50:16.944: INFO: Trying to get logs from node node-2 pod pod-projected-configmaps-0e818a87-cff8-4b4f-b5ac-5f1cc27ee952 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 12 09:50:16.973: INFO: Waiting for pod pod-projected-configmaps-0e818a87-cff8-4b4f-b5ac-5f1cc27ee952 to disappear
Mar 12 09:50:16.976: INFO: Pod pod-projected-configmaps-0e818a87-cff8-4b4f-b5ac-5f1cc27ee952 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:50:16.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8461" for this suite.
Mar 12 09:50:22.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:50:23.099: INFO: namespace projected-8461 deletion completed in 6.117687359s

• [SLOW TEST:20.262 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:50:23.099: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-4d9a9c06-1b47-4c72-b214-be05ea7158af
STEP: Creating a pod to test consume secrets
Mar 12 09:50:23.178: INFO: Waiting up to 5m0s for pod "pod-secrets-e18a186a-ffac-4e1f-b724-c76c36d4bbc3" in namespace "secrets-9235" to be "success or failure"
Mar 12 09:50:23.182: INFO: Pod "pod-secrets-e18a186a-ffac-4e1f-b724-c76c36d4bbc3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.892923ms
Mar 12 09:50:25.187: INFO: Pod "pod-secrets-e18a186a-ffac-4e1f-b724-c76c36d4bbc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008427622s
Mar 12 09:50:27.191: INFO: Pod "pod-secrets-e18a186a-ffac-4e1f-b724-c76c36d4bbc3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01335587s
Mar 12 09:50:29.196: INFO: Pod "pod-secrets-e18a186a-ffac-4e1f-b724-c76c36d4bbc3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018081805s
Mar 12 09:50:31.201: INFO: Pod "pod-secrets-e18a186a-ffac-4e1f-b724-c76c36d4bbc3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023364573s
Mar 12 09:50:33.206: INFO: Pod "pod-secrets-e18a186a-ffac-4e1f-b724-c76c36d4bbc3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.027911869s
Mar 12 09:50:35.211: INFO: Pod "pod-secrets-e18a186a-ffac-4e1f-b724-c76c36d4bbc3": Phase="Running", Reason="", readiness=true. Elapsed: 12.032553474s
Mar 12 09:50:37.229: INFO: Pod "pod-secrets-e18a186a-ffac-4e1f-b724-c76c36d4bbc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.051350947s
STEP: Saw pod success
Mar 12 09:50:37.230: INFO: Pod "pod-secrets-e18a186a-ffac-4e1f-b724-c76c36d4bbc3" satisfied condition "success or failure"
Mar 12 09:50:37.233: INFO: Trying to get logs from node node-3 pod pod-secrets-e18a186a-ffac-4e1f-b724-c76c36d4bbc3 container secret-volume-test: <nil>
STEP: delete the pod
Mar 12 09:50:37.266: INFO: Waiting for pod pod-secrets-e18a186a-ffac-4e1f-b724-c76c36d4bbc3 to disappear
Mar 12 09:50:37.269: INFO: Pod pod-secrets-e18a186a-ffac-4e1f-b724-c76c36d4bbc3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:50:37.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9235" for this suite.
Mar 12 09:50:43.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:50:43.390: INFO: namespace secrets-9235 deletion completed in 6.115845571s

• [SLOW TEST:20.291 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:50:43.391: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-c04715fa-055e-432e-a20e-1ed2edbfd585
STEP: Creating a pod to test consume secrets
Mar 12 09:50:43.529: INFO: Waiting up to 5m0s for pod "pod-secrets-7fb211cb-2885-470a-a72c-e77e3c55423f" in namespace "secrets-2749" to be "success or failure"
Mar 12 09:50:43.532: INFO: Pod "pod-secrets-7fb211cb-2885-470a-a72c-e77e3c55423f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.019145ms
Mar 12 09:50:45.537: INFO: Pod "pod-secrets-7fb211cb-2885-470a-a72c-e77e3c55423f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007627246s
Mar 12 09:50:47.541: INFO: Pod "pod-secrets-7fb211cb-2885-470a-a72c-e77e3c55423f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011885776s
Mar 12 09:50:49.548: INFO: Pod "pod-secrets-7fb211cb-2885-470a-a72c-e77e3c55423f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018529718s
Mar 12 09:50:51.553: INFO: Pod "pod-secrets-7fb211cb-2885-470a-a72c-e77e3c55423f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023294483s
Mar 12 09:50:53.557: INFO: Pod "pod-secrets-7fb211cb-2885-470a-a72c-e77e3c55423f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.027917165s
Mar 12 09:50:55.562: INFO: Pod "pod-secrets-7fb211cb-2885-470a-a72c-e77e3c55423f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.032474292s
Mar 12 09:50:57.569: INFO: Pod "pod-secrets-7fb211cb-2885-470a-a72c-e77e3c55423f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.039468582s
STEP: Saw pod success
Mar 12 09:50:57.569: INFO: Pod "pod-secrets-7fb211cb-2885-470a-a72c-e77e3c55423f" satisfied condition "success or failure"
Mar 12 09:50:57.573: INFO: Trying to get logs from node node-3 pod pod-secrets-7fb211cb-2885-470a-a72c-e77e3c55423f container secret-volume-test: <nil>
STEP: delete the pod
Mar 12 09:50:57.604: INFO: Waiting for pod pod-secrets-7fb211cb-2885-470a-a72c-e77e3c55423f to disappear
Mar 12 09:50:57.610: INFO: Pod pod-secrets-7fb211cb-2885-470a-a72c-e77e3c55423f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:50:57.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2749" for this suite.
Mar 12 09:51:03.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:51:03.724: INFO: namespace secrets-2749 deletion completed in 6.106743984s
STEP: Destroying namespace "secret-namespace-2887" for this suite.
Mar 12 09:51:09.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:51:09.836: INFO: namespace secret-namespace-2887 deletion completed in 6.11218182s

• [SLOW TEST:26.445 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:51:09.836: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 12 09:51:10.590: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 12 09:51:12.601: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:51:14.608: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:51:16.608: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:51:18.607: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:51:20.607: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:51:22.606: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603470, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 12 09:51:25.661: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 09:51:25.665: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9906-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:51:31.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8490" for this suite.
Mar 12 09:51:37.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:51:37.976: INFO: namespace webhook-8490 deletion completed in 6.102591858s
STEP: Destroying namespace "webhook-8490-markers" for this suite.
Mar 12 09:51:43.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:51:44.105: INFO: namespace webhook-8490-markers deletion completed in 6.128740525s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:34.292 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:51:44.129: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-6e976acc-83c7-4253-b060-9b321373ce31
STEP: Creating a pod to test consume configMaps
Mar 12 09:51:44.217: INFO: Waiting up to 5m0s for pod "pod-configmaps-6caedcb4-2105-4d0e-9ac0-733d5e4c082f" in namespace "configmap-277" to be "success or failure"
Mar 12 09:51:44.222: INFO: Pod "pod-configmaps-6caedcb4-2105-4d0e-9ac0-733d5e4c082f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.582237ms
Mar 12 09:51:46.226: INFO: Pod "pod-configmaps-6caedcb4-2105-4d0e-9ac0-733d5e4c082f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009372098s
Mar 12 09:51:48.231: INFO: Pod "pod-configmaps-6caedcb4-2105-4d0e-9ac0-733d5e4c082f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013880058s
Mar 12 09:51:50.239: INFO: Pod "pod-configmaps-6caedcb4-2105-4d0e-9ac0-733d5e4c082f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021709674s
Mar 12 09:51:52.243: INFO: Pod "pod-configmaps-6caedcb4-2105-4d0e-9ac0-733d5e4c082f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026185985s
Mar 12 09:51:54.248: INFO: Pod "pod-configmaps-6caedcb4-2105-4d0e-9ac0-733d5e4c082f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030923914s
Mar 12 09:51:56.255: INFO: Pod "pod-configmaps-6caedcb4-2105-4d0e-9ac0-733d5e4c082f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.037529556s
Mar 12 09:51:58.260: INFO: Pod "pod-configmaps-6caedcb4-2105-4d0e-9ac0-733d5e4c082f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.042535871s
STEP: Saw pod success
Mar 12 09:51:58.260: INFO: Pod "pod-configmaps-6caedcb4-2105-4d0e-9ac0-733d5e4c082f" satisfied condition "success or failure"
Mar 12 09:51:58.263: INFO: Trying to get logs from node node-3 pod pod-configmaps-6caedcb4-2105-4d0e-9ac0-733d5e4c082f container configmap-volume-test: <nil>
STEP: delete the pod
Mar 12 09:51:58.289: INFO: Waiting for pod pod-configmaps-6caedcb4-2105-4d0e-9ac0-733d5e4c082f to disappear
Mar 12 09:51:58.292: INFO: Pod pod-configmaps-6caedcb4-2105-4d0e-9ac0-733d5e4c082f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:51:58.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-277" for this suite.
Mar 12 09:52:04.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:52:04.410: INFO: namespace configmap-277 deletion completed in 6.112803088s

• [SLOW TEST:20.281 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:52:04.410: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-5
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-5
STEP: Deleting pre-stop pod
Mar 12 09:52:37.511: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:52:37.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5" for this suite.
Mar 12 09:53:21.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:53:21.653: INFO: namespace prestop-5 deletion completed in 44.128646301s

• [SLOW TEST:77.243 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:53:21.654: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 09:53:21.734: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar 12 09:53:21.757: INFO: Number of nodes with available pods: 0
Mar 12 09:53:21.757: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:53:22.766: INFO: Number of nodes with available pods: 0
Mar 12 09:53:22.766: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:53:23.767: INFO: Number of nodes with available pods: 0
Mar 12 09:53:23.767: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:53:24.768: INFO: Number of nodes with available pods: 0
Mar 12 09:53:24.768: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:53:25.767: INFO: Number of nodes with available pods: 0
Mar 12 09:53:25.767: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:53:26.768: INFO: Number of nodes with available pods: 0
Mar 12 09:53:26.768: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:53:27.767: INFO: Number of nodes with available pods: 0
Mar 12 09:53:27.767: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:53:28.807: INFO: Number of nodes with available pods: 0
Mar 12 09:53:28.807: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:53:29.768: INFO: Number of nodes with available pods: 0
Mar 12 09:53:29.768: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:53:30.768: INFO: Number of nodes with available pods: 0
Mar 12 09:53:30.768: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:53:31.768: INFO: Number of nodes with available pods: 0
Mar 12 09:53:31.768: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:53:32.767: INFO: Number of nodes with available pods: 0
Mar 12 09:53:32.767: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:53:33.815: INFO: Number of nodes with available pods: 0
Mar 12 09:53:33.815: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:53:34.767: INFO: Number of nodes with available pods: 3
Mar 12 09:53:34.767: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar 12 09:53:34.798: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:34.798: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:34.798: INFO: Wrong image for pod: daemon-set-wrvbg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:35.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:35.808: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:35.808: INFO: Wrong image for pod: daemon-set-wrvbg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:36.807: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:36.807: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:36.807: INFO: Wrong image for pod: daemon-set-wrvbg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:37.807: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:37.807: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:37.807: INFO: Wrong image for pod: daemon-set-wrvbg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:37.807: INFO: Pod daemon-set-wrvbg is not available
Mar 12 09:53:38.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:38.808: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:38.808: INFO: Pod daemon-set-twrpr is not available
Mar 12 09:53:39.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:39.808: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:39.808: INFO: Pod daemon-set-twrpr is not available
Mar 12 09:53:40.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:40.808: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:40.808: INFO: Pod daemon-set-twrpr is not available
Mar 12 09:53:41.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:41.808: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:41.808: INFO: Pod daemon-set-twrpr is not available
Mar 12 09:53:42.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:42.808: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:42.808: INFO: Pod daemon-set-twrpr is not available
Mar 12 09:53:43.812: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:43.812: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:43.812: INFO: Pod daemon-set-twrpr is not available
Mar 12 09:53:44.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:44.808: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:44.808: INFO: Pod daemon-set-twrpr is not available
Mar 12 09:53:45.807: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:45.807: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:45.807: INFO: Pod daemon-set-twrpr is not available
Mar 12 09:53:46.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:46.808: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:46.808: INFO: Pod daemon-set-twrpr is not available
Mar 12 09:53:47.807: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:47.807: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:47.807: INFO: Pod daemon-set-twrpr is not available
Mar 12 09:53:48.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:48.808: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:48.808: INFO: Pod daemon-set-twrpr is not available
Mar 12 09:53:49.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:49.808: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:49.808: INFO: Pod daemon-set-twrpr is not available
Mar 12 09:53:50.807: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:50.807: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:50.807: INFO: Pod daemon-set-twrpr is not available
Mar 12 09:53:51.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:51.808: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:51.808: INFO: Pod daemon-set-nv6fc is not available
Mar 12 09:53:52.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:52.808: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:52.808: INFO: Pod daemon-set-nv6fc is not available
Mar 12 09:53:53.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:53.808: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:53.808: INFO: Pod daemon-set-nv6fc is not available
Mar 12 09:53:54.807: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:54.807: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:54.807: INFO: Pod daemon-set-nv6fc is not available
Mar 12 09:53:55.807: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:55.807: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:55.807: INFO: Pod daemon-set-nv6fc is not available
Mar 12 09:53:56.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:56.808: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:56.808: INFO: Pod daemon-set-nv6fc is not available
Mar 12 09:53:57.809: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:57.809: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:57.809: INFO: Pod daemon-set-nv6fc is not available
Mar 12 09:53:58.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:58.808: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:58.808: INFO: Pod daemon-set-nv6fc is not available
Mar 12 09:53:59.809: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:59.809: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:53:59.809: INFO: Pod daemon-set-nv6fc is not available
Mar 12 09:54:00.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:54:00.808: INFO: Wrong image for pod: daemon-set-nv6fc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:54:00.808: INFO: Pod daemon-set-nv6fc is not available
Mar 12 09:54:01.810: INFO: Pod daemon-set-g69tv is not available
Mar 12 09:54:01.810: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:54:02.808: INFO: Pod daemon-set-g69tv is not available
Mar 12 09:54:02.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:54:03.808: INFO: Pod daemon-set-g69tv is not available
Mar 12 09:54:03.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:54:04.808: INFO: Pod daemon-set-g69tv is not available
Mar 12 09:54:04.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:54:05.810: INFO: Pod daemon-set-g69tv is not available
Mar 12 09:54:05.810: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:54:06.809: INFO: Pod daemon-set-g69tv is not available
Mar 12 09:54:06.809: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:54:07.808: INFO: Pod daemon-set-g69tv is not available
Mar 12 09:54:07.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:54:08.808: INFO: Pod daemon-set-g69tv is not available
Mar 12 09:54:08.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:54:09.808: INFO: Pod daemon-set-g69tv is not available
Mar 12 09:54:09.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:54:10.808: INFO: Pod daemon-set-g69tv is not available
Mar 12 09:54:10.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:54:11.808: INFO: Pod daemon-set-g69tv is not available
Mar 12 09:54:11.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:54:12.808: INFO: Pod daemon-set-g69tv is not available
Mar 12 09:54:12.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:54:13.808: INFO: Pod daemon-set-g69tv is not available
Mar 12 09:54:13.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:54:14.808: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:54:15.807: INFO: Wrong image for pod: daemon-set-kx2dc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 12 09:54:15.807: INFO: Pod daemon-set-kx2dc is not available
Mar 12 09:54:16.808: INFO: Pod daemon-set-chmh9 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Mar 12 09:54:16.822: INFO: Number of nodes with available pods: 2
Mar 12 09:54:16.822: INFO: Node node-2 is running more than one daemon pod
Mar 12 09:54:17.832: INFO: Number of nodes with available pods: 2
Mar 12 09:54:17.832: INFO: Node node-2 is running more than one daemon pod
Mar 12 09:54:18.832: INFO: Number of nodes with available pods: 2
Mar 12 09:54:18.832: INFO: Node node-2 is running more than one daemon pod
Mar 12 09:54:19.835: INFO: Number of nodes with available pods: 2
Mar 12 09:54:19.835: INFO: Node node-2 is running more than one daemon pod
Mar 12 09:54:20.832: INFO: Number of nodes with available pods: 2
Mar 12 09:54:20.832: INFO: Node node-2 is running more than one daemon pod
Mar 12 09:54:21.832: INFO: Number of nodes with available pods: 2
Mar 12 09:54:21.832: INFO: Node node-2 is running more than one daemon pod
Mar 12 09:54:22.833: INFO: Number of nodes with available pods: 2
Mar 12 09:54:22.833: INFO: Node node-2 is running more than one daemon pod
Mar 12 09:54:23.832: INFO: Number of nodes with available pods: 2
Mar 12 09:54:23.832: INFO: Node node-2 is running more than one daemon pod
Mar 12 09:54:24.833: INFO: Number of nodes with available pods: 2
Mar 12 09:54:24.833: INFO: Node node-2 is running more than one daemon pod
Mar 12 09:54:25.832: INFO: Number of nodes with available pods: 2
Mar 12 09:54:25.832: INFO: Node node-2 is running more than one daemon pod
Mar 12 09:54:26.834: INFO: Number of nodes with available pods: 2
Mar 12 09:54:26.834: INFO: Node node-2 is running more than one daemon pod
Mar 12 09:54:27.831: INFO: Number of nodes with available pods: 2
Mar 12 09:54:27.831: INFO: Node node-2 is running more than one daemon pod
Mar 12 09:54:28.834: INFO: Number of nodes with available pods: 3
Mar 12 09:54:28.834: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9849, will wait for the garbage collector to delete the pods
Mar 12 09:54:28.916: INFO: Deleting DaemonSet.extensions daemon-set took: 9.873109ms
Mar 12 09:54:29.017: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.503769ms
Mar 12 09:54:41.520: INFO: Number of nodes with available pods: 0
Mar 12 09:54:41.520: INFO: Number of running nodes: 0, number of available pods: 0
Mar 12 09:54:41.523: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9849/daemonsets","resourceVersion":"166034"},"items":null}

Mar 12 09:54:41.526: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9849/pods","resourceVersion":"166034"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:54:41.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9849" for this suite.
Mar 12 09:54:47.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:54:47.652: INFO: namespace daemonsets-9849 deletion completed in 6.107404014s

• [SLOW TEST:85.998 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:54:47.653: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1258
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1258
STEP: creating replication controller externalsvc in namespace services-1258
I0312 09:54:47.770548      24 runners.go:184] Created replication controller with name: externalsvc, namespace: services-1258, replica count: 2
I0312 09:54:50.821234      24 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 09:54:53.821473      24 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 09:54:56.821683      24 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 09:54:59.821992      24 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0312 09:55:02.822347      24 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Mar 12 09:55:02.859: INFO: Creating new exec pod
Mar 12 09:55:16.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-675793433 exec --namespace=services-1258 execpodvcrsz -- /bin/sh -x -c nslookup nodeport-service'
Mar 12 09:55:17.459: INFO: stderr: "+ nslookup nodeport-service\n"
Mar 12 09:55:17.459: INFO: stdout: "Server:\t\t10.233.0.3\nAddress:\t10.233.0.3#53\n\nnodeport-service.services-1258.svc.cluster.local\tcanonical name = externalsvc.services-1258.svc.cluster.local.\nName:\texternalsvc.services-1258.svc.cluster.local\nAddress: 10.233.53.204\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1258, will wait for the garbage collector to delete the pods
Mar 12 09:55:17.523: INFO: Deleting ReplicationController externalsvc took: 9.720133ms
Mar 12 09:55:17.624: INFO: Terminating ReplicationController externalsvc pods took: 100.247013ms
Mar 12 09:55:31.651: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:55:31.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1258" for this suite.
Mar 12 09:55:37.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:55:37.784: INFO: namespace services-1258 deletion completed in 6.108773521s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:50.132 seconds]
[sig-network] Services
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:55:37.784: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 12 09:55:38.136: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 12 09:55:40.149: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:55:42.155: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:55:44.159: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:55:46.154: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:55:48.154: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:55:50.154: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603738, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 12 09:55:53.172: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 09:55:53.177: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7296-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:55:59.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7357" for this suite.
Mar 12 09:56:05.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:56:05.449: INFO: namespace webhook-7357 deletion completed in 6.105457202s
STEP: Destroying namespace "webhook-7357-markers" for this suite.
Mar 12 09:56:11.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:56:11.580: INFO: namespace webhook-7357-markers deletion completed in 6.130617599s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:33.814 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:56:11.598: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 09:56:11.665: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-22df14fa-d876-46d9-b42b-e76057bdf8b2" in namespace "security-context-test-4089" to be "success or failure"
Mar 12 09:56:11.669: INFO: Pod "busybox-readonly-false-22df14fa-d876-46d9-b42b-e76057bdf8b2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.845597ms
Mar 12 09:56:13.673: INFO: Pod "busybox-readonly-false-22df14fa-d876-46d9-b42b-e76057bdf8b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008379388s
Mar 12 09:56:15.679: INFO: Pod "busybox-readonly-false-22df14fa-d876-46d9-b42b-e76057bdf8b2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013638989s
Mar 12 09:56:17.683: INFO: Pod "busybox-readonly-false-22df14fa-d876-46d9-b42b-e76057bdf8b2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018312802s
Mar 12 09:56:19.688: INFO: Pod "busybox-readonly-false-22df14fa-d876-46d9-b42b-e76057bdf8b2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023136196s
Mar 12 09:56:21.693: INFO: Pod "busybox-readonly-false-22df14fa-d876-46d9-b42b-e76057bdf8b2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02826713s
Mar 12 09:56:23.698: INFO: Pod "busybox-readonly-false-22df14fa-d876-46d9-b42b-e76057bdf8b2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.033197232s
Mar 12 09:56:25.703: INFO: Pod "busybox-readonly-false-22df14fa-d876-46d9-b42b-e76057bdf8b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.038328651s
Mar 12 09:56:25.703: INFO: Pod "busybox-readonly-false-22df14fa-d876-46d9-b42b-e76057bdf8b2" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:56:25.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4089" for this suite.
Mar 12 09:56:31.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:56:31.821: INFO: namespace security-context-test-4089 deletion completed in 6.112569918s

• [SLOW TEST:20.223 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:56:31.821: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-e9cb64d4-bcc2-4d60-b13a-a40b4d91e500
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:56:31.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9237" for this suite.
Mar 12 09:56:37.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:56:38.048: INFO: namespace configmap-9237 deletion completed in 6.119267922s

• [SLOW TEST:6.226 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:56:38.048: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 09:56:38.133: INFO: Create a RollingUpdate DaemonSet
Mar 12 09:56:38.145: INFO: Check that daemon pods launch on every node of the cluster
Mar 12 09:56:38.155: INFO: Number of nodes with available pods: 0
Mar 12 09:56:38.155: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:56:39.172: INFO: Number of nodes with available pods: 0
Mar 12 09:56:39.172: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:56:40.167: INFO: Number of nodes with available pods: 0
Mar 12 09:56:40.167: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:56:41.166: INFO: Number of nodes with available pods: 0
Mar 12 09:56:41.166: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:56:42.165: INFO: Number of nodes with available pods: 0
Mar 12 09:56:42.165: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:56:43.168: INFO: Number of nodes with available pods: 0
Mar 12 09:56:43.168: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:56:44.167: INFO: Number of nodes with available pods: 0
Mar 12 09:56:44.167: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:56:45.167: INFO: Number of nodes with available pods: 0
Mar 12 09:56:45.167: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:56:46.166: INFO: Number of nodes with available pods: 0
Mar 12 09:56:46.166: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:56:47.166: INFO: Number of nodes with available pods: 0
Mar 12 09:56:47.166: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:56:48.167: INFO: Number of nodes with available pods: 0
Mar 12 09:56:48.167: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:56:49.166: INFO: Number of nodes with available pods: 0
Mar 12 09:56:49.166: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:56:50.165: INFO: Number of nodes with available pods: 0
Mar 12 09:56:50.165: INFO: Node node-1 is running more than one daemon pod
Mar 12 09:56:51.166: INFO: Number of nodes with available pods: 3
Mar 12 09:56:51.166: INFO: Number of running nodes: 3, number of available pods: 3
Mar 12 09:56:51.166: INFO: Update the DaemonSet to trigger a rollout
Mar 12 09:56:51.176: INFO: Updating DaemonSet daemon-set
Mar 12 09:57:02.197: INFO: Roll back the DaemonSet before rollout is complete
Mar 12 09:57:02.208: INFO: Updating DaemonSet daemon-set
Mar 12 09:57:02.208: INFO: Make sure DaemonSet rollback is complete
Mar 12 09:57:02.212: INFO: Wrong image for pod: daemon-set-mjfjz. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 12 09:57:02.212: INFO: Pod daemon-set-mjfjz is not available
Mar 12 09:57:03.227: INFO: Wrong image for pod: daemon-set-mjfjz. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 12 09:57:03.227: INFO: Pod daemon-set-mjfjz is not available
Mar 12 09:57:04.226: INFO: Pod daemon-set-zv4tg is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3134, will wait for the garbage collector to delete the pods
Mar 12 09:57:04.300: INFO: Deleting DaemonSet.extensions daemon-set took: 9.150342ms
Mar 12 09:57:04.400: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.310484ms
Mar 12 09:57:06.404: INFO: Number of nodes with available pods: 0
Mar 12 09:57:06.404: INFO: Number of running nodes: 0, number of available pods: 0
Mar 12 09:57:06.407: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3134/daemonsets","resourceVersion":"166619"},"items":null}

Mar 12 09:57:06.409: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3134/pods","resourceVersion":"166619"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:57:06.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3134" for this suite.
Mar 12 09:57:12.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:57:12.531: INFO: namespace daemonsets-3134 deletion completed in 6.104618325s

• [SLOW TEST:34.484 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:57:12.532: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 12 09:57:12.600: INFO: Waiting up to 5m0s for pod "pod-13d2a8ff-d4b6-477d-8af7-c0f75bddff2e" in namespace "emptydir-8241" to be "success or failure"
Mar 12 09:57:12.604: INFO: Pod "pod-13d2a8ff-d4b6-477d-8af7-c0f75bddff2e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.712058ms
Mar 12 09:57:14.609: INFO: Pod "pod-13d2a8ff-d4b6-477d-8af7-c0f75bddff2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008895028s
Mar 12 09:57:16.614: INFO: Pod "pod-13d2a8ff-d4b6-477d-8af7-c0f75bddff2e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013582938s
Mar 12 09:57:18.627: INFO: Pod "pod-13d2a8ff-d4b6-477d-8af7-c0f75bddff2e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026080252s
Mar 12 09:57:20.631: INFO: Pod "pod-13d2a8ff-d4b6-477d-8af7-c0f75bddff2e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.030263419s
Mar 12 09:57:22.635: INFO: Pod "pod-13d2a8ff-d4b6-477d-8af7-c0f75bddff2e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.034859589s
Mar 12 09:57:24.640: INFO: Pod "pod-13d2a8ff-d4b6-477d-8af7-c0f75bddff2e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.040003486s
Mar 12 09:57:26.646: INFO: Pod "pod-13d2a8ff-d4b6-477d-8af7-c0f75bddff2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.045194415s
STEP: Saw pod success
Mar 12 09:57:26.646: INFO: Pod "pod-13d2a8ff-d4b6-477d-8af7-c0f75bddff2e" satisfied condition "success or failure"
Mar 12 09:57:26.649: INFO: Trying to get logs from node node-3 pod pod-13d2a8ff-d4b6-477d-8af7-c0f75bddff2e container test-container: <nil>
STEP: delete the pod
Mar 12 09:57:26.684: INFO: Waiting for pod pod-13d2a8ff-d4b6-477d-8af7-c0f75bddff2e to disappear
Mar 12 09:57:26.688: INFO: Pod pod-13d2a8ff-d4b6-477d-8af7-c0f75bddff2e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:57:26.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8241" for this suite.
Mar 12 09:57:32.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:57:32.801: INFO: namespace emptydir-8241 deletion completed in 6.107807769s

• [SLOW TEST:20.270 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:57:32.801: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 12 09:57:32.844: INFO: Creating deployment "test-recreate-deployment"
Mar 12 09:57:32.857: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 12 09:57:32.864: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar 12 09:57:34.872: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 12 09:57:34.876: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:57:36.881: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:57:38.881: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:57:40.881: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:57:42.881: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:57:44.881: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603852, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:57:46.882: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 12 09:57:46.892: INFO: Updating deployment test-recreate-deployment
Mar 12 09:57:46.892: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar 12 09:57:46.998: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6606 /apis/apps/v1/namespaces/deployment-6606/deployments/test-recreate-deployment b3483605-dd6c-497c-974b-9cb5fd0b5efe 166783 2 2020-03-12 09:57:32 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001afa9e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-03-12 09:57:46 +0000 UTC,LastTransitionTime:2020-03-12 09:57:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-03-12 09:57:46 +0000 UTC,LastTransitionTime:2020-03-12 09:57:32 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar 12 09:57:47.002: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-6606 /apis/apps/v1/namespaces/deployment-6606/replicasets/test-recreate-deployment-5f94c574ff 9a2235ae-1c61-4dfe-b23d-45cd4dfe1851 166781 1 2020-03-12 09:57:46 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment b3483605-dd6c-497c-974b-9cb5fd0b5efe 0xc0047ac557 0xc0047ac558}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0047ac5b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 12 09:57:47.002: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 12 09:57:47.002: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-6606 /apis/apps/v1/namespaces/deployment-6606/replicasets/test-recreate-deployment-68fc85c7bb 893d74e0-afa1-4666-9108-dd215c2c42b5 166772 2 2020-03-12 09:57:32 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment b3483605-dd6c-497c-974b-9cb5fd0b5efe 0xc0047ac627 0xc0047ac628}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0047ac688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 12 09:57:47.006: INFO: Pod "test-recreate-deployment-5f94c574ff-qcp7s" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-qcp7s test-recreate-deployment-5f94c574ff- deployment-6606 /api/v1/namespaces/deployment-6606/pods/test-recreate-deployment-5f94c574ff-qcp7s 661efdae-cba1-4a92-83af-96e2b6ebc8df 166784 0 2020-03-12 09:57:46 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 9a2235ae-1c61-4dfe-b23d-45cd4dfe1851 0xc0047acaf7 0xc0047acaf8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7mrk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7mrk,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7mrk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:57:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:57:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:57:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-12 09:57:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.20.5,PodIP:,StartTime:2020-03-12 09:57:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:57:47.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6606" for this suite.
Mar 12 09:57:53.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:57:53.122: INFO: namespace deployment-6606 deletion completed in 6.11196601s

• [SLOW TEST:20.321 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:57:53.123: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-2110c3e7-5a4d-4dce-8207-a0bd2103f244
STEP: Creating a pod to test consume configMaps
Mar 12 09:57:53.184: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-89d4288c-8f4b-4f87-8136-ee7a7938f664" in namespace "projected-191" to be "success or failure"
Mar 12 09:57:53.186: INFO: Pod "pod-projected-configmaps-89d4288c-8f4b-4f87-8136-ee7a7938f664": Phase="Pending", Reason="", readiness=false. Elapsed: 2.834019ms
Mar 12 09:57:55.190: INFO: Pod "pod-projected-configmaps-89d4288c-8f4b-4f87-8136-ee7a7938f664": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00645888s
Mar 12 09:57:57.195: INFO: Pod "pod-projected-configmaps-89d4288c-8f4b-4f87-8136-ee7a7938f664": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011030447s
Mar 12 09:57:59.199: INFO: Pod "pod-projected-configmaps-89d4288c-8f4b-4f87-8136-ee7a7938f664": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015665904s
Mar 12 09:58:01.204: INFO: Pod "pod-projected-configmaps-89d4288c-8f4b-4f87-8136-ee7a7938f664": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020735615s
Mar 12 09:58:03.210: INFO: Pod "pod-projected-configmaps-89d4288c-8f4b-4f87-8136-ee7a7938f664": Phase="Pending", Reason="", readiness=false. Elapsed: 10.026699466s
Mar 12 09:58:05.215: INFO: Pod "pod-projected-configmaps-89d4288c-8f4b-4f87-8136-ee7a7938f664": Phase="Pending", Reason="", readiness=false. Elapsed: 12.03135104s
Mar 12 09:58:07.220: INFO: Pod "pod-projected-configmaps-89d4288c-8f4b-4f87-8136-ee7a7938f664": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.036023953s
STEP: Saw pod success
Mar 12 09:58:07.220: INFO: Pod "pod-projected-configmaps-89d4288c-8f4b-4f87-8136-ee7a7938f664" satisfied condition "success or failure"
Mar 12 09:58:07.229: INFO: Trying to get logs from node node-3 pod pod-projected-configmaps-89d4288c-8f4b-4f87-8136-ee7a7938f664 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 12 09:58:07.259: INFO: Waiting for pod pod-projected-configmaps-89d4288c-8f4b-4f87-8136-ee7a7938f664 to disappear
Mar 12 09:58:07.262: INFO: Pod pod-projected-configmaps-89d4288c-8f4b-4f87-8136-ee7a7938f664 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:58:07.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-191" for this suite.
Mar 12 09:58:13.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:58:13.396: INFO: namespace projected-191 deletion completed in 6.128026794s

• [SLOW TEST:20.274 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:58:13.397: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 12 09:58:13.743: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 12 09:58:15.755: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:58:17.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:58:19.761: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:58:21.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:58:23.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 12 09:58:25.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719603893, loc:(*time.Location)(0x78896e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 12 09:58:28.772: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:58:28.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6183" for this suite.
Mar 12 09:58:34.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:58:34.986: INFO: namespace webhook-6183 deletion completed in 6.098026545s
STEP: Destroying namespace "webhook-6183-markers" for this suite.
Mar 12 09:58:40.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:58:41.082: INFO: namespace webhook-6183-markers deletion completed in 6.096196692s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.703 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 12 09:58:41.101: INFO: >>> kubeConfig: /tmp/kubeconfig-675793433
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 12 09:58:41.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8409" for this suite.
Mar 12 09:58:47.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 12 09:58:47.270: INFO: namespace tables-8409 deletion completed in 6.112451577s

• [SLOW TEST:6.170 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.6-beta.0.3+72c30166b2105c/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSMar 12 09:58:47.271: INFO: Running AfterSuite actions on all nodes
Mar 12 09:58:47.271: INFO: Running AfterSuite actions on node 1
Mar 12 09:58:47.271: INFO: Skipping dumping logs from cluster

Ran 276 of 4731 Specs in 10023.916 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4455 Skipped
PASS

Ginkgo ran 1 suite in 2h47m8.141862383s
Test Suite Passed
